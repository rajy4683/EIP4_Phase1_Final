{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCP_SimpleCNN_Assignment5_wrn_acc_sth.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EIP4_Phase1_Final/blob/master/SimpleCNNSeparable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "\n",
        "#import keras_one_cycle_clr as ktool\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ikbd0UBLQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6OZ3Li_RAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOEkwuxHDDws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_contrib.callbacks import CyclicLR\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PZZE92dNYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "\n",
        "\n",
        "#class PersonDataGenerator(keras.utils.Sequence):\n",
        "class PersonDataGenerator(Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,normalize=False,aug_flow=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.aug_flow=aug_flow\n",
        "        self.on_epoch_end()\n",
        "        #print(\"Shuffle = \",self.shuffle)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        #print(batch_slice)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        #print(items[\"image_path\"])\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "\n",
        "\n",
        "        if(self.aug_flow is not None):\n",
        "            image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
        "\n",
        "        if(self.normalize == True):\n",
        "            train_mean = np.mean(image, axis=(0,1,2))\n",
        "            train_std = np.std(image, axis=(0,1,2))\n",
        "            #print(train_mean, train_std)\n",
        "            normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "            image = normalize(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YffxshA1XEo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_old_df = pd.read_csv('/content/gdrive/My Drive/train_df_simplecnn2_widrn_acc_1577201016.csv')\n",
        "train_old_df.shape\n",
        "train_df = train_old_df\n",
        "val_old_df = pd.read_csv('/content/gdrive/My Drive/val_df_simplecnn2_widrn_acc_1577201016.csv')\n",
        "val_old_df.shape\n",
        "val_df = val_old_df\n",
        "print(train_df.shape, val_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vdCgZKoo3LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "def get_curr_time():\n",
        "    return int(datetime.utcnow().strftime(\"%s\"))\n",
        "\n",
        "model_name_itr = 'simplecnn2_widrn_acc_run2_sth_'+str(get_curr_time())\n",
        "gdrive_home_path=\"/content/gdrive/My Drive/\"\n",
        "train_csv=gdrive_home_path+\"train_df_\"+model_name_itr+\".csv\"\n",
        "val_csv=gdrive_home_path+\"val_df_\"+model_name_itr+\".csv\"\n",
        "json_file=gdrive_home_path+\"json_\"+model_name_itr+\".json\"\n",
        "png_file=gdrive_home_path+\"png_\"+model_name_itr+\".png\"\n",
        "weights_file=gdrive_home_path+\"h5_\"+model_name_itr+\".h5\"\n",
        "\n",
        "print(\"Model-name:\",model_name_itr)\n",
        "print(train_csv,val_csv,json_file,png_file,weights_file)\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "# /content/gdrive/My Drive/train_df_simplecnn2_widrn_acc_1577201016.csv \n",
        "# /content/gdrive/My Drive/val_df_simplecnn2_widrn_acc_1577201016.csv \n",
        "# /content/gdrive/My Drive/json_simplecnn2_widrn_acc_1577201016.json \n",
        "# /content/gdrive/My Drive/png_simplecnn2_widrn_acc_1577201016.png \n",
        "# /content/gdrive/My Drive/h5_simplecnn2_widrn_acc_1577201016.h5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkL1cNdVYFns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model-name:\",model_name_itr)\n",
        "for var_name in [train_csv,val_csv,json_file,png_file,weights_file]:\n",
        "  print(var_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6SokzO8Q8bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def equalize_classwise_dist(df_to_equalize, selector_column, classwise_dist):\n",
        "    max_value = np.max(classwise_dist)\n",
        "    max_column = selector_column[np.argmax(classwise_dist)]\n",
        "    equalizer_const = [ np.floor((max_value-value)/value).astype('int') for value in classwise_dist]\n",
        "    print(equalizer_const)\n",
        "    print(equalizer_const[0])\n",
        "    for counter in range(len(equalizer_const)):\n",
        "        local_df = pd.DataFrame()\n",
        "        column_to_select = selector_column[counter]\n",
        "        for value in range(equalizer_const[counter]):\n",
        "            #,equalizer_const[counter]) # The actual class\n",
        "            #print(df_to_equalize[df_to_equalize[column_to_select]==1].shape[0])\n",
        "            local_df = local_df.append(df_to_equalize[df_to_equalize[column_to_select]==1])\n",
        "\n",
        "        if (local_df.empty == False):\n",
        "            print(\"Appending \",local_df.shape[0],\" rows for \",selector_column[counter])\n",
        "            print(\"Current count for \",selector_column[counter], \":\",df_to_equalize[df_to_equalize[column_to_select]==1].shape[0])\n",
        "            #df_to_equalize = df_to_equalize.append(local_df, ignore_index=True)\n",
        "            #df_to_equalize.reset_index()\n",
        "            #df_to_equalize=pd.concat([df_to_equalize,local_df], axis=0,ignore_index=True)\n",
        "\n",
        "            print(\"New count for \",selector_column[counter], \":\",df_to_equalize[df_to_equalize[column_to_select]==1].shape[0])\n",
        "    print(local_df.shape)\n",
        "    return local_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_weights = [\"gender_output\", \"imagequality_ouput\",\"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\",\"emotion_output\", \"pose_output\"]\n",
        "col_splits = [_gender_cols_, _imagequality_cols_,_age_cols_, _weight_cols_, _carryingbag_cols_, _footwear_cols_, _emotion_cols_, _bodypose_cols_]\n",
        "def get_dist(train_df,equalize_classes=False):\n",
        "    loss_weights = {}\n",
        "    index=0\n",
        "    for selector_column in col_splits:\n",
        "        print(selector_column)\n",
        "        count = []\n",
        "        percentile = []\n",
        "        for age_split in selector_column:\n",
        "            count.append( train_df[selector_column][train_df[age_split] == 1].shape[0])\n",
        "        #print(count, np.round((count/11537.0)*100.0, 2))\n",
        "\n",
        "        max_val = np.max(count)\n",
        "        total_count = np.float32(train_df.shape[0])\n",
        "        #print(count, )\n",
        "        count_weights= [np.round(max_val/current_val,3) for current_val in count]\n",
        "        print(count_weights)\n",
        "        #np.round((np.asarray(count)/total)*100.0, 2)\n",
        "\n",
        "        #print(\"Top Class:\",selector_column[np.argmax(count)],\"Max Count\",np.max(count))\n",
        "        #print(\"Bottom Class:\",selector_column[np.argmin(count)])\n",
        "        #weights_dist = dict(zip(selector_column, count_weights))\n",
        "        \n",
        "        #print(weights_dist)\n",
        "        weights_vals_dist={}\n",
        "        index_val=0\n",
        "        for y in range(len(count_weights)):\n",
        "            weights_vals_dist[y]=count_weights[y]\n",
        "            print(weights_vals_dist[y],y)\n",
        "            #index_val+=index_val\n",
        "            \n",
        "            #loss_weights[output_weights[index]]={x,y}\n",
        "        loss_weights[output_weights[index]]=weights_vals_dist\n",
        "        #if equalize_classes == True:\n",
        "        #    expanded_df = equalize_classwise_dist(train_df, selector_column, count)\n",
        "        #    train_df = train_df.append(expanded_df, ignore_index=True)\n",
        "        index+=1\n",
        "    #print(loss_weights)\n",
        "    return train_df,loss_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CcVktArAxJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-P-l7ZuSd-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_df = get_dist(test_df, equalize_classes=True)\n",
        "test_df = train_df\n",
        "_,loss_weights_train=get_dist(test_df, equalize_classes=False)\n",
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=32\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,1.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,1.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "                             #batch_size=BATCH_SIZE)\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_zwfsq5qPZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_batch(data_df, batch_size=32, shuffle=True,normalize=True, selected_field='age_output'):\n",
        "    new_batch = PersonDataGenerator(data_df, batch_size,shuffle, normalize)\n",
        "    images, targets = next(iter(new_batch))\n",
        "    num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "    labels = np.asarray([ np.argmax(targets['age_output'][pos]) for pos in range(len(targets['age_output'])) ])\n",
        "    return images,labels, targets, len(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZu45hUsPkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, y_train, targets, len_train = get_image_batch(train_df, batch_size=32,normalize=True, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usX4GK8btqZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, y_test, targets_test, len_test = get_image_batch(val_df, batch_size=32,normalize=True, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJhDLpyRBJq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_mean_std_for_batch(datagen_process):\n",
        "    image_val,target = next(iter(datagen_process))\n",
        "    print(image_val.shape)\n",
        "    print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )\n",
        "    #print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyX3wQN8ZQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "\n",
        "images_test, targets_test = next(iter(valid_gen))\n",
        "\n",
        "print(num_units)\n",
        "#print(np.mean(images.round(2), axis=(0,1,2)),np.std(images.round(2), axis=(0,1,2)) )\n",
        "#print(np.mean(images_test.round(2), axis=(0,1,2)),np.std(images_test.round(2), axis=(0,1,2)) )\n",
        "\n",
        "print_mean_std_for_batch(train_gen)\n",
        "print_mean_std_for_batch(valid_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YlmUZre7St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_units={'age': 5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjP5xE2CcM_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_single_image(image):\n",
        "    cv2_imshow(cv2.resize(image, (image.shape[1], image.shape[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FdbGK4nb8gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cv2_imshow(cv2.resize(images_norm[0], (images_norm[0].shape[1], images_norm[0].shape[0])))\n",
        "#images_norm.shape\n",
        "#images_test[0].shape\n",
        "display_single_image(images[10])\n",
        "#display_single_image(images_test_norm[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6-3atp3iAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import time,math\n",
        "############# Weights initializer #################\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMcoEEmPApbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJcOZ_VAQzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMD4IZRjCV80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=32\n",
        "LEARNING_RATE=2\n",
        "EPOCHS=50\n",
        "MOMENTUM=0.9\n",
        "train_df.shape\n",
        "batches_per_epoch = train_df.shape[0]//BATCH_SIZE + 1\n",
        "batches_per_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNuv-GtNAXaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D,SeparableConv2D\n",
        "from keras.layers.convolutional import AveragePooling2D\n",
        "from keras.layers import GlobalMaxPooling2D,GlobalAveragePooling2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import add\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDdi1kMt-ubh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_c=1e-5\n",
        "momentum_c=0.9\n",
        "class ResNet_DavidNet:\n",
        "    ## Static Values\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def conv_builder(input_data, \n",
        "                     channels_out, \n",
        "                     kernel_size=(3,3), \n",
        "                     padding=\"SAME\", \n",
        "                     strides=(1,1), \n",
        "                     use_dropout=True, \n",
        "                     activation=None,\n",
        "                     kernel_initializer='he_normal',\n",
        "                     separable=False):\n",
        "        if (separable == True):\n",
        "            x = SeparableConv2D(filters=channels_out,\n",
        "                kernel_size=kernel_size, \n",
        "                padding=padding,\n",
        "                strides=strides, \n",
        "                use_bias=False)(input_data)\n",
        "        else:    \n",
        "            x = Conv2D(filters=channels_out,\n",
        "                    kernel_size=kernel_size, \n",
        "                    padding=padding,\n",
        "                    strides=strides, \n",
        "                    use_bias=False )(input_data)\n",
        "\n",
        "        if use_dropout:\n",
        "            x = Dropout(0.05)(x)\n",
        "\n",
        "        x = BatchNormalization(epsilon=epsilon_c, momentum=momentum_c)(x)\n",
        "\n",
        "        if activation != None:\n",
        "            x =  Activation(activation)(x)\n",
        "        \n",
        "        return x\n",
        "    @staticmethod\n",
        "    ### Builds a stack of\n",
        "    #### (3x3)|(3x3 Stride=2)|MaxPool2D \n",
        "    def build_conv_max_pool_blocks(inputs_data, \n",
        "                                    channels_out, \n",
        "                                    use_dropout=True, \n",
        "                                    activation=None,                                    \n",
        "                                    separable=False,\n",
        "                                    pool=True):\n",
        "        \n",
        "        conv_layer = ResNet_DavidNet.conv_builder(inputs_data,\n",
        "                                        channels_out,\n",
        "                                        use_dropout=True,\n",
        "                                        activation=activation,\n",
        "                                        kernel_initializer='he_normal',\n",
        "                                        separable=separable)                                      \n",
        "        conv_layer = ResNet_DavidNet.conv_builder(conv_layer,\n",
        "                                        channels_out,\n",
        "                                        use_dropout=True,\n",
        "                                        activation=activation,\n",
        "                                        kernel_initializer='he_normal',\n",
        "                                        strides=(2,2),\n",
        "                                        separable=separable)\n",
        "        if pool == True:\n",
        "            conv_layer = MaxPooling2D()(conv_layer)\n",
        "        \n",
        "        return conv_layer\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def residual_block(inputs_data, \n",
        "                       channels_out, \n",
        "                       pool_layer=False, \n",
        "                       res=False,\n",
        "                      ):\n",
        "        #shortcut=data\n",
        "\n",
        "        convBn = ResNet_DavidNet.conv_builder(inputs_data,\n",
        "                                        channels_out,\n",
        "                                        use_dropout=True,\n",
        "                                        activation='relu',\n",
        "                                        )\n",
        "        if pool_layer:\n",
        "            convBn = MaxPooling2D()(convBn)   \n",
        "        if res:\n",
        "            # Add two Conv Layers\n",
        "            x = ResNet_DavidNet.conv_builder(convBn, \n",
        "                                        channels_out,\n",
        "                                        use_dropout=True,\n",
        "                                        activation='relu',\n",
        "                                        )\n",
        "            x = ResNet_DavidNet.conv_builder(x, \n",
        "                                        channels_out,\n",
        "                                        use_dropout=True,\n",
        "                                        activation='relu',\n",
        "                                        )\n",
        "            convBn = add([convBn, x])\n",
        "        ### Return the constructed layer\n",
        "        return convBn\n",
        "    @staticmethod\n",
        "    def get_onexone_conv(attribute_str, input_layer,num_units_in):\n",
        "        attr_name = attribute_str+'_output'\n",
        "\n",
        "        return Flatten(name=attr_name)((ResNet_DavidNet.conv_builder(input_layer,\n",
        "                                                                     channels_out=5,\n",
        "                                                                     kernel_size=(1,1),\n",
        "                                                                     strides=(1,1),\n",
        "                                                                     activation='softmax',\n",
        "                                                                     kernel_initializer='he_normal')(input_layer)))\n",
        "    @staticmethod\n",
        "    def build(inputShape, \n",
        "              num_units_in=num_units, \n",
        "              channels_out=64, \n",
        "              output_field=None, \n",
        "              num_layers=3,\n",
        "              separable = False):\n",
        "    #\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "        \n",
        "        inputs_data = Input(shape=inputShape)\n",
        "        print(inputs_data)\n",
        "        \n",
        "        # for layer_count in range(num_layers):\n",
        "\n",
        "        #     model = ResNet_DavidNet.build_conv_max_pool_blocks(inputs_data, \n",
        "        #                                                         channels_out, \n",
        "        #                                                         use_dropout=True, \n",
        "        #                                                         activation='relu',                                    \n",
        "        #                                                         separable=separable,\n",
        "        #                                                         pool=True )\n",
        "        #     channels_out = channels_out**2\n",
        "        #     #if(layer_count == 0):\n",
        "        #     inputs_data = model\n",
        "        model = ResNet_DavidNet.conv_builder(inputs_data, \n",
        "                                             channels_out,\n",
        "                                             use_dropout=True,\n",
        "                                             activation='relu',\n",
        "                                             separable=separable)\n",
        "        #inputs_data = model\n",
        "        model = ResNet_DavidNet.build_conv_max_pool_blocks(model, \n",
        "                                                           channels_out, \n",
        "                                                           use_dropout=True, \n",
        "                                                           activation='relu',                                    \n",
        "                                                          separable=separable,\n",
        "                                                          pool=True )\n",
        "        #inputs_data = model\n",
        "        model = ResNet_DavidNet.build_conv_max_pool_blocks(model, \n",
        "                                                           channels_out*2, \n",
        "                                                           use_dropout=True, \n",
        "                                                           activation='relu',                                    \n",
        "                                                          separable=separable,\n",
        "                                                          pool=True )\n",
        "        model = ResNet_DavidNet.build_conv_max_pool_blocks(model, \n",
        "                                                           channels_out*4, \n",
        "                                                           use_dropout=True, \n",
        "                                                           activation='relu',                                    \n",
        "                                                          separable=separable,\n",
        "                                                          pool=True )\n",
        "        model = ResNet_DavidNet.build_conv_max_pool_blocks(model, \n",
        "                                                           channels_out*8, \n",
        "                                                           use_dropout=True, \n",
        "                                                           activation='relu',                                    \n",
        "                                                          separable=separable,\n",
        "                                                          pool=True )\n",
        "\n",
        "        # model = ResNet_DavidNet.conv_builder(inputs_data, channels_out,\n",
        "        #                                      use_dropout=True,\n",
        "        #                                      activation='relu',\n",
        "        #                                      separable=separable) #\n",
        "        # model = ResNet_DavidNet.residual_block(model, channels_out*2, \n",
        "        #                                        pool_layer=True, \n",
        "        #                                        res=False,\n",
        "        #                                        separable=separable)\n",
        "        \n",
        "        # model = ResNet_DavidNet.residual_block(model, \n",
        "        #                                         channels_out*4,\n",
        "        #                                         pool_layer=True,\n",
        "        #                                         res=False,\n",
        "        #                                        separable=separable)\n",
        "        # model = ResNet_DavidNet.residual_block(model, \n",
        "        #                                         channels_out*8,\n",
        "        #                                         pool_layer=True,\n",
        "        #                                         res=False,\n",
        "        #                                        separable=separable)\n",
        "        \n",
        "        model = GlobalMaxPooling2D()(model)\n",
        "        # #model = GlobalAveragePooling2D()(model)\n",
        "        # #model = Dense(128, activation=\"relu\")(model)\n",
        "        # #model = Dropout(0.3)(model)\n",
        "        if(output_field == None):\n",
        "            output_vals = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(model) for name in num_units.keys()]\n",
        "        else:\n",
        "            output_vals = Dense(num_units[output_field], activation=\"softmax\", name=f\"{output_field}_output\")(model)\n",
        "\n",
        "        #output_vals = Flatten(name=\"flatten\")(output_vals)\n",
        "        #Flatten(name=attr_name)\n",
        "        \n",
        "        #ResNet_DavidNet.get_onexone_conv('age',model,num_units_in) #for index in num_units_in.keys()]\n",
        "        model = Model(inputs=inputs_data, outputs=output_vals)\n",
        "\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAxcc9wwiaUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(224,224,3)\n",
        "modelv2 = ResNet_DavidNet.build(inputShape=input_shape,channels_out=64,separable=True)#,output_field='age') \n",
        "                    #    num_units_in=num_units, \n",
        "                    #    64)\n",
        "modelv2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo6lWzUhirkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.utils import plot_model\n",
        "plot_model(modelv2, 'latest.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xlOmISPvKwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\t#plt.plot(N, self.H[\"acc\"], label=\"train_acc\")\n",
        "\t\t\t#plt.plot(N, self.H[\"val_acc\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48fxik3vU9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "plotPath = png_file\n",
        "#os.path.sep.join([\"/content/gdrive/My Drive/\", \"resnet_fashion_mnist.png\"])\n",
        "jsonPath = json_file\n",
        "#os.path.sep.join([\"/content/gdrive/My Drive/\", \"resnet_fashion_mnist.json\"])\n",
        "print(plotPath,jsonPath)\n",
        "\n",
        "\n",
        "#/content/gdrive/My Drive/png_simplecnn2_widrn_acc_1577201016.png \n",
        "#/content/gdrive/My Drive/json_simplecnn2_widrn_acc_1577201016.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EjF8SnVDp_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=[0]*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDwKOA71Vyw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#0.30007923\n",
        "#0.25668284\n",
        "## Stable --->\n",
        "#Min: 7.108769 0.22178908\n",
        "#Min: 7.2756057 0.12301114\n",
        "#Min: 7.295935 0.24042512\n",
        "#Min: 7.7571077 0.030115802\n",
        "#Min: 7.5900717 0.01655656\n",
        "#Min: 7.1677036 0.21655203\n",
        "#Min: 7.195501 0.10309692\n",
        "#Min: 8.308885 0.09440613\n",
        "MOMENTUM=0.9\n",
        "input_shape=(224,224,3)\n",
        "modelv2 = ResNet_DavidNet.build(inputShape=input_shape,channels_out=64,separable=True)\n",
        "#modelv2 = ResNet_DavidNet.build(inputShape=input_shape,channels_out=32)\n",
        "modelv2.compile(\n",
        "    #optimizer=SGD(lr=2.2915473*0.01,momentum=MOMENTUM, nesterov=True),\n",
        "    optimizer=SGD(lr=0.23201092*0.1),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),     \n",
        "    #weighted_metrics=[\"accuracy\"]\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "lr_finder = LRFinder_new(modelv2)\n",
        "lr_finder.find_generator(train_gen,start_lr=0.5, end_lr=10,epochs=20\n",
        "                         ,steps_per_epoch=20)\n",
        "                         #,class_weight=class_weight)\n",
        "\n",
        "#print(lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "#print(lr_finder.lrs[np.argmin(lr_finder.losses)])\n",
        "\n",
        "print(\"#Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"#Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu7_G3uQ48E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CCmMusyYlqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss(n_skip_beginning=2, n_skip_end=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YufjEI2xfvYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQpclcR_NHIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.min(lr_finder.losses))\n",
        "#(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKucpPfiNVGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54_7yDhKelKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.max(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNjWKxWPchDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_finder.get_best_lr(1,1,1))\n",
        "print(lr_finder.get_best_lr(10,1,1))\n",
        "print(lr_finder.get_best_lr(20,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECN7l8KJYoLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss(n_skip_end=1,x_scale='linear')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2efVbTjpSs_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss_change(sma=1, n_skip_beginning=1, n_skip_end=1, y_lim=(np.min(lr_finder.get_derivatives(sma=1)), np.max(lr_finder.get_derivatives(sma=1))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWmlx6nBS8zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "derivatives_2 = lr_finder.get_derivatives(sma=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbIlOuz1BwvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERK2PRn3Bw6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_y=np.linspace(0, np.int(EPOCHS), np.int(EPOCHS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WpHD8hvnu0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join('/content/gdrive/', 'My Drive')\n",
        "def get_curr_time():\n",
        "    return int(datetime.utcnow().strftime(\"%s\"))\n",
        "model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr)\n",
        "print(model_name)\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Q4S94kBxAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE=0.5037587*0.1\n",
        "EPOCHS=100\n",
        "test_y = np.linspace(0,EPOCHS,EPOCHS)\n",
        "x=[0, (EPOCHS+1)//5, EPOCHS]\n",
        "y=[LEARNING_RATE*0.01, LEARNING_RATE, LEARNING_RATE*0.01]\n",
        "interp_lr = np.interp(test_y, x, y)\n",
        "def one_lr_schedule(epoch):\n",
        "    print(\"lr:\",interp_lr[epoch])\n",
        "    return interp_lr[epoch]\n",
        "#interp_values = np.interp(, [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)\n",
        "callbacks = [checkpoint, lr_scheduler,TrainingMonitor(figPath=plotPath,\n",
        "                                                      jsonPath=jsonPath,startAt=2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp9C4a81oJS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(test_y,interp_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htY5Fj1RIMGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "def reset_keras(per_process_gpu_memory_fraction=1.0):\n",
        "    \"\"\"\n",
        "    Reset Keras session and set GPU configuration as well as collect unused memory.\n",
        "    This is adapted from [jaycangel's post on fastai forum](https://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/18).\n",
        "    Calling this before any training will clear Keras session. Hence, a Keras model must be redefined and compiled again.\n",
        "    It can be used in during hyperparameter scan or K-fold validation when model training is invoked several times.\n",
        "    :param per_process_gpu_memory_fraction: tensorflow's config.gpu_options.per_process_gpu_memory_fraction\n",
        "    \"\"\"\n",
        "    sess = K.get_session()\n",
        "    K.clear_session()\n",
        "    sess.close()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = per_process_gpu_memory_fraction\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    K.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN0waQ-WIO9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_keras()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDdosDO1PMlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=[0]*200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnKUCEBcZAea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Min: 7.108769 0.22178908\n",
        "#Min: 7.2756057 0.12301114\n",
        "#Min: 7.295935 0.24042512\n",
        "#Min: 7.7571077 0.030115802\n",
        "#Min: 7.5900717 0.01655656\n",
        "MOMENTUM=0.9\n",
        "#LEARNING_RATE=0.21655203\n",
        "#LEARNING_RATE=6.7237797*0.1-----> no learning\n",
        "LEARNING_RATE=0.5037587*0.1\n",
        "#optimizer=SGD(lr=LEARNING_RATE)\n",
        "input_shape=(224,224,3)\n",
        "\n",
        "optimizer=SGD(lr=one_lr_schedule(0))\n",
        "#0.101295\n",
        "#*0.1\n",
        "#0.22178908\n",
        "loss_weights_compile = {'gender_output': 2, 'image_quality_output': 3, 'age_output': 5, 'weight_output': 4, 'bag_output': 3, 'pose_output': 3, 'footwear_output': 3, 'emotion_output': 4}\n",
        "modelv2 = ResNet_DavidNet.build(inputShape=input_shape,channels_out=64,separable=True)\n",
        "modelv2.load_weights('/content/gdrive/My Drive/assignment5_simplecnn2_widrn_acc_run2_1577249111_model.002.h5')\n",
        "modelv2.compile(\n",
        "    #optimizer=SGD(lr=0.22178908*0.01,momentum=MOMENTUM, nesterov=True),\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    loss_weights=loss_weights_compile,\n",
        "    weighted_metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7jbWtc07NDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[0]=modelv2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=False,\n",
        "    workers=4, \n",
        "    epochs=100,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=loss_weights_train,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MhqdnsS_cn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/assignment5_simplecnn2_widrn_acc_1577201016_model.047.h5\" \"/content/gdrive/My Drive/simplecnn2_widrn_round1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CabmU5tzN7CI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_simplecnn2_widrn_acc_1577201016.json\" \"/content/gdrive/My Drive/back_json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFBF_Awu_Oz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[1]=modelv2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=False,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=loss_weights_train,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isCGlxzluCxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelv2.save_weights('/content/gdrive/My Drive/h5_simplecnn2_widrn_acc_1577201016.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7N2b7Nnv9Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_simplecnn2_widrn_acc_1577201016.json\" \"/content/gdrive/My Drive/back_json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO-cIeAJNfyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = [0] * 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modelv2 = ResNet_DavidNet.build(inputShape=input_shape,channels_out=32)\n",
        "# modelv2.compile(\n",
        "#     #optimizer=SGD(lr=2.2915473*0.01,momentum=MOMENTUM, nesterov=True),\n",
        "#     optimizer=SGD(lr=0.23201092*0.1),\n",
        "#     loss=tf.keras.losses.CategoricalCrossentropy(),     \n",
        "#     weighted_metrics=[\"accuracy\"]\n",
        "# )\n",
        "history[2]=modelv2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=False,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=loss_weights_train,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IECaFK0SOYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[3]=modelv2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=False,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=loss_weights_train,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[3]=modelv2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=False,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=loss_weights_train,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oClynqTAo1ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay713OKAo4By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PRgJh98o4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcR0ayLo1iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LRFinder_new:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            self.model.stop_training = True\n",
        "            print(\"Training halted as loss reached maximum\",loss)\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        # If x_train contains data for multiple inputs, use length of the first input.\n",
        "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
        "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
        "\n",
        "        # Compute number of batches and LR multiplier\n",
        "        num_batches = epochs * N / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=batch_size, epochs=epochs,\n",
        "                       callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "        if steps_per_epoch is None:\n",
        "            try:\n",
        "                steps_per_epoch = len(generator)\n",
        "            except (ValueError, NotImplementedError) as e:\n",
        "                raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                        ' generator based on the '\n",
        "                        '`keras.utils.Sequence`'\n",
        "                        ' class. Please specify `steps_per_epoch` '\n",
        "                        'or use the `keras.utils.Sequence` class.')\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit_generator(generator=generator,\n",
        "                                 epochs=epochs,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 callbacks=[callback],\n",
        "                                 **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(x_scale)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK6kQZKMK61J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}