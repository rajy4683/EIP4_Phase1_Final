{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WRN_A5_rkg_wrn_gml_rkg_rjy_gml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EIP4_Phase1_Final/blob/master/WRNSmall_A5_rkg_wrn_gml_rkg_rjy_gml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "2540a78e-1c4d-4a21-9c74-f672a3e03b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "36a76ba9-ca2a-4d6a-9dcf-6bb89ed6df5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6OZ3Li_RAJ",
        "colab_type": "code",
        "outputId": "f80e8e4e-a08d-4373-840e-261371719cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "f5adcd24-b355-4922-cd1e-df1f23fdc95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "8b0e1963-870e-46df-8b83-7877cac0c385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PZZE92dNYM",
        "colab_type": "code",
        "outputId": "7baf6820-e863-4d17-c6f1-960d37c1637a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "one_hot_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "#class PersonDataGenerator(keras.utils.Sequence):\n",
        "class PersonDataGenerator(Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,normalize=False,aug_flow=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.on_epoch_end()\n",
        "        self.aug_flow=aug_flow\n",
        "        #print(\"Shuffle = \",self.shuffle)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        #print(batch_slice)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        #print(items[\"image_path\"])\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        if(self.aug_flow is not None):\n",
        "            image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
        "        if(self.normalize == True):\n",
        "            train_mean = np.mean(image, axis=(0,1,2))\n",
        "            train_std = np.std(image, axis=(0,1,2))\n",
        "            #print(train_mean, train_std)\n",
        "            normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "            image = normalize(image)\n",
        "\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "3a11a1f3-7b5f-4a31-f290-7ed51e4eed24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmFlcRB7pOTD",
        "colab_type": "code",
        "outputId": "e1528d5a-3560-4663-d500-342859c8cb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_old_df =pd.read_csv('/content/gdrive/My Drive/WRN_Base/train_df_wrn2_widrn_acc_1577202722.csv') \n",
        "train_df = train_old_df\n",
        "val_old_df = pd.read_csv('/content/gdrive/My Drive/WRN_Base/val_df_wrn2_widrn_acc_1577202722.csv')\n",
        "val_df=val_old_df\n",
        "print(train_df.shape, val_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11537, 28) (2036, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIYrhzCAbWv0",
        "colab_type": "code",
        "outputId": "fb87b0ae-72f9-4070-d976-3a92ed171e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from datetime import datetime\n",
        "def get_curr_time():\n",
        "    return int(datetime.utcnow().strftime(\"%s\"))\n",
        "\n",
        "model_name_itr = 'wrn2_rkg_fresh_wrn_rjy2_gml2_'+str(get_curr_time())\n",
        "gdrive_home_path=\"/content/gdrive/My Drive/WRN_Base/\"\n",
        "train_csv=gdrive_home_path+\"train_df_\"+model_name_itr+\".csv\"\n",
        "val_csv=gdrive_home_path+\"val_df_\"+model_name_itr+\".csv\"\n",
        "json_file=gdrive_home_path+\"json_\"+model_name_itr+\".json\"\n",
        "png_file=gdrive_home_path+\"png_\"+model_name_itr+\".png\"\n",
        "weights_file=gdrive_home_path+\"h5_\"+model_name_itr+\".h5\"\n",
        "\n",
        "print(\"Model-name:\",model_name_itr)\n",
        "print(train_csv,val_csv,json_file,png_file,weights_file)\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "# /content/gdrive/My Drive/train_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/val_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722.json\n",
        "#  /content/gdrive/My Drive/png_wrn2_widrn_acc_1577202722.png \n",
        "#  /content/gdrive/My Drive/h5_wrn2_widrn_acc_1577202722.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model-name: wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484\n",
            "/content/gdrive/My Drive/WRN_Base/train_df_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.csv /content/gdrive/My Drive/WRN_Base/val_df_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.csv /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json /content/gdrive/My Drive/WRN_Base/png_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.png /content/gdrive/My Drive/WRN_Base/h5_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1L33i_QqLRe",
        "colab_type": "code",
        "outputId": "05faa680-2803-4a95-b90a-00ab603dc526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "print(\"Model-name:\",model_name_itr)\n",
        "for var_name in [train_csv,val_csv,json_file,png_file,weights_file]:\n",
        "  print(var_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model-name: wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484\n",
            "/content/gdrive/My Drive/WRN_Base/train_df_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.csv\n",
            "/content/gdrive/My Drive/WRN_Base/val_df_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.csv\n",
            "/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json\n",
            "/content/gdrive/My Drive/WRN_Base/png_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.png\n",
            "/content/gdrive/My Drive/WRN_Base/h5_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdL4WRuG2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_weights = [\"gender_output\", \"imagequality_ouput\",\"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\",\"emotion_output\", \"pose_output\"]\n",
        "col_splits = [_gender_cols_, _imagequality_cols_,_age_cols_, _weight_cols_, _carryingbag_cols_, _footwear_cols_, _emotion_cols_, _bodypose_cols_]\n",
        "def get_dist(train_df,equalize_classes=False):\n",
        "    loss_weights = {}\n",
        "    index=0\n",
        "    for selector_column in col_splits:\n",
        "        print(selector_column)\n",
        "        count = []\n",
        "        percentile = []\n",
        "        for age_split in selector_column:\n",
        "            count.append( train_df[selector_column][train_df[age_split] == 1].shape[0])\n",
        "        #print(count, np.round((count/11537.0)*100.0, 2))\n",
        "\n",
        "        max_val = np.max(count)\n",
        "        total_count = np.float32(train_df.shape[0])\n",
        "        #print(count, )\n",
        "        count_weights= [np.round(max_val/current_val,3) for current_val in count]\n",
        "        print(count_weights)\n",
        "        print(np.round((np.asarray(count)/total_count)*100.0, 2))\n",
        "\n",
        "        #print(\"Top Class:\",selector_column[np.argmax(count)],\"Max Count\",np.max(count))\n",
        "        #print(\"Bottom Class:\",selector_column[np.argmin(count)])\n",
        "        #weights_dist = dict(zip(selector_column, count_weights))\n",
        "        \n",
        "        #print(weights_dist)\n",
        "        weights_vals_dist={}\n",
        "        index_val=0\n",
        "        for y in range(len(count_weights)):\n",
        "            weights_vals_dist[y]=count_weights[y]\n",
        "            print(weights_vals_dist[y],y)\n",
        "            #index_val+=index_val\n",
        "            \n",
        "            #loss_weights[output_weights[index]]={x,y}\n",
        "        loss_weights[output_weights[index]]=weights_vals_dist\n",
        "        #if equalize_classes == True:\n",
        "        #    expanded_df = equalize_classwise_dist(train_df, selector_column, count)\n",
        "        #    train_df = train_df.append(expanded_df, ignore_index=True)\n",
        "        index+=1\n",
        "    #print(loss_weights)\n",
        "    return train_df,loss_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcR5l-K0HESp",
        "colab_type": "code",
        "outputId": "c19efde2-cb2d-41b1-e310-8c3755a3dcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_,loss_weights_train=get_dist(train_df, equalize_classes=False)\n",
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gender_female', 'gender_male']\n",
            "[1.304, 1.0]\n",
            "[43.41 56.59]\n",
            "1.304 0\n",
            "1.0 1\n",
            "['imagequality_Average', 'imagequality_Bad', 'imagequality_Good']\n",
            "[1.0, 3.361, 1.979]\n",
            "[55.47 16.5  28.03]\n",
            "1.0 0\n",
            "3.361 1\n",
            "1.979 2\n",
            "['age_15-25', 'age_25-35', 'age_35-45', 'age_45-55', 'age_55+']\n",
            "[2.172, 1.0, 1.589, 3.556, 7.326]\n",
            "[18.36 39.88 25.1  11.22  5.44]\n",
            "2.172 0\n",
            "1.0 1\n",
            "1.589 2\n",
            "3.556 3\n",
            "7.326 4\n",
            "['weight_normal-healthy', 'weight_over-weight', 'weight_slightly-overweight', 'weight_underweight']\n",
            "[1.0, 9.632, 2.753, 9.931]\n",
            "[63.79  6.62 23.17  6.42]\n",
            "1.0 0\n",
            "9.632 1\n",
            "2.753 2\n",
            "9.931 3\n",
            "['carryingbag_Daily/Office/Work Bag', 'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None']\n",
            "[1.667, 5.837, 1.0]\n",
            "[33.86  9.67 56.46]\n",
            "1.667 0\n",
            "5.837 1\n",
            "1.0 2\n",
            "['footwear_CantSee', 'footwear_Fancy', 'footwear_Normal']\n",
            "[1.23, 2.38, 1.0]\n",
            "[36.4  18.82 44.78]\n",
            "1.23 0\n",
            "2.38 1\n",
            "1.0 2\n",
            "['emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral', 'emotion_Sad']\n",
            "[6.454, 6.05, 1.0, 12.056]\n",
            "[11.04 11.78 71.27  5.91]\n",
            "6.454 0\n",
            "6.05 1\n",
            "1.0 2\n",
            "12.056 3\n",
            "['bodypose_Back', 'bodypose_Front-Frontish', 'bodypose_Side']\n",
            "[3.836, 1.0, 2.786]\n",
            "[16.1  61.74 22.16]\n",
            "3.836 0\n",
            "1.0 1\n",
            "2.786 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output': {0: 2.172, 1: 1.0, 2: 1.589, 3: 3.556, 4: 7.326},\n",
              " 'bag_output': {0: 1.667, 1: 5.837, 2: 1.0},\n",
              " 'emotion_output': {0: 6.454, 1: 6.05, 2: 1.0, 3: 12.056},\n",
              " 'footwear_output': {0: 1.23, 1: 2.38, 2: 1.0},\n",
              " 'gender_output': {0: 1.304, 1: 1.0},\n",
              " 'imagequality_ouput': {0: 1.0, 1: 3.361, 2: 1.979},\n",
              " 'pose_output': {0: 3.836, 1: 1.0, 2: 2.786},\n",
              " 'weight_output': {0: 1.0, 1: 9.632, 2: 2.753, 3: 9.931}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZIV48it799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(iter(batches))\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hco1q4yMxji3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "BATCH_SIZE=32\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2y9D1I-udt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CROP_LENGTH=64\n",
        "train_crops = crop_generator(train_gen, CROP_LENGTH)\n",
        "valid_crops = crop_generator(valid_gen, CROP_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BbkJCyZXZx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_zwfsq5qPZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_batch(data_df, batch_size=32, shuffle=True,normalize=True, selected_field='age_output'):\n",
        "    new_batch = PersonDataGenerator(data_df, batch_size,shuffle, normalize)\n",
        "    images, targets = next(iter(new_batch))\n",
        "    num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "    labels = np.asarray([ np.argmax(targets['age_output'][pos]) for pos in range(len(targets['age_output'])) ])\n",
        "    return images,labels, targets, len(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZu45hUsPkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, y_train, targets, len_train = get_image_batch(train_df, batch_size=32,normalize=True, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usX4GK8btqZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, y_test, targets_test, len_test = get_image_batch(val_df, batch_size=32,normalize=True, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cb1XGwFvwmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_mean_std_for_batch(datagen_process):\n",
        "    image_val,target = next(iter(datagen_process))\n",
        "    print(image_val.shape)\n",
        "    print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )\n",
        "    #print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyX3wQN8ZQz",
        "colab_type": "code",
        "outputId": "76359847-c31b-4b21-ed47-7100f7bbe83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "\n",
        "images_test, targets_test = next(iter(valid_gen))\n",
        "\n",
        "print(num_units)\n",
        "#print(np.mean(images.round(2), axis=(0,1,2)),np.std(images.round(2), axis=(0,1,2)) )\n",
        "#print(np.mean(images_test.round(2), axis=(0,1,2)),np.std(images_test.round(2), axis=(0,1,2)) )\n",
        "\n",
        "print_mean_std_for_batch(train_gen)\n",
        "print_mean_std_for_batch(valid_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gender': 2, 'image_quality': 3, 'age': 5, 'weight': 4, 'bag': 3, 'pose': 3, 'footwear': 3, 'emotion': 4}\n",
            "(32, 224, 224, 3)\n",
            "[-0.00021646 -0.00139709 -0.00333309] [0.99797153 0.9986378  1.0010139 ]\n",
            "(32, 224, 224, 3)\n",
            "[ 0.00087246 -0.00039059 -0.00225037] [0.99544924 0.99689215 1.004353  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uwZZR7lpd54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, targets_test = next(iter(valid_gen))\n",
        "num_units_test = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets_test.items()}\n",
        "num_units_test\n",
        "#images.shape\n",
        "y_test  = np.asarray([ np.argmax(targets_test['age_output'][pos]) for pos in range(len(targets_test['age_output'])) ]) ## Taking the argmax to select the correct class\n",
        "cv2_imshow(cv2.resize(images_test[0], (images_test[0].shape[1], images_test[0].shape[0])))\n",
        "y_test.shape\n",
        "y_test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjP5xE2CcM_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_single_image(image):\n",
        "    cv2_imshow(cv2.resize(image, (image.shape[1], image.shape[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TKbaDy5pO_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_train, len_test = len(images), len(images_test)\n",
        "\n",
        "# train_mean = np.mean(images, axis=(0,1,2))\n",
        "# train_std = np.std(images, axis=(0,1,2))\n",
        "\n",
        "# normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n",
        "# #pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "# images_norm = normalize(images)\n",
        "# images_test_norm = normalize(images_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FdbGK4nb8gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cv2_imshow(cv2.resize(images_norm[0], (images_norm[0].shape[1], images_norm[0].shape[0])))\n",
        "#images_norm.shape\n",
        "#images_test[0].shape\n",
        "display_single_image(images[10])\n",
        "#display_single_image(images_test_norm[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6-3atp3iAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import time,math\n",
        "############# Weights initializer #################\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMcoEEmPApbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06AAPOdlNSKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSxGw9KOLfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weight_decay = 0.0005\n",
        "weight_decay = 0.001\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1# if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0O2LzQOPzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1,num_units_in=num_units):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    #x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "    output_layers = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(x) for name in num_units_in.keys()]\n",
        "    model = Model(ip, output_layers)\n",
        "\n",
        "    #if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "    #return x\n",
        "\n",
        "def add_final_layer(model,num_units_in):\n",
        "    output_vals = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(model) for name in num_units_in.keys()]\n",
        "    return output_vals\n",
        "\n",
        "def create_model(dropout=0.05):\n",
        "    from keras.utils import plot_model\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "\n",
        "    init = (224, 224, 3)\n",
        "\n",
        "    #wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=2, dropout=0.0)\n",
        "    wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=2, dropout=dropout)\n",
        "    return wrn_28_10\n",
        "    #output_layers = add_final_layer(wrn_28_10_backbone, num_units)\n",
        "    #model = Model(Input(shape=init), output_layers)\n",
        "    #wrn_28_10.summary()\n",
        "\n",
        "    #plot_model(wrn_28_10, \"WRN-16-2.png\", show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJcOZ_VAQzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5leq1Jrxz3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJhxE2F1HmKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrn_28_10.compile(\n",
        "#     optimizer=SGD(lr=0.5),\n",
        "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#     # loss_weights=loss_weights, \n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.compile(\n",
        "    optimizer=SGD(lr=0.049203925),\n",
        "    #,momentum=MOMENTUM, nesterov=True),\n",
        "    #optimizer=SGD(lr=0.01191919191919192),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),     \n",
        "    weighted_metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfzXsa5sVZbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Min: 12.520438 0.57330394\n",
        "#Min: 12.54081 0.586987\n",
        "#Min: 12.628042 0.18920188\n",
        "del wrn_28_10\n",
        "#Min: 7.8906326 1.3513402\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.compile(\n",
        "    optimizer=SGD(lr=0.049203925),\n",
        "    #,momentum=MOMENTUM, nesterov=True),\n",
        "    #optimizer=SGD(lr=0.01191919191919192),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    loss_weights=loss_weights_compile,  \n",
        "    #metrics=[\"accuracy\"]\n",
        "    weighted_metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "lr_finder = LRFinder_new(wrn_28_10)\n",
        "lr_finder.find_generator(train_gen,start_lr=0.01, end_lr=0.1,epochs=10,steps_per_epoch=20)#, class_weight=loss_weights_train)\n",
        "\n",
        "\n",
        "print(\"#Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"#Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyzEOah2Z_mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_finder.get_best_lr(1,1,1))\n",
        "print(lr_finder.get_best_lr(10,1,1))\n",
        "print(lr_finder.get_best_lr(20,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND4LUezqnx96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses = pd.DataFrame({'lrs':lr_finder.lrs, 'losses':lr_finder.losses})\n",
        "lrs_to_losses.loc[lrs_to_losses.shape[0]-1]\n",
        "sma_10_lr = lr_finder.get_best_lr(10,1,1)\n",
        "lrs_to_losses.loc[lrs_to_losses[lrs_to_losses['lrs'].between(sma_10_lr*0.1, sma_10_lr) ]['losses'].idxmin()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PasUsGbpr80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6LOmnuLSdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.min(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMw7fR8XQK1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2TfaqULMg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss(n_skip_end=1)\n",
        "#lr_finder.plot_loss(n_skip_end=1,x_scale='linear')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qS64vScOD5",
        "colab_type": "code",
        "outputId": "66e9348e-702d-4743-b101-408bb39edc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#wrn_28_10=create_model()\n",
        "from keras.models import load_model\n",
        "# import the necessary packages\n",
        "from keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# class LossHistory(keras.callbacks.Callback):\n",
        "# \tdef on_train_begin(self, logs={}):\n",
        "# \t\tprint(\"Clearing saved content on training start\")\n",
        "# \t\tself.losses = []\n",
        "# \t\tself.best = np.Inf\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0, backup_hist=True):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\t\tself.backup_hist = backup_hist\n",
        "\t\tprint(\"JSON path:\",self.jsonPath)\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\t\t#self.losses = []\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath) and (self.backup_hist == True):\n",
        "\t\t\t\t#self.H = json.loads(open(self.jsonPath).read())\n",
        "\t\t\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t\t\tprint(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\t\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\n",
        "\t\t\t\t# # check to see if a starting epoch was supplied\n",
        "\t\t\t\t# if self.startAt > 0:\n",
        "\t\t\t\t# \t# loop over the entries in the history log and\n",
        "\t\t\t\t# \t# trim any entries that are past the starting\n",
        "\t\t\t\t# \t# epoch\n",
        "\t\t\t\t# \tfor k in self.H.keys():\n",
        "\t\t\t\t# \t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\tdef on_train_end(self, logs={}):\t\t\n",
        "\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t#print(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\t\tprint(\"Current JSON PATH:\",self.jsonPath)\n",
        "\t\tprint(\"Final JSON PATH:\",backup_file_name)\t\t\n",
        "\n",
        "import os\n",
        "plotPath = png_file\n",
        "jsonPath = json_file\n",
        "print(plotPath,jsonPath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Base/png_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.png /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC-pZQP-WaNO",
        "colab_type": "code",
        "outputId": "a65ce846-ece6-49f6-e459-5c8571b8925c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "# #                              save_best_only=True)\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True,mode='min')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.{epoch:03d}.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1HpOmnIWPtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=50\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#LEARNING_RATE=0.18883973*0.1\n",
        "#LEARNING_RATE=0.5037587*0.1\n",
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=100\n",
        "test_y = np.linspace(0,EPOCHS,EPOCHS)\n",
        "x=[0, (EPOCHS+1)//5, EPOCHS]\n",
        "y=[LEARNING_RATE*0.01, LEARNING_RATE, LEARNING_RATE*0.0001]\n",
        "interp_lr = np.interp(test_y, x, y)\n",
        "def one_lr_schedule(epoch):\n",
        "    # if(epoch <= 15):\n",
        "    #     print(\"lr:\",interp_lr[epoch+84],epoch)\n",
        "    #     return interp_lr[epoch+84]\n",
        "    print(\"lr:\",interp_lr[epoch],epoch)\n",
        "    return interp_lr[epoch]\n",
        "#interp_values = np.interp(, [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)\n",
        "# callbacks = [checkpoint, lr_scheduler,TrainingMonitor(figPath=plotPath,\n",
        "#                                                       jsonPath=jsonPath,startAt=2)]\n",
        "callbacks = [checkpoint, clr,TrainingMonitor(figPath=plotPath,\n",
        "                                                      jsonPath=jsonPath,startAt=0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHx46QbROE-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join('/content/gdrive/', 'My Drive/WRN_Base')\n",
        "\n",
        "\n",
        "def generate_new_callbacks(steps_per_epoch=50,epoch_count=50,min_lr=0.00001, max_lr=0.1,patience=25):\n",
        "    model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr+\"_\"+str(get_curr_time()))\n",
        "    print(model_name)\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    filepath = os.path.join(save_dir, model_name)\n",
        "    print(filepath)\n",
        "    local_clr = CyclicLR(base_lr=min_lr,\n",
        "                            max_lr=max_lr,\n",
        "                            step_size=steps_per_epoch*4,\n",
        "                            mode='triangular')\n",
        "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "    training_mon = TrainingMonitor(figPath=plotPath,\n",
        "                                   jsonPath=jsonPath,\n",
        "                                   startAt=0)\n",
        "            ########## Introduced after 2x100 Epochs\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                    patience=patience, \n",
        "                                                    restore_best_weights=True)\n",
        "    print(\"Returning new callback array with steps_per_epoch=\",steps_per_epoch,\n",
        "          \"min_lr=\",min_lr,\n",
        "          \"max_lr=\",max_lr,\n",
        "          \"epoch_count=\",epoch_count\n",
        "          )\n",
        "    callback_array = [local_clr, checkpoint, early_stop, training_mon]\n",
        "    return callback_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09u1PORzY1LS",
        "colab_type": "code",
        "outputId": "9267ba56-9519-4d54-a7b7-936708dc865b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.callbacks import CyclicLR\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-cvhc88hq\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-cvhc88hq\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.4)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101065 sha256=906fddde90915cd2801227d1aa5302d2609e9dd6f0062a24cdfa5ade40ab0c2f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uj50dg7t/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUomQO7jBhQ",
        "colab_type": "code",
        "outputId": "49c2e408-a7f6-417b-ca83-141f2982ebb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output': {0: 2.172, 1: 1.0, 2: 1.589, 3: 3.556, 4: 7.326},\n",
              " 'bag_output': {0: 1.667, 1: 5.837, 2: 1.0},\n",
              " 'emotion_output': {0: 6.454, 1: 6.05, 2: 1.0, 3: 12.056},\n",
              " 'footwear_output': {0: 1.23, 1: 2.38, 2: 1.0},\n",
              " 'gender_output': {0: 1.304, 1: 1.0},\n",
              " 'imagequality_ouput': {0: 1.0, 1: 3.361, 2: 1.979},\n",
              " 'pose_output': {0: 3.836, 1: 1.0, 2: 2.786},\n",
              " 'weight_output': {0: 1.0, 1: 9.632, 2: 2.753, 3: 9.931}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG49QJrPglub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model_iterations(model,\n",
        "                         re_compile=True,\n",
        "                         epoch_count=50, \n",
        "                         steps_per_epoch=50, \n",
        "                         min_lr=LEARNING_RATE*0.01, \n",
        "                         max_lr=LEARNING_RATE,\n",
        "                         loss_weights_compile={},\n",
        "                         loss_weights_train={}                         \n",
        "                         ):\n",
        "\n",
        "\n",
        "    if re_compile == True:\n",
        "        model.compile(\n",
        "            #optimizer=SGD(lr=1.3513402*0.1),\n",
        "            optimizer=SGD(lr=min_lr),\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            loss_weights=loss_weights_compile,\n",
        "            #weighted_metrics=[\"accuracy\"]\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        workers=4, \n",
        "        epochs=1,\n",
        "        verbose=1,\n",
        "        class_weight=loss_weights_train,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=generate_new_callbacks(steps_per_epoch=steps_per_epoch, min_lrm=in_lr, max_lr=max_lr, epoch_count=epoch_count)\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzipUJFJjYRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10=create_model()\n",
        "run_model_iterations(wrn_28_10,\n",
        "                    re_compile=True,\n",
        "                    epoch_count=1, \n",
        "                    steps_per_epoch=1, \n",
        "                    min_lr=LEARNING_RATE*0.01, \n",
        "                    max_lr=LEARNING_RATE,\n",
        "                    loss_weights_compile={},\n",
        "                    loss_weights_train={})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay1_bpi-aW_I",
        "colab_type": "code",
        "outputId": "2e8f893b-dc8e-4b5b-ad43-ffe9eb70aa9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json  to: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json1577473064_backup\n",
            "Epoch 1/100\n",
            "30/30 [==============================] - 42s 1s/step - loss: 58.7930 - gender_output_loss: 0.7844 - image_quality_output_loss: 1.1017 - age_output_loss: 3.2201 - weight_output_loss: 3.3636 - bag_output_loss: 1.9356 - pose_output_loss: 1.8879 - footwear_output_loss: 1.4434 - emotion_output_loss: 3.8562 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.2635 - age_output_acc: 0.2937 - weight_output_acc: 0.5698 - bag_output_acc: 0.5406 - pose_output_acc: 0.4750 - footwear_output_acc: 0.4167 - emotion_output_acc: 0.5562 - val_loss: 29.9642 - val_gender_output_loss: 0.6910 - val_image_quality_output_loss: 1.0982 - val_age_output_loss: 1.5928 - val_weight_output_loss: 1.3173 - val_bag_output_loss: 1.0713 - val_pose_output_loss: 1.0729 - val_footwear_output_loss: 1.0934 - val_emotion_output_loss: 1.2939 - val_gender_output_acc: 0.5427 - val_image_quality_output_acc: 0.3581 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4658 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 29.96419, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.001.h5\n",
            "Epoch 2/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 55.9438 - gender_output_loss: 0.7841 - image_quality_output_loss: 1.0814 - age_output_loss: 3.1774 - weight_output_loss: 3.2252 - bag_output_loss: 1.7424 - pose_output_loss: 1.8630 - footwear_output_loss: 1.4841 - emotion_output_loss: 3.4440 - gender_output_acc: 0.5528 - image_quality_output_acc: 0.5172 - age_output_acc: 0.4041 - weight_output_acc: 0.6239 - bag_output_acc: 0.5700 - pose_output_acc: 0.6358 - footwear_output_acc: 0.4246 - emotion_output_acc: 0.6983\n",
            "30/30 [==============================] - 30s 996ms/step - loss: 55.8715 - gender_output_loss: 0.7825 - image_quality_output_loss: 1.0802 - age_output_loss: 3.2050 - weight_output_loss: 3.2222 - bag_output_loss: 1.7381 - pose_output_loss: 1.8547 - footwear_output_loss: 1.4821 - emotion_output_loss: 3.4123 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.5188 - age_output_acc: 0.4010 - weight_output_acc: 0.6229 - bag_output_acc: 0.5708 - pose_output_acc: 0.6365 - footwear_output_acc: 0.4219 - emotion_output_acc: 0.6990 - val_loss: 26.7106 - val_gender_output_loss: 0.6886 - val_image_quality_output_loss: 1.0564 - val_age_output_loss: 1.4926 - val_weight_output_loss: 1.0705 - val_bag_output_loss: 0.9788 - val_pose_output_loss: 0.9625 - val_footwear_output_loss: 1.0688 - val_emotion_output_loss: 0.9525 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4325 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00002: val_loss improved from 29.96419 to 26.71060, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.002.h5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.6293 - gender_output_loss: 0.7716 - image_quality_output_loss: 1.0223 - age_output_loss: 3.0206 - weight_output_loss: 2.6444 - bag_output_loss: 1.5466 - pose_output_loss: 1.7517 - footwear_output_loss: 1.4138 - emotion_output_loss: 2.7591 - gender_output_acc: 0.5802 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3865 - weight_output_acc: 0.6354 - bag_output_acc: 0.5719 - pose_output_acc: 0.6208 - footwear_output_acc: 0.4615 - emotion_output_acc: 0.7063 - val_loss: 25.6523 - val_gender_output_loss: 0.6914 - val_image_quality_output_loss: 1.0015 - val_age_output_loss: 1.4329 - val_weight_output_loss: 0.9887 - val_bag_output_loss: 0.9263 - val_pose_output_loss: 0.9293 - val_footwear_output_loss: 1.0373 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4276 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00003: val_loss improved from 26.71060 to 25.65228, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.003.h5\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.3099 - gender_output_loss: 0.7732 - image_quality_output_loss: 1.0001 - age_output_loss: 2.8457 - weight_output_loss: 2.3595 - bag_output_loss: 1.5591 - pose_output_loss: 1.8764 - footwear_output_loss: 1.3800 - emotion_output_loss: 2.7422 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5406 - age_output_acc: 0.4198 - weight_output_acc: 0.6479 - bag_output_acc: 0.5667 - pose_output_acc: 0.5885 - footwear_output_acc: 0.4552 - emotion_output_acc: 0.7031 - val_loss: 25.5784 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 0.9911 - val_age_output_loss: 1.4253 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.9234 - val_pose_output_loss: 0.9382 - val_footwear_output_loss: 1.0237 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4350 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00004: val_loss improved from 25.65228 to 25.57838, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.004.h5\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.9483 - gender_output_loss: 0.7751 - image_quality_output_loss: 0.9863 - age_output_loss: 2.9464 - weight_output_loss: 2.5370 - bag_output_loss: 1.4155 - pose_output_loss: 1.7745 - footwear_output_loss: 1.3815 - emotion_output_loss: 2.8576 - gender_output_acc: 0.5677 - image_quality_output_acc: 0.5427 - age_output_acc: 0.3781 - weight_output_acc: 0.6490 - bag_output_acc: 0.5833 - pose_output_acc: 0.6000 - footwear_output_acc: 0.4542 - emotion_output_acc: 0.7010 - val_loss: 25.5847 - val_gender_output_loss: 0.6899 - val_image_quality_output_loss: 0.9868 - val_age_output_loss: 1.4224 - val_weight_output_loss: 0.9887 - val_bag_output_loss: 0.9237 - val_pose_output_loss: 0.9415 - val_footwear_output_loss: 1.0255 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4301 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 25.57838\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.8879 - gender_output_loss: 0.7849 - image_quality_output_loss: 0.9648 - age_output_loss: 2.9574 - weight_output_loss: 2.7217 - bag_output_loss: 1.6717 - pose_output_loss: 1.8501 - footwear_output_loss: 1.4696 - emotion_output_loss: 2.6560 - gender_output_acc: 0.5458 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3917 - weight_output_acc: 0.6240 - bag_output_acc: 0.5427 - pose_output_acc: 0.5906 - footwear_output_acc: 0.4406 - emotion_output_acc: 0.7188 - val_loss: 25.5381 - val_gender_output_loss: 0.6898 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.4229 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9252 - val_pose_output_loss: 0.9305 - val_footwear_output_loss: 1.0260 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4420 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00006: val_loss improved from 25.57838 to 25.53813, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.006.h5\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.5651 - gender_output_loss: 0.7698 - image_quality_output_loss: 0.9747 - age_output_loss: 2.8704 - weight_output_loss: 2.7961 - bag_output_loss: 1.5466 - pose_output_loss: 1.8741 - footwear_output_loss: 1.4471 - emotion_output_loss: 2.1964 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5677 - age_output_acc: 0.4094 - weight_output_acc: 0.6073 - bag_output_acc: 0.5823 - pose_output_acc: 0.5917 - footwear_output_acc: 0.4417 - emotion_output_acc: 0.7479 - val_loss: 25.5127 - val_gender_output_loss: 0.6911 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.4181 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9228 - val_pose_output_loss: 0.9296 - val_footwear_output_loss: 1.0263 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4435 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00007: val_loss improved from 25.53813 to 25.51268, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.007.h5\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.2067 - gender_output_loss: 0.7770 - image_quality_output_loss: 0.9714 - age_output_loss: 2.7563 - weight_output_loss: 2.4999 - bag_output_loss: 1.6071 - pose_output_loss: 1.7145 - footwear_output_loss: 1.3923 - emotion_output_loss: 2.5428 - gender_output_acc: 0.5656 - image_quality_output_acc: 0.5656 - age_output_acc: 0.3875 - weight_output_acc: 0.6385 - bag_output_acc: 0.5740 - pose_output_acc: 0.6219 - footwear_output_acc: 0.4656 - emotion_output_acc: 0.7146 - val_loss: 25.4904 - val_gender_output_loss: 0.6914 - val_image_quality_output_loss: 0.9872 - val_age_output_loss: 1.4171 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9231 - val_pose_output_loss: 0.9281 - val_footwear_output_loss: 1.0240 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4474 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00008: val_loss improved from 25.51268 to 25.49040, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.008.h5\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.1659 - gender_output_loss: 0.7919 - image_quality_output_loss: 0.9714 - age_output_loss: 2.6955 - weight_output_loss: 2.5419 - bag_output_loss: 1.6230 - pose_output_loss: 1.7040 - footwear_output_loss: 1.4251 - emotion_output_loss: 2.7840 - gender_output_acc: 0.5333 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4344 - weight_output_acc: 0.6510 - bag_output_acc: 0.5417 - pose_output_acc: 0.6292 - footwear_output_acc: 0.4396 - emotion_output_acc: 0.7115 - val_loss: 25.4900 - val_gender_output_loss: 0.6906 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.4171 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9228 - val_pose_output_loss: 0.9283 - val_footwear_output_loss: 1.0246 - val_emotion_output_loss: 0.9135 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4469 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00008: val_loss improved from 25.51268 to 25.49040, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.008.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 25.49040 to 25.49005, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.009.h5\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.5830 - gender_output_loss: 0.7670 - image_quality_output_loss: 0.9723 - age_output_loss: 2.8932 - weight_output_loss: 2.7765 - bag_output_loss: 1.4712 - pose_output_loss: 1.7092 - footwear_output_loss: 1.3964 - emotion_output_loss: 2.6509 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3958 - weight_output_acc: 0.6385 - bag_output_acc: 0.5656 - pose_output_acc: 0.6260 - footwear_output_acc: 0.4479 - emotion_output_acc: 0.7052 - val_loss: 25.4932 - val_gender_output_loss: 0.6924 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.4187 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9229 - val_pose_output_loss: 0.9274 - val_footwear_output_loss: 1.0197 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4598 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 25.49005\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.5186 - gender_output_loss: 0.7726 - image_quality_output_loss: 0.9965 - age_output_loss: 2.9214 - weight_output_loss: 2.5106 - bag_output_loss: 1.5988 - pose_output_loss: 1.6912 - footwear_output_loss: 1.4015 - emotion_output_loss: 2.7065 - gender_output_acc: 0.5740 - image_quality_output_acc: 0.5354 - age_output_acc: 0.3979 - weight_output_acc: 0.6458 - bag_output_acc: 0.5781 - pose_output_acc: 0.6271 - footwear_output_acc: 0.4344 - emotion_output_acc: 0.7167 - val_loss: 25.5087 - val_gender_output_loss: 0.6928 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.4186 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.9254 - val_pose_output_loss: 0.9302 - val_footwear_output_loss: 1.0183 - val_emotion_output_loss: 0.9146 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5040 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 25.49005\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 47.9024 - gender_output_loss: 0.7694 - image_quality_output_loss: 0.9782 - age_output_loss: 2.7827 - weight_output_loss: 2.3556 - bag_output_loss: 1.6544 - pose_output_loss: 1.7920 - footwear_output_loss: 1.3398 - emotion_output_loss: 2.7319 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4000 - weight_output_acc: 0.6500 - bag_output_acc: 0.5427 - pose_output_acc: 0.6042 - footwear_output_acc: 0.4750 - emotion_output_acc: 0.7094 - val_loss: 25.5865 - val_gender_output_loss: 0.6900 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4248 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9262 - val_pose_output_loss: 0.9307 - val_footwear_output_loss: 1.0289 - val_emotion_output_loss: 0.9201 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.3983 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 25.49005\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.1005 - gender_output_loss: 0.7808 - image_quality_output_loss: 0.9786 - age_output_loss: 2.7286 - weight_output_loss: 2.6163 - bag_output_loss: 1.7409 - pose_output_loss: 1.7192 - footwear_output_loss: 1.3782 - emotion_output_loss: 2.6048 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4073 - weight_output_acc: 0.6448 - bag_output_acc: 0.5479 - pose_output_acc: 0.6219 - footwear_output_acc: 0.4740 - emotion_output_acc: 0.7167 - val_loss: 25.5045 - val_gender_output_loss: 0.6885 - val_image_quality_output_loss: 0.9857 - val_age_output_loss: 1.4168 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.9268 - val_pose_output_loss: 0.9276 - val_footwear_output_loss: 1.0224 - val_emotion_output_loss: 0.9168 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4291 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 25.49005\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.0316 - gender_output_loss: 0.7647 - image_quality_output_loss: 0.9788 - age_output_loss: 2.8551 - weight_output_loss: 2.5999 - bag_output_loss: 1.5446 - pose_output_loss: 1.7396 - footwear_output_loss: 1.4223 - emotion_output_loss: 2.8413 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3885 - weight_output_acc: 0.6302 - bag_output_acc: 0.5823 - pose_output_acc: 0.6219 - footwear_output_acc: 0.4302 - emotion_output_acc: 0.7031 - val_loss: 25.4948 - val_gender_output_loss: 0.6897 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.4189 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.9228 - val_pose_output_loss: 0.9288 - val_footwear_output_loss: 1.0179 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5174 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 25.49005\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 25.49005\n",
            "Epoch 15/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 48.7700 - gender_output_loss: 0.7807 - image_quality_output_loss: 0.9635 - age_output_loss: 2.8407 - weight_output_loss: 2.7639 - bag_output_loss: 1.3543 - pose_output_loss: 1.6933 - footwear_output_loss: 1.4289 - emotion_output_loss: 2.8413 - gender_output_acc: 0.5539 - image_quality_output_acc: 0.5744 - age_output_acc: 0.3966 - weight_output_acc: 0.6228 - bag_output_acc: 0.5970 - pose_output_acc: 0.6239 - footwear_output_acc: 0.4591 - emotion_output_acc: 0.6994\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.6678 - gender_output_loss: 0.7815 - image_quality_output_loss: 0.9655 - age_output_loss: 2.8118 - weight_output_loss: 2.7810 - bag_output_loss: 1.3560 - pose_output_loss: 1.6985 - footwear_output_loss: 1.4289 - emotion_output_loss: 2.8253 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4000 - weight_output_acc: 0.6219 - bag_output_acc: 0.5948 - pose_output_acc: 0.6229 - footwear_output_acc: 0.4604 - emotion_output_acc: 0.7000 - val_loss: 25.4782 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.4159 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.9210 - val_pose_output_loss: 0.9286 - val_footwear_output_loss: 1.0149 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4955 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00015: val_loss improved from 25.49005 to 25.47817, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.015.h5\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 50.6098 - gender_output_loss: 0.7632 - image_quality_output_loss: 0.9966 - age_output_loss: 2.9302 - weight_output_loss: 2.7430 - bag_output_loss: 1.6900 - pose_output_loss: 1.7785 - footwear_output_loss: 1.4115 - emotion_output_loss: 2.9127 - gender_output_acc: 0.5885 - image_quality_output_acc: 0.5427 - age_output_acc: 0.3958 - weight_output_acc: 0.6302 - bag_output_acc: 0.5417 - pose_output_acc: 0.6083 - footwear_output_acc: 0.4531 - emotion_output_acc: 0.6969 - val_loss: 25.4721 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 0.9863 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9210 - val_pose_output_loss: 0.9279 - val_footwear_output_loss: 1.0153 - val_emotion_output_loss: 0.9173 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5129 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00016: val_loss improved from 25.47817 to 25.47212, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.016.h5\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.6124 - gender_output_loss: 0.7789 - image_quality_output_loss: 0.9699 - age_output_loss: 2.9680 - weight_output_loss: 2.6828 - bag_output_loss: 1.6871 - pose_output_loss: 1.6635 - footwear_output_loss: 1.3815 - emotion_output_loss: 2.5296 - gender_output_acc: 0.5552 - image_quality_output_acc: 0.5719 - age_output_acc: 0.3771 - weight_output_acc: 0.6354 - bag_output_acc: 0.5479 - pose_output_acc: 0.6344 - footwear_output_acc: 0.4656 - emotion_output_acc: 0.7323 - val_loss: 25.4614 - val_gender_output_loss: 0.6876 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.4172 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9213 - val_pose_output_loss: 0.9276 - val_footwear_output_loss: 1.0146 - val_emotion_output_loss: 0.9146 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5159 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00017: val_loss improved from 25.47212 to 25.46141, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.017.h5\n",
            "\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 50.1889 - gender_output_loss: 0.7928 - image_quality_output_loss: 0.9960 - age_output_loss: 3.0248 - weight_output_loss: 2.7927 - bag_output_loss: 1.4812 - pose_output_loss: 1.8182 - footwear_output_loss: 1.4145 - emotion_output_loss: 2.7865 - gender_output_acc: 0.5271 - image_quality_output_acc: 0.5396 - age_output_acc: 0.3875 - weight_output_acc: 0.6198 - bag_output_acc: 0.5708 - pose_output_acc: 0.5990 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.6979 - val_loss: 25.4538 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9860 - val_age_output_loss: 1.4193 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9210 - val_pose_output_loss: 0.9277 - val_footwear_output_loss: 1.0138 - val_emotion_output_loss: 0.9133 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5134 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00018: val_loss improved from 25.46141 to 25.45380, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.018.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 25.47212 to 25.46141, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.017.h5\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.6904 - gender_output_loss: 0.7719 - image_quality_output_loss: 0.9767 - age_output_loss: 2.9509 - weight_output_loss: 2.7577 - bag_output_loss: 1.6228 - pose_output_loss: 1.8267 - footwear_output_loss: 1.4103 - emotion_output_loss: 2.6717 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3865 - weight_output_acc: 0.6198 - bag_output_acc: 0.5635 - pose_output_acc: 0.5896 - footwear_output_acc: 0.4833 - emotion_output_acc: 0.7125 - val_loss: 25.4981 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9857 - val_age_output_loss: 1.4205 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9208 - val_pose_output_loss: 0.9335 - val_footwear_output_loss: 1.0166 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4737 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 25.45380\n",
            "Epoch 20/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 46.9774 - gender_output_loss: 0.7732 - image_quality_output_loss: 0.9791 - age_output_loss: 2.9047 - weight_output_loss: 2.4081 - bag_output_loss: 1.5845 - pose_output_loss: 1.7018 - footwear_output_loss: 1.3157 - emotion_output_loss: 2.4698 - gender_output_acc: 0.5679 - image_quality_output_acc: 0.5593 - age_output_acc: 0.4192 - weight_output_acc: 0.6422 - bag_output_acc: 0.5517 - pose_output_acc: 0.6282 - footwear_output_acc: 0.4957 - emotion_output_acc: 0.7295\n",
            "Epoch 00019: val_loss did not improve from 25.45380\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.9327 - gender_output_loss: 0.7716 - image_quality_output_loss: 0.9792 - age_output_loss: 2.8947 - weight_output_loss: 2.4427 - bag_output_loss: 1.6049 - pose_output_loss: 1.6974 - footwear_output_loss: 1.3180 - emotion_output_loss: 2.4303 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4167 - weight_output_acc: 0.6396 - bag_output_acc: 0.5510 - pose_output_acc: 0.6292 - footwear_output_acc: 0.4958 - emotion_output_acc: 0.7333 - val_loss: 25.4276 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9216 - val_pose_output_loss: 0.9268 - val_footwear_output_loss: 1.0120 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4702 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00020: val_loss improved from 25.45380 to 25.42758, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.020.h5\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.9598 - gender_output_loss: 0.7810 - image_quality_output_loss: 0.9489 - age_output_loss: 2.8164 - weight_output_loss: 2.4746 - bag_output_loss: 1.5264 - pose_output_loss: 1.7691 - footwear_output_loss: 1.4062 - emotion_output_loss: 2.7133 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4125 - weight_output_acc: 0.6240 - bag_output_acc: 0.5719 - pose_output_acc: 0.6125 - footwear_output_acc: 0.4760 - emotion_output_acc: 0.7104 - val_loss: 25.4983 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4213 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.9216 - val_pose_output_loss: 0.9269 - val_footwear_output_loss: 1.0116 - val_emotion_output_loss: 0.9186 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5258 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00020: val_loss improved from 25.45380 to 25.42758, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.020.h5\n",
            "Epoch 00021: val_loss did not improve from 25.42758\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 45.8857 - gender_output_loss: 0.7688 - image_quality_output_loss: 1.0044 - age_output_loss: 2.6785 - weight_output_loss: 2.2965 - bag_output_loss: 1.4991 - pose_output_loss: 1.8187 - footwear_output_loss: 1.3972 - emotion_output_loss: 2.4324 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5281 - age_output_acc: 0.4323 - weight_output_acc: 0.6750 - bag_output_acc: 0.5771 - pose_output_acc: 0.6010 - footwear_output_acc: 0.4646 - emotion_output_acc: 0.7271 - val_loss: 25.4220 - val_gender_output_loss: 0.6858 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9209 - val_pose_output_loss: 0.9279 - val_footwear_output_loss: 1.0098 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5536 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00022: val_loss improved from 25.42758 to 25.42203, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.022.h5\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.8482 - gender_output_loss: 0.7779 - image_quality_output_loss: 0.9741 - age_output_loss: 2.8171 - weight_output_loss: 2.3268 - bag_output_loss: 1.6062 - pose_output_loss: 1.6881 - footwear_output_loss: 1.3489 - emotion_output_loss: 2.5641 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3990 - weight_output_acc: 0.6750 - bag_output_acc: 0.5573 - pose_output_acc: 0.6313 - footwear_output_acc: 0.4677 - emotion_output_acc: 0.7167 - val_loss: 25.4153 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9848 - val_age_output_loss: 1.4146 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9211 - val_pose_output_loss: 0.9268 - val_footwear_output_loss: 1.0102 - val_emotion_output_loss: 0.9130 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5585 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00023: val_loss improved from 25.42203 to 25.41535, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.023.h5\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.5251 - gender_output_loss: 0.7563 - image_quality_output_loss: 0.9912 - age_output_loss: 2.9746 - weight_output_loss: 2.4660 - bag_output_loss: 1.5615 - pose_output_loss: 1.6432 - footwear_output_loss: 1.4082 - emotion_output_loss: 2.7613 - gender_output_acc: 0.5958 - image_quality_output_acc: 0.5354 - age_output_acc: 0.3823 - weight_output_acc: 0.6396 - bag_output_acc: 0.5698 - pose_output_acc: 0.6375 - footwear_output_acc: 0.4448 - emotion_output_acc: 0.7052 - val_loss: 25.4286 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.9213 - val_pose_output_loss: 0.9268 - val_footwear_output_loss: 1.0097 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5437 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 25.41535\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.7398 - gender_output_loss: 0.7797 - image_quality_output_loss: 0.9889 - age_output_loss: 2.8366 - weight_output_loss: 2.3765 - bag_output_loss: 1.7014 - pose_output_loss: 1.6740 - footwear_output_loss: 1.3941 - emotion_output_loss: 2.3886 - gender_output_acc: 0.5531 - image_quality_output_acc: 0.5427 - age_output_acc: 0.4000 - weight_output_acc: 0.6729 - bag_output_acc: 0.5323 - pose_output_acc: 0.6354 - footwear_output_acc: 0.4615 - emotion_output_acc: 0.7354 - val_loss: 25.4122 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4156 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.9210 - val_pose_output_loss: 0.9267 - val_footwear_output_loss: 1.0088 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5556 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00025: val_loss improved from 25.41535 to 25.41217, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.025.h5\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.9585 - gender_output_loss: 0.7638 - image_quality_output_loss: 1.0050 - age_output_loss: 2.9910 - weight_output_loss: 2.8335 - bag_output_loss: 1.6035 - pose_output_loss: 1.7478 - footwear_output_loss: 1.3619 - emotion_output_loss: 2.4804 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5260 - age_output_acc: 0.3854 - weight_output_acc: 0.5990 - bag_output_acc: 0.5667 - pose_output_acc: 0.6146 - footwear_output_acc: 0.4979 - emotion_output_acc: 0.7250 - val_loss: 25.4475 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9864 - val_age_output_loss: 1.4196 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.9214 - val_pose_output_loss: 0.9266 - val_footwear_output_loss: 1.0077 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5432 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 25.41217\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.7376 - gender_output_loss: 0.7941 - image_quality_output_loss: 0.9881 - age_output_loss: 2.7190 - weight_output_loss: 2.5196 - bag_output_loss: 1.5790 - pose_output_loss: 1.6579 - footwear_output_loss: 1.4341 - emotion_output_loss: 2.4755 - gender_output_acc: 0.5198 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4031 - weight_output_acc: 0.6562 - bag_output_acc: 0.5552 - pose_output_acc: 0.6333 - footwear_output_acc: 0.4490 - emotion_output_acc: 0.7344 - val_loss: 25.4121 - val_gender_output_loss: 0.6820 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4156 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9199 - val_pose_output_loss: 0.9285 - val_footwear_output_loss: 1.0049 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5481 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00027: val_loss improved from 25.41217 to 25.41208, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.027.h5\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 49.1257 - gender_output_loss: 0.7657 - image_quality_output_loss: 0.9425 - age_output_loss: 2.8447 - weight_output_loss: 2.7722 - bag_output_loss: 1.5131 - pose_output_loss: 1.7972 - footwear_output_loss: 1.3592 - emotion_output_loss: 2.7770 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5885 - age_output_acc: 0.3958 - weight_output_acc: 0.6313 - bag_output_acc: 0.5875 - pose_output_acc: 0.6031 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.6938 - val_loss: 25.4102 - val_gender_output_loss: 0.6818 - val_image_quality_output_loss: 0.9891 - val_age_output_loss: 1.4134 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9213 - val_pose_output_loss: 0.9270 - val_footwear_output_loss: 1.0062 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4970 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00028: val_loss improved from 25.41208 to 25.41020, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.028.h5\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 47.2616 - gender_output_loss: 0.7774 - image_quality_output_loss: 0.9970 - age_output_loss: 2.6044 - weight_output_loss: 2.3919 - bag_output_loss: 1.6702 - pose_output_loss: 1.8317 - footwear_output_loss: 1.3759 - emotion_output_loss: 2.6516 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5365 - age_output_acc: 0.4375 - weight_output_acc: 0.6542 - bag_output_acc: 0.5323 - pose_output_acc: 0.5948 - footwear_output_acc: 0.4938 - emotion_output_acc: 0.7135 - val_loss: 25.3904 - val_gender_output_loss: 0.6812 - val_image_quality_output_loss: 0.9851 - val_age_output_loss: 1.4141 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9200 - val_pose_output_loss: 0.9292 - val_footwear_output_loss: 1.0068 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5069 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00029: val_loss improved from 25.41020 to 25.39042, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.029.h5\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 49.0068 - gender_output_loss: 0.7678 - image_quality_output_loss: 0.9710 - age_output_loss: 2.8401 - weight_output_loss: 2.6904 - bag_output_loss: 1.5387 - pose_output_loss: 1.6822 - footwear_output_loss: 1.4000 - emotion_output_loss: 2.8450 - gender_output_acc: 0.5719 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4042 - weight_output_acc: 0.6177 - bag_output_acc: 0.5708 - pose_output_acc: 0.6323 - footwear_output_acc: 0.4719 - emotion_output_acc: 0.7010 - val_loss: 25.3632 - val_gender_output_loss: 0.6817 - val_image_quality_output_loss: 0.9856 - val_age_output_loss: 1.4112 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9175 - val_pose_output_loss: 0.9259 - val_footwear_output_loss: 0.9997 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5595 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00030: val_loss improved from 25.39042 to 25.36317, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.030.h5\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 49.1620 - gender_output_loss: 0.7565 - image_quality_output_loss: 0.9858 - age_output_loss: 2.8969 - weight_output_loss: 2.6949 - bag_output_loss: 1.5022 - pose_output_loss: 1.7852 - footwear_output_loss: 1.3626 - emotion_output_loss: 2.7908 - gender_output_acc: 0.5885 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3917 - weight_output_acc: 0.6146 - bag_output_acc: 0.5771 - pose_output_acc: 0.6010 - footwear_output_acc: 0.4667 - emotion_output_acc: 0.6958 - val_loss: 25.3561 - val_gender_output_loss: 0.6820 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9172 - val_pose_output_loss: 0.9269 - val_footwear_output_loss: 1.0003 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5516 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00031: val_loss improved from 25.36317 to 25.35614, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.031.h5\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 47.8859 - gender_output_loss: 0.7548 - image_quality_output_loss: 0.9865 - age_output_loss: 2.9944 - weight_output_loss: 2.4592 - bag_output_loss: 1.4939 - pose_output_loss: 1.8336 - footwear_output_loss: 1.3423 - emotion_output_loss: 2.5316 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5458 - age_output_acc: 0.3917 - weight_output_acc: 0.6458 - bag_output_acc: 0.5771 - pose_output_acc: 0.5917 - footwear_output_acc: 0.4938 - emotion_output_acc: 0.7292 - val_loss: 25.3550 - val_gender_output_loss: 0.6818 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.4129 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9172 - val_pose_output_loss: 0.9284 - val_footwear_output_loss: 0.9990 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5526 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00032: val_loss improved from 25.35614 to 25.35502, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.032.h5\n",
            "Epoch 33/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 49.0277 - gender_output_loss: 0.7566 - image_quality_output_loss: 0.9563 - age_output_loss: 3.0868 - weight_output_loss: 2.5751 - bag_output_loss: 1.6655 - pose_output_loss: 1.6870 - footwear_output_loss: 1.4290 - emotion_output_loss: 2.5898 - gender_output_acc: 0.5916 - image_quality_output_acc: 0.5787 - age_output_acc: 0.3664 - weight_output_acc: 0.6412 - bag_output_acc: 0.5636 - pose_output_acc: 0.6293 - footwear_output_acc: 0.4612 - emotion_output_acc: 0.7188\n",
            "Epoch 00032: val_loss improved from 25.35614 to 25.35502, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.032.h5\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.9222 - gender_output_loss: 0.7593 - image_quality_output_loss: 0.9569 - age_output_loss: 3.1059 - weight_output_loss: 2.5569 - bag_output_loss: 1.6857 - pose_output_loss: 1.6663 - footwear_output_loss: 1.4280 - emotion_output_loss: 2.5572 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5802 - age_output_acc: 0.3688 - weight_output_acc: 0.6417 - bag_output_acc: 0.5604 - pose_output_acc: 0.6344 - footwear_output_acc: 0.4615 - emotion_output_acc: 0.7208 - val_loss: 25.3472 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.9171 - val_pose_output_loss: 0.9269 - val_footwear_output_loss: 0.9988 - val_emotion_output_loss: 0.9109 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5590 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00033: val_loss improved from 25.35502 to 25.34721, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.033.h5\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.3750 - gender_output_loss: 0.7802 - image_quality_output_loss: 0.9743 - age_output_loss: 2.7898 - weight_output_loss: 2.4371 - bag_output_loss: 1.4273 - pose_output_loss: 1.6633 - footwear_output_loss: 1.3896 - emotion_output_loss: 2.7726 - gender_output_acc: 0.5417 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4125 - weight_output_acc: 0.6438 - bag_output_acc: 0.5833 - pose_output_acc: 0.6365 - footwear_output_acc: 0.4646 - emotion_output_acc: 0.7010 - val_loss: 25.3830 - val_gender_output_loss: 0.6802 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4126 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.9183 - val_pose_output_loss: 0.9263 - val_footwear_output_loss: 0.9990 - val_emotion_output_loss: 0.9206 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5600 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00033: val_loss improved from 25.35502 to 25.34721, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.033.h5\n",
            "Epoch 00034: val_loss did not improve from 25.34721\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 50.0353 - gender_output_loss: 0.7807 - image_quality_output_loss: 0.9881 - age_output_loss: 3.0590 - weight_output_loss: 2.6056 - bag_output_loss: 1.6263 - pose_output_loss: 1.6580 - footwear_output_loss: 1.3726 - emotion_output_loss: 2.8982 - gender_output_acc: 0.5417 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3729 - weight_output_acc: 0.6302 - bag_output_acc: 0.5521 - pose_output_acc: 0.6365 - footwear_output_acc: 0.4948 - emotion_output_acc: 0.6938 - val_loss: 25.3777 - val_gender_output_loss: 0.6779 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.4173 - val_weight_output_loss: 0.9800 - val_bag_output_loss: 0.9170 - val_pose_output_loss: 0.9261 - val_footwear_output_loss: 0.9962 - val_emotion_output_loss: 0.9164 - val_gender_output_acc: 0.5466 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5600 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 25.34721\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.5005 - gender_output_loss: 0.7635 - image_quality_output_loss: 0.9857 - age_output_loss: 2.7791 - weight_output_loss: 2.5375 - bag_output_loss: 1.4874 - pose_output_loss: 1.8166 - footwear_output_loss: 1.3913 - emotion_output_loss: 2.8314 - gender_output_acc: 0.5760 - image_quality_output_acc: 0.5573 - age_output_acc: 0.4208 - weight_output_acc: 0.6448 - bag_output_acc: 0.5771 - pose_output_acc: 0.5958 - footwear_output_acc: 0.4854 - emotion_output_acc: 0.7063 - val_loss: 25.3419 - val_gender_output_loss: 0.6792 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4097 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.9145 - val_pose_output_loss: 0.9275 - val_footwear_output_loss: 0.9951 - val_emotion_output_loss: 0.9169 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5570 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00036: val_loss improved from 25.34721 to 25.34186, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.036.h5\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.0618 - gender_output_loss: 0.7672 - image_quality_output_loss: 1.0063 - age_output_loss: 2.8574 - weight_output_loss: 2.4754 - bag_output_loss: 1.4705 - pose_output_loss: 1.7256 - footwear_output_loss: 1.3509 - emotion_output_loss: 2.7790 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5333 - age_output_acc: 0.4125 - weight_output_acc: 0.6417 - bag_output_acc: 0.5771 - pose_output_acc: 0.6198 - footwear_output_acc: 0.4823 - emotion_output_acc: 0.7031 - val_loss: 25.3453 - val_gender_output_loss: 0.6783 - val_image_quality_output_loss: 0.9871 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.9162 - val_pose_output_loss: 0.9265 - val_footwear_output_loss: 0.9938 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5516 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 25.34186\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.1347 - gender_output_loss: 0.7742 - image_quality_output_loss: 0.9898 - age_output_loss: 2.8787 - weight_output_loss: 2.6292 - bag_output_loss: 1.5895 - pose_output_loss: 1.7798 - footwear_output_loss: 1.3342 - emotion_output_loss: 2.5441 - gender_output_acc: 0.5531 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3823 - weight_output_acc: 0.6354 - bag_output_acc: 0.5656 - pose_output_acc: 0.6104 - footwear_output_acc: 0.4885 - emotion_output_acc: 0.7250 - val_loss: 25.3097 - val_gender_output_loss: 0.6753 - val_image_quality_output_loss: 0.9860 - val_age_output_loss: 1.4104 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.9144 - val_pose_output_loss: 0.9266 - val_footwear_output_loss: 0.9916 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5486 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.0618 - gender_output_loss: 0.7672 - image_quality_output_loss: 1.0063 - age_output_loss: 2.8574 - weight_output_loss: 2.4754 - bag_output_loss: 1.4705 - pose_output_loss: 1.7256 - footwear_output_loss: 1.3509 - emotion_output_loss: 2.7790 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5333 - age_output_acc: 0.4125 - weight_output_acc: 0.6417 - bag_output_acc: 0.5771 - pose_output_acc: 0.6198 - footwear_output_acc: 0.4823 - emotion_output_acc: 0.7031 - val_loss: 25.3453 - val_gender_output_loss: 0.6783 - val_image_quality_output_loss: 0.9871 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.9162 - val_pose_output_loss: 0.9265 - val_footwear_output_loss: 0.9938 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5516 - val_emotion_output_acc: 0.7073\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00038: val_loss improved from 25.34186 to 25.30974, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.038.h5\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 50.2465 - gender_output_loss: 0.7644 - image_quality_output_loss: 0.9827 - age_output_loss: 3.0582 - weight_output_loss: 2.6170 - bag_output_loss: 1.5985 - pose_output_loss: 1.6557 - footwear_output_loss: 1.4024 - emotion_output_loss: 2.9625 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3927 - weight_output_acc: 0.6271 - bag_output_acc: 0.5688 - pose_output_acc: 0.6333 - footwear_output_acc: 0.4875 - emotion_output_acc: 0.6885 - val_loss: 25.4347 - val_gender_output_loss: 0.6757 - val_image_quality_output_loss: 0.9879 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9164 - val_pose_output_loss: 0.9262 - val_footwear_output_loss: 0.9978 - val_emotion_output_loss: 0.9255 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5174 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 25.30974\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.9636 - gender_output_loss: 0.7707 - image_quality_output_loss: 0.9840 - age_output_loss: 2.8682 - weight_output_loss: 2.5894 - bag_output_loss: 1.5440 - pose_output_loss: 1.7872 - footwear_output_loss: 1.3521 - emotion_output_loss: 2.5659 - gender_output_acc: 0.5573 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4146 - weight_output_acc: 0.6490 - bag_output_acc: 0.5802 - pose_output_acc: 0.6083 - footwear_output_acc: 0.4948 - emotion_output_acc: 0.7188 - val_loss: 25.3059 - val_gender_output_loss: 0.6745 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.4111 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9144 - val_pose_output_loss: 0.9262 - val_footwear_output_loss: 0.9919 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5471 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00040: val_loss improved from 25.30974 to 25.30587, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.040.h5\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.1487 - gender_output_loss: 0.7540 - image_quality_output_loss: 0.9621 - age_output_loss: 3.0036 - weight_output_loss: 2.6335 - bag_output_loss: 1.5602 - pose_output_loss: 1.7278 - footwear_output_loss: 1.4192 - emotion_output_loss: 2.7120 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3948 - weight_output_acc: 0.6365 - bag_output_acc: 0.5740 - pose_output_acc: 0.6125 - footwear_output_acc: 0.4667 - emotion_output_acc: 0.7083 - val_loss: 25.3096 - val_gender_output_loss: 0.6742 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.4113 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.9142 - val_pose_output_loss: 0.9270 - val_footwear_output_loss: 0.9906 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5461 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 25.30587\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 49.3750 - gender_output_loss: 0.7711 - image_quality_output_loss: 0.9786 - age_output_loss: 2.8045 - weight_output_loss: 2.8600 - bag_output_loss: 1.6085 - pose_output_loss: 1.7080 - footwear_output_loss: 1.4172 - emotion_output_loss: 2.7607 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3969 - weight_output_acc: 0.6125 - bag_output_acc: 0.5531 - pose_output_acc: 0.6219 - footwear_output_acc: 0.4677 - emotion_output_acc: 0.7073 - val_loss: 25.2938 - val_gender_output_loss: 0.6723 - val_image_quality_output_loss: 0.9851 - val_age_output_loss: 1.4098 - val_weight_output_loss: 0.9820 - val_bag_output_loss: 0.9123 - val_pose_output_loss: 0.9269 - val_footwear_output_loss: 0.9887 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5575 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "\n",
            "Epoch 00042: val_loss improved from 25.30587 to 25.29381, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.042.h5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.9267 - gender_output_loss: 0.7679 - image_quality_output_loss: 0.9845 - age_output_loss: 2.8579 - weight_output_loss: 2.5353 - bag_output_loss: 1.6049 - pose_output_loss: 1.7489 - footwear_output_loss: 1.3984 - emotion_output_loss: 2.5689 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5448 - age_output_acc: 0.4135 - weight_output_acc: 0.6479 - bag_output_acc: 0.5750 - pose_output_acc: 0.6177 - footwear_output_acc: 0.4552 - emotion_output_acc: 0.7208 - val_loss: 25.3192 - val_gender_output_loss: 0.6742 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.4111 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.9145 - val_pose_output_loss: 0.9275 - val_footwear_output_loss: 0.9907 - val_emotion_output_loss: 0.9133 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5461 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 25.29381\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 46.4347 - gender_output_loss: 0.7797 - image_quality_output_loss: 0.9536 - age_output_loss: 2.7095 - weight_output_loss: 2.2708 - bag_output_loss: 1.5760 - pose_output_loss: 1.7349 - footwear_output_loss: 1.3621 - emotion_output_loss: 2.6027 - gender_output_acc: 0.5385 - image_quality_output_acc: 0.5906 - age_output_acc: 0.4260 - weight_output_acc: 0.6448 - bag_output_acc: 0.4938 - pose_output_acc: 0.6198 - footwear_output_acc: 0.4885 - emotion_output_acc: 0.7198 - val_loss: 25.3063 - val_gender_output_loss: 0.6718 - val_image_quality_output_loss: 0.9894 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.9186 - val_pose_output_loss: 0.9252 - val_footwear_output_loss: 0.9874 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5635 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5511 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 25.29381\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 25.29381\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.8092 - gender_output_loss: 0.7647 - image_quality_output_loss: 0.9745 - age_output_loss: 2.9945 - weight_output_loss: 2.8141 - bag_output_loss: 1.6370 - pose_output_loss: 1.7386 - footwear_output_loss: 1.4007 - emotion_output_loss: 2.4333 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3896 - weight_output_acc: 0.6083 - bag_output_acc: 0.5406 - pose_output_acc: 0.6198 - footwear_output_acc: 0.4917 - emotion_output_acc: 0.7271 - val_loss: 25.2749 - val_gender_output_loss: 0.6687 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9853 - val_bag_output_loss: 0.9116 - val_pose_output_loss: 0.9252 - val_footwear_output_loss: 0.9829 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5541 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00045: val_loss improved from 25.29381 to 25.27488, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.045.h5\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.1298 - gender_output_loss: 0.7745 - image_quality_output_loss: 1.0048 - age_output_loss: 2.8646 - weight_output_loss: 2.5232 - bag_output_loss: 1.5289 - pose_output_loss: 1.7874 - footwear_output_loss: 1.3376 - emotion_output_loss: 2.6677 - gender_output_acc: 0.5458 - image_quality_output_acc: 0.5219 - age_output_acc: 0.3833 - weight_output_acc: 0.6479 - bag_output_acc: 0.5708 - pose_output_acc: 0.6031 - footwear_output_acc: 0.5073 - emotion_output_acc: 0.7083 - val_loss: 25.2469 - val_gender_output_loss: 0.6689 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9103 - val_pose_output_loss: 0.9256 - val_footwear_output_loss: 0.9801 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.5471 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5531 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00046: val_loss improved from 25.27488 to 25.24687, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.046.h5\n",
            "Epoch 47/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 47.2136 - gender_output_loss: 0.7577 - image_quality_output_loss: 0.9545 - age_output_loss: 2.7881 - weight_output_loss: 2.4239 - bag_output_loss: 1.6217 - pose_output_loss: 1.6867 - footwear_output_loss: 1.3291 - emotion_output_loss: 2.6334 - gender_output_acc: 0.5830 - image_quality_output_acc: 0.5797 - age_output_acc: 0.3901 - weight_output_acc: 0.6616 - bag_output_acc: 0.5679 - pose_output_acc: 0.6272 - footwear_output_acc: 0.5172 - emotion_output_acc: 0.7155\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.0504 - gender_output_loss: 0.7558 - image_quality_output_loss: 0.9558 - age_output_loss: 2.7714 - weight_output_loss: 2.4162 - bag_output_loss: 1.6119 - pose_output_loss: 1.6838 - footwear_output_loss: 1.3293 - emotion_output_loss: 2.6247 - gender_output_acc: 0.5865 - image_quality_output_acc: 0.5771 - age_output_acc: 0.3906 - weight_output_acc: 0.6625 - bag_output_acc: 0.5719 - pose_output_acc: 0.6281 - footwear_output_acc: 0.5156 - emotion_output_acc: 0.7177 - val_loss: 25.2403 - val_gender_output_loss: 0.6721 - val_image_quality_output_loss: 0.9851 - val_age_output_loss: 1.4056 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9113 - val_pose_output_loss: 0.9253 - val_footwear_output_loss: 0.9800 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.5432 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5600 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00047: val_loss improved from 25.24687 to 25.24032, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.047.h5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 46.8490 - gender_output_loss: 0.7494 - image_quality_output_loss: 0.9880 - age_output_loss: 2.7174 - weight_output_loss: 2.5190 - bag_output_loss: 1.4534 - pose_output_loss: 1.7300 - footwear_output_loss: 1.3430 - emotion_output_loss: 2.6159 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3812 - weight_output_acc: 0.6396 - bag_output_acc: 0.5844 - pose_output_acc: 0.6187 - footwear_output_acc: 0.5052 - emotion_output_acc: 0.7073 - val_loss: 25.2478 - val_gender_output_loss: 0.6724 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4066 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9118 - val_pose_output_loss: 0.9250 - val_footwear_output_loss: 0.9805 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5580 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 25.24032\n",
            "Epoch 49/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 48.5551 - gender_output_loss: 0.7694 - image_quality_output_loss: 0.9771 - age_output_loss: 3.0296 - weight_output_loss: 2.5252 - bag_output_loss: 1.5687 - pose_output_loss: 1.5777 - footwear_output_loss: 1.3074 - emotion_output_loss: 2.7665 - gender_output_acc: 0.5582 - image_quality_output_acc: 0.5614 - age_output_acc: 0.4019 - weight_output_acc: 0.6315 - bag_output_acc: 0.5485 - pose_output_acc: 0.6552 - footwear_output_acc: 0.5269 - emotion_output_acc: 0.7101Epoch 49/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.4761 - gender_output_loss: 0.7690 - image_quality_output_loss: 0.9744 - age_output_loss: 3.0324 - weight_output_loss: 2.5107 - bag_output_loss: 1.5694 - pose_output_loss: 1.5841 - footwear_output_loss: 1.3049 - emotion_output_loss: 2.7524 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4000 - weight_output_acc: 0.6323 - bag_output_acc: 0.5479 - pose_output_acc: 0.6542 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7104 - val_loss: 25.2560 - val_gender_output_loss: 0.6719 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.4073 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.9114 - val_pose_output_loss: 0.9250 - val_footwear_output_loss: 0.9820 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5357 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 25.24032\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.5234 - gender_output_loss: 0.7663 - image_quality_output_loss: 0.9641 - age_output_loss: 2.7327 - weight_output_loss: 2.4492 - bag_output_loss: 1.5875 - pose_output_loss: 1.6902 - footwear_output_loss: 1.3645 - emotion_output_loss: 2.7435 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4229 - weight_output_acc: 0.6458 - bag_output_acc: 0.5448 - pose_output_acc: 0.6260 - footwear_output_acc: 0.5115 - emotion_output_acc: 0.7063 - val_loss: 25.2513 - val_gender_output_loss: 0.6718 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4048 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9116 - val_pose_output_loss: 0.9283 - val_footwear_output_loss: 0.9830 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5298 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 25.24032\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.8969 - gender_output_loss: 0.7528 - image_quality_output_loss: 0.9778 - age_output_loss: 2.9753 - weight_output_loss: 2.4401 - bag_output_loss: 1.6095 - pose_output_loss: 1.7074 - footwear_output_loss: 1.3443 - emotion_output_loss: 2.5820 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3865 - weight_output_acc: 0.6531 - bag_output_acc: 0.5635 - pose_output_acc: 0.6240 - footwear_output_acc: 0.5104 - emotion_output_acc: 0.7229 - val_loss: 25.2613 - val_gender_output_loss: 0.6694 - val_image_quality_output_loss: 0.9857 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.9108 - val_pose_output_loss: 0.9254 - val_footwear_output_loss: 0.9807 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.5432 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5501 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 25.24032\n",
            "Epoch 52/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 47.9402 - gender_output_loss: 0.7715 - image_quality_output_loss: 0.9897 - age_output_loss: 2.7672 - weight_output_loss: 2.3454 - bag_output_loss: 1.6497 - pose_output_loss: 1.8447 - footwear_output_loss: 1.4086 - emotion_output_loss: 2.6914 - gender_output_acc: 0.5571 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4041 - weight_output_acc: 0.6616 - bag_output_acc: 0.5603 - pose_output_acc: 0.5894 - footwear_output_acc: 0.4903 - emotion_output_acc: 0.7177\n",
            "Epoch 00051: val_loss did not improve from 25.24032\n",
            "30/30 [==============================] - 33s 1s/step - loss: 47.9121 - gender_output_loss: 0.7709 - image_quality_output_loss: 0.9873 - age_output_loss: 2.8068 - weight_output_loss: 2.3608 - bag_output_loss: 1.6436 - pose_output_loss: 1.8470 - footwear_output_loss: 1.4115 - emotion_output_loss: 2.6362 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3990 - weight_output_acc: 0.6562 - bag_output_acc: 0.5573 - pose_output_acc: 0.5885 - footwear_output_acc: 0.4854 - emotion_output_acc: 0.7229 - val_loss: 25.2510 - val_gender_output_loss: 0.6659 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4042 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.9101 - val_pose_output_loss: 0.9273 - val_footwear_output_loss: 0.9923 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.5551 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5174 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 25.24032\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.5707 - gender_output_loss: 0.7685 - image_quality_output_loss: 0.9894 - age_output_loss: 2.7486 - weight_output_loss: 2.4696 - bag_output_loss: 1.5710 - pose_output_loss: 1.7576 - footwear_output_loss: 1.3468 - emotion_output_loss: 2.6816 - gender_output_acc: 0.5635 - image_quality_output_acc: 0.5385 - age_output_acc: 0.3979 - weight_output_acc: 0.6302 - bag_output_acc: 0.5521 - pose_output_acc: 0.6094 - footwear_output_acc: 0.4927 - emotion_output_acc: 0.7104 - val_loss: 25.2055 - val_gender_output_loss: 0.6666 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4071 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.9095 - val_pose_output_loss: 0.9262 - val_footwear_output_loss: 0.9757 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.5903 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5506 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00053: val_loss improved from 25.24032 to 25.20554, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.053.h5\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.4091 - gender_output_loss: 0.7733 - image_quality_output_loss: 0.9612 - age_output_loss: 2.9963 - weight_output_loss: 2.6614 - bag_output_loss: 1.6249 - pose_output_loss: 1.7408 - footwear_output_loss: 1.3572 - emotion_output_loss: 2.7286 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4062 - weight_output_acc: 0.6344 - bag_output_acc: 0.5510 - pose_output_acc: 0.6125 - footwear_output_acc: 0.5219 - emotion_output_acc: 0.7073 - val_loss: 25.1925 - val_gender_output_loss: 0.6615 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4056 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.9094 - val_pose_output_loss: 0.9259 - val_footwear_output_loss: 0.9728 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.5759 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5531 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00054: val_loss improved from 25.20554 to 25.19252, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.054.h5\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.5897 - gender_output_loss: 0.7447 - image_quality_output_loss: 0.9885 - age_output_loss: 2.8930 - weight_output_loss: 2.6901 - bag_output_loss: 1.4626 - pose_output_loss: 1.7449 - footwear_output_loss: 1.3476 - emotion_output_loss: 2.7297 - gender_output_acc: 0.5990 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4062 - weight_output_acc: 0.6406 - bag_output_acc: 0.5917 - pose_output_acc: 0.6104 - footwear_output_acc: 0.4979 - emotion_output_acc: 0.7042 - val_loss: 25.2294 - val_gender_output_loss: 0.6694 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4084 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9109 - val_pose_output_loss: 0.9270 - val_footwear_output_loss: 0.9701 - val_emotion_output_loss: 0.9106 - val_gender_output_acc: 0.5561 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5630 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 25.19252\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 46.3304 - gender_output_loss: 0.7671 - image_quality_output_loss: 0.9939 - age_output_loss: 2.7444 - weight_output_loss: 2.3896 - bag_output_loss: 1.4379 - pose_output_loss: 1.7449 - footwear_output_loss: 1.3798 - emotion_output_loss: 2.5273 - gender_output_acc: 0.5604 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4010 - weight_output_acc: 0.6542 - bag_output_acc: 0.5760 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5052 - emotion_output_acc: 0.7135 - val_loss: 25.2064 - val_gender_output_loss: 0.6671 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4058 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9111 - val_pose_output_loss: 0.9252 - val_footwear_output_loss: 0.9700 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.5685 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5615 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 25.19252\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.2523 - gender_output_loss: 0.7541 - image_quality_output_loss: 0.9718 - age_output_loss: 2.8053 - weight_output_loss: 2.5410 - bag_output_loss: 1.5246 - pose_output_loss: 1.8123 - footwear_output_loss: 1.3602 - emotion_output_loss: 2.4951 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3896 - weight_output_acc: 0.6521 - bag_output_acc: 0.5802 - pose_output_acc: 0.6062 - footwear_output_acc: 0.5042 - emotion_output_acc: 0.7198 - val_loss: 25.1898 - val_gender_output_loss: 0.6668 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9101 - val_pose_output_loss: 0.9253 - val_footwear_output_loss: 0.9687 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5441 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00057: val_loss improved from 25.19252 to 25.18978, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.057.h5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.1294 - gender_output_loss: 0.7557 - image_quality_output_loss: 1.0081 - age_output_loss: 2.6037 - weight_output_loss: 2.7387 - bag_output_loss: 1.6791 - pose_output_loss: 1.7422 - footwear_output_loss: 1.3062 - emotion_output_loss: 2.7126 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5312 - age_output_acc: 0.4146 - weight_output_acc: 0.6260 - bag_output_acc: 0.5375 - pose_output_acc: 0.6198 - footwear_output_acc: 0.5365 - emotion_output_acc: 0.7083 - val_loss: 25.1949 - val_gender_output_loss: 0.6676 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9098 - val_pose_output_loss: 0.9249 - val_footwear_output_loss: 0.9693 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.5506 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5496 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 25.18978\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 48.8754 - gender_output_loss: 0.7565 - image_quality_output_loss: 0.9620 - age_output_loss: 2.9849 - weight_output_loss: 2.8078 - bag_output_loss: 1.4830 - pose_output_loss: 1.7709 - footwear_output_loss: 1.3328 - emotion_output_loss: 2.6011 - gender_output_acc: 0.5823 - image_quality_output_acc: 0.5656 - age_output_acc: 0.3833 - weight_output_acc: 0.6115 - bag_output_acc: 0.5844 - pose_output_acc: 0.6094 - footwear_output_acc: 0.5135 - emotion_output_acc: 0.7177 - val_loss: 25.1993 - val_gender_output_loss: 0.6650 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.4031 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.9122 - val_pose_output_loss: 0.9245 - val_footwear_output_loss: 0.9696 - val_emotion_output_loss: 0.9107 - val_gender_output_acc: 0.5531 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5605 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 25.18978\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 49.0974 - gender_output_loss: 0.7368 - image_quality_output_loss: 0.9801 - age_output_loss: 2.9571 - weight_output_loss: 2.7986 - bag_output_loss: 1.4991 - pose_output_loss: 1.6894 - footwear_output_loss: 1.3396 - emotion_output_loss: 2.7379 - gender_output_acc: 0.6083 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3844 - weight_output_acc: 0.6187 - bag_output_acc: 0.5854 - pose_output_acc: 0.6292 - footwear_output_acc: 0.5260 - emotion_output_acc: 0.7073 - val_loss: 25.2611 - val_gender_output_loss: 0.6706 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4090 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.9139 - val_pose_output_loss: 0.9260 - val_footwear_output_loss: 0.9770 - val_emotion_output_loss: 0.9133 - val_gender_output_acc: 0.5466 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5109 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 25.18978\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.8615 - gender_output_loss: 0.7597 - image_quality_output_loss: 0.9829 - age_output_loss: 2.9223 - weight_output_loss: 2.6682 - bag_output_loss: 1.6171 - pose_output_loss: 1.7230 - footwear_output_loss: 1.3347 - emotion_output_loss: 2.6875 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5490 - age_output_acc: 0.3833 - weight_output_acc: 0.6083 - bag_output_acc: 0.5656 - pose_output_acc: 0.6198 - footwear_output_acc: 0.5156 - emotion_output_acc: 0.7104 - val_loss: 25.2309 - val_gender_output_loss: 0.6593 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.9103 - val_pose_output_loss: 0.9249 - val_footwear_output_loss: 0.9722 - val_emotion_output_loss: 0.9133 - val_gender_output_acc: 0.5670 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5432 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 25.18978\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.9598 - gender_output_loss: 0.7471 - image_quality_output_loss: 0.9482 - age_output_loss: 2.9634 - weight_output_loss: 2.8044 - bag_output_loss: 1.5008 - pose_output_loss: 1.8462 - footwear_output_loss: 1.3644 - emotion_output_loss: 2.8226 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5865 - age_output_acc: 0.3792 - weight_output_acc: 0.6271 - bag_output_acc: 0.5833 - pose_output_acc: 0.5885 - footwear_output_acc: 0.5073 - emotion_output_acc: 0.6969 - val_loss: 25.2015 - val_gender_output_loss: 0.6649 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.4078 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9093 - val_pose_output_loss: 0.9278 - val_footwear_output_loss: 0.9666 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.5561 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5441 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 25.18978\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.6049 - gender_output_loss: 0.7723 - image_quality_output_loss: 1.0156 - age_output_loss: 2.8860 - weight_output_loss: 2.5415 - bag_output_loss: 1.7116 - pose_output_loss: 1.7259 - footwear_output_loss: 1.3047 - emotion_output_loss: 2.6742 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.5198 - age_output_acc: 0.3823 - weight_output_acc: 0.6469 - bag_output_acc: 0.5437 - pose_output_acc: 0.6146 - footwear_output_acc: 0.5219 - emotion_output_acc: 0.7135 - val_loss: 25.1763 - val_gender_output_loss: 0.6627 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4057 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.9078 - val_pose_output_loss: 0.9260 - val_footwear_output_loss: 0.9627 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.5724 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5407 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00063: val_loss improved from 25.18978 to 25.17626, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.063.h5\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.7752 - gender_output_loss: 0.7693 - image_quality_output_loss: 0.9649 - age_output_loss: 2.8382 - weight_output_loss: 2.6569 - bag_output_loss: 1.6328 - pose_output_loss: 1.6662 - footwear_output_loss: 1.3819 - emotion_output_loss: 2.5203 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5750 - age_output_acc: 0.3885 - weight_output_acc: 0.6260 - bag_output_acc: 0.5427 - pose_output_acc: 0.6344 - footwear_output_acc: 0.5115 - emotion_output_acc: 0.7250 - val_loss: 25.1818 - val_gender_output_loss: 0.6628 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4074 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9075 - val_pose_output_loss: 0.9255 - val_footwear_output_loss: 0.9625 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.5774 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5342 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 25.17626\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.2858 - gender_output_loss: 0.7555 - image_quality_output_loss: 0.9780 - age_output_loss: 2.8519 - weight_output_loss: 2.5174 - bag_output_loss: 1.5815 - pose_output_loss: 1.7249 - footwear_output_loss: 1.3601 - emotion_output_loss: 2.7446 - gender_output_acc: 0.6031 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4062 - weight_output_acc: 0.6323 - bag_output_acc: 0.5667 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5010 - emotion_output_acc: 0.7031 - val_loss: 25.1681 - val_gender_output_loss: 0.6619 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.4064 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.9086 - val_pose_output_loss: 0.9245 - val_footwear_output_loss: 0.9635 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.5739 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5570 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00065: val_loss improved from 25.17626 to 25.16810, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.065.h5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.8184 - gender_output_loss: 0.7705 - image_quality_output_loss: 0.9762 - age_output_loss: 2.8609 - weight_output_loss: 2.4391 - bag_output_loss: 1.4433 - pose_output_loss: 1.6560 - footwear_output_loss: 1.3519 - emotion_output_loss: 2.5803 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3896 - weight_output_acc: 0.6438 - bag_output_acc: 0.5865 - pose_output_acc: 0.6313 - footwear_output_acc: 0.5052 - emotion_output_acc: 0.7104 - val_loss: 25.1412 - val_gender_output_loss: 0.6622 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4030 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.9094 - val_pose_output_loss: 0.9261 - val_footwear_output_loss: 0.9594 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5784 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5387 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00065: val_loss improved from 25.17626 to 25.16810, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.065.h5\n",
            "Epoch 00066: val_loss improved from 25.16810 to 25.14116, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.066.h5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.4984 - gender_output_loss: 0.7521 - image_quality_output_loss: 0.9923 - age_output_loss: 2.8661 - weight_output_loss: 2.6025 - bag_output_loss: 1.5720 - pose_output_loss: 1.6725 - footwear_output_loss: 1.3582 - emotion_output_loss: 3.0118 - gender_output_acc: 0.5823 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3969 - weight_output_acc: 0.6510 - bag_output_acc: 0.5750 - pose_output_acc: 0.6292 - footwear_output_acc: 0.5198 - emotion_output_acc: 0.6865 - val_loss: 25.1815 - val_gender_output_loss: 0.6632 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.4017 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9066 - val_pose_output_loss: 0.9270 - val_footwear_output_loss: 0.9689 - val_emotion_output_loss: 0.9129 - val_gender_output_acc: 0.5590 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5253 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 25.14116\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.3475 - gender_output_loss: 0.7617 - image_quality_output_loss: 0.9649 - age_output_loss: 2.5987 - weight_output_loss: 2.5948 - bag_output_loss: 1.5264 - pose_output_loss: 1.8045 - footwear_output_loss: 1.3065 - emotion_output_loss: 2.4673 - gender_output_acc: 0.5802 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4219 - weight_output_acc: 0.6396 - bag_output_acc: 0.5573 - pose_output_acc: 0.6052 - footwear_output_acc: 0.5260 - emotion_output_acc: 0.7354 - val_loss: 25.2219 - val_gender_output_loss: 0.6667 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4029 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9082 - val_pose_output_loss: 0.9293 - val_footwear_output_loss: 0.9787 - val_emotion_output_loss: 0.9109 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5546 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5347 - val_emotion_output_acc: 0.7073\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00068: val_loss did not improve from 25.14116\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.4822 - gender_output_loss: 0.7453 - image_quality_output_loss: 0.9861 - age_output_loss: 2.9481 - weight_output_loss: 2.5026 - bag_output_loss: 1.6129 - pose_output_loss: 1.7014 - footwear_output_loss: 1.3539 - emotion_output_loss: 2.7073 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.5458 - age_output_acc: 0.4021 - weight_output_acc: 0.6396 - bag_output_acc: 0.5542 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5198 - emotion_output_acc: 0.7083 - val_loss: 25.1211 - val_gender_output_loss: 0.6631 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.3998 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9071 - val_pose_output_loss: 0.9244 - val_footwear_output_loss: 0.9552 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.5660 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5565 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 25.14116\n",
            "\n",
            "Epoch 00069: val_loss improved from 25.14116 to 25.12109, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.069.h5\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.3251 - gender_output_loss: 0.7571 - image_quality_output_loss: 0.9829 - age_output_loss: 2.9076 - weight_output_loss: 2.5527 - bag_output_loss: 1.4623 - pose_output_loss: 1.7736 - footwear_output_loss: 1.3167 - emotion_output_loss: 2.4942 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4167 - weight_output_acc: 0.6438 - bag_output_acc: 0.5698 - pose_output_acc: 0.6073 - footwear_output_acc: 0.5271 - emotion_output_acc: 0.7135 - val_loss: 25.1477 - val_gender_output_loss: 0.6634 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4013 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9081 - val_pose_output_loss: 0.9248 - val_footwear_output_loss: 0.9574 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.5655 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5640 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 25.12109\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.7123 - gender_output_loss: 0.7604 - image_quality_output_loss: 0.9682 - age_output_loss: 2.6712 - weight_output_loss: 2.5865 - bag_output_loss: 1.5036 - pose_output_loss: 1.8447 - footwear_output_loss: 1.3051 - emotion_output_loss: 2.4795 - gender_output_acc: 0.5823 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4125 - weight_output_acc: 0.6313 - bag_output_acc: 0.5740 - pose_output_acc: 0.6000 - footwear_output_acc: 0.5302 - emotion_output_acc: 0.7260 - val_loss: 25.1101 - val_gender_output_loss: 0.6609 - val_image_quality_output_loss: 0.9826 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.9080 - val_pose_output_loss: 0.9242 - val_footwear_output_loss: 0.9565 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.5828 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5516 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00071: val_loss improved from 25.12109 to 25.11012, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.071.h5\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.4303 - gender_output_loss: 0.7502 - image_quality_output_loss: 0.9935 - age_output_loss: 2.9282 - weight_output_loss: 2.3359 - bag_output_loss: 1.5288 - pose_output_loss: 1.6663 - footwear_output_loss: 1.3198 - emotion_output_loss: 2.6898 - gender_output_acc: 0.6052 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3927 - weight_output_acc: 0.6667 - bag_output_acc: 0.5479 - pose_output_acc: 0.6333 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.7219 - val_loss: 25.1138 - val_gender_output_loss: 0.6595 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4016 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9082 - val_pose_output_loss: 0.9233 - val_footwear_output_loss: 0.9561 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.5863 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5466 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 25.11012\n",
            "Epoch 73/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 48.2305 - gender_output_loss: 0.7403 - image_quality_output_loss: 0.9717 - age_output_loss: 2.9350 - weight_output_loss: 2.6772 - bag_output_loss: 1.5554 - pose_output_loss: 1.6962 - footwear_output_loss: 1.3051 - emotion_output_loss: 2.6078 - gender_output_acc: 0.6013 - image_quality_output_acc: 0.5690 - age_output_acc: 0.3976 - weight_output_acc: 0.6131 - bag_output_acc: 0.5571 - pose_output_acc: 0.6272 - footwear_output_acc: 0.5453 - emotion_output_acc: 0.7155\n",
            "Epoch 00072: val_loss did not improve from 25.11012\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.4347 - gender_output_loss: 0.7416 - image_quality_output_loss: 0.9726 - age_output_loss: 2.9449 - weight_output_loss: 2.6636 - bag_output_loss: 1.5811 - pose_output_loss: 1.7159 - footwear_output_loss: 1.3090 - emotion_output_loss: 2.6222 - gender_output_acc: 0.5990 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3938 - weight_output_acc: 0.6156 - bag_output_acc: 0.5573 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7135 - val_loss: 25.1058 - val_gender_output_loss: 0.6567 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4026 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.9068 - val_pose_output_loss: 0.9234 - val_footwear_output_loss: 0.9543 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5838 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5521 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00073: val_loss improved from 25.11012 to 25.10579, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.073.h5\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.4347 - gender_output_loss: 0.7416 - image_quality_output_loss: 0.9726 - age_output_loss: 2.9449 - weight_output_loss: 2.6636 - bag_output_loss: 1.5811 - pose_output_loss: 1.7159 - footwear_output_loss: 1.3090 - emotion_output_loss: 2.6222 - gender_output_acc: 0.5990 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3938 - weight_output_acc: 0.6156 - bag_output_acc: 0.5573 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7135 - val_loss: 25.1058 - val_gender_output_loss: 0.6567 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4026 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.9068 - val_pose_output_loss: 0.9234 - val_footwear_output_loss: 0.9543 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5838 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5521 - val_emotion_output_acc: 0.7073\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.6053 - gender_output_loss: 0.7468 - image_quality_output_loss: 0.9890 - age_output_loss: 2.9381 - weight_output_loss: 2.6244 - bag_output_loss: 1.4686 - pose_output_loss: 1.7000 - footwear_output_loss: 1.3863 - emotion_output_loss: 2.7480 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3792 - weight_output_acc: 0.6583 - bag_output_acc: 0.6031 - pose_output_acc: 0.6219 - footwear_output_acc: 0.4969 - emotion_output_acc: 0.7052 - val_loss: 25.1613 - val_gender_output_loss: 0.6588 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4041 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.9081 - val_pose_output_loss: 0.9247 - val_footwear_output_loss: 0.9564 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.5724 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5536 - val_emotion_output_acc: 0.7073\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00074: val_loss did not improve from 25.10579\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.6724 - gender_output_loss: 0.7591 - image_quality_output_loss: 0.9751 - age_output_loss: 3.0270 - weight_output_loss: 2.9127 - bag_output_loss: 1.5406 - pose_output_loss: 1.7350 - footwear_output_loss: 1.3157 - emotion_output_loss: 2.6657 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3812 - weight_output_acc: 0.6094 - bag_output_acc: 0.5792 - pose_output_acc: 0.6094 - footwear_output_acc: 0.5448 - emotion_output_acc: 0.7198 - val_loss: 25.1664 - val_gender_output_loss: 0.6605 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.9097 - val_pose_output_loss: 0.9256 - val_footwear_output_loss: 0.9560 - val_emotion_output_loss: 0.9091 - val_gender_output_acc: 0.5680 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5412 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 25.10579\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.7651 - gender_output_loss: 0.7539 - image_quality_output_loss: 0.9838 - age_output_loss: 2.7254 - weight_output_loss: 2.3257 - bag_output_loss: 1.5011 - pose_output_loss: 1.8482 - footwear_output_loss: 1.3346 - emotion_output_loss: 2.6143 - gender_output_acc: 0.6010 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4229 - weight_output_acc: 0.6594 - bag_output_acc: 0.5833 - pose_output_acc: 0.5917 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.7177 - val_loss: 25.1480 - val_gender_output_loss: 0.6588 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9911 - val_bag_output_loss: 0.9099 - val_pose_output_loss: 0.9235 - val_footwear_output_loss: 0.9505 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.5833 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5402 - val_emotion_output_acc: 0.7073\n",
            "30/30 [==============================]\n",
            "Epoch 00076: val_loss did not improve from 25.10579\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.6046 - gender_output_loss: 0.7609 - image_quality_output_loss: 0.9713 - age_output_loss: 2.7229 - weight_output_loss: 2.2647 - bag_output_loss: 1.6251 - pose_output_loss: 1.7068 - footwear_output_loss: 1.3447 - emotion_output_loss: 2.6333 - gender_output_acc: 0.5927 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4177 - weight_output_acc: 0.6573 - bag_output_acc: 0.5427 - pose_output_acc: 0.6240 - footwear_output_acc: 0.5208 - emotion_output_acc: 0.7135 - val_loss: 25.0991 - val_gender_output_loss: 0.6552 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.9085 - val_pose_output_loss: 0.9276 - val_footwear_output_loss: 0.9549 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.5903 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5556 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5461 - val_emotion_output_acc: 0.7073\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: val_loss improved from 25.10579 to 25.09905, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.077.h5\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 48.0633 - gender_output_loss: 0.7773 - image_quality_output_loss: 0.9733 - age_output_loss: 2.8343 - weight_output_loss: 2.6106 - bag_output_loss: 1.5375 - pose_output_loss: 1.6205 - footwear_output_loss: 1.3056 - emotion_output_loss: 2.7682 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4104 - weight_output_acc: 0.6406 - bag_output_acc: 0.5458 - pose_output_acc: 0.6385 - footwear_output_acc: 0.5573 - emotion_output_acc: 0.6990 - val_loss: 25.1163 - val_gender_output_loss: 0.6580 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4024 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.9080 - val_pose_output_loss: 0.9255 - val_footwear_output_loss: 0.9526 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.6062 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5610 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5471 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 25.09905\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.5353 - gender_output_loss: 0.7560 - image_quality_output_loss: 0.9943 - age_output_loss: 2.7074 - weight_output_loss: 2.5102 - bag_output_loss: 1.4895 - pose_output_loss: 1.7873 - footwear_output_loss: 1.3394 - emotion_output_loss: 2.4825 - gender_output_acc: 0.6094 - image_quality_output_acc: 0.5354 - age_output_acc: 0.4229 - weight_output_acc: 0.6510 - bag_output_acc: 0.5385 - pose_output_acc: 0.6146 - footwear_output_acc: 0.5333 - emotion_output_acc: 0.7292 - val_loss: 25.0797 - val_gender_output_loss: 0.6554 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4003 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.9083 - val_pose_output_loss: 0.9222 - val_footwear_output_loss: 0.9481 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.5928 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5680 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00079: val_loss improved from 25.09905 to 25.07972, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.079.h5\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 49.3045 - gender_output_loss: 0.7391 - image_quality_output_loss: 0.9817 - age_output_loss: 3.1628 - weight_output_loss: 2.6112 - bag_output_loss: 1.6134 - pose_output_loss: 1.7826 - footwear_output_loss: 1.2577 - emotion_output_loss: 2.6099 - gender_output_acc: 0.6115 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3604 - weight_output_acc: 0.6313 - bag_output_acc: 0.5792 - pose_output_acc: 0.6062 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.7188 - val_loss: 25.0597 - val_gender_output_loss: 0.6534 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9056 - val_pose_output_loss: 0.9219 - val_footwear_output_loss: 0.9443 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.5878 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5551 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5451 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "\n",
            "Epoch 00080: val_loss improved from 25.07972 to 25.05968, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.080.h5\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 47.7310 - gender_output_loss: 0.7503 - image_quality_output_loss: 0.9863 - age_output_loss: 2.8380 - weight_output_loss: 2.5584 - bag_output_loss: 1.6432 - pose_output_loss: 1.7225 - footwear_output_loss: 1.3466 - emotion_output_loss: 2.5514 - gender_output_acc: 0.5865 - image_quality_output_acc: 0.5490 - age_output_acc: 0.4104 - weight_output_acc: 0.6344 - bag_output_acc: 0.5510 - pose_output_acc: 0.6208 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7198 - val_loss: 25.0768 - val_gender_output_loss: 0.6549 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.4018 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9060 - val_pose_output_loss: 0.9223 - val_footwear_output_loss: 0.9461 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.5858 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5635 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5605 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 25.05968\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.2943 - gender_output_loss: 0.7498 - image_quality_output_loss: 0.9501 - age_output_loss: 2.8100 - weight_output_loss: 2.6417 - bag_output_loss: 1.5419 - pose_output_loss: 1.7676 - footwear_output_loss: 1.3269 - emotion_output_loss: 2.7280 - gender_output_acc: 0.5990 - image_quality_output_acc: 0.5844 - age_output_acc: 0.3781 - weight_output_acc: 0.6208 - bag_output_acc: 0.5677 - pose_output_acc: 0.6115 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.7073 - val_loss: 25.0931 - val_gender_output_loss: 0.6595 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4016 - val_weight_output_loss: 0.9820 - val_bag_output_loss: 0.9079 - val_pose_output_loss: 0.9241 - val_footwear_output_loss: 0.9462 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.5888 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5570 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 25.05968\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.1892 - gender_output_loss: 0.7539 - image_quality_output_loss: 0.9733 - age_output_loss: 2.7932 - weight_output_loss: 2.6073 - bag_output_loss: 1.6810 - pose_output_loss: 1.6723 - footwear_output_loss: 1.3422 - emotion_output_loss: 2.6904 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4135 - weight_output_acc: 0.6260 - bag_output_acc: 0.5490 - pose_output_acc: 0.6281 - footwear_output_acc: 0.5125 - emotion_output_acc: 0.7104 - val_loss: 25.1792 - val_gender_output_loss: 0.6586 - val_image_quality_output_loss: 0.9825 - val_age_output_loss: 1.3968 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.9066 - val_pose_output_loss: 0.9251 - val_footwear_output_loss: 0.9800 - val_emotion_output_loss: 0.9186 - val_gender_output_acc: 0.5739 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4876 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 25.05968\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 47.1850 - gender_output_loss: 0.7582 - image_quality_output_loss: 0.9969 - age_output_loss: 2.7141 - weight_output_loss: 2.3737 - bag_output_loss: 1.4953 - pose_output_loss: 1.7147 - footwear_output_loss: 1.2995 - emotion_output_loss: 2.8088 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5260 - age_output_acc: 0.3948 - weight_output_acc: 0.6479 - bag_output_acc: 0.5604 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5479 - emotion_output_acc: 0.6948 - val_loss: 25.0716 - val_gender_output_loss: 0.6513 - val_image_quality_output_loss: 0.9825 - val_age_output_loss: 1.3999 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9069 - val_pose_output_loss: 0.9228 - val_footwear_output_loss: 0.9484 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5938 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5531 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 25.05968\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.3151 - gender_output_loss: 0.7539 - image_quality_output_loss: 0.9808 - age_output_loss: 2.8774 - weight_output_loss: 2.5494 - bag_output_loss: 1.6356 - pose_output_loss: 1.6849 - footwear_output_loss: 1.2817 - emotion_output_loss: 2.4825 - gender_output_acc: 0.5865 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4010 - weight_output_acc: 0.6469 - bag_output_acc: 0.5542 - pose_output_acc: 0.6302 - footwear_output_acc: 0.5552 - emotion_output_acc: 0.7240 - val_loss: 25.0935 - val_gender_output_loss: 0.6562 - val_image_quality_output_loss: 0.9810 - val_age_output_loss: 1.4070 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9073 - val_pose_output_loss: 0.9238 - val_footwear_output_loss: 0.9413 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5843 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5655 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 25.05968\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 46.8098 - gender_output_loss: 0.7453 - image_quality_output_loss: 0.9802 - age_output_loss: 2.7432 - weight_output_loss: 2.5498 - bag_output_loss: 1.4872 - pose_output_loss: 1.7195 - footwear_output_loss: 1.3108 - emotion_output_loss: 2.5657 - gender_output_acc: 0.5969 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4156 - weight_output_acc: 0.6365 - bag_output_acc: 0.5656 - pose_output_acc: 0.6240 - footwear_output_acc: 0.5354 - emotion_output_acc: 0.7240 - val_loss: 25.0809 - val_gender_output_loss: 0.6566 - val_image_quality_output_loss: 0.9810 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9086 - val_pose_output_loss: 0.9219 - val_footwear_output_loss: 0.9464 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.5769 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5476 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 25.05968\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.2307 - gender_output_loss: 0.7545 - image_quality_output_loss: 0.9576 - age_output_loss: 2.8524 - weight_output_loss: 2.5436 - bag_output_loss: 1.5487 - pose_output_loss: 1.8006 - footwear_output_loss: 1.2989 - emotion_output_loss: 2.7222 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4146 - weight_output_acc: 0.6344 - bag_output_acc: 0.5510 - pose_output_acc: 0.5969 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7042 - val_loss: 25.0788 - val_gender_output_loss: 0.6535 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.3999 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.9072 - val_pose_output_loss: 0.9229 - val_footwear_output_loss: 0.9466 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5799 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5521 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 25.05968\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.9260 - gender_output_loss: 0.7667 - image_quality_output_loss: 1.0020 - age_output_loss: 2.8015 - weight_output_loss: 2.3864 - bag_output_loss: 1.6163 - pose_output_loss: 1.6847 - footwear_output_loss: 1.3295 - emotion_output_loss: 2.8075 - gender_output_acc: 0.5635 - image_quality_output_acc: 0.5281 - age_output_acc: 0.3927 - weight_output_acc: 0.6531 - bag_output_acc: 0.5448 - pose_output_acc: 0.6333 - footwear_output_acc: 0.5469 - emotion_output_acc: 0.7073 - val_loss: 25.0543 - val_gender_output_loss: 0.6521 - val_image_quality_output_loss: 0.9801 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9064 - val_pose_output_loss: 0.9212 - val_footwear_output_loss: 0.9428 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.5947 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5625 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5476 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00088: val_loss improved from 25.05968 to 25.05430, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.088.h5\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 50.3037 - gender_output_loss: 0.7502 - image_quality_output_loss: 0.9799 - age_output_loss: 2.9487 - weight_output_loss: 2.6138 - bag_output_loss: 1.6100 - pose_output_loss: 1.8094 - footwear_output_loss: 1.3085 - emotion_output_loss: 3.0252 - gender_output_acc: 0.6042 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3927 - weight_output_acc: 0.6365 - bag_output_acc: 0.5510 - pose_output_acc: 0.6000 - footwear_output_acc: 0.5365 - emotion_output_acc: 0.6802 - val_loss: 25.0881 - val_gender_output_loss: 0.6524 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.4017 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9062 - val_pose_output_loss: 0.9220 - val_footwear_output_loss: 0.9427 - val_emotion_output_loss: 0.9145 - val_gender_output_acc: 0.5967 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5486 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 25.05430\n",
            "Epoch 90/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 48.4535 - gender_output_loss: 0.7260 - image_quality_output_loss: 0.9709 - age_output_loss: 2.8515 - weight_output_loss: 2.7068 - bag_output_loss: 1.6675 - pose_output_loss: 1.7143 - footwear_output_loss: 1.3263 - emotion_output_loss: 2.6260 - gender_output_acc: 0.6347 - image_quality_output_acc: 0.5603 - age_output_acc: 0.3761 - weight_output_acc: 0.6250 - bag_output_acc: 0.5571 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5334 - emotion_output_acc: 0.7101\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.5645 - gender_output_loss: 0.7287 - image_quality_output_loss: 0.9664 - age_output_loss: 2.8591 - weight_output_loss: 2.7064 - bag_output_loss: 1.6704 - pose_output_loss: 1.7276 - footwear_output_loss: 1.3273 - emotion_output_loss: 2.6347 - gender_output_acc: 0.6292 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3719 - weight_output_acc: 0.6208 - bag_output_acc: 0.5562 - pose_output_acc: 0.6187 - footwear_output_acc: 0.5323 - emotion_output_acc: 0.7094 - val_loss: 25.0674 - val_gender_output_loss: 0.6528 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.3986 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9074 - val_pose_output_loss: 0.9216 - val_footwear_output_loss: 0.9483 - val_emotion_output_loss: 0.9109 - val_gender_output_acc: 0.5838 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5407 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 25.05430\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 47.4377 - gender_output_loss: 0.7673 - image_quality_output_loss: 0.9520 - age_output_loss: 2.9759 - weight_output_loss: 2.5922 - bag_output_loss: 1.4448 - pose_output_loss: 1.5976 - footwear_output_loss: 1.3132 - emotion_output_loss: 2.5836 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5719 - age_output_acc: 0.3698 - weight_output_acc: 0.6323 - bag_output_acc: 0.5844 - pose_output_acc: 0.6490 - footwear_output_acc: 0.5448 - emotion_output_acc: 0.7135 - val_loss: 25.0800 - val_gender_output_loss: 0.6497 - val_image_quality_output_loss: 0.9804 - val_age_output_loss: 1.4020 - val_weight_output_loss: 0.9907 - val_bag_output_loss: 0.9079 - val_pose_output_loss: 0.9205 - val_footwear_output_loss: 0.9403 - val_emotion_output_loss: 0.9109 - val_gender_output_acc: 0.6136 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5402 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 25.05430\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 46.8017 - gender_output_loss: 0.7648 - image_quality_output_loss: 0.9798 - age_output_loss: 2.7544 - weight_output_loss: 2.5850 - bag_output_loss: 1.5359 - pose_output_loss: 1.6906 - footwear_output_loss: 1.3483 - emotion_output_loss: 2.4834 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4042 - weight_output_acc: 0.6552 - bag_output_acc: 0.5688 - pose_output_acc: 0.6240 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7396 - val_loss: 25.0888 - val_gender_output_loss: 0.6497 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9860 - val_bag_output_loss: 0.9103 - val_pose_output_loss: 0.9230 - val_footwear_output_loss: 0.9452 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.6027 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5417 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 25.05430\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 25.05430\n",
            "Epoch 93/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 48.5878 - gender_output_loss: 0.7552 - image_quality_output_loss: 0.9706 - age_output_loss: 2.8300 - weight_output_loss: 2.7085 - bag_output_loss: 1.4901 - pose_output_loss: 1.7852 - footwear_output_loss: 1.2733 - emotion_output_loss: 2.7722 - gender_output_acc: 0.6045 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4009 - weight_output_acc: 0.6401 - bag_output_acc: 0.5841 - pose_output_acc: 0.6034 - footwear_output_acc: 0.5517 - emotion_output_acc: 0.7101\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.6371 - gender_output_loss: 0.7567 - image_quality_output_loss: 0.9693 - age_output_loss: 2.8332 - weight_output_loss: 2.6754 - bag_output_loss: 1.4876 - pose_output_loss: 1.7645 - footwear_output_loss: 1.2699 - emotion_output_loss: 2.8251 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4042 - weight_output_acc: 0.6417 - bag_output_acc: 0.5823 - pose_output_acc: 0.6083 - footwear_output_acc: 0.5531 - emotion_output_acc: 0.7063 - val_loss: 25.1678 - val_gender_output_loss: 0.6569 - val_image_quality_output_loss: 0.9801 - val_age_output_loss: 1.4045 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.9061 - val_pose_output_loss: 0.9222 - val_footwear_output_loss: 0.9596 - val_emotion_output_loss: 0.9203 - val_gender_output_acc: 0.5818 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5625 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5015 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 25.05430\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.0531 - gender_output_loss: 0.7488 - image_quality_output_loss: 0.9767 - age_output_loss: 2.6705 - weight_output_loss: 2.5955 - bag_output_loss: 1.5040 - pose_output_loss: 1.8703 - footwear_output_loss: 1.3216 - emotion_output_loss: 2.5347 - gender_output_acc: 0.6000 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4208 - weight_output_acc: 0.6240 - bag_output_acc: 0.5750 - pose_output_acc: 0.5833 - footwear_output_acc: 0.5385 - emotion_output_acc: 0.7156 - val_loss: 25.0711 - val_gender_output_loss: 0.6540 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.4028 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.9050 - val_pose_output_loss: 0.9224 - val_footwear_output_loss: 0.9504 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.5799 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5541 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5193 - val_emotion_output_acc: 0.7073\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 25.05430\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 47.3850 - gender_output_loss: 0.7616 - image_quality_output_loss: 1.0030 - age_output_loss: 2.9710 - weight_output_loss: 2.3964 - bag_output_loss: 1.6026 - pose_output_loss: 1.7075 - footwear_output_loss: 1.3161 - emotion_output_loss: 2.4979 - gender_output_acc: 0.5865 - image_quality_output_acc: 0.5260 - age_output_acc: 0.4094 - weight_output_acc: 0.6385 - bag_output_acc: 0.5542 - pose_output_acc: 0.6271 - footwear_output_acc: 0.5260 - emotion_output_acc: 0.7219 - val_loss: 25.0543 - val_gender_output_loss: 0.6513 - val_image_quality_output_loss: 0.9810 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.9044 - val_pose_output_loss: 0.9199 - val_footwear_output_loss: 0.9501 - val_emotion_output_loss: 0.9068 - val_gender_output_acc: 0.5903 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5193 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00095: val_loss improved from 25.05430 to 25.05429, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.095.h5\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 48.0286 - gender_output_loss: 0.7356 - image_quality_output_loss: 0.9699 - age_output_loss: 2.8786 - weight_output_loss: 2.5322 - bag_output_loss: 1.5354 - pose_output_loss: 1.7290 - footwear_output_loss: 1.3269 - emotion_output_loss: 2.7079 - gender_output_acc: 0.6104 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3802 - weight_output_acc: 0.6354 - bag_output_acc: 0.5583 - pose_output_acc: 0.6146 - footwear_output_acc: 0.5323 - emotion_output_acc: 0.7073 - val_loss: 25.0521 - val_gender_output_loss: 0.6511 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.4032 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.9039 - val_pose_output_loss: 0.9201 - val_footwear_output_loss: 0.9499 - val_emotion_output_loss: 0.9080 - val_gender_output_acc: 0.5908 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5580 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5278 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00096: val_loss improved from 25.05429 to 25.05208, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.096.h5\n",
            "Epoch 97/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 46.5929 - gender_output_loss: 0.7558 - image_quality_output_loss: 0.9795 - age_output_loss: 2.8607 - weight_output_loss: 2.4462 - bag_output_loss: 1.5293 - pose_output_loss: 1.6951 - footwear_output_loss: 1.3163 - emotion_output_loss: 2.4517 - gender_output_acc: 0.5970 - image_quality_output_acc: 0.5431 - age_output_acc: 0.4019 - weight_output_acc: 0.6358 - bag_output_acc: 0.5636 - pose_output_acc: 0.6336 - footwear_output_acc: 0.5388 - emotion_output_acc: 0.7252\n",
            "30/30 [==============================] - 30s 1s/step - loss: 46.7318 - gender_output_loss: 0.7557 - image_quality_output_loss: 0.9783 - age_output_loss: 2.8550 - weight_output_loss: 2.4292 - bag_output_loss: 1.5401 - pose_output_loss: 1.6952 - footwear_output_loss: 1.3209 - emotion_output_loss: 2.4951 - gender_output_acc: 0.5969 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4052 - weight_output_acc: 0.6375 - bag_output_acc: 0.5625 - pose_output_acc: 0.6333 - footwear_output_acc: 0.5385 - emotion_output_acc: 0.7219 - val_loss: 25.0263 - val_gender_output_loss: 0.6501 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.4029 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.9037 - val_pose_output_loss: 0.9196 - val_footwear_output_loss: 0.9412 - val_emotion_output_loss: 0.9083 - val_gender_output_acc: 0.6007 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5556 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5437 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00097: val_loss improved from 25.05208 to 25.02634, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.097.h5\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 47.5803 - gender_output_loss: 0.7334 - image_quality_output_loss: 0.9722 - age_output_loss: 2.8525 - weight_output_loss: 2.5639 - bag_output_loss: 1.3890 - pose_output_loss: 1.6966 - footwear_output_loss: 1.2768 - emotion_output_loss: 2.7573 - gender_output_acc: 0.6198 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4052 - weight_output_acc: 0.6333 - bag_output_acc: 0.5938 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5375 - emotion_output_acc: 0.7135 - val_loss: 25.0520 - val_gender_output_loss: 0.6515 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9086 - val_pose_output_loss: 0.9199 - val_footwear_output_loss: 0.9484 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.5848 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5273 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 25.02634\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 45.3516 - gender_output_loss: 0.7547 - image_quality_output_loss: 0.9699 - age_output_loss: 2.7395 - weight_output_loss: 2.2149 - bag_output_loss: 1.6593 - pose_output_loss: 1.7578 - footwear_output_loss: 1.3078 - emotion_output_loss: 2.3013 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4385 - weight_output_acc: 0.6677 - bag_output_acc: 0.5448 - pose_output_acc: 0.6073 - footwear_output_acc: 0.5312 - emotion_output_acc: 0.7396 - val_loss: 25.0682 - val_gender_output_loss: 0.6470 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4024 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.9082 - val_pose_output_loss: 0.9221 - val_footwear_output_loss: 0.9527 - val_emotion_output_loss: 0.9099 - val_gender_output_acc: 0.6017 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5278 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 25.02634\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.2032 - gender_output_loss: 0.7424 - image_quality_output_loss: 0.9793 - age_output_loss: 2.8784 - weight_output_loss: 2.6362 - bag_output_loss: 1.5531 - pose_output_loss: 1.7921 - footwear_output_loss: 1.3161 - emotion_output_loss: 2.8607 - gender_output_acc: 0.6010 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3792 - weight_output_acc: 0.6271 - bag_output_acc: 0.5865 - pose_output_acc: 0.6042 - footwear_output_acc: 0.5406 - emotion_output_acc: 0.6990 - val_loss: 25.1076 - val_gender_output_loss: 0.6589 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.4014 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.9089 - val_pose_output_loss: 0.9212 - val_footwear_output_loss: 0.9453 - val_emotion_output_loss: 0.9185 - val_gender_output_acc: 0.5699 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5268 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 25.02634\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff04130bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPUfY_NmI35",
        "colab_type": "code",
        "outputId": "477ec90e-92af-420e-af11-0ed41eb63939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.097.h5')\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n",
            "Returning new callback array with steps_per_epoch= 100 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 79s 786ms/step - loss: 47.6813 - gender_output_loss: 0.7465 - image_quality_output_loss: 0.9754 - age_output_loss: 2.8559 - weight_output_loss: 2.5449 - bag_output_loss: 1.5544 - pose_output_loss: 1.7511 - footwear_output_loss: 1.3008 - emotion_output_loss: 2.6082 - gender_output_acc: 0.6050 - image_quality_output_acc: 0.5553 - age_output_acc: 0.3925 - weight_output_acc: 0.6344 - bag_output_acc: 0.5631 - pose_output_acc: 0.6144 - footwear_output_acc: 0.5472 - emotion_output_acc: 0.7169 - val_loss: 25.0048 - val_gender_output_loss: 0.6538 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.4004 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.9042 - val_pose_output_loss: 0.9196 - val_footwear_output_loss: 0.9347 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.5893 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5660 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5595 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 25.00479, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.001.h5\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 71s 713ms/step - loss: 47.5823 - gender_output_loss: 0.7464 - image_quality_output_loss: 0.9830 - age_output_loss: 2.7773 - weight_output_loss: 2.5620 - bag_output_loss: 1.5375 - pose_output_loss: 1.7240 - footwear_output_loss: 1.2991 - emotion_output_loss: 2.6795 - gender_output_acc: 0.5903 - image_quality_output_acc: 0.5497 - age_output_acc: 0.4059 - weight_output_acc: 0.6444 - bag_output_acc: 0.5678 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5447 - emotion_output_acc: 0.7069 - val_loss: 25.0699 - val_gender_output_loss: 0.6571 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.4000 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9091 - val_pose_output_loss: 0.9235 - val_footwear_output_loss: 0.9448 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.5868 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5570 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5536 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 25.00479\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 71s 712ms/step - loss: 48.3150 - gender_output_loss: 0.7436 - image_quality_output_loss: 0.9679 - age_output_loss: 2.9262 - weight_output_loss: 2.5874 - bag_output_loss: 1.5421 - pose_output_loss: 1.7776 - footwear_output_loss: 1.3205 - emotion_output_loss: 2.6498 - gender_output_acc: 0.6091 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3941 - weight_output_acc: 0.6344 - bag_output_acc: 0.5653 - pose_output_acc: 0.6063 - footwear_output_acc: 0.5309 - emotion_output_acc: 0.7131 - val_loss: 25.0408 - val_gender_output_loss: 0.6446 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.3947 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9096 - val_pose_output_loss: 0.9197 - val_footwear_output_loss: 0.9586 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.6066 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.4980 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 25.00479\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 72s 718ms/step - loss: 47.9021 - gender_output_loss: 0.7620 - image_quality_output_loss: 0.9836 - age_output_loss: 2.8686 - weight_output_loss: 2.5434 - bag_output_loss: 1.6057 - pose_output_loss: 1.6620 - footwear_output_loss: 1.3056 - emotion_output_loss: 2.6671 - gender_output_acc: 0.5834 - image_quality_output_acc: 0.5466 - age_output_acc: 0.4028 - weight_output_acc: 0.6425 - bag_output_acc: 0.5537 - pose_output_acc: 0.6303 - footwear_output_acc: 0.5375 - emotion_output_acc: 0.7131 - val_loss: 25.0800 - val_gender_output_loss: 0.6596 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4056 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.9094 - val_pose_output_loss: 0.9178 - val_footwear_output_loss: 0.9400 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5689 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5595 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 25.00479\n",
            "Epoch 5/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.4713 - gender_output_loss: 0.7486 - image_quality_output_loss: 0.9690 - age_output_loss: 2.8478 - weight_output_loss: 2.5104 - bag_output_loss: 1.5178 - pose_output_loss: 1.6934 - footwear_output_loss: 1.2917 - emotion_output_loss: 2.6688 - gender_output_acc: 0.5931 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3943 - weight_output_acc: 0.6389 - bag_output_acc: 0.5653 - pose_output_acc: 0.6278 - footwear_output_acc: 0.5461 - emotion_output_acc: 0.7118\n",
            "Epoch 00004: val_loss did not improve from 25.00479\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 47.3599 - gender_output_loss: 0.7487 - image_quality_output_loss: 0.9685 - age_output_loss: 2.8428 - weight_output_loss: 2.5085 - bag_output_loss: 1.5116 - pose_output_loss: 1.6975 - footwear_output_loss: 1.2922 - emotion_output_loss: 2.6490 - gender_output_acc: 0.5934 - image_quality_output_acc: 0.5588 - age_output_acc: 0.3947 - weight_output_acc: 0.6391 - bag_output_acc: 0.5659 - pose_output_acc: 0.6269 - footwear_output_acc: 0.5463 - emotion_output_acc: 0.7137 - val_loss: 25.0876 - val_gender_output_loss: 0.6545 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.4018 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9083 - val_pose_output_loss: 0.9222 - val_footwear_output_loss: 0.9551 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.6066 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5322 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 25.00479\n",
            "Epoch 6/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 48.2095 - gender_output_loss: 0.7492 - image_quality_output_loss: 0.9682 - age_output_loss: 2.8456 - weight_output_loss: 2.6297 - bag_output_loss: 1.6026 - pose_output_loss: 1.7627 - footwear_output_loss: 1.2966 - emotion_output_loss: 2.6487 - gender_output_acc: 0.6035 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4012 - weight_output_acc: 0.6301 - bag_output_acc: 0.5571 - pose_output_acc: 0.6095 - footwear_output_acc: 0.5590 - emotion_output_acc: 0.7137Epoch 6/100\n",
            "100/100 [==============================] - 70s 702ms/step - loss: 48.1995 - gender_output_loss: 0.7487 - image_quality_output_loss: 0.9696 - age_output_loss: 2.8483 - weight_output_loss: 2.6337 - bag_output_loss: 1.5982 - pose_output_loss: 1.7605 - footwear_output_loss: 1.2966 - emotion_output_loss: 2.6450 - gender_output_acc: 0.6041 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4009 - weight_output_acc: 0.6303 - bag_output_acc: 0.5572 - pose_output_acc: 0.6097 - footwear_output_acc: 0.5588 - emotion_output_acc: 0.7141 - val_loss: 24.9609 - val_gender_output_loss: 0.6474 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.3977 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9043 - val_pose_output_loss: 0.9182 - val_footwear_output_loss: 0.9324 - val_emotion_output_loss: 0.9095 - val_gender_output_acc: 0.6042 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5640 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00006: val_loss improved from 25.00479 to 24.96088, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.006.h5\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 71s 712ms/step - loss: 47.9000 - gender_output_loss: 0.7386 - image_quality_output_loss: 0.9753 - age_output_loss: 2.8205 - weight_output_loss: 2.6000 - bag_output_loss: 1.5316 - pose_output_loss: 1.7316 - footwear_output_loss: 1.3126 - emotion_output_loss: 2.6894 - gender_output_acc: 0.6134 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4019 - weight_output_acc: 0.6388 - bag_output_acc: 0.5650 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5431 - emotion_output_acc: 0.7113 - val_loss: 24.9847 - val_gender_output_loss: 0.6472 - val_image_quality_output_loss: 0.9753 - val_age_output_loss: 1.4024 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9040 - val_pose_output_loss: 0.9147 - val_footwear_output_loss: 0.9317 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.6126 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5546 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 24.96088\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 73s 725ms/step - loss: 48.0412 - gender_output_loss: 0.7395 - image_quality_output_loss: 0.9787 - age_output_loss: 2.8972 - weight_output_loss: 2.4591 - bag_output_loss: 1.5672 - pose_output_loss: 1.6949 - footwear_output_loss: 1.2851 - emotion_output_loss: 2.7663 - gender_output_acc: 0.6175 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3803 - weight_output_acc: 0.6466 - bag_output_acc: 0.5722 - pose_output_acc: 0.6213 - footwear_output_acc: 0.5497 - emotion_output_acc: 0.7025 - val_loss: 24.9574 - val_gender_output_loss: 0.6413 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.4008 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9038 - val_pose_output_loss: 0.9148 - val_footwear_output_loss: 0.9337 - val_emotion_output_loss: 0.9108 - val_gender_output_acc: 0.6096 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5585 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5481 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00008: val_loss improved from 24.96088 to 24.95738, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.008.h5\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 47.7848 - gender_output_loss: 0.7456 - image_quality_output_loss: 0.9818 - age_output_loss: 2.8148 - weight_output_loss: 2.6136 - bag_output_loss: 1.5737 - pose_output_loss: 1.7417 - footwear_output_loss: 1.2664 - emotion_output_loss: 2.6334 - gender_output_acc: 0.6066 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4028 - weight_output_acc: 0.6303 - bag_output_acc: 0.5678 - pose_output_acc: 0.6138 - footwear_output_acc: 0.5563 - emotion_output_acc: 0.7153 - val_loss: 24.9407 - val_gender_output_loss: 0.6425 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.3975 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9051 - val_pose_output_loss: 0.9137 - val_footwear_output_loss: 0.9346 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.6111 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5491 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00009: val_loss improved from 24.95738 to 24.94069, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.009.h5\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 70s 697ms/step - loss: 47.4659 - gender_output_loss: 0.7510 - image_quality_output_loss: 0.9638 - age_output_loss: 2.8044 - weight_output_loss: 2.6176 - bag_output_loss: 1.5560 - pose_output_loss: 1.7293 - footwear_output_loss: 1.2999 - emotion_output_loss: 2.5734 - gender_output_acc: 0.5997 - image_quality_output_acc: 0.5637 - age_output_acc: 0.4100 - weight_output_acc: 0.6272 - bag_output_acc: 0.5722 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5450 - emotion_output_acc: 0.7181 - val_loss: 24.9776 - val_gender_output_loss: 0.6471 - val_image_quality_output_loss: 0.9749 - val_age_output_loss: 1.4033 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9059 - val_pose_output_loss: 0.9159 - val_footwear_output_loss: 0.9360 - val_emotion_output_loss: 0.9074 - val_gender_output_acc: 0.6017 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5665 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5580 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 24.94069\n",
            "Epoch 11/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.6232 - gender_output_loss: 0.7431 - image_quality_output_loss: 0.9709 - age_output_loss: 2.8441 - weight_output_loss: 2.4442 - bag_output_loss: 1.5344 - pose_output_loss: 1.7472 - footwear_output_loss: 1.3085 - emotion_output_loss: 2.7024 - gender_output_acc: 0.6095 - image_quality_output_acc: 0.5470 - age_output_acc: 0.3987 - weight_output_acc: 0.6518 - bag_output_acc: 0.5597 - pose_output_acc: 0.6121 - footwear_output_acc: 0.5543 - emotion_output_acc: 0.7090\n",
            "Epoch 00010: val_loss did not improve from 24.94069\n",
            "100/100 [==============================] - 72s 723ms/step - loss: 47.6509 - gender_output_loss: 0.7434 - image_quality_output_loss: 0.9720 - age_output_loss: 2.8383 - weight_output_loss: 2.4380 - bag_output_loss: 1.5333 - pose_output_loss: 1.7483 - footwear_output_loss: 1.3107 - emotion_output_loss: 2.7179 - gender_output_acc: 0.6097 - image_quality_output_acc: 0.5463 - age_output_acc: 0.3987 - weight_output_acc: 0.6525 - bag_output_acc: 0.5603 - pose_output_acc: 0.6116 - footwear_output_acc: 0.5528 - emotion_output_acc: 0.7075 - val_loss: 24.9571 - val_gender_output_loss: 0.6425 - val_image_quality_output_loss: 0.9784 - val_age_output_loss: 1.3966 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.9030 - val_pose_output_loss: 0.9211 - val_footwear_output_loss: 0.9324 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.6270 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5685 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5446 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 24.94069\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 48.4532 - gender_output_loss: 0.7432 - image_quality_output_loss: 0.9726 - age_output_loss: 2.8397 - weight_output_loss: 2.7204 - bag_output_loss: 1.5159 - pose_output_loss: 1.7387 - footwear_output_loss: 1.2869 - emotion_output_loss: 2.7377 - gender_output_acc: 0.6022 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3941 - weight_output_acc: 0.6181 - bag_output_acc: 0.5725 - pose_output_acc: 0.6147 - footwear_output_acc: 0.5409 - emotion_output_acc: 0.7081 - val_loss: 25.0203 - val_gender_output_loss: 0.6484 - val_image_quality_output_loss: 0.9776 - val_age_output_loss: 1.4031 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.9086 - val_pose_output_loss: 0.9155 - val_footwear_output_loss: 0.9341 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.5987 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5590 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 24.94069\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 72s 716ms/step - loss: 47.1978 - gender_output_loss: 0.7387 - image_quality_output_loss: 0.9691 - age_output_loss: 2.8229 - weight_output_loss: 2.5373 - bag_output_loss: 1.5888 - pose_output_loss: 1.7020 - footwear_output_loss: 1.3160 - emotion_output_loss: 2.5408 - gender_output_acc: 0.6122 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4028 - weight_output_acc: 0.6388 - bag_output_acc: 0.5634 - pose_output_acc: 0.6209 - footwear_output_acc: 0.5384 - emotion_output_acc: 0.7197 - val_loss: 24.9561 - val_gender_output_loss: 0.6493 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9074 - val_pose_output_loss: 0.9129 - val_footwear_output_loss: 0.9254 - val_emotion_output_loss: 0.9077 - val_gender_output_acc: 0.5942 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5541 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5670 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 24.94069\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 24.94069\n",
            "Epoch 14/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.7820 - gender_output_loss: 0.7433 - image_quality_output_loss: 0.9735 - age_output_loss: 2.8647 - weight_output_loss: 2.4727 - bag_output_loss: 1.5373 - pose_output_loss: 1.7015 - footwear_output_loss: 1.2795 - emotion_output_loss: 2.7468 - gender_output_acc: 0.6086 - image_quality_output_acc: 0.5533 - age_output_acc: 0.3968 - weight_output_acc: 0.6493 - bag_output_acc: 0.5694 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5524 - emotion_output_acc: 0.7049Epoch 14/100\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 47.7754 - gender_output_loss: 0.7428 - image_quality_output_loss: 0.9731 - age_output_loss: 2.8680 - weight_output_loss: 2.4695 - bag_output_loss: 1.5372 - pose_output_loss: 1.6969 - footwear_output_loss: 1.2811 - emotion_output_loss: 2.7474 - gender_output_acc: 0.6097 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3956 - weight_output_acc: 0.6491 - bag_output_acc: 0.5703 - pose_output_acc: 0.6234 - footwear_output_acc: 0.5519 - emotion_output_acc: 0.7047 - val_loss: 24.9102 - val_gender_output_loss: 0.6380 - val_image_quality_output_loss: 0.9753 - val_age_output_loss: 1.4026 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.9039 - val_pose_output_loss: 0.9102 - val_footwear_output_loss: 0.9209 - val_emotion_output_loss: 0.9086 - val_gender_output_acc: 0.6329 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5660 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5724 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00014: val_loss improved from 24.94069 to 24.91022, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.014.h5\n",
            "Epoch 15/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 46.8402 - gender_output_loss: 0.7371 - image_quality_output_loss: 0.9647 - age_output_loss: 2.7846 - weight_output_loss: 2.6161 - bag_output_loss: 1.5710 - pose_output_loss: 1.7161 - footwear_output_loss: 1.2737 - emotion_output_loss: 2.4585 - gender_output_acc: 0.6143 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4100 - weight_output_acc: 0.6389 - bag_output_acc: 0.5672 - pose_output_acc: 0.6212 - footwear_output_acc: 0.5562 - emotion_output_acc: 0.7298\n",
            "100/100 [==============================] - 76s 755ms/step - loss: 46.7915 - gender_output_loss: 0.7364 - image_quality_output_loss: 0.9639 - age_output_loss: 2.7772 - weight_output_loss: 2.6137 - bag_output_loss: 1.5697 - pose_output_loss: 1.7159 - footwear_output_loss: 1.2758 - emotion_output_loss: 2.4562 - gender_output_acc: 0.6153 - image_quality_output_acc: 0.5619 - age_output_acc: 0.4103 - weight_output_acc: 0.6388 - bag_output_acc: 0.5672 - pose_output_acc: 0.6213 - footwear_output_acc: 0.5550 - emotion_output_acc: 0.7300 - val_loss: 24.8902 - val_gender_output_loss: 0.6399 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.3952 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.9059 - val_pose_output_loss: 0.9094 - val_footwear_output_loss: 0.9302 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.6116 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5640 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5605 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00015: val_loss improved from 24.91022 to 24.89019, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.015.h5\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 46.7381 - gender_output_loss: 0.7462 - image_quality_output_loss: 0.9723 - age_output_loss: 2.7431 - weight_output_loss: 2.4077 - bag_output_loss: 1.5780 - pose_output_loss: 1.7284 - footwear_output_loss: 1.2798 - emotion_output_loss: 2.6051 - gender_output_acc: 0.6134 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4094 - weight_output_acc: 0.6553 - bag_output_acc: 0.5553 - pose_output_acc: 0.6188 - footwear_output_acc: 0.5578 - emotion_output_acc: 0.7166 - val_loss: 24.9076 - val_gender_output_loss: 0.6354 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.3974 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9035 - val_pose_output_loss: 0.9091 - val_footwear_output_loss: 0.9426 - val_emotion_output_loss: 0.9081 - val_gender_output_acc: 0.6290 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5650 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5412 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 24.89019\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 74s 745ms/step - loss: 48.1941 - gender_output_loss: 0.7342 - image_quality_output_loss: 0.9664 - age_output_loss: 2.9179 - weight_output_loss: 2.6368 - bag_output_loss: 1.4987 - pose_output_loss: 1.7292 - footwear_output_loss: 1.3065 - emotion_output_loss: 2.6769 - gender_output_acc: 0.6169 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3794 - weight_output_acc: 0.6209 - bag_output_acc: 0.5731 - pose_output_acc: 0.6134 - footwear_output_acc: 0.5494 - emotion_output_acc: 0.7097 - val_loss: 24.9323 - val_gender_output_loss: 0.6365 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4063 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9004 - val_pose_output_loss: 0.9103 - val_footwear_output_loss: 0.9293 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.6171 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5670 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5635 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 24.89019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 24.89019\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 48.1156 - gender_output_loss: 0.7331 - image_quality_output_loss: 0.9719 - age_output_loss: 2.9018 - weight_output_loss: 2.5517 - bag_output_loss: 1.5138 - pose_output_loss: 1.7246 - footwear_output_loss: 1.2605 - emotion_output_loss: 2.7504 - gender_output_acc: 0.6253 - image_quality_output_acc: 0.5491 - age_output_acc: 0.3975 - weight_output_acc: 0.6394 - bag_output_acc: 0.5737 - pose_output_acc: 0.6147 - footwear_output_acc: 0.5528 - emotion_output_acc: 0.7056 - val_loss: 24.9382 - val_gender_output_loss: 0.6391 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.3949 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9003 - val_pose_output_loss: 0.9119 - val_footwear_output_loss: 0.9230 - val_emotion_output_loss: 0.9225 - val_gender_output_acc: 0.6131 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5694 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 24.89019\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 75s 754ms/step - loss: 47.0393 - gender_output_loss: 0.7311 - image_quality_output_loss: 0.9756 - age_output_loss: 2.7873 - weight_output_loss: 2.4728 - bag_output_loss: 1.5897 - pose_output_loss: 1.7053 - footwear_output_loss: 1.3019 - emotion_output_loss: 2.5913 - gender_output_acc: 0.6247 - image_quality_output_acc: 0.5475 - age_output_acc: 0.3922 - weight_output_acc: 0.6453 - bag_output_acc: 0.5588 - pose_output_acc: 0.6197 - footwear_output_acc: 0.5484 - emotion_output_acc: 0.7175 - val_loss: 24.9167 - val_gender_output_loss: 0.6405 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.3942 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9067 - val_pose_output_loss: 0.9111 - val_footwear_output_loss: 0.9342 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.6062 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5501 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 24.89019\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 75s 750ms/step - loss: 48.4047 - gender_output_loss: 0.7349 - image_quality_output_loss: 0.9775 - age_output_loss: 2.9026 - weight_output_loss: 2.5890 - bag_output_loss: 1.5142 - pose_output_loss: 1.7318 - footwear_output_loss: 1.2936 - emotion_output_loss: 2.7687 - gender_output_acc: 0.6219 - image_quality_output_acc: 0.5481 - age_output_acc: 0.3966 - weight_output_acc: 0.6341 - bag_output_acc: 0.5606 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5444 - emotion_output_acc: 0.7037 - val_loss: 24.8377 - val_gender_output_loss: 0.6407 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.3935 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8976 - val_pose_output_loss: 0.9087 - val_footwear_output_loss: 0.9274 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.6166 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5709 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5680 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00020: val_loss improved from 24.89019 to 24.83770, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.020.h5\n",
            "Epoch 21/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.4214 - gender_output_loss: 0.7485 - image_quality_output_loss: 0.9668 - age_output_loss: 2.8156 - weight_output_loss: 2.6090 - bag_output_loss: 1.5340 - pose_output_loss: 1.7045 - footwear_output_loss: 1.2666 - emotion_output_loss: 2.6131 - gender_output_acc: 0.6042 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3980 - weight_output_acc: 0.6379 - bag_output_acc: 0.5751 - pose_output_acc: 0.6215 - footwear_output_acc: 0.5713 - emotion_output_acc: 0.7162\n",
            "100/100 [==============================] - 75s 750ms/step - loss: 47.3836 - gender_output_loss: 0.7482 - image_quality_output_loss: 0.9664 - age_output_loss: 2.8078 - weight_output_loss: 2.6005 - bag_output_loss: 1.5331 - pose_output_loss: 1.7036 - footwear_output_loss: 1.2679 - emotion_output_loss: 2.6189 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.5603 - age_output_acc: 0.3991 - weight_output_acc: 0.6391 - bag_output_acc: 0.5753 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5703 - emotion_output_acc: 0.7156 - val_loss: 24.8569 - val_gender_output_loss: 0.6346 - val_image_quality_output_loss: 0.9712 - val_age_output_loss: 1.3986 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.9013 - val_pose_output_loss: 0.9092 - val_footwear_output_loss: 0.9263 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.6379 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5580 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 24.83770\n",
            "Epoch 22/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.3914 - gender_output_loss: 0.7300 - image_quality_output_loss: 0.9597 - age_output_loss: 2.8218 - weight_output_loss: 2.5891 - bag_output_loss: 1.5482 - pose_output_loss: 1.7477 - footwear_output_loss: 1.2573 - emotion_output_loss: 2.5893 - gender_output_acc: 0.6282 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4037 - weight_output_acc: 0.6354 - bag_output_acc: 0.5641 - pose_output_acc: 0.6098 - footwear_output_acc: 0.5657 - emotion_output_acc: 0.7197\n",
            "Epoch 00021: val_loss did not improve from 24.83770\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 77s 775ms/step - loss: 47.3944 - gender_output_loss: 0.7301 - image_quality_output_loss: 0.9597 - age_output_loss: 2.8332 - weight_output_loss: 2.5895 - bag_output_loss: 1.5451 - pose_output_loss: 1.7410 - footwear_output_loss: 1.2566 - emotion_output_loss: 2.5858 - gender_output_acc: 0.6269 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4041 - weight_output_acc: 0.6356 - bag_output_acc: 0.5641 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5659 - emotion_output_acc: 0.7200 - val_loss: 24.8164 - val_gender_output_loss: 0.6312 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.3980 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8968 - val_pose_output_loss: 0.9075 - val_footwear_output_loss: 0.9175 - val_emotion_output_loss: 0.9097 - val_gender_output_acc: 0.6324 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5665 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5709 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00022: val_loss improved from 24.83770 to 24.81644, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.022.h5\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 75s 754ms/step - loss: 48.0819 - gender_output_loss: 0.7264 - image_quality_output_loss: 0.9606 - age_output_loss: 2.8925 - weight_output_loss: 2.4952 - bag_output_loss: 1.5125 - pose_output_loss: 1.7672 - footwear_output_loss: 1.2665 - emotion_output_loss: 2.7710 - gender_output_acc: 0.6372 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3891 - weight_output_acc: 0.6425 - bag_output_acc: 0.5822 - pose_output_acc: 0.6028 - footwear_output_acc: 0.5637 - emotion_output_acc: 0.7016 - val_loss: 24.8875 - val_gender_output_loss: 0.6299 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4007 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8978 - val_pose_output_loss: 0.9077 - val_footwear_output_loss: 0.9252 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.6270 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5704 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 24.81644\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 76s 758ms/step - loss: 47.7490 - gender_output_loss: 0.7320 - image_quality_output_loss: 0.9821 - age_output_loss: 2.7867 - weight_output_loss: 2.6146 - bag_output_loss: 1.5723 - pose_output_loss: 1.6835 - footwear_output_loss: 1.2731 - emotion_output_loss: 2.7051 - gender_output_acc: 0.6228 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3959 - weight_output_acc: 0.6266 - bag_output_acc: 0.5687 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5544 - emotion_output_acc: 0.7087 - val_loss: 24.8187 - val_gender_output_loss: 0.6266 - val_image_quality_output_loss: 0.9735 - val_age_output_loss: 1.3971 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8990 - val_pose_output_loss: 0.9041 - val_footwear_output_loss: 0.9188 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.6404 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5660 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 24.81644\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 75s 754ms/step - loss: 48.0819 - gender_output_loss: 0.7264 - image_quality_output_loss: 0.9606 - age_output_loss: 2.8925 - weight_output_loss: 2.4952 - bag_output_loss: 1.5125 - pose_output_loss: 1.7672 - footwear_output_loss: 1.2665 - emotion_output_loss: 2.7710 - gender_output_acc: 0.6372 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3891 - weight_output_acc: 0.6425 - bag_output_acc: 0.5822 - pose_output_acc: 0.6028 - footwear_output_acc: 0.5637 - emotion_output_acc: 0.7016 - val_loss: 24.8875 - val_gender_output_loss: 0.6299 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4007 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8978 - val_pose_output_loss: 0.9077 - val_footwear_output_loss: 0.9252 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.6270 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5704 - val_emotion_output_acc: 0.7073\n",
            "100/100 [==============================] - 76s 761ms/step - loss: 46.7540 - gender_output_loss: 0.7272 - image_quality_output_loss: 0.9651 - age_output_loss: 2.7801 - weight_output_loss: 2.5509 - bag_output_loss: 1.5445 - pose_output_loss: 1.6871 - footwear_output_loss: 1.2839 - emotion_output_loss: 2.5345 - gender_output_acc: 0.6253 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4078 - weight_output_acc: 0.6416 - bag_output_acc: 0.5653 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5513 - emotion_output_acc: 0.7191 - val_loss: 24.7634 - val_gender_output_loss: 0.6260 - val_image_quality_output_loss: 0.9723 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8955 - val_pose_output_loss: 0.9055 - val_footwear_output_loss: 0.9152 - val_emotion_output_loss: 0.9083 - val_gender_output_acc: 0.6409 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5675 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5729 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00025: val_loss improved from 24.81644 to 24.76336, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.025.h5\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 77s 770ms/step - loss: 47.1650 - gender_output_loss: 0.7330 - image_quality_output_loss: 0.9730 - age_output_loss: 2.8868 - weight_output_loss: 2.4295 - bag_output_loss: 1.5192 - pose_output_loss: 1.7115 - footwear_output_loss: 1.2665 - emotion_output_loss: 2.6242 - gender_output_acc: 0.6222 - image_quality_output_acc: 0.5466 - age_output_acc: 0.3912 - weight_output_acc: 0.6459 - bag_output_acc: 0.5753 - pose_output_acc: 0.6184 - footwear_output_acc: 0.5509 - emotion_output_acc: 0.7184 - val_loss: 24.8239 - val_gender_output_loss: 0.6308 - val_image_quality_output_loss: 0.9725 - val_age_output_loss: 1.3981 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8993 - val_pose_output_loss: 0.9052 - val_footwear_output_loss: 0.9160 - val_emotion_output_loss: 0.9074 - val_gender_output_acc: 0.6265 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5650 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5804 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 24.76336\n",
            "Epoch 27/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 48.0403 - gender_output_loss: 0.7341 - image_quality_output_loss: 0.9629 - age_output_loss: 2.9019 - weight_output_loss: 2.6163 - bag_output_loss: 1.5614 - pose_output_loss: 1.7393 - footwear_output_loss: 1.2525 - emotion_output_loss: 2.6472 - gender_output_acc: 0.6212 - image_quality_output_acc: 0.5647 - age_output_acc: 0.3867 - weight_output_acc: 0.6297 - bag_output_acc: 0.5549 - pose_output_acc: 0.6158 - footwear_output_acc: 0.5641 - emotion_output_acc: 0.7153\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 48.0458 - gender_output_loss: 0.7342 - image_quality_output_loss: 0.9632 - age_output_loss: 2.8928 - weight_output_loss: 2.6157 - bag_output_loss: 1.5625 - pose_output_loss: 1.7383 - footwear_output_loss: 1.2551 - emotion_output_loss: 2.6566 - gender_output_acc: 0.6209 - image_quality_output_acc: 0.5650 - age_output_acc: 0.3872 - weight_output_acc: 0.6297 - bag_output_acc: 0.5541 - pose_output_acc: 0.6159 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7147 - val_loss: 24.8342 - val_gender_output_loss: 0.6279 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.3964 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9026 - val_pose_output_loss: 0.9093 - val_footwear_output_loss: 0.9136 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.6255 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5709 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 24.76336\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 47.7153 - gender_output_loss: 0.7361 - image_quality_output_loss: 0.9627 - age_output_loss: 2.7807 - weight_output_loss: 2.6195 - bag_output_loss: 1.5615 - pose_output_loss: 1.6905 - footwear_output_loss: 1.2819 - emotion_output_loss: 2.7061 - gender_output_acc: 0.6284 - image_quality_output_acc: 0.5600 - age_output_acc: 0.3941 - weight_output_acc: 0.6250 - bag_output_acc: 0.5641 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5503 - emotion_output_acc: 0.7084 - val_loss: 24.8607 - val_gender_output_loss: 0.6412 - val_image_quality_output_loss: 0.9709 - val_age_output_loss: 1.3999 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8988 - val_pose_output_loss: 0.9044 - val_footwear_output_loss: 0.9215 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.6042 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5650 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5689 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 24.76336\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 78s 775ms/step - loss: 47.6113 - gender_output_loss: 0.7295 - image_quality_output_loss: 0.9815 - age_output_loss: 2.8188 - weight_output_loss: 2.5693 - bag_output_loss: 1.5118 - pose_output_loss: 1.6748 - footwear_output_loss: 1.2718 - emotion_output_loss: 2.7283 - gender_output_acc: 0.6178 - image_quality_output_acc: 0.5397 - age_output_acc: 0.4019 - weight_output_acc: 0.6431 - bag_output_acc: 0.5763 - pose_output_acc: 0.6259 - footwear_output_acc: 0.5588 - emotion_output_acc: 0.7066 - val_loss: 24.7801 - val_gender_output_loss: 0.6323 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.3972 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8960 - val_pose_output_loss: 0.9063 - val_footwear_output_loss: 0.9089 - val_emotion_output_loss: 0.9097 - val_gender_output_acc: 0.6225 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5863 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 24.76336\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 47.0895 - gender_output_loss: 0.7382 - image_quality_output_loss: 0.9674 - age_output_loss: 2.8912 - weight_output_loss: 2.5045 - bag_output_loss: 1.5655 - pose_output_loss: 1.7183 - footwear_output_loss: 1.2471 - emotion_output_loss: 2.5166 - gender_output_acc: 0.6144 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3928 - weight_output_acc: 0.6422 - bag_output_acc: 0.5731 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5600 - emotion_output_acc: 0.7197 - val_loss: 24.7637 - val_gender_output_loss: 0.6337 - val_image_quality_output_loss: 0.9651 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8978 - val_pose_output_loss: 0.9044 - val_footwear_output_loss: 0.9126 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.6151 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4102 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5615 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5744 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 24.76336\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 46.8688 - gender_output_loss: 0.7189 - image_quality_output_loss: 0.9588 - age_output_loss: 2.7689 - weight_output_loss: 2.5459 - bag_output_loss: 1.5250 - pose_output_loss: 1.7281 - footwear_output_loss: 1.2641 - emotion_output_loss: 2.5817 - gender_output_acc: 0.6437 - image_quality_output_acc: 0.5581 - age_output_acc: 0.4119 - weight_output_acc: 0.6397 - bag_output_acc: 0.5809 - pose_output_acc: 0.6106 - footwear_output_acc: 0.5609 - emotion_output_acc: 0.7188 - val_loss: 24.7329 - val_gender_output_loss: 0.6269 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8897 - val_pose_output_loss: 0.8985 - val_footwear_output_loss: 0.9185 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.6300 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4077 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5714 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5709 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00031: val_loss improved from 24.76336 to 24.73285, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.031.h5\n",
            "Epoch 32/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.5182 - gender_output_loss: 0.7203 - image_quality_output_loss: 0.9694 - age_output_loss: 2.8579 - weight_output_loss: 2.5333 - bag_output_loss: 1.5645 - pose_output_loss: 1.6611 - footwear_output_loss: 1.2600 - emotion_output_loss: 2.6813 - gender_output_acc: 0.6414 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3920 - weight_output_acc: 0.6335 - bag_output_acc: 0.5616 - pose_output_acc: 0.6297 - footwear_output_acc: 0.5628 - emotion_output_acc: 0.7134\n",
            "100/100 [==============================] - 76s 761ms/step - loss: 47.5841 - gender_output_loss: 0.7201 - image_quality_output_loss: 0.9683 - age_output_loss: 2.8621 - weight_output_loss: 2.5386 - bag_output_loss: 1.5703 - pose_output_loss: 1.6632 - footwear_output_loss: 1.2583 - emotion_output_loss: 2.6850 - gender_output_acc: 0.6419 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3909 - weight_output_acc: 0.6331 - bag_output_acc: 0.5606 - pose_output_acc: 0.6291 - footwear_output_acc: 0.5637 - emotion_output_acc: 0.7131 - val_loss: 24.6429 - val_gender_output_loss: 0.6219 - val_image_quality_output_loss: 0.9666 - val_age_output_loss: 1.3905 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8882 - val_pose_output_loss: 0.8984 - val_footwear_output_loss: 0.9057 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.6384 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4127 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5749 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5843 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00032: val_loss improved from 24.73285 to 24.64287, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.032.h5\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 78s 783ms/step - loss: 47.7356 - gender_output_loss: 0.7199 - image_quality_output_loss: 0.9564 - age_output_loss: 2.8300 - weight_output_loss: 2.4881 - bag_output_loss: 1.5275 - pose_output_loss: 1.7161 - footwear_output_loss: 1.2746 - emotion_output_loss: 2.7834 - gender_output_acc: 0.6459 - image_quality_output_acc: 0.5694 - age_output_acc: 0.3978 - weight_output_acc: 0.6444 - bag_output_acc: 0.5787 - pose_output_acc: 0.6166 - footwear_output_acc: 0.5563 - emotion_output_acc: 0.7037 - val_loss: 24.6299 - val_gender_output_loss: 0.6180 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.3900 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.8894 - val_pose_output_loss: 0.8981 - val_footwear_output_loss: 0.9075 - val_emotion_output_loss: 0.9044 - val_gender_output_acc: 0.6463 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5769 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5823 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00033: val_loss improved from 24.64287 to 24.62985, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.033.h5\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 76s 763ms/step - loss: 47.2631 - gender_output_loss: 0.7129 - image_quality_output_loss: 0.9662 - age_output_loss: 2.7920 - weight_output_loss: 2.6263 - bag_output_loss: 1.4657 - pose_output_loss: 1.7283 - footwear_output_loss: 1.2544 - emotion_output_loss: 2.6457 - gender_output_acc: 0.6519 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4103 - weight_output_acc: 0.6238 - bag_output_acc: 0.5819 - pose_output_acc: 0.6116 - footwear_output_acc: 0.5619 - emotion_output_acc: 0.7131 - val_loss: 24.8200 - val_gender_output_loss: 0.6334 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.3981 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.9006 - val_pose_output_loss: 0.9000 - val_footwear_output_loss: 0.9110 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.6131 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.5640 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 24.62985\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 47.7303 - gender_output_loss: 0.7295 - image_quality_output_loss: 0.9723 - age_output_loss: 2.7456 - weight_output_loss: 2.5543 - bag_output_loss: 1.5840 - pose_output_loss: 1.7021 - footwear_output_loss: 1.2701 - emotion_output_loss: 2.7750 - gender_output_acc: 0.6266 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3950 - weight_output_acc: 0.6325 - bag_output_acc: 0.5550 - pose_output_acc: 0.6203 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7041 - val_loss: 24.7638 - val_gender_output_loss: 0.6181 - val_image_quality_output_loss: 0.9681 - val_age_output_loss: 1.3891 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.9047 - val_pose_output_loss: 0.9008 - val_footwear_output_loss: 0.9209 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.6473 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5665 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5749 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 24.62985\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 78s 783ms/step - loss: 47.2811 - gender_output_loss: 0.7181 - image_quality_output_loss: 0.9651 - age_output_loss: 2.9208 - weight_output_loss: 2.4835 - bag_output_loss: 1.5561 - pose_output_loss: 1.7149 - footwear_output_loss: 1.2710 - emotion_output_loss: 2.5611 - gender_output_acc: 0.6350 - image_quality_output_acc: 0.5494 - age_output_acc: 0.3944 - weight_output_acc: 0.6550 - bag_output_acc: 0.5691 - pose_output_acc: 0.6141 - footwear_output_acc: 0.5600 - emotion_output_acc: 0.7203 - val_loss: 24.7389 - val_gender_output_loss: 0.6167 - val_image_quality_output_loss: 0.9678 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.8929 - val_pose_output_loss: 0.8979 - val_footwear_output_loss: 0.9384 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.6453 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5675 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5441 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 24.62985\n",
            "Epoch 37/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 48.0387 - gender_output_loss: 0.7125 - image_quality_output_loss: 0.9715 - age_output_loss: 2.7730 - weight_output_loss: 2.6782 - bag_output_loss: 1.5336 - pose_output_loss: 1.7412 - footwear_output_loss: 1.2683 - emotion_output_loss: 2.7510 - gender_output_acc: 0.6496 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3949 - weight_output_acc: 0.6266 - bag_output_acc: 0.5827 - pose_output_acc: 0.6080 - footwear_output_acc: 0.5565 - emotion_output_acc: 0.7052\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00036: val_loss did not improve from 24.62985\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 48.0533 - gender_output_loss: 0.7114 - image_quality_output_loss: 0.9715 - age_output_loss: 2.7737 - weight_output_loss: 2.6780 - bag_output_loss: 1.5374 - pose_output_loss: 1.7367 - footwear_output_loss: 1.2675 - emotion_output_loss: 2.7556 - gender_output_acc: 0.6506 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3947 - weight_output_acc: 0.6262 - bag_output_acc: 0.5819 - pose_output_acc: 0.6088 - footwear_output_acc: 0.5572 - emotion_output_acc: 0.7047 - val_loss: 24.6848 - val_gender_output_loss: 0.6284 - val_image_quality_output_loss: 0.9670 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8936 - val_pose_output_loss: 0.8983 - val_footwear_output_loss: 0.9018 - val_emotion_output_loss: 0.9081 - val_gender_output_acc: 0.6270 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5714 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5828 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 24.62985\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 78s 775ms/step - loss: 46.8079 - gender_output_loss: 0.7182 - image_quality_output_loss: 0.9614 - age_output_loss: 2.8584 - weight_output_loss: 2.5155 - bag_output_loss: 1.5530 - pose_output_loss: 1.6898 - footwear_output_loss: 1.2527 - emotion_output_loss: 2.5145 - gender_output_acc: 0.6425 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3941 - weight_output_acc: 0.6388 - bag_output_acc: 0.5747 - pose_output_acc: 0.6191 - footwear_output_acc: 0.5584 - emotion_output_acc: 0.7241 - val_loss: 24.6804 - val_gender_output_loss: 0.6154 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.3942 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8925 - val_pose_output_loss: 0.8979 - val_footwear_output_loss: 0.9132 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.6498 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4087 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5764 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5749 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 24.62985\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 47.6495 - gender_output_loss: 0.7207 - image_quality_output_loss: 0.9594 - age_output_loss: 2.8258 - weight_output_loss: 2.5385 - bag_output_loss: 1.5282 - pose_output_loss: 1.7120 - footwear_output_loss: 1.2816 - emotion_output_loss: 2.7278 - gender_output_acc: 0.6322 - image_quality_output_acc: 0.5603 - age_output_acc: 0.4006 - weight_output_acc: 0.6441 - bag_output_acc: 0.5706 - pose_output_acc: 0.6159 - footwear_output_acc: 0.5534 - emotion_output_acc: 0.7053 - val_loss: 24.6472 - val_gender_output_loss: 0.6147 - val_image_quality_output_loss: 0.9663 - val_age_output_loss: 1.3900 - val_weight_output_loss: 0.9739 - val_bag_output_loss: 0.8865 - val_pose_output_loss: 0.8918 - val_footwear_output_loss: 0.9272 - val_emotion_output_loss: 0.9097 - val_gender_output_acc: 0.6572 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5794 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5541 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 24.62985\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 79s 788ms/step - loss: 45.7373 - gender_output_loss: 0.7222 - image_quality_output_loss: 0.9751 - age_output_loss: 2.7573 - weight_output_loss: 2.4062 - bag_output_loss: 1.4835 - pose_output_loss: 1.6642 - footwear_output_loss: 1.2441 - emotion_output_loss: 2.4971 - gender_output_acc: 0.6316 - image_quality_output_acc: 0.5353 - age_output_acc: 0.4081 - weight_output_acc: 0.6416 - bag_output_acc: 0.5656 - pose_output_acc: 0.6256 - footwear_output_acc: 0.5731 - emotion_output_acc: 0.7253 - val_loss: 24.5865 - val_gender_output_loss: 0.6101 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.3888 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8894 - val_pose_output_loss: 0.8927 - val_footwear_output_loss: 0.9032 - val_emotion_output_loss: 0.9067 - val_gender_output_acc: 0.6528 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5784 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5868 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00040: val_loss improved from 24.62985 to 24.58651, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.040.h5\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 77s 765ms/step - loss: 47.1301 - gender_output_loss: 0.7173 - image_quality_output_loss: 0.9560 - age_output_loss: 2.8297 - weight_output_loss: 2.6581 - bag_output_loss: 1.5100 - pose_output_loss: 1.6179 - footwear_output_loss: 1.2443 - emotion_output_loss: 2.6108 - gender_output_acc: 0.6466 - image_quality_output_acc: 0.5622 - age_output_acc: 0.3984 - weight_output_acc: 0.6284 - bag_output_acc: 0.5716 - pose_output_acc: 0.6353 - footwear_output_acc: 0.5722 - emotion_output_acc: 0.7163 - val_loss: 24.6656 - val_gender_output_loss: 0.6079 - val_image_quality_output_loss: 0.9670 - val_age_output_loss: 1.3923 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.8905 - val_pose_output_loss: 0.8915 - val_footwear_output_loss: 0.9148 - val_emotion_output_loss: 0.9144 - val_gender_output_acc: 0.6627 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5779 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 24.58651\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 77s 767ms/step - loss: 47.3798 - gender_output_loss: 0.7060 - image_quality_output_loss: 0.9602 - age_output_loss: 2.7987 - weight_output_loss: 2.4274 - bag_output_loss: 1.5186 - pose_output_loss: 1.7213 - footwear_output_loss: 1.2732 - emotion_output_loss: 2.7825 - gender_output_acc: 0.6559 - image_quality_output_acc: 0.5578 - age_output_acc: 0.4053 - weight_output_acc: 0.6441 - bag_output_acc: 0.5837 - pose_output_acc: 0.6072 - footwear_output_acc: 0.5566 - emotion_output_acc: 0.7006 - val_loss: 24.7108 - val_gender_output_loss: 0.6295 - val_image_quality_output_loss: 0.9623 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9002 - val_pose_output_loss: 0.8969 - val_footwear_output_loss: 0.9002 - val_emotion_output_loss: 0.9098 - val_gender_output_acc: 0.6200 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5640 - val_pose_output_acc: 0.6171 - val_footwear_output_acc: 0.5868 - val_emotion_output_acc: 0.7073\n",
            "Epoch 42/100\n",
            "Epoch 00042: val_loss did not improve from 24.58651\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 77s 774ms/step - loss: 47.7231 - gender_output_loss: 0.7226 - image_quality_output_loss: 0.9574 - age_output_loss: 2.9297 - weight_output_loss: 2.4792 - bag_output_loss: 1.5822 - pose_output_loss: 1.7487 - footwear_output_loss: 1.2385 - emotion_output_loss: 2.6410 - gender_output_acc: 0.6359 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3881 - weight_output_acc: 0.6469 - bag_output_acc: 0.5706 - pose_output_acc: 0.6072 - footwear_output_acc: 0.5697 - emotion_output_acc: 0.7147 - val_loss: 24.6121 - val_gender_output_loss: 0.6127 - val_image_quality_output_loss: 0.9607 - val_age_output_loss: 1.3891 - val_weight_output_loss: 0.9680 - val_bag_output_loss: 0.8896 - val_pose_output_loss: 0.8899 - val_footwear_output_loss: 0.9232 - val_emotion_output_loss: 0.9119 - val_gender_output_acc: 0.6508 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5734 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5521 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 24.58651\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 79s 787ms/step - loss: 47.2696 - gender_output_loss: 0.7150 - image_quality_output_loss: 0.9510 - age_output_loss: 2.7986 - weight_output_loss: 2.5741 - bag_output_loss: 1.5240 - pose_output_loss: 1.6671 - footwear_output_loss: 1.2748 - emotion_output_loss: 2.6818 - gender_output_acc: 0.6375 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3934 - weight_output_acc: 0.6397 - bag_output_acc: 0.5716 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5572 - emotion_output_acc: 0.7069 - val_loss: 24.6885 - val_gender_output_loss: 0.6145 - val_image_quality_output_loss: 0.9655 - val_age_output_loss: 1.4013 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8895 - val_pose_output_loss: 0.8963 - val_footwear_output_loss: 0.9014 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.6419 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.5680 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5789 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 24.58651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 45/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 48.3052 - gender_output_loss: 0.7195 - image_quality_output_loss: 0.9573 - age_output_loss: 2.8810 - weight_output_loss: 2.6274 - bag_output_loss: 1.5046 - pose_output_loss: 1.7545 - footwear_output_loss: 1.2845 - emotion_output_loss: 2.7576 - gender_output_acc: 0.6424 - image_quality_output_acc: 0.5644 - age_output_acc: 0.3980 - weight_output_acc: 0.6370 - bag_output_acc: 0.5745 - pose_output_acc: 0.6048 - footwear_output_acc: 0.5499 - emotion_output_acc: 0.7049\n",
            "100/100 [==============================] - 76s 763ms/step - loss: 48.3736 - gender_output_loss: 0.7195 - image_quality_output_loss: 0.9584 - age_output_loss: 2.8802 - weight_output_loss: 2.6336 - bag_output_loss: 1.5059 - pose_output_loss: 1.7508 - footwear_output_loss: 1.2838 - emotion_output_loss: 2.7726 - gender_output_acc: 0.6412 - image_quality_output_acc: 0.5641 - age_output_acc: 0.3987 - weight_output_acc: 0.6366 - bag_output_acc: 0.5744 - pose_output_acc: 0.6056 - footwear_output_acc: 0.5506 - emotion_output_acc: 0.7037 - val_loss: 24.6610 - val_gender_output_loss: 0.6094 - val_image_quality_output_loss: 0.9576 - val_age_output_loss: 1.3921 - val_weight_output_loss: 0.9800 - val_bag_output_loss: 0.8892 - val_pose_output_loss: 0.8965 - val_footwear_output_loss: 0.8982 - val_emotion_output_loss: 0.9244 - val_gender_output_acc: 0.6483 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5655 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5799 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 24.58651\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 76s 759ms/step - loss: 47.2486 - gender_output_loss: 0.7234 - image_quality_output_loss: 0.9593 - age_output_loss: 2.8654 - weight_output_loss: 2.5437 - bag_output_loss: 1.5612 - pose_output_loss: 1.6513 - footwear_output_loss: 1.2528 - emotion_output_loss: 2.6202 - gender_output_acc: 0.6338 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3944 - weight_output_acc: 0.6316 - bag_output_acc: 0.5787 - pose_output_acc: 0.6284 - footwear_output_acc: 0.5650 - emotion_output_acc: 0.7153 - val_loss: 24.5886 - val_gender_output_loss: 0.6057 - val_image_quality_output_loss: 0.9553 - val_age_output_loss: 1.3926 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8947 - val_pose_output_loss: 0.8866 - val_footwear_output_loss: 0.9088 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.6642 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5680 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5769 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 24.58651\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 79s 786ms/step - loss: 46.5454 - gender_output_loss: 0.7138 - image_quality_output_loss: 0.9676 - age_output_loss: 2.7794 - weight_output_loss: 2.4836 - bag_output_loss: 1.5697 - pose_output_loss: 1.7020 - footwear_output_loss: 1.2438 - emotion_output_loss: 2.5365 - gender_output_acc: 0.6356 - image_quality_output_acc: 0.5447 - age_output_acc: 0.4106 - weight_output_acc: 0.6363 - bag_output_acc: 0.5666 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5675 - emotion_output_acc: 0.7234 - val_loss: 24.5143 - val_gender_output_loss: 0.6032 - val_image_quality_output_loss: 0.9560 - val_age_output_loss: 1.3891 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.8874 - val_pose_output_loss: 0.8900 - val_footwear_output_loss: 0.8949 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.6622 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5729 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5823 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00047: val_loss improved from 24.58651 to 24.51431, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.047.h5\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 47.1714 - gender_output_loss: 0.7169 - image_quality_output_loss: 0.9580 - age_output_loss: 2.7970 - weight_output_loss: 2.5338 - bag_output_loss: 1.5265 - pose_output_loss: 1.7542 - footwear_output_loss: 1.2571 - emotion_output_loss: 2.6278 - gender_output_acc: 0.6434 - image_quality_output_acc: 0.5578 - age_output_acc: 0.4034 - weight_output_acc: 0.6375 - bag_output_acc: 0.5784 - pose_output_acc: 0.6091 - footwear_output_acc: 0.5728 - emotion_output_acc: 0.7141 - val_loss: 24.5321 - val_gender_output_loss: 0.5995 - val_image_quality_output_loss: 0.9544 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8854 - val_pose_output_loss: 0.8905 - val_footwear_output_loss: 0.9007 - val_emotion_output_loss: 0.9107 - val_gender_output_acc: 0.6696 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5779 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5823 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 24.51431\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 77s 770ms/step - loss: 47.4639 - gender_output_loss: 0.7141 - image_quality_output_loss: 0.9586 - age_output_loss: 2.8721 - weight_output_loss: 2.4517 - bag_output_loss: 1.5502 - pose_output_loss: 1.6932 - footwear_output_loss: 1.2697 - emotion_output_loss: 2.7102 - gender_output_acc: 0.6541 - image_quality_output_acc: 0.5481 - age_output_acc: 0.4009 - weight_output_acc: 0.6447 - bag_output_acc: 0.5866 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5569 - emotion_output_acc: 0.7087 - val_loss: 24.5357 - val_gender_output_loss: 0.5986 - val_image_quality_output_loss: 0.9558 - val_age_output_loss: 1.3944 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8886 - val_pose_output_loss: 0.8870 - val_footwear_output_loss: 0.9011 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.6652 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5774 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5799 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 24.51431\n",
            "Epoch 50/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.4826 - gender_output_loss: 0.6986 - image_quality_output_loss: 0.9595 - age_output_loss: 2.8086 - weight_output_loss: 2.6980 - bag_output_loss: 1.4844 - pose_output_loss: 1.6867 - footwear_output_loss: 1.2126 - emotion_output_loss: 2.6838 - gender_output_acc: 0.6496 - image_quality_output_acc: 0.5505 - age_output_acc: 0.3968 - weight_output_acc: 0.6266 - bag_output_acc: 0.5833 - pose_output_acc: 0.6174 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7080Epoch 50/100\n",
            "100/100 [==============================] - 77s 772ms/step - loss: 47.4498 - gender_output_loss: 0.6974 - image_quality_output_loss: 0.9592 - age_output_loss: 2.8086 - weight_output_loss: 2.7037 - bag_output_loss: 1.4796 - pose_output_loss: 1.6865 - footwear_output_loss: 1.2118 - emotion_output_loss: 2.6763 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5509 - age_output_acc: 0.3969 - weight_output_acc: 0.6262 - bag_output_acc: 0.5841 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5794 - emotion_output_acc: 0.7084 - val_loss: 24.6038 - val_gender_output_loss: 0.6097 - val_image_quality_output_loss: 0.9618 - val_age_output_loss: 1.3853 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8901 - val_pose_output_loss: 0.8904 - val_footwear_output_loss: 0.9143 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.6567 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5789 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5685 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 24.51431\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 79s 788ms/step - loss: 46.9130 - gender_output_loss: 0.7146 - image_quality_output_loss: 0.9406 - age_output_loss: 2.8179 - weight_output_loss: 2.5219 - bag_output_loss: 1.5423 - pose_output_loss: 1.6773 - footwear_output_loss: 1.3059 - emotion_output_loss: 2.5831 - gender_output_acc: 0.6366 - image_quality_output_acc: 0.5737 - age_output_acc: 0.3937 - weight_output_acc: 0.6441 - bag_output_acc: 0.5681 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5459 - emotion_output_acc: 0.7163 - val_loss: 24.7243 - val_gender_output_loss: 0.6045 - val_image_quality_output_loss: 0.9547 - val_age_output_loss: 1.4054 - val_weight_output_loss: 0.9982 - val_bag_output_loss: 0.8957 - val_pose_output_loss: 0.8887 - val_footwear_output_loss: 0.9187 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.6523 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.5694 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5546 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 24.51431\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 24.51431\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 46.7668 - gender_output_loss: 0.7179 - image_quality_output_loss: 0.9459 - age_output_loss: 2.8659 - weight_output_loss: 2.4726 - bag_output_loss: 1.5089 - pose_output_loss: 1.7407 - footwear_output_loss: 1.2412 - emotion_output_loss: 2.5415 - gender_output_acc: 0.6512 - image_quality_output_acc: 0.5641 - age_output_acc: 0.3859 - weight_output_acc: 0.6359 - bag_output_acc: 0.5706 - pose_output_acc: 0.6084 - footwear_output_acc: 0.5681 - emotion_output_acc: 0.7247 - val_loss: 24.7627 - val_gender_output_loss: 0.6108 - val_image_quality_output_loss: 0.9523 - val_age_output_loss: 1.3972 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.9010 - val_pose_output_loss: 0.8972 - val_footwear_output_loss: 0.9133 - val_emotion_output_loss: 0.9236 - val_gender_output_acc: 0.6518 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6195 - val_footwear_output_acc: 0.5799 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 24.51431\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 77s 770ms/step - loss: 47.2093 - gender_output_loss: 0.7119 - image_quality_output_loss: 0.9605 - age_output_loss: 2.8068 - weight_output_loss: 2.6265 - bag_output_loss: 1.5250 - pose_output_loss: 1.6997 - footwear_output_loss: 1.2481 - emotion_output_loss: 2.6073 - gender_output_acc: 0.6534 - image_quality_output_acc: 0.5491 - age_output_acc: 0.3947 - weight_output_acc: 0.6341 - bag_output_acc: 0.5822 - pose_output_acc: 0.6188 - footwear_output_acc: 0.5722 - emotion_output_acc: 0.7147 - val_loss: 24.5597 - val_gender_output_loss: 0.6081 - val_image_quality_output_loss: 0.9499 - val_age_output_loss: 1.3935 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8914 - val_pose_output_loss: 0.8911 - val_footwear_output_loss: 0.9078 - val_emotion_output_loss: 0.9061 - val_gender_output_acc: 0.6607 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5729 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5729 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 24.51431\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 78s 780ms/step - loss: 47.6744 - gender_output_loss: 0.7166 - image_quality_output_loss: 0.9657 - age_output_loss: 2.8259 - weight_output_loss: 2.5328 - bag_output_loss: 1.5587 - pose_output_loss: 1.6556 - footwear_output_loss: 1.2575 - emotion_output_loss: 2.7733 - gender_output_acc: 0.6359 - image_quality_output_acc: 0.5447 - age_output_acc: 0.4031 - weight_output_acc: 0.6391 - bag_output_acc: 0.5806 - pose_output_acc: 0.6287 - footwear_output_acc: 0.5584 - emotion_output_acc: 0.7009 - val_loss: 24.7646 - val_gender_output_loss: 0.5976 - val_image_quality_output_loss: 0.9533 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9969 - val_bag_output_loss: 0.8920 - val_pose_output_loss: 0.8936 - val_footwear_output_loss: 0.9228 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.6607 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5719 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5620 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 24.51431\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 46.7351 - gender_output_loss: 0.6976 - image_quality_output_loss: 0.9489 - age_output_loss: 2.7873 - weight_output_loss: 2.5877 - bag_output_loss: 1.4734 - pose_output_loss: 1.6634 - footwear_output_loss: 1.2352 - emotion_output_loss: 2.6234 - gender_output_acc: 0.6584 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3984 - weight_output_acc: 0.6347 - bag_output_acc: 0.5900 - pose_output_acc: 0.6259 - footwear_output_acc: 0.5669 - emotion_output_acc: 0.7156 - val_loss: 24.5060 - val_gender_output_loss: 0.5954 - val_image_quality_output_loss: 0.9482 - val_age_output_loss: 1.3844 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8881 - val_pose_output_loss: 0.8907 - val_footwear_output_loss: 0.9018 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.6652 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5809 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.5759 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00055: val_loss improved from 24.51431 to 24.50602, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.055.h5\n",
            "Epoch 56/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 46.3887 - gender_output_loss: 0.6939 - image_quality_output_loss: 0.9571 - age_output_loss: 2.7230 - weight_output_loss: 2.5069 - bag_output_loss: 1.5411 - pose_output_loss: 1.6590 - footwear_output_loss: 1.2556 - emotion_output_loss: 2.6020 - gender_output_acc: 0.6648 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4135 - weight_output_acc: 0.6345 - bag_output_acc: 0.5795 - pose_output_acc: 0.6212 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7188\n",
            "100/100 [==============================] - 77s 767ms/step - loss: 46.3610 - gender_output_loss: 0.6934 - image_quality_output_loss: 0.9565 - age_output_loss: 2.7349 - weight_output_loss: 2.5076 - bag_output_loss: 1.5362 - pose_output_loss: 1.6599 - footwear_output_loss: 1.2547 - emotion_output_loss: 2.5866 - gender_output_acc: 0.6653 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4131 - weight_output_acc: 0.6350 - bag_output_acc: 0.5809 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5588 - emotion_output_acc: 0.7203 - val_loss: 24.4613 - val_gender_output_loss: 0.5955 - val_image_quality_output_loss: 0.9555 - val_age_output_loss: 1.3834 - val_weight_output_loss: 0.9715 - val_bag_output_loss: 0.8820 - val_pose_output_loss: 0.8858 - val_footwear_output_loss: 0.9098 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.6687 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5873 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00056: val_loss improved from 24.50602 to 24.46128, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.056.h5\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 76s 765ms/step - loss: 47.5174 - gender_output_loss: 0.7084 - image_quality_output_loss: 0.9534 - age_output_loss: 2.8700 - weight_output_loss: 2.4802 - bag_output_loss: 1.5334 - pose_output_loss: 1.7037 - footwear_output_loss: 1.2522 - emotion_output_loss: 2.7258 - gender_output_acc: 0.6484 - image_quality_output_acc: 0.5572 - age_output_acc: 0.3931 - weight_output_acc: 0.6419 - bag_output_acc: 0.5747 - pose_output_acc: 0.6075 - footwear_output_acc: 0.5641 - emotion_output_acc: 0.7028 - val_loss: 24.4760 - val_gender_output_loss: 0.5893 - val_image_quality_output_loss: 0.9469 - val_age_output_loss: 1.3892 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8857 - val_pose_output_loss: 0.8865 - val_footwear_output_loss: 0.9067 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4107 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5804 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5784 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 24.46128\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 79s 789ms/step - loss: 47.6293 - gender_output_loss: 0.7004 - image_quality_output_loss: 0.9498 - age_output_loss: 2.8541 - weight_output_loss: 2.6101 - bag_output_loss: 1.5326 - pose_output_loss: 1.7129 - footwear_output_loss: 1.2722 - emotion_output_loss: 2.6620 - gender_output_acc: 0.6553 - image_quality_output_acc: 0.5563 - age_output_acc: 0.3916 - weight_output_acc: 0.6316 - bag_output_acc: 0.5819 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5547 - emotion_output_acc: 0.7116 - val_loss: 24.5469 - val_gender_output_loss: 0.5928 - val_image_quality_output_loss: 0.9553 - val_age_output_loss: 1.3967 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.8892 - val_pose_output_loss: 0.8868 - val_footwear_output_loss: 0.9047 - val_emotion_output_loss: 0.9099 - val_gender_output_acc: 0.6677 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5709 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.5779 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 24.46128\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 46.7971 - gender_output_loss: 0.7097 - image_quality_output_loss: 0.9477 - age_output_loss: 2.9462 - weight_output_loss: 2.4091 - bag_output_loss: 1.5332 - pose_output_loss: 1.7096 - footwear_output_loss: 1.2582 - emotion_output_loss: 2.5183 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3950 - weight_output_acc: 0.6503 - bag_output_acc: 0.5800 - pose_output_acc: 0.6134 - footwear_output_acc: 0.5606 - emotion_output_acc: 0.7212 - val_loss: 24.7812 - val_gender_output_loss: 0.5962 - val_image_quality_output_loss: 0.9507 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9938 - val_bag_output_loss: 0.9016 - val_pose_output_loss: 0.8909 - val_footwear_output_loss: 0.9318 - val_emotion_output_loss: 0.9195 - val_gender_output_acc: 0.6741 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.5724 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5471 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 24.46128\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 77s 772ms/step - loss: 47.1081 - gender_output_loss: 0.7082 - image_quality_output_loss: 0.9508 - age_output_loss: 2.7431 - weight_output_loss: 2.5702 - bag_output_loss: 1.4973 - pose_output_loss: 1.7348 - footwear_output_loss: 1.2100 - emotion_output_loss: 2.7100 - gender_output_acc: 0.6416 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4059 - weight_output_acc: 0.6456 - bag_output_acc: 0.5828 - pose_output_acc: 0.6122 - footwear_output_acc: 0.5944 - emotion_output_acc: 0.7050 - val_loss: 25.4622 - val_gender_output_loss: 0.6361 - val_image_quality_output_loss: 0.9712 - val_age_output_loss: 1.4215 - val_weight_output_loss: 1.0290 - val_bag_output_loss: 0.9213 - val_pose_output_loss: 0.9041 - val_footwear_output_loss: 1.0326 - val_emotion_output_loss: 0.9460 - val_gender_output_acc: 0.6176 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3586 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.5640 - val_pose_output_acc: 0.6171 - val_footwear_output_acc: 0.4802 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 24.46128\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 77s 769ms/step - loss: 47.7827 - gender_output_loss: 0.7116 - image_quality_output_loss: 0.9565 - age_output_loss: 2.8014 - weight_output_loss: 2.6453 - bag_output_loss: 1.5555 - pose_output_loss: 1.6524 - footwear_output_loss: 1.2794 - emotion_output_loss: 2.7436 - gender_output_acc: 0.6363 - image_quality_output_acc: 0.5588 - age_output_acc: 0.3969 - weight_output_acc: 0.6238 - bag_output_acc: 0.5716 - pose_output_acc: 0.6238 - footwear_output_acc: 0.5588 - emotion_output_acc: 0.7066 - val_loss: 24.4538 - val_gender_output_loss: 0.5877 - val_image_quality_output_loss: 0.9506 - val_age_output_loss: 1.3832 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.8907 - val_pose_output_loss: 0.8855 - val_footwear_output_loss: 0.9029 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.6835 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5789 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5858 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00061: val_loss improved from 24.46128 to 24.45379, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.061.h5\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 78s 779ms/step - loss: 46.5477 - gender_output_loss: 0.6933 - image_quality_output_loss: 0.9497 - age_output_loss: 2.7910 - weight_output_loss: 2.5336 - bag_output_loss: 1.5428 - pose_output_loss: 1.6399 - footwear_output_loss: 1.2473 - emotion_output_loss: 2.5770 - gender_output_acc: 0.6616 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3916 - weight_output_acc: 0.6347 - bag_output_acc: 0.5828 - pose_output_acc: 0.6247 - footwear_output_acc: 0.5687 - emotion_output_acc: 0.7203 - val_loss: 24.4339 - val_gender_output_loss: 0.5915 - val_image_quality_output_loss: 0.9496 - val_age_output_loss: 1.3850 - val_weight_output_loss: 0.9703 - val_bag_output_loss: 0.8876 - val_pose_output_loss: 0.8860 - val_footwear_output_loss: 0.9007 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5799 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5833 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00062: val_loss improved from 24.45379 to 24.43388, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.062.h5\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 77s 765ms/step - loss: 47.0877 - gender_output_loss: 0.7015 - image_quality_output_loss: 0.9526 - age_output_loss: 2.8182 - weight_output_loss: 2.5093 - bag_output_loss: 1.5236 - pose_output_loss: 1.6888 - footwear_output_loss: 1.2542 - emotion_output_loss: 2.6721 - gender_output_acc: 0.6647 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3956 - weight_output_acc: 0.6403 - bag_output_acc: 0.5666 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5722 - emotion_output_acc: 0.7094 - val_loss: 24.3972 - val_gender_output_loss: 0.5813 - val_image_quality_output_loss: 0.9465 - val_age_output_loss: 1.3836 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8875 - val_pose_output_loss: 0.8820 - val_footwear_output_loss: 0.9022 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5818 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5744 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00063: val_loss improved from 24.43388 to 24.39721, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.063.h5\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 76s 762ms/step - loss: 47.4123 - gender_output_loss: 0.6975 - image_quality_output_loss: 0.9616 - age_output_loss: 2.8640 - weight_output_loss: 2.5632 - bag_output_loss: 1.5191 - pose_output_loss: 1.6839 - footwear_output_loss: 1.2356 - emotion_output_loss: 2.6810 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5438 - age_output_acc: 0.3969 - weight_output_acc: 0.6281 - bag_output_acc: 0.5787 - pose_output_acc: 0.6159 - footwear_output_acc: 0.5797 - emotion_output_acc: 0.7069 - val_loss: 24.4073 - val_gender_output_loss: 0.5878 - val_image_quality_output_loss: 0.9422 - val_age_output_loss: 1.3844 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8877 - val_pose_output_loss: 0.8775 - val_footwear_output_loss: 0.9049 - val_emotion_output_loss: 0.9063 - val_gender_output_acc: 0.6766 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5804 - val_pose_output_acc: 0.6215 - val_footwear_output_acc: 0.5799 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 24.39721\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 78s 778ms/step - loss: 46.1913 - gender_output_loss: 0.7082 - image_quality_output_loss: 0.9316 - age_output_loss: 2.8037 - weight_output_loss: 2.4755 - bag_output_loss: 1.4799 - pose_output_loss: 1.6953 - footwear_output_loss: 1.2278 - emotion_output_loss: 2.5362 - gender_output_acc: 0.6466 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4012 - weight_output_acc: 0.6484 - bag_output_acc: 0.5847 - pose_output_acc: 0.6134 - footwear_output_acc: 0.5691 - emotion_output_acc: 0.7181 - val_loss: 24.4154 - val_gender_output_loss: 0.5901 - val_image_quality_output_loss: 0.9450 - val_age_output_loss: 1.3874 - val_weight_output_loss: 0.9737 - val_bag_output_loss: 0.8866 - val_pose_output_loss: 0.8799 - val_footwear_output_loss: 0.9026 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.6791 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5848 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.5804 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 24.39721\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 76s 763ms/step - loss: 47.0156 - gender_output_loss: 0.7120 - image_quality_output_loss: 0.9572 - age_output_loss: 2.8553 - weight_output_loss: 2.5730 - bag_output_loss: 1.5028 - pose_output_loss: 1.6404 - footwear_output_loss: 1.2537 - emotion_output_loss: 2.6141 - gender_output_acc: 0.6453 - image_quality_output_acc: 0.5491 - age_output_acc: 0.4091 - weight_output_acc: 0.6306 - bag_output_acc: 0.5934 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5697 - emotion_output_acc: 0.7125 - val_loss: 24.4016 - val_gender_output_loss: 0.5878 - val_image_quality_output_loss: 0.9443 - val_age_output_loss: 1.3832 - val_weight_output_loss: 0.9727 - val_bag_output_loss: 0.8926 - val_pose_output_loss: 0.8812 - val_footwear_output_loss: 0.8992 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.6776 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5739 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.5794 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 24.39721\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 76s 762ms/step - loss: 46.8590 - gender_output_loss: 0.6973 - image_quality_output_loss: 0.9419 - age_output_loss: 2.8593 - weight_output_loss: 2.5314 - bag_output_loss: 1.5423 - pose_output_loss: 1.7194 - footwear_output_loss: 1.2469 - emotion_output_loss: 2.5320 - gender_output_acc: 0.6637 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3909 - weight_output_acc: 0.6412 - bag_output_acc: 0.5834 - pose_output_acc: 0.6147 - footwear_output_acc: 0.5716 - emotion_output_acc: 0.7191 - val_loss: 24.4913 - val_gender_output_loss: 0.5944 - val_image_quality_output_loss: 0.9491 - val_age_output_loss: 1.3945 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.8874 - val_pose_output_loss: 0.8876 - val_footwear_output_loss: 0.8992 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.6622 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5818 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5779 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 24.39721\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 76s 756ms/step - loss: 47.7140 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9415 - age_output_loss: 2.8378 - weight_output_loss: 2.6507 - bag_output_loss: 1.5338 - pose_output_loss: 1.6988 - footwear_output_loss: 1.2469 - emotion_output_loss: 2.7059 - gender_output_acc: 0.6744 - image_quality_output_acc: 0.5634 - age_output_acc: 0.3953 - weight_output_acc: 0.6319 - bag_output_acc: 0.5809 - pose_output_acc: 0.6134 - footwear_output_acc: 0.5622 - emotion_output_acc: 0.7103 - val_loss: 24.8497 - val_gender_output_loss: 0.6418 - val_image_quality_output_loss: 0.9460 - val_age_output_loss: 1.4187 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8997 - val_pose_output_loss: 0.9002 - val_footwear_output_loss: 0.9002 - val_emotion_output_loss: 0.9238 - val_gender_output_acc: 0.6012 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5675 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.5818 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 24.39721\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 78s 777ms/step - loss: 46.7478 - gender_output_loss: 0.7171 - image_quality_output_loss: 0.9587 - age_output_loss: 2.7446 - weight_output_loss: 2.5578 - bag_output_loss: 1.4920 - pose_output_loss: 1.7111 - footwear_output_loss: 1.2849 - emotion_output_loss: 2.6069 - gender_output_acc: 0.6459 - image_quality_output_acc: 0.5453 - age_output_acc: 0.3947 - weight_output_acc: 0.6412 - bag_output_acc: 0.5797 - pose_output_acc: 0.6103 - footwear_output_acc: 0.5600 - emotion_output_acc: 0.7141 - val_loss: 24.3586 - val_gender_output_loss: 0.5917 - val_image_quality_output_loss: 0.9445 - val_age_output_loss: 1.3835 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.8844 - val_pose_output_loss: 0.8766 - val_footwear_output_loss: 0.8973 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.6900 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.6200 - val_footwear_output_acc: 0.5833 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00069: val_loss improved from 24.39721 to 24.35860, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.069.h5\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 76s 763ms/step - loss: 46.8204 - gender_output_loss: 0.6944 - image_quality_output_loss: 0.9438 - age_output_loss: 2.8297 - weight_output_loss: 2.4605 - bag_output_loss: 1.5823 - pose_output_loss: 1.6433 - footwear_output_loss: 1.2403 - emotion_output_loss: 2.6375 - gender_output_acc: 0.6613 - image_quality_output_acc: 0.5575 - age_output_acc: 0.4091 - weight_output_acc: 0.6400 - bag_output_acc: 0.5741 - pose_output_acc: 0.6266 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7103 - val_loss: 24.4980 - val_gender_output_loss: 0.5980 - val_image_quality_output_loss: 0.9431 - val_age_output_loss: 1.3962 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8959 - val_pose_output_loss: 0.8819 - val_footwear_output_loss: 0.8992 - val_emotion_output_loss: 0.9106 - val_gender_output_acc: 0.6518 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5784 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5863 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 24.35860\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 46.9114 - gender_output_loss: 0.6985 - image_quality_output_loss: 0.9481 - age_output_loss: 2.8180 - weight_output_loss: 2.4767 - bag_output_loss: 1.5377 - pose_output_loss: 1.6836 - footwear_output_loss: 1.2477 - emotion_output_loss: 2.6556 - gender_output_acc: 0.6569 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3987 - weight_output_acc: 0.6384 - bag_output_acc: 0.5791 - pose_output_acc: 0.6116 - footwear_output_acc: 0.5691 - emotion_output_acc: 0.7097 - val_loss: 24.3846 - val_gender_output_loss: 0.5877 - val_image_quality_output_loss: 0.9460 - val_age_output_loss: 1.3857 - val_weight_output_loss: 0.9735 - val_bag_output_loss: 0.8890 - val_pose_output_loss: 0.8758 - val_footwear_output_loss: 0.8936 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.6711 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5744 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5938 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 24.35860\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 78s 784ms/step - loss: 46.9380 - gender_output_loss: 0.7021 - image_quality_output_loss: 0.9345 - age_output_loss: 2.7845 - weight_output_loss: 2.6579 - bag_output_loss: 1.4891 - pose_output_loss: 1.6612 - footwear_output_loss: 1.2185 - emotion_output_loss: 2.6327 - gender_output_acc: 0.6512 - image_quality_output_acc: 0.5691 - age_output_acc: 0.4041 - weight_output_acc: 0.6353 - bag_output_acc: 0.5806 - pose_output_acc: 0.6209 - footwear_output_acc: 0.5856 - emotion_output_acc: 0.7153 - val_loss: 24.3329 - val_gender_output_loss: 0.5799 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.3864 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.8808 - val_pose_output_loss: 0.8710 - val_footwear_output_loss: 0.8968 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.6905 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5878 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5923 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00072: val_loss improved from 24.35860 to 24.33290, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.072.h5\n",
            "Epoch 73/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 46.0883 - gender_output_loss: 0.6881 - image_quality_output_loss: 0.9494 - age_output_loss: 2.7635 - weight_output_loss: 2.4202 - bag_output_loss: 1.4775 - pose_output_loss: 1.6676 - footwear_output_loss: 1.2199 - emotion_output_loss: 2.6224 - gender_output_acc: 0.6692 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3930 - weight_output_acc: 0.6477 - bag_output_acc: 0.5966 - pose_output_acc: 0.6155 - footwear_output_acc: 0.5789 - emotion_output_acc: 0.7112\n",
            "Epoch 00072: val_loss improved from 24.35860 to 24.33290, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.072.h5\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 46.1371 - gender_output_loss: 0.6879 - image_quality_output_loss: 0.9492 - age_output_loss: 2.7640 - weight_output_loss: 2.4241 - bag_output_loss: 1.4864 - pose_output_loss: 1.6673 - footwear_output_loss: 1.2211 - emotion_output_loss: 2.6245 - gender_output_acc: 0.6697 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3931 - weight_output_acc: 0.6469 - bag_output_acc: 0.5959 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5784 - emotion_output_acc: 0.7113 - val_loss: 24.4377 - val_gender_output_loss: 0.5890 - val_image_quality_output_loss: 0.9476 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8872 - val_pose_output_loss: 0.8754 - val_footwear_output_loss: 0.9109 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.6731 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6230 - val_footwear_output_acc: 0.5749 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 24.33290\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 47.1390 - gender_output_loss: 0.7000 - image_quality_output_loss: 0.9220 - age_output_loss: 2.8308 - weight_output_loss: 2.6384 - bag_output_loss: 1.5465 - pose_output_loss: 1.6428 - footwear_output_loss: 1.2365 - emotion_output_loss: 2.6207 - gender_output_acc: 0.6584 - image_quality_output_acc: 0.5744 - age_output_acc: 0.3972 - weight_output_acc: 0.6294 - bag_output_acc: 0.5734 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5731 - emotion_output_acc: 0.7153 - val_loss: 24.3070 - val_gender_output_loss: 0.5786 - val_image_quality_output_loss: 0.9360 - val_age_output_loss: 1.3847 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8786 - val_pose_output_loss: 0.8743 - val_footwear_output_loss: 0.8943 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.6895 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4043 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.5853 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00074: val_loss improved from 24.33290 to 24.30698, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.074.h5\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 47.3614 - gender_output_loss: 0.7018 - image_quality_output_loss: 0.9594 - age_output_loss: 2.8629 - weight_output_loss: 2.5730 - bag_output_loss: 1.4938 - pose_output_loss: 1.6604 - footwear_output_loss: 1.2591 - emotion_output_loss: 2.6889 - gender_output_acc: 0.6472 - image_quality_output_acc: 0.5416 - age_output_acc: 0.4025 - weight_output_acc: 0.6341 - bag_output_acc: 0.5900 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5622 - emotion_output_acc: 0.7103 - val_loss: 24.6892 - val_gender_output_loss: 0.6084 - val_image_quality_output_loss: 0.9439 - val_age_output_loss: 1.4078 - val_weight_output_loss: 0.9972 - val_bag_output_loss: 0.8898 - val_pose_output_loss: 0.8896 - val_footwear_output_loss: 0.9119 - val_emotion_output_loss: 0.9172 - val_gender_output_acc: 0.6394 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.5764 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5769 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 24.30698\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 78s 781ms/step - loss: 46.5765 - gender_output_loss: 0.6877 - image_quality_output_loss: 0.9496 - age_output_loss: 2.7941 - weight_output_loss: 2.4561 - bag_output_loss: 1.5530 - pose_output_loss: 1.7327 - footwear_output_loss: 1.2343 - emotion_output_loss: 2.5753 - gender_output_acc: 0.6744 - image_quality_output_acc: 0.5572 - age_output_acc: 0.4069 - weight_output_acc: 0.6478 - bag_output_acc: 0.5759 - pose_output_acc: 0.6003 - footwear_output_acc: 0.5722 - emotion_output_acc: 0.7178 - val_loss: 24.4270 - val_gender_output_loss: 0.5882 - val_image_quality_output_loss: 0.9443 - val_age_output_loss: 1.3890 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8862 - val_pose_output_loss: 0.8904 - val_footwear_output_loss: 0.8967 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.6815 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5784 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.5888 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 24.30698\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 46.6235 - gender_output_loss: 0.6994 - image_quality_output_loss: 0.9383 - age_output_loss: 2.7955 - weight_output_loss: 2.5245 - bag_output_loss: 1.5080 - pose_output_loss: 1.6865 - footwear_output_loss: 1.2449 - emotion_output_loss: 2.5979 - gender_output_acc: 0.6569 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4003 - weight_output_acc: 0.6388 - bag_output_acc: 0.5781 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5634 - emotion_output_acc: 0.7159 - val_loss: 24.3794 - val_gender_output_loss: 0.5869 - val_image_quality_output_loss: 0.9374 - val_age_output_loss: 1.3919 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.8811 - val_pose_output_loss: 0.8890 - val_footwear_output_loss: 0.8916 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.6756 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5853 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.5898 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 24.30698\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 76s 756ms/step - loss: 47.3685 - gender_output_loss: 0.7098 - image_quality_output_loss: 0.9469 - age_output_loss: 2.8579 - weight_output_loss: 2.4850 - bag_output_loss: 1.5443 - pose_output_loss: 1.6587 - footwear_output_loss: 1.2800 - emotion_output_loss: 2.7183 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5613 - age_output_acc: 0.3925 - weight_output_acc: 0.6434 - bag_output_acc: 0.5853 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5475 - emotion_output_acc: 0.7031 - val_loss: 24.2783 - val_gender_output_loss: 0.5755 - val_image_quality_output_loss: 0.9396 - val_age_output_loss: 1.3817 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.8795 - val_pose_output_loss: 0.8789 - val_footwear_output_loss: 0.8830 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.6835 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5962 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00078: val_loss improved from 24.30698 to 24.27831, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.078.h5\n",
            "Epoch 79/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 47.0252 - gender_output_loss: 0.6939 - image_quality_output_loss: 0.9528 - age_output_loss: 2.7753 - weight_output_loss: 2.6490 - bag_output_loss: 1.4682 - pose_output_loss: 1.6532 - footwear_output_loss: 1.2551 - emotion_output_loss: 2.6711 - gender_output_acc: 0.6537 - image_quality_output_acc: 0.5473 - age_output_acc: 0.4066 - weight_output_acc: 0.6250 - bag_output_acc: 0.5944 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5631 - emotion_output_acc: 0.7099\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 47.1479 - gender_output_loss: 0.6921 - image_quality_output_loss: 0.9529 - age_output_loss: 2.7921 - weight_output_loss: 2.6524 - bag_output_loss: 1.4670 - pose_output_loss: 1.6535 - footwear_output_loss: 1.2565 - emotion_output_loss: 2.6832 - gender_output_acc: 0.6550 - image_quality_output_acc: 0.5469 - age_output_acc: 0.4056 - weight_output_acc: 0.6244 - bag_output_acc: 0.5938 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5622 - emotion_output_acc: 0.7084 - val_loss: 24.3035 - val_gender_output_loss: 0.5762 - val_image_quality_output_loss: 0.9381 - val_age_output_loss: 1.3843 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8789 - val_pose_output_loss: 0.8737 - val_footwear_output_loss: 0.8863 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.6825 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.6230 - val_footwear_output_acc: 0.5947 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 24.27831\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 75s 746ms/step - loss: 46.9850 - gender_output_loss: 0.6885 - image_quality_output_loss: 0.9416 - age_output_loss: 2.8796 - weight_output_loss: 2.5025 - bag_output_loss: 1.5563 - pose_output_loss: 1.6690 - footwear_output_loss: 1.2280 - emotion_output_loss: 2.6107 - gender_output_acc: 0.6731 - image_quality_output_acc: 0.5591 - age_output_acc: 0.4016 - weight_output_acc: 0.6384 - bag_output_acc: 0.5831 - pose_output_acc: 0.6222 - footwear_output_acc: 0.5834 - emotion_output_acc: 0.7128 - val_loss: 24.2859 - val_gender_output_loss: 0.5734 - val_image_quality_output_loss: 0.9367 - val_age_output_loss: 1.3859 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8835 - val_pose_output_loss: 0.8723 - val_footwear_output_loss: 0.8863 - val_emotion_output_loss: 0.9067 - val_gender_output_acc: 0.6840 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4043 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5823 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5947 - val_emotion_output_acc: 0.7078\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 24.27831\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 72s 722ms/step - loss: 46.3949 - gender_output_loss: 0.6874 - image_quality_output_loss: 0.9565 - age_output_loss: 2.8116 - weight_output_loss: 2.5345 - bag_output_loss: 1.5370 - pose_output_loss: 1.6535 - footwear_output_loss: 1.2400 - emotion_output_loss: 2.5205 - gender_output_acc: 0.6681 - image_quality_output_acc: 0.5459 - age_output_acc: 0.4050 - weight_output_acc: 0.6416 - bag_output_acc: 0.5687 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5644 - emotion_output_acc: 0.7272 - val_loss: 24.3162 - val_gender_output_loss: 0.5729 - val_image_quality_output_loss: 0.9523 - val_age_output_loss: 1.3830 - val_weight_output_loss: 0.9742 - val_bag_output_loss: 0.8816 - val_pose_output_loss: 0.8732 - val_footwear_output_loss: 0.8983 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.6820 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.6210 - val_footwear_output_acc: 0.5848 - val_emotion_output_acc: 0.7078\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 24.27831\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 72s 722ms/step - loss: 46.5497 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9465 - age_output_loss: 2.7513 - weight_output_loss: 2.5088 - bag_output_loss: 1.4761 - pose_output_loss: 1.7093 - footwear_output_loss: 1.2431 - emotion_output_loss: 2.6472 - gender_output_acc: 0.6700 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4000 - weight_output_acc: 0.6322 - bag_output_acc: 0.5891 - pose_output_acc: 0.6078 - footwear_output_acc: 0.5609 - emotion_output_acc: 0.7097 - val_loss: 24.2930 - val_gender_output_loss: 0.5766 - val_image_quality_output_loss: 0.9394 - val_age_output_loss: 1.3882 - val_weight_output_loss: 0.9751 - val_bag_output_loss: 0.8782 - val_pose_output_loss: 0.8740 - val_footwear_output_loss: 0.8827 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.6796 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5893 - val_pose_output_acc: 0.6245 - val_footwear_output_acc: 0.5997 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 24.27831\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 74s 740ms/step - loss: 47.4936 - gender_output_loss: 0.6962 - image_quality_output_loss: 0.9317 - age_output_loss: 2.8376 - weight_output_loss: 2.6587 - bag_output_loss: 1.5439 - pose_output_loss: 1.6614 - footwear_output_loss: 1.2540 - emotion_output_loss: 2.6663 - gender_output_acc: 0.6516 - image_quality_output_acc: 0.5716 - age_output_acc: 0.3884 - weight_output_acc: 0.6281 - bag_output_acc: 0.5794 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5647 - emotion_output_acc: 0.7062 - val_loss: 24.3277 - val_gender_output_loss: 0.5841 - val_image_quality_output_loss: 0.9463 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8830 - val_pose_output_loss: 0.8730 - val_footwear_output_loss: 0.8856 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.6736 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.4127 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5823 - val_pose_output_acc: 0.6250 - val_footwear_output_acc: 0.5992 - val_emotion_output_acc: 0.7078\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 24.27831\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 73s 726ms/step - loss: 47.1571 - gender_output_loss: 0.7047 - image_quality_output_loss: 0.9422 - age_output_loss: 2.7459 - weight_output_loss: 2.5786 - bag_output_loss: 1.5494 - pose_output_loss: 1.6258 - footwear_output_loss: 1.2423 - emotion_output_loss: 2.7534 - gender_output_acc: 0.6609 - image_quality_output_acc: 0.5634 - age_output_acc: 0.3941 - weight_output_acc: 0.6356 - bag_output_acc: 0.5784 - pose_output_acc: 0.6294 - footwear_output_acc: 0.5756 - emotion_output_acc: 0.7025 - val_loss: 24.6776 - val_gender_output_loss: 0.6038 - val_image_quality_output_loss: 0.9524 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.9017 - val_pose_output_loss: 0.8795 - val_footwear_output_loss: 0.9184 - val_emotion_output_loss: 0.9333 - val_gender_output_acc: 0.6533 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5645 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.5704 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 24.27831\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 24.27831\n",
            "Epoch 85/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 46.8463 - gender_output_loss: 0.7028 - image_quality_output_loss: 0.9448 - age_output_loss: 2.8794 - weight_output_loss: 2.3986 - bag_output_loss: 1.5185 - pose_output_loss: 1.6855 - footwear_output_loss: 1.2562 - emotion_output_loss: 2.6488 - gender_output_acc: 0.6578 - image_quality_output_acc: 0.5590 - age_output_acc: 0.3905 - weight_output_acc: 0.6493 - bag_output_acc: 0.5814 - pose_output_acc: 0.6121 - footwear_output_acc: 0.5574 - emotion_output_acc: 0.7153Epoch 85/100\n",
            "100/100 [==============================] - 72s 724ms/step - loss: 46.9383 - gender_output_loss: 0.7019 - image_quality_output_loss: 0.9442 - age_output_loss: 2.8999 - weight_output_loss: 2.4214 - bag_output_loss: 1.5140 - pose_output_loss: 1.6869 - footwear_output_loss: 1.2542 - emotion_output_loss: 2.6383 - gender_output_acc: 0.6591 - image_quality_output_acc: 0.5591 - age_output_acc: 0.3881 - weight_output_acc: 0.6475 - bag_output_acc: 0.5831 - pose_output_acc: 0.6112 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7159 - val_loss: 24.6622 - val_gender_output_loss: 0.6008 - val_image_quality_output_loss: 0.9481 - val_age_output_loss: 1.4145 - val_weight_output_loss: 0.9894 - val_bag_output_loss: 0.8953 - val_pose_output_loss: 0.8850 - val_footwear_output_loss: 0.9082 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.6448 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.3686 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5749 - val_pose_output_acc: 0.6200 - val_footwear_output_acc: 0.5749 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 24.27831\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 72s 716ms/step - loss: 46.5801 - gender_output_loss: 0.6955 - image_quality_output_loss: 0.9573 - age_output_loss: 2.7403 - weight_output_loss: 2.6222 - bag_output_loss: 1.4907 - pose_output_loss: 1.6855 - footwear_output_loss: 1.2370 - emotion_output_loss: 2.5820 - gender_output_acc: 0.6619 - image_quality_output_acc: 0.5450 - age_output_acc: 0.4088 - weight_output_acc: 0.6309 - bag_output_acc: 0.5894 - pose_output_acc: 0.6112 - footwear_output_acc: 0.5678 - emotion_output_acc: 0.7188 - val_loss: 24.3161 - val_gender_output_loss: 0.5765 - val_image_quality_output_loss: 0.9339 - val_age_output_loss: 1.3843 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8765 - val_pose_output_loss: 0.8693 - val_footwear_output_loss: 0.9097 - val_emotion_output_loss: 0.9107 - val_gender_output_acc: 0.6850 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5928 - val_pose_output_acc: 0.6195 - val_footwear_output_acc: 0.5818 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 24.27831\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 47.5699 - gender_output_loss: 0.6948 - image_quality_output_loss: 0.9487 - age_output_loss: 2.9150 - weight_output_loss: 2.5142 - bag_output_loss: 1.5303 - pose_output_loss: 1.6812 - footwear_output_loss: 1.2419 - emotion_output_loss: 2.7118 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5506 - age_output_acc: 0.3994 - weight_output_acc: 0.6397 - bag_output_acc: 0.5806 - pose_output_acc: 0.6109 - footwear_output_acc: 0.5756 - emotion_output_acc: 0.7031 - val_loss: 24.2883 - val_gender_output_loss: 0.5831 - val_image_quality_output_loss: 0.9440 - val_age_output_loss: 1.3874 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8811 - val_pose_output_loss: 0.8629 - val_footwear_output_loss: 0.8868 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.6766 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5858 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5928 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 24.27831\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 71s 711ms/step - loss: 45.8739 - gender_output_loss: 0.6935 - image_quality_output_loss: 0.9433 - age_output_loss: 2.7955 - weight_output_loss: 2.4547 - bag_output_loss: 1.5308 - pose_output_loss: 1.6679 - footwear_output_loss: 1.2183 - emotion_output_loss: 2.4769 - gender_output_acc: 0.6603 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3909 - weight_output_acc: 0.6475 - bag_output_acc: 0.5925 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5716 - emotion_output_acc: 0.7269 - val_loss: 24.3251 - val_gender_output_loss: 0.5865 - val_image_quality_output_loss: 0.9394 - val_age_output_loss: 1.3894 - val_weight_output_loss: 0.9775 - val_bag_output_loss: 0.8873 - val_pose_output_loss: 0.8634 - val_footwear_output_loss: 0.8973 - val_emotion_output_loss: 0.9056 - val_gender_output_acc: 0.6696 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5813 - val_pose_output_acc: 0.6245 - val_footwear_output_acc: 0.5838 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 24.27831\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 24.27831\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 46.6866 - gender_output_loss: 0.6849 - image_quality_output_loss: 0.9338 - age_output_loss: 2.8415 - weight_output_loss: 2.6533 - bag_output_loss: 1.5054 - pose_output_loss: 1.6671 - footwear_output_loss: 1.2058 - emotion_output_loss: 2.5201 - gender_output_acc: 0.6697 - image_quality_output_acc: 0.5644 - age_output_acc: 0.3963 - weight_output_acc: 0.6222 - bag_output_acc: 0.5884 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5894 - emotion_output_acc: 0.7191 - val_loss: 24.4044 - val_gender_output_loss: 0.5843 - val_image_quality_output_loss: 0.9387 - val_age_output_loss: 1.3978 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.8845 - val_pose_output_loss: 0.8693 - val_footwear_output_loss: 0.8929 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.6776 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.6319 - val_footwear_output_acc: 0.5933 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 24.27831\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 71s 714ms/step - loss: 46.7024 - gender_output_loss: 0.6937 - image_quality_output_loss: 0.9418 - age_output_loss: 2.7288 - weight_output_loss: 2.4587 - bag_output_loss: 1.4817 - pose_output_loss: 1.6244 - footwear_output_loss: 1.2507 - emotion_output_loss: 2.8018 - gender_output_acc: 0.6716 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4091 - weight_output_acc: 0.6488 - bag_output_acc: 0.5816 - pose_output_acc: 0.6253 - footwear_output_acc: 0.5616 - emotion_output_acc: 0.7013 - val_loss: 24.2738 - val_gender_output_loss: 0.5788 - val_image_quality_output_loss: 0.9350 - val_age_output_loss: 1.3896 - val_weight_output_loss: 0.9737 - val_bag_output_loss: 0.8837 - val_pose_output_loss: 0.8671 - val_footwear_output_loss: 0.8886 - val_emotion_output_loss: 0.9062 - val_gender_output_acc: 0.6711 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5779 - val_pose_output_acc: 0.6215 - val_footwear_output_acc: 0.5933 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00090: val_loss improved from 24.27831 to 24.27377, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.090.h5\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 70s 701ms/step - loss: 46.0242 - gender_output_loss: 0.6919 - image_quality_output_loss: 0.9497 - age_output_loss: 2.7387 - weight_output_loss: 2.3768 - bag_output_loss: 1.5333 - pose_output_loss: 1.6817 - footwear_output_loss: 1.2451 - emotion_output_loss: 2.6021 - gender_output_acc: 0.6703 - image_quality_output_acc: 0.5553 - age_output_acc: 0.4119 - weight_output_acc: 0.6553 - bag_output_acc: 0.5778 - pose_output_acc: 0.6141 - footwear_output_acc: 0.5800 - emotion_output_acc: 0.7141 - val_loss: 24.2998 - val_gender_output_loss: 0.5731 - val_image_quality_output_loss: 0.9346 - val_age_output_loss: 1.3821 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8828 - val_pose_output_loss: 0.8750 - val_footwear_output_loss: 0.8984 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.6835 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5784 - val_pose_output_acc: 0.6270 - val_footwear_output_acc: 0.5903 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 24.27377\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 46.8742 - gender_output_loss: 0.6954 - image_quality_output_loss: 0.9387 - age_output_loss: 2.8058 - weight_output_loss: 2.6762 - bag_output_loss: 1.4558 - pose_output_loss: 1.6875 - footwear_output_loss: 1.2547 - emotion_output_loss: 2.5761 - gender_output_acc: 0.6572 - image_quality_output_acc: 0.5603 - age_output_acc: 0.3994 - weight_output_acc: 0.6172 - bag_output_acc: 0.5906 - pose_output_acc: 0.6131 - footwear_output_acc: 0.5519 - emotion_output_acc: 0.7203 - val_loss: 24.3906 - val_gender_output_loss: 0.5775 - val_image_quality_output_loss: 0.9360 - val_age_output_loss: 1.3849 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.9004 - val_pose_output_loss: 0.8702 - val_footwear_output_loss: 0.9011 - val_emotion_output_loss: 0.9097 - val_gender_output_acc: 0.6930 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.5675 - val_pose_output_acc: 0.6255 - val_footwear_output_acc: 0.5779 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 24.27377\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 24.27377\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 46.9192 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9493 - age_output_loss: 2.8257 - weight_output_loss: 2.5173 - bag_output_loss: 1.5330 - pose_output_loss: 1.6368 - footwear_output_loss: 1.2195 - emotion_output_loss: 2.6845 - gender_output_acc: 0.6653 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3966 - weight_output_acc: 0.6428 - bag_output_acc: 0.5787 - pose_output_acc: 0.6209 - footwear_output_acc: 0.5869 - emotion_output_acc: 0.7075 - val_loss: 24.4445 - val_gender_output_loss: 0.6044 - val_image_quality_output_loss: 0.9451 - val_age_output_loss: 1.3880 - val_weight_output_loss: 0.9892 - val_bag_output_loss: 0.8985 - val_pose_output_loss: 0.8651 - val_footwear_output_loss: 0.8925 - val_emotion_output_loss: 0.9107 - val_gender_output_acc: 0.6558 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5680 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5942 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 24.27377\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 72s 722ms/step - loss: 47.9340 - gender_output_loss: 0.6848 - image_quality_output_loss: 0.9312 - age_output_loss: 2.9637 - weight_output_loss: 2.6857 - bag_output_loss: 1.5513 - pose_output_loss: 1.6506 - footwear_output_loss: 1.2348 - emotion_output_loss: 2.6522 - gender_output_acc: 0.6684 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3813 - weight_output_acc: 0.6281 - bag_output_acc: 0.5750 - pose_output_acc: 0.6213 - footwear_output_acc: 0.5741 - emotion_output_acc: 0.7113 - val_loss: 24.2657 - val_gender_output_loss: 0.5689 - val_image_quality_output_loss: 0.9352 - val_age_output_loss: 1.3884 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.8829 - val_pose_output_loss: 0.8586 - val_footwear_output_loss: 0.8957 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.6920 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5799 - val_pose_output_acc: 0.6305 - val_footwear_output_acc: 0.5883 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00094: val_loss improved from 24.27377 to 24.26571, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.094.h5\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 95/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 47.7518 - gender_output_loss: 0.6910 - image_quality_output_loss: 0.9566 - age_output_loss: 2.8644 - weight_output_loss: 2.5787 - bag_output_loss: 1.5212 - pose_output_loss: 1.6372 - footwear_output_loss: 1.2147 - emotion_output_loss: 2.8133 - gender_output_acc: 0.6622 - image_quality_output_acc: 0.5438 - age_output_acc: 0.4044 - weight_output_acc: 0.6322 - bag_output_acc: 0.5806 - pose_output_acc: 0.6213 - footwear_output_acc: 0.5822 - emotion_output_acc: 0.6994 - val_loss: 24.2749 - val_gender_output_loss: 0.5775 - val_image_quality_output_loss: 0.9431 - val_age_output_loss: 1.3914 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.8546 - val_footwear_output_loss: 0.8963 - val_emotion_output_loss: 0.9076 - val_gender_output_acc: 0.6756 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5769 - val_pose_output_acc: 0.6285 - val_footwear_output_acc: 0.5843 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 24.26571\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 45.9408 - gender_output_loss: 0.6803 - image_quality_output_loss: 0.9336 - age_output_loss: 2.7338 - weight_output_loss: 2.4351 - bag_output_loss: 1.4820 - pose_output_loss: 1.6533 - footwear_output_loss: 1.2309 - emotion_output_loss: 2.6250 - gender_output_acc: 0.6797 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4106 - weight_output_acc: 0.6447 - bag_output_acc: 0.5978 - pose_output_acc: 0.6203 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7156 - val_loss: 24.1491 - val_gender_output_loss: 0.5638 - val_image_quality_output_loss: 0.9388 - val_age_output_loss: 1.3813 - val_weight_output_loss: 0.9736 - val_bag_output_loss: 0.8801 - val_pose_output_loss: 0.8569 - val_footwear_output_loss: 0.8855 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7014 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6240 - val_footwear_output_acc: 0.5977 - val_emotion_output_acc: 0.7068\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: val_loss improved from 24.26571 to 24.14910, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.096.h5\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 72s 715ms/step - loss: 46.0030 - gender_output_loss: 0.6885 - image_quality_output_loss: 0.9407 - age_output_loss: 2.7535 - weight_output_loss: 2.4688 - bag_output_loss: 1.5381 - pose_output_loss: 1.6700 - footwear_output_loss: 1.2507 - emotion_output_loss: 2.5235 - gender_output_acc: 0.6669 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3903 - weight_output_acc: 0.6422 - bag_output_acc: 0.5766 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5759 - emotion_output_acc: 0.7216 - val_loss: 24.1528 - val_gender_output_loss: 0.5684 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.3829 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8814 - val_pose_output_loss: 0.8523 - val_footwear_output_loss: 0.8851 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.6875 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5838 - val_pose_output_acc: 0.6265 - val_footwear_output_acc: 0.5982 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 24.14910\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 74s 742ms/step - loss: 46.2445 - gender_output_loss: 0.6905 - image_quality_output_loss: 0.9509 - age_output_loss: 2.8625 - weight_output_loss: 2.4627 - bag_output_loss: 1.5374 - pose_output_loss: 1.6493 - footwear_output_loss: 1.2440 - emotion_output_loss: 2.4929 - gender_output_acc: 0.6678 - image_quality_output_acc: 0.5478 - age_output_acc: 0.3934 - weight_output_acc: 0.6437 - bag_output_acc: 0.5775 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5759 - emotion_output_acc: 0.7212 - val_loss: 24.2750 - val_gender_output_loss: 0.5808 - val_image_quality_output_loss: 0.9468 - val_age_output_loss: 1.3899 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.8855 - val_pose_output_loss: 0.8596 - val_footwear_output_loss: 0.8838 - val_emotion_output_loss: 0.9072 - val_gender_output_acc: 0.6776 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5794 - val_pose_output_acc: 0.6295 - val_footwear_output_acc: 0.5952 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 24.14910\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 74s 743ms/step - loss: 46.6285 - gender_output_loss: 0.6901 - image_quality_output_loss: 0.9492 - age_output_loss: 2.7387 - weight_output_loss: 2.5415 - bag_output_loss: 1.5271 - pose_output_loss: 1.6400 - footwear_output_loss: 1.2610 - emotion_output_loss: 2.6613 - gender_output_acc: 0.6622 - image_quality_output_acc: 0.5481 - age_output_acc: 0.4062 - weight_output_acc: 0.6344 - bag_output_acc: 0.5913 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5719 - emotion_output_acc: 0.7109 - val_loss: 24.4440 - val_gender_output_loss: 0.5754 - val_image_quality_output_loss: 0.9432 - val_age_output_loss: 1.3855 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8861 - val_pose_output_loss: 0.8704 - val_footwear_output_loss: 0.9497 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.6756 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5863 - val_pose_output_acc: 0.6270 - val_footwear_output_acc: 0.5273 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 24.14910\n",
            "Epoch 100/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 46.5453 - gender_output_loss: 0.7054 - image_quality_output_loss: 0.9325 - age_output_loss: 2.7912 - weight_output_loss: 2.5277 - bag_output_loss: 1.4899 - pose_output_loss: 1.7065 - footwear_output_loss: 1.2032 - emotion_output_loss: 2.6064 - gender_output_acc: 0.6585 - image_quality_output_acc: 0.5751 - age_output_acc: 0.4040 - weight_output_acc: 0.6376 - bag_output_acc: 0.6004 - pose_output_acc: 0.6092 - footwear_output_acc: 0.5770 - emotion_output_acc: 0.7156\n",
            "100/100 [==============================] - 75s 755ms/step - loss: 46.4630 - gender_output_loss: 0.7060 - image_quality_output_loss: 0.9321 - age_output_loss: 2.7848 - weight_output_loss: 2.5194 - bag_output_loss: 1.4883 - pose_output_loss: 1.7076 - footwear_output_loss: 1.2045 - emotion_output_loss: 2.5981 - gender_output_acc: 0.6572 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4047 - weight_output_acc: 0.6388 - bag_output_acc: 0.6006 - pose_output_acc: 0.6088 - footwear_output_acc: 0.5763 - emotion_output_acc: 0.7163 - val_loss: 24.9290 - val_gender_output_loss: 0.6146 - val_image_quality_output_loss: 0.9596 - val_age_output_loss: 1.4350 - val_weight_output_loss: 0.9948 - val_bag_output_loss: 0.9009 - val_pose_output_loss: 0.9123 - val_footwear_output_loss: 0.9291 - val_emotion_output_loss: 0.9148 - val_gender_output_acc: 0.6379 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3586 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.5734 - val_pose_output_acc: 0.6136 - val_footwear_output_acc: 0.5660 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 24.14910\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff03ffd7e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11dXe5JmI1I",
        "colab_type": "code",
        "outputId": "24a4f470-0898-427e-dd79-7587d32d2a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=200\n",
        "EPOCHS=50\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.096.h5')\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n",
            "Returning new callback array with steps_per_epoch= 200 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 50\n",
            "Backing up history file: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json  to: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json1577487087_backup\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 144s 721ms/step - loss: 46.5579 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9427 - age_output_loss: 2.7753 - weight_output_loss: 2.5206 - bag_output_loss: 1.5339 - pose_output_loss: 1.6517 - footwear_output_loss: 1.2282 - emotion_output_loss: 2.6303 - gender_output_acc: 0.6695 - image_quality_output_acc: 0.5573 - age_output_acc: 0.3981 - weight_output_acc: 0.6394 - bag_output_acc: 0.5812 - pose_output_acc: 0.6139 - footwear_output_acc: 0.5767 - emotion_output_acc: 0.7113 - val_loss: 24.2748 - val_gender_output_loss: 0.5787 - val_image_quality_output_loss: 0.9431 - val_age_output_loss: 1.3910 - val_weight_output_loss: 0.9800 - val_bag_output_loss: 0.8828 - val_pose_output_loss: 0.8586 - val_footwear_output_loss: 0.8957 - val_emotion_output_loss: 0.9020 - val_gender_output_acc: 0.6860 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.6275 - val_footwear_output_acc: 0.5863 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 24.27478, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.001.h5\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 135s 675ms/step - loss: 46.6248 - gender_output_loss: 0.6907 - image_quality_output_loss: 0.9384 - age_output_loss: 2.8486 - weight_output_loss: 2.5191 - bag_output_loss: 1.4933 - pose_output_loss: 1.6419 - footwear_output_loss: 1.2410 - emotion_output_loss: 2.6061 - gender_output_acc: 0.6666 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4014 - weight_output_acc: 0.6375 - bag_output_acc: 0.5902 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5739 - emotion_output_acc: 0.7147 - val_loss: 24.2265 - val_gender_output_loss: 0.5756 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3864 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8770 - val_pose_output_loss: 0.8564 - val_footwear_output_loss: 0.8910 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.6796 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.6290 - val_footwear_output_acc: 0.5987 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00002: val_loss improved from 24.27478 to 24.22654, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.002.h5\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 131s 656ms/step - loss: 46.2567 - gender_output_loss: 0.6900 - image_quality_output_loss: 0.9417 - age_output_loss: 2.7245 - weight_output_loss: 2.4976 - bag_output_loss: 1.5034 - pose_output_loss: 1.6355 - footwear_output_loss: 1.2267 - emotion_output_loss: 2.6580 - gender_output_acc: 0.6702 - image_quality_output_acc: 0.5558 - age_output_acc: 0.4102 - weight_output_acc: 0.6403 - bag_output_acc: 0.5870 - pose_output_acc: 0.6209 - footwear_output_acc: 0.5770 - emotion_output_acc: 0.7103 - val_loss: 24.2639 - val_gender_output_loss: 0.5757 - val_image_quality_output_loss: 0.9357 - val_age_output_loss: 1.3803 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.8786 - val_pose_output_loss: 0.8687 - val_footwear_output_loss: 0.8828 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.6939 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.6210 - val_footwear_output_acc: 0.5952 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 24.22654\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 133s 667ms/step - loss: 47.0284 - gender_output_loss: 0.6885 - image_quality_output_loss: 0.9449 - age_output_loss: 2.8996 - weight_output_loss: 2.5429 - bag_output_loss: 1.5188 - pose_output_loss: 1.6582 - footwear_output_loss: 1.2422 - emotion_output_loss: 2.6056 - gender_output_acc: 0.6605 - image_quality_output_acc: 0.5559 - age_output_acc: 0.3873 - weight_output_acc: 0.6380 - bag_output_acc: 0.5859 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5752 - emotion_output_acc: 0.7156 - val_loss: 24.2111 - val_gender_output_loss: 0.5859 - val_image_quality_output_loss: 0.9372 - val_age_output_loss: 1.3865 - val_weight_output_loss: 0.9700 - val_bag_output_loss: 0.8849 - val_pose_output_loss: 0.8568 - val_footwear_output_loss: 0.8905 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.6716 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.5928 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00004: val_loss improved from 24.22654 to 24.21105, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.004.h5\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 132s 659ms/step - loss: 46.9063 - gender_output_loss: 0.6871 - image_quality_output_loss: 0.9448 - age_output_loss: 2.7808 - weight_output_loss: 2.6273 - bag_output_loss: 1.5376 - pose_output_loss: 1.6442 - footwear_output_loss: 1.2418 - emotion_output_loss: 2.6290 - gender_output_acc: 0.6680 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3998 - weight_output_acc: 0.6294 - bag_output_acc: 0.5742 - pose_output_acc: 0.6184 - footwear_output_acc: 0.5709 - emotion_output_acc: 0.7111 - val_loss: 24.2410 - val_gender_output_loss: 0.5699 - val_image_quality_output_loss: 0.9414 - val_age_output_loss: 1.3885 - val_weight_output_loss: 0.9748 - val_bag_output_loss: 0.8858 - val_pose_output_loss: 0.8613 - val_footwear_output_loss: 0.8944 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.6880 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5863 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5823 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 24.21105\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 137s 686ms/step - loss: 46.7739 - gender_output_loss: 0.7012 - image_quality_output_loss: 0.9370 - age_output_loss: 2.8408 - weight_output_loss: 2.4731 - bag_output_loss: 1.4848 - pose_output_loss: 1.6641 - footwear_output_loss: 1.2385 - emotion_output_loss: 2.6756 - gender_output_acc: 0.6641 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4023 - weight_output_acc: 0.6439 - bag_output_acc: 0.5944 - pose_output_acc: 0.6127 - footwear_output_acc: 0.5780 - emotion_output_acc: 0.7108 - val_loss: 24.2353 - val_gender_output_loss: 0.5706 - val_image_quality_output_loss: 0.9341 - val_age_output_loss: 1.3861 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8873 - val_pose_output_loss: 0.8584 - val_footwear_output_loss: 0.8932 - val_emotion_output_loss: 0.9019 - val_gender_output_acc: 0.6840 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.5848 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.5908 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 24.21105\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 137s 683ms/step - loss: 46.5602 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9436 - age_output_loss: 2.7847 - weight_output_loss: 2.5480 - bag_output_loss: 1.5217 - pose_output_loss: 1.6367 - footwear_output_loss: 1.2133 - emotion_output_loss: 2.6325 - gender_output_acc: 0.6709 - image_quality_output_acc: 0.5567 - age_output_acc: 0.3994 - weight_output_acc: 0.6369 - bag_output_acc: 0.5820 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5811 - emotion_output_acc: 0.7108 - val_loss: 24.1043 - val_gender_output_loss: 0.5732 - val_image_quality_output_loss: 0.9300 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8804 - val_pose_output_loss: 0.8449 - val_footwear_output_loss: 0.8889 - val_emotion_output_loss: 0.8991 - val_gender_output_acc: 0.6875 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5858 - val_pose_output_acc: 0.6295 - val_footwear_output_acc: 0.5947 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00007: val_loss improved from 24.21105 to 24.10434, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.007.h5\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 139s 696ms/step - loss: 46.4098 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9331 - age_output_loss: 2.8455 - weight_output_loss: 2.5440 - bag_output_loss: 1.5054 - pose_output_loss: 1.6205 - footwear_output_loss: 1.2178 - emotion_output_loss: 2.5662 - gender_output_acc: 0.6678 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4044 - weight_output_acc: 0.6358 - bag_output_acc: 0.5916 - pose_output_acc: 0.6188 - footwear_output_acc: 0.5834 - emotion_output_acc: 0.7153 - val_loss: 24.0662 - val_gender_output_loss: 0.5667 - val_image_quality_output_loss: 0.9332 - val_age_output_loss: 1.3794 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8813 - val_pose_output_loss: 0.8436 - val_footwear_output_loss: 0.8812 - val_emotion_output_loss: 0.9014 - val_gender_output_acc: 0.6915 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5813 - val_pose_output_acc: 0.6300 - val_footwear_output_acc: 0.6032 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00008: val_loss improved from 24.10434 to 24.06619, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.008.h5\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 140s 698ms/step - loss: 46.4190 - gender_output_loss: 0.6754 - image_quality_output_loss: 0.9377 - age_output_loss: 2.7988 - weight_output_loss: 2.5031 - bag_output_loss: 1.5225 - pose_output_loss: 1.6112 - footwear_output_loss: 1.2297 - emotion_output_loss: 2.6358 - gender_output_acc: 0.6786 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3988 - weight_output_acc: 0.6402 - bag_output_acc: 0.5845 - pose_output_acc: 0.6209 - footwear_output_acc: 0.5766 - emotion_output_acc: 0.7131 - val_loss: 24.1163 - val_gender_output_loss: 0.5779 - val_image_quality_output_loss: 0.9364 - val_age_output_loss: 1.3798 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.8829 - val_pose_output_loss: 0.8443 - val_footwear_output_loss: 0.8843 - val_emotion_output_loss: 0.9012 - val_gender_output_acc: 0.6716 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5853 - val_pose_output_acc: 0.6310 - val_footwear_output_acc: 0.5967 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 24.06619\n",
            "Epoch 10/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 47.0422 - gender_output_loss: 0.6828 - image_quality_output_loss: 0.9320 - age_output_loss: 2.8314 - weight_output_loss: 2.5512 - bag_output_loss: 1.5332 - pose_output_loss: 1.6451 - footwear_output_loss: 1.2168 - emotion_output_loss: 2.6953 - gender_output_acc: 0.6735 - image_quality_output_acc: 0.5658 - age_output_acc: 0.3984 - weight_output_acc: 0.6340 - bag_output_acc: 0.5850 - pose_output_acc: 0.6140 - footwear_output_acc: 0.5807 - emotion_output_acc: 0.7060\n",
            "Epoch 00009: val_loss did not improve from 24.06619\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 139s 694ms/step - loss: 47.0452 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9321 - age_output_loss: 2.8322 - weight_output_loss: 2.5516 - bag_output_loss: 1.5362 - pose_output_loss: 1.6457 - footwear_output_loss: 1.2160 - emotion_output_loss: 2.6925 - gender_output_acc: 0.6733 - image_quality_output_acc: 0.5655 - age_output_acc: 0.3986 - weight_output_acc: 0.6338 - bag_output_acc: 0.5845 - pose_output_acc: 0.6139 - footwear_output_acc: 0.5809 - emotion_output_acc: 0.7061 - val_loss: 24.2565 - val_gender_output_loss: 0.5855 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3880 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8845 - val_pose_output_loss: 0.8712 - val_footwear_output_loss: 0.8824 - val_emotion_output_loss: 0.9018 - val_gender_output_acc: 0.6647 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.5838 - val_pose_output_acc: 0.6305 - val_footwear_output_acc: 0.5938 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 24.06619\n",
            "Epoch 11/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 45.6710 - gender_output_loss: 0.6900 - image_quality_output_loss: 0.9466 - age_output_loss: 2.7575 - weight_output_loss: 2.4888 - bag_output_loss: 1.4995 - pose_output_loss: 1.6204 - footwear_output_loss: 1.2414 - emotion_output_loss: 2.4944 - gender_output_acc: 0.6720 - image_quality_output_acc: 0.5485 - age_output_acc: 0.4028 - weight_output_acc: 0.6451 - bag_output_acc: 0.5961 - pose_output_acc: 0.6247 - footwear_output_acc: 0.5669 - emotion_output_acc: 0.7222\n",
            "200/200 [==============================] - 140s 700ms/step - loss: 45.6549 - gender_output_loss: 0.6898 - image_quality_output_loss: 0.9463 - age_output_loss: 2.7543 - weight_output_loss: 2.4891 - bag_output_loss: 1.5003 - pose_output_loss: 1.6231 - footwear_output_loss: 1.2427 - emotion_output_loss: 2.4904 - gender_output_acc: 0.6720 - image_quality_output_acc: 0.5489 - age_output_acc: 0.4028 - weight_output_acc: 0.6450 - bag_output_acc: 0.5961 - pose_output_acc: 0.6239 - footwear_output_acc: 0.5664 - emotion_output_acc: 0.7225 - val_loss: 24.3764 - val_gender_output_loss: 0.5753 - val_image_quality_output_loss: 0.9394 - val_age_output_loss: 1.3922 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.8930 - val_pose_output_loss: 0.8561 - val_footwear_output_loss: 0.9091 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.6855 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5774 - val_pose_output_acc: 0.6354 - val_footwear_output_acc: 0.5813 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 24.06619\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 138s 690ms/step - loss: 47.2694 - gender_output_loss: 0.6926 - image_quality_output_loss: 0.9429 - age_output_loss: 2.8549 - weight_output_loss: 2.5624 - bag_output_loss: 1.5002 - pose_output_loss: 1.6354 - footwear_output_loss: 1.2507 - emotion_output_loss: 2.7265 - gender_output_acc: 0.6630 - image_quality_output_acc: 0.5553 - age_output_acc: 0.3977 - weight_output_acc: 0.6381 - bag_output_acc: 0.5830 - pose_output_acc: 0.6180 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7084 - val_loss: 24.2064 - val_gender_output_loss: 0.5733 - val_image_quality_output_loss: 0.9337 - val_age_output_loss: 1.3858 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8818 - val_pose_output_loss: 0.8605 - val_footwear_output_loss: 0.8867 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.6905 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5868 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5928 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 24.06619\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 139s 696ms/step - loss: 46.0353 - gender_output_loss: 0.6828 - image_quality_output_loss: 0.9465 - age_output_loss: 2.7492 - weight_output_loss: 2.5061 - bag_output_loss: 1.5429 - pose_output_loss: 1.6321 - footwear_output_loss: 1.2127 - emotion_output_loss: 2.5595 - gender_output_acc: 0.6766 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4037 - weight_output_acc: 0.6386 - bag_output_acc: 0.5897 - pose_output_acc: 0.6172 - footwear_output_acc: 0.5777 - emotion_output_acc: 0.7147 - val_loss: 24.1273 - val_gender_output_loss: 0.5665 - val_image_quality_output_loss: 0.9357 - val_age_output_loss: 1.3862 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8850 - val_pose_output_loss: 0.8403 - val_footwear_output_loss: 0.8980 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.6979 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6285 - val_footwear_output_acc: 0.5789 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 24.06619\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 138s 691ms/step - loss: 46.6963 - gender_output_loss: 0.6860 - image_quality_output_loss: 0.9368 - age_output_loss: 2.8484 - weight_output_loss: 2.5529 - bag_output_loss: 1.4818 - pose_output_loss: 1.6576 - footwear_output_loss: 1.2296 - emotion_output_loss: 2.6129 - gender_output_acc: 0.6666 - image_quality_output_acc: 0.5559 - age_output_acc: 0.3930 - weight_output_acc: 0.6338 - bag_output_acc: 0.5923 - pose_output_acc: 0.6097 - footwear_output_acc: 0.5680 - emotion_output_acc: 0.7131 - val_loss: 24.3381 - val_gender_output_loss: 0.5807 - val_image_quality_output_loss: 0.9379 - val_age_output_loss: 1.3961 - val_weight_output_loss: 0.9994 - val_bag_output_loss: 0.8912 - val_pose_output_loss: 0.8412 - val_footwear_output_loss: 0.8968 - val_emotion_output_loss: 0.9149 - val_gender_output_acc: 0.6791 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.5789 - val_pose_output_acc: 0.6394 - val_footwear_output_acc: 0.5888 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 24.06619\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 141s 703ms/step - loss: 46.6461 - gender_output_loss: 0.6801 - image_quality_output_loss: 0.9315 - age_output_loss: 2.8087 - weight_output_loss: 2.5789 - bag_output_loss: 1.5160 - pose_output_loss: 1.5968 - footwear_output_loss: 1.2371 - emotion_output_loss: 2.6428 - gender_output_acc: 0.6683 - image_quality_output_acc: 0.5617 - age_output_acc: 0.3969 - weight_output_acc: 0.6328 - bag_output_acc: 0.5819 - pose_output_acc: 0.6269 - footwear_output_acc: 0.5744 - emotion_output_acc: 0.7119 - val_loss: 24.1442 - val_gender_output_loss: 0.5625 - val_image_quality_output_loss: 0.9462 - val_age_output_loss: 1.3832 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.8829 - val_pose_output_loss: 0.8342 - val_footwear_output_loss: 0.8930 - val_emotion_output_loss: 0.9031 - val_gender_output_acc: 0.6964 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5913 - val_pose_output_acc: 0.6384 - val_footwear_output_acc: 0.5952 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 24.06619\n",
            "200/200 [==============================]Epoch 16/50\n",
            "200/200 [==============================] - 140s 700ms/step - loss: 46.5881 - gender_output_loss: 0.6753 - image_quality_output_loss: 0.9264 - age_output_loss: 2.8145 - weight_output_loss: 2.5353 - bag_output_loss: 1.5243 - pose_output_loss: 1.5966 - footwear_output_loss: 1.2164 - emotion_output_loss: 2.6648 - gender_output_acc: 0.6816 - image_quality_output_acc: 0.5623 - age_output_acc: 0.3988 - weight_output_acc: 0.6405 - bag_output_acc: 0.5866 - pose_output_acc: 0.6230 - footwear_output_acc: 0.5795 - emotion_output_acc: 0.7087 - val_loss: 23.9624 - val_gender_output_loss: 0.5604 - val_image_quality_output_loss: 0.9252 - val_age_output_loss: 1.3801 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8814 - val_pose_output_loss: 0.8215 - val_footwear_output_loss: 0.8824 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.7014 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5848 - val_pose_output_acc: 0.6369 - val_footwear_output_acc: 0.5967 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00016: val_loss improved from 24.06619 to 23.96238, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.016.h5\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 141s 704ms/step - loss: 46.2803 - gender_output_loss: 0.6757 - image_quality_output_loss: 0.9395 - age_output_loss: 2.7931 - weight_output_loss: 2.4781 - bag_output_loss: 1.5193 - pose_output_loss: 1.5792 - footwear_output_loss: 1.2317 - emotion_output_loss: 2.6547 - gender_output_acc: 0.6825 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4081 - weight_output_acc: 0.6411 - bag_output_acc: 0.5869 - pose_output_acc: 0.6309 - footwear_output_acc: 0.5745 - emotion_output_acc: 0.7102 - val_loss: 24.0732 - val_gender_output_loss: 0.5606 - val_image_quality_output_loss: 0.9336 - val_age_output_loss: 1.3825 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8834 - val_pose_output_loss: 0.8275 - val_footwear_output_loss: 0.8914 - val_emotion_output_loss: 0.9056 - val_gender_output_acc: 0.7093 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6369 - val_footwear_output_acc: 0.5962 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 23.96238\n",
            "Epoch 18/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 45.8507 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9306 - age_output_loss: 2.7593 - weight_output_loss: 2.5134 - bag_output_loss: 1.4853 - pose_output_loss: 1.6300 - footwear_output_loss: 1.2176 - emotion_output_loss: 2.5515 - gender_output_acc: 0.6811 - image_quality_output_acc: 0.5664 - age_output_acc: 0.4062 - weight_output_acc: 0.6383 - bag_output_acc: 0.5903 - pose_output_acc: 0.6131 - footwear_output_acc: 0.5815 - emotion_output_acc: 0.7169\n",
            "Epoch 00017: val_loss did not improve from 23.96238Epoch 18/50\n",
            "200/200 [==============================] - 141s 703ms/step - loss: 45.8482 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9306 - age_output_loss: 2.7641 - weight_output_loss: 2.5125 - bag_output_loss: 1.4841 - pose_output_loss: 1.6287 - footwear_output_loss: 1.2161 - emotion_output_loss: 2.5494 - gender_output_acc: 0.6808 - image_quality_output_acc: 0.5658 - age_output_acc: 0.4055 - weight_output_acc: 0.6381 - bag_output_acc: 0.5905 - pose_output_acc: 0.6133 - footwear_output_acc: 0.5822 - emotion_output_acc: 0.7169 - val_loss: 24.0862 - val_gender_output_loss: 0.5721 - val_image_quality_output_loss: 0.9274 - val_age_output_loss: 1.3857 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8780 - val_pose_output_loss: 0.8451 - val_footwear_output_loss: 0.8873 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.6806 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.6389 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 23.96238\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 137s 683ms/step - loss: 46.6719 - gender_output_loss: 0.6782 - image_quality_output_loss: 0.9428 - age_output_loss: 2.8146 - weight_output_loss: 2.5566 - bag_output_loss: 1.5258 - pose_output_loss: 1.5889 - footwear_output_loss: 1.2164 - emotion_output_loss: 2.6658 - gender_output_acc: 0.6788 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4030 - weight_output_acc: 0.6362 - bag_output_acc: 0.5844 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5812 - emotion_output_acc: 0.7083 - val_loss: 24.2342 - val_gender_output_loss: 0.5579 - val_image_quality_output_loss: 0.9298 - val_age_output_loss: 1.3901 - val_weight_output_loss: 1.0067 - val_bag_output_loss: 0.8822 - val_pose_output_loss: 0.8509 - val_footwear_output_loss: 0.8953 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.7128 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.6334 - val_footwear_output_acc: 0.5833 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 23.96238\n",
            "Epoch 20/50\n",
            "\n",
            "200/200 [==============================] - 137s 683ms/step - loss: 46.3160 - gender_output_loss: 0.6801 - image_quality_output_loss: 0.9332 - age_output_loss: 2.8004 - weight_output_loss: 2.5168 - bag_output_loss: 1.5066 - pose_output_loss: 1.6648 - footwear_output_loss: 1.2265 - emotion_output_loss: 2.5778 - gender_output_acc: 0.6797 - image_quality_output_acc: 0.5639 - age_output_acc: 0.3994 - weight_output_acc: 0.6386 - bag_output_acc: 0.5908 - pose_output_acc: 0.6080 - footwear_output_acc: 0.5755 - emotion_output_acc: 0.7177 - val_loss: 24.3443 - val_gender_output_loss: 0.6072 - val_image_quality_output_loss: 0.9425 - val_age_output_loss: 1.4139 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8896 - val_pose_output_loss: 0.8430 - val_footwear_output_loss: 0.8773 - val_emotion_output_loss: 0.9108 - val_gender_output_acc: 0.6409 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5709 - val_pose_output_acc: 0.6434 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 23.96238\n",
            "Epoch 21/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 47.0127 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9368 - age_output_loss: 2.8572 - weight_output_loss: 2.5466 - bag_output_loss: 1.5287 - pose_output_loss: 1.6072 - footwear_output_loss: 1.2307 - emotion_output_loss: 2.6939 - gender_output_acc: 0.6732 - image_quality_output_acc: 0.5551 - age_output_acc: 0.3979 - weight_output_acc: 0.6382 - bag_output_acc: 0.5787 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5804 - emotion_output_acc: 0.7085\n",
            "200/200 [==============================] - 133s 667ms/step - loss: 46.9649 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9375 - age_output_loss: 2.8531 - weight_output_loss: 2.5442 - bag_output_loss: 1.5274 - pose_output_loss: 1.6071 - footwear_output_loss: 1.2295 - emotion_output_loss: 2.6894 - gender_output_acc: 0.6734 - image_quality_output_acc: 0.5547 - age_output_acc: 0.3983 - weight_output_acc: 0.6386 - bag_output_acc: 0.5791 - pose_output_acc: 0.6223 - footwear_output_acc: 0.5808 - emotion_output_acc: 0.7087 - val_loss: 24.1664 - val_gender_output_loss: 0.5833 - val_image_quality_output_loss: 0.9351 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8811 - val_pose_output_loss: 0.8323 - val_footwear_output_loss: 0.8901 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.6657 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5843 - val_pose_output_acc: 0.6285 - val_footwear_output_acc: 0.5878 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 23.96238\n",
            "Epoch 22/50\n",
            "199/200 [============================>.] - ETA: 0s - loss: 45.7930 - gender_output_loss: 0.6748 - image_quality_output_loss: 0.9334 - age_output_loss: 2.7615 - weight_output_loss: 2.5205 - bag_output_loss: 1.4679 - pose_output_loss: 1.5872 - footwear_output_loss: 1.2190 - emotion_output_loss: 2.5787 - gender_output_acc: 0.6782 - image_quality_output_acc: 0.5554 - age_output_acc: 0.4117 - weight_output_acc: 0.6390 - bag_output_acc: 0.5974 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5774 - emotion_output_acc: 0.7150\n",
            "200/200 [==============================] - 134s 670ms/step - loss: 45.8041 - gender_output_loss: 0.6750 - image_quality_output_loss: 0.9333 - age_output_loss: 2.7594 - weight_output_loss: 2.5183 - bag_output_loss: 1.4701 - pose_output_loss: 1.5856 - footwear_output_loss: 1.2185 - emotion_output_loss: 2.5849 - gender_output_acc: 0.6777 - image_quality_output_acc: 0.5555 - age_output_acc: 0.4119 - weight_output_acc: 0.6395 - bag_output_acc: 0.5973 - pose_output_acc: 0.6223 - footwear_output_acc: 0.5777 - emotion_output_acc: 0.7144 - val_loss: 24.0780 - val_gender_output_loss: 0.5666 - val_image_quality_output_loss: 0.9321 - val_age_output_loss: 1.3835 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8909 - val_pose_output_loss: 0.8265 - val_footwear_output_loss: 0.8901 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.6880 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5779 - val_pose_output_acc: 0.6354 - val_footwear_output_acc: 0.5843 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 23.96238\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 132s 658ms/step - loss: 46.3500 - gender_output_loss: 0.6784 - image_quality_output_loss: 0.9323 - age_output_loss: 2.7871 - weight_output_loss: 2.5582 - bag_output_loss: 1.5523 - pose_output_loss: 1.5964 - footwear_output_loss: 1.2259 - emotion_output_loss: 2.5899 - gender_output_acc: 0.6733 - image_quality_output_acc: 0.5569 - age_output_acc: 0.3961 - weight_output_acc: 0.6330 - bag_output_acc: 0.5873 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5780 - emotion_output_acc: 0.7152 - val_loss: 24.0153 - val_gender_output_loss: 0.5726 - val_image_quality_output_loss: 0.9435 - val_age_output_loss: 1.3811 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8855 - val_pose_output_loss: 0.8193 - val_footwear_output_loss: 0.8808 - val_emotion_output_loss: 0.9001 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5868 - val_pose_output_acc: 0.6389 - val_footwear_output_acc: 0.5992 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 23.96238\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 23.96238\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 131s 655ms/step - loss: 46.0635 - gender_output_loss: 0.6608 - image_quality_output_loss: 0.9405 - age_output_loss: 2.7858 - weight_output_loss: 2.4813 - bag_output_loss: 1.4777 - pose_output_loss: 1.5643 - footwear_output_loss: 1.2317 - emotion_output_loss: 2.6593 - gender_output_acc: 0.6842 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4027 - weight_output_acc: 0.6409 - bag_output_acc: 0.5994 - pose_output_acc: 0.6267 - footwear_output_acc: 0.5728 - emotion_output_acc: 0.7091 - val_loss: 23.8614 - val_gender_output_loss: 0.5587 - val_image_quality_output_loss: 0.9288 - val_age_output_loss: 1.3823 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8768 - val_pose_output_loss: 0.8037 - val_footwear_output_loss: 0.8741 - val_emotion_output_loss: 0.8966 - val_gender_output_acc: 0.6949 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5878 - val_pose_output_acc: 0.6424 - val_footwear_output_acc: 0.6052 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00024: val_loss improved from 23.96238 to 23.86140, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.024.h5\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 129s 644ms/step - loss: 45.9804 - gender_output_loss: 0.6739 - image_quality_output_loss: 0.9286 - age_output_loss: 2.7765 - weight_output_loss: 2.5400 - bag_output_loss: 1.5177 - pose_output_loss: 1.6043 - footwear_output_loss: 1.2027 - emotion_output_loss: 2.5578 - gender_output_acc: 0.6792 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4022 - weight_output_acc: 0.6350 - bag_output_acc: 0.5902 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5858 - emotion_output_acc: 0.7155 - val_loss: 23.9358 - val_gender_output_loss: 0.5662 - val_image_quality_output_loss: 0.9292 - val_age_output_loss: 1.3838 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.8779 - val_pose_output_loss: 0.8125 - val_footwear_output_loss: 0.8780 - val_emotion_output_loss: 0.8999 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.6518 - val_footwear_output_acc: 0.6007 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 23.86140\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 131s 653ms/step - loss: 46.2298 - gender_output_loss: 0.6772 - image_quality_output_loss: 0.9327 - age_output_loss: 2.8005 - weight_output_loss: 2.5411 - bag_output_loss: 1.4993 - pose_output_loss: 1.5963 - footwear_output_loss: 1.2246 - emotion_output_loss: 2.6009 - gender_output_acc: 0.6806 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3942 - weight_output_acc: 0.6387 - bag_output_acc: 0.5931 - pose_output_acc: 0.6212 - footwear_output_acc: 0.5755 - emotion_output_acc: 0.7123 - val_loss: 23.9896 - val_gender_output_loss: 0.5783 - val_image_quality_output_loss: 0.9454 - val_age_output_loss: 1.3850 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8826 - val_pose_output_loss: 0.7972 - val_footwear_output_loss: 0.8905 - val_emotion_output_loss: 0.9027 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6543 - val_footwear_output_acc: 0.5928 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 23.86140\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 132s 661ms/step - loss: 46.1419 - gender_output_loss: 0.6741 - image_quality_output_loss: 0.9319 - age_output_loss: 2.7954 - weight_output_loss: 2.5065 - bag_output_loss: 1.5029 - pose_output_loss: 1.5864 - footwear_output_loss: 1.2107 - emotion_output_loss: 2.6239 - gender_output_acc: 0.6792 - image_quality_output_acc: 0.5611 - age_output_acc: 0.4019 - weight_output_acc: 0.6403 - bag_output_acc: 0.5962 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5780 - emotion_output_acc: 0.7125 - val_loss: 24.4754 - val_gender_output_loss: 0.5877 - val_image_quality_output_loss: 0.9419 - val_age_output_loss: 1.4013 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.9107 - val_pose_output_loss: 0.8750 - val_footwear_output_loss: 0.8885 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.6647 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.5694 - val_pose_output_acc: 0.6300 - val_footwear_output_acc: 0.5923 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 23.86140\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 129s 643ms/step - loss: 46.6491 - gender_output_loss: 0.6717 - image_quality_output_loss: 0.9366 - age_output_loss: 2.7928 - weight_output_loss: 2.5684 - bag_output_loss: 1.5095 - pose_output_loss: 1.5799 - footwear_output_loss: 1.2286 - emotion_output_loss: 2.6977 - gender_output_acc: 0.6839 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3927 - weight_output_acc: 0.6400 - bag_output_acc: 0.5934 - pose_output_acc: 0.6255 - footwear_output_acc: 0.5759 - emotion_output_acc: 0.7058 - val_loss: 24.2312 - val_gender_output_loss: 0.5843 - val_image_quality_output_loss: 0.9336 - val_age_output_loss: 1.3911 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8921 - val_pose_output_loss: 0.8683 - val_footwear_output_loss: 0.8846 - val_emotion_output_loss: 0.9028 - val_gender_output_acc: 0.6791 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5685 - val_pose_output_acc: 0.6468 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 23.86140Epoch 28/50\n",
            "\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 129s 643ms/step - loss: 46.1370 - gender_output_loss: 0.6844 - image_quality_output_loss: 0.9371 - age_output_loss: 2.8420 - weight_output_loss: 2.5022 - bag_output_loss: 1.5252 - pose_output_loss: 1.5927 - footwear_output_loss: 1.2043 - emotion_output_loss: 2.5553 - gender_output_acc: 0.6697 - image_quality_output_acc: 0.5597 - age_output_acc: 0.4006 - weight_output_acc: 0.6383 - bag_output_acc: 0.5897 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5864 - emotion_output_acc: 0.7192 - val_loss: 23.9334 - val_gender_output_loss: 0.5585 - val_image_quality_output_loss: 0.9275 - val_age_output_loss: 1.3723 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.8937 - val_pose_output_loss: 0.8235 - val_footwear_output_loss: 0.8749 - val_emotion_output_loss: 0.8993 - val_gender_output_acc: 0.7034 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5789 - val_pose_output_acc: 0.6319 - val_footwear_output_acc: 0.6052 - val_emotion_output_acc: 0.7073\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 23.86140\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 126s 632ms/step - loss: 45.4114 - gender_output_loss: 0.6736 - image_quality_output_loss: 0.9314 - age_output_loss: 2.7324 - weight_output_loss: 2.4580 - bag_output_loss: 1.5092 - pose_output_loss: 1.5782 - footwear_output_loss: 1.2104 - emotion_output_loss: 2.5457 - gender_output_acc: 0.6827 - image_quality_output_acc: 0.5592 - age_output_acc: 0.4081 - weight_output_acc: 0.6409 - bag_output_acc: 0.5956 - pose_output_acc: 0.6289 - footwear_output_acc: 0.5789 - emotion_output_acc: 0.7184 - val_loss: 23.9189 - val_gender_output_loss: 0.5770 - val_image_quality_output_loss: 0.9378 - val_age_output_loss: 1.3803 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.8898 - val_pose_output_loss: 0.7903 - val_footwear_output_loss: 0.8788 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.6959 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5833 - val_pose_output_acc: 0.6503 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 23.86140\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 128s 639ms/step - loss: 46.8227 - gender_output_loss: 0.6799 - image_quality_output_loss: 0.9369 - age_output_loss: 2.8318 - weight_output_loss: 2.6132 - bag_output_loss: 1.5190 - pose_output_loss: 1.5752 - footwear_output_loss: 1.2284 - emotion_output_loss: 2.6633 - gender_output_acc: 0.6745 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3936 - weight_output_acc: 0.6303 - bag_output_acc: 0.5878 - pose_output_acc: 0.6228 - footwear_output_acc: 0.5802 - emotion_output_acc: 0.7081 - val_loss: 23.9031 - val_gender_output_loss: 0.5678 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3841 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.8876 - val_pose_output_loss: 0.7799 - val_footwear_output_loss: 0.8752 - val_emotion_output_loss: 0.9012 - val_gender_output_acc: 0.6900 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5784 - val_pose_output_acc: 0.6488 - val_footwear_output_acc: 0.6066 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 23.86140\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 127s 635ms/step - loss: 45.8042 - gender_output_loss: 0.6624 - image_quality_output_loss: 0.9262 - age_output_loss: 2.7370 - weight_output_loss: 2.5087 - bag_output_loss: 1.4844 - pose_output_loss: 1.5377 - footwear_output_loss: 1.2049 - emotion_output_loss: 2.6620 - gender_output_acc: 0.6902 - image_quality_output_acc: 0.5581 - age_output_acc: 0.4056 - weight_output_acc: 0.6384 - bag_output_acc: 0.5959 - pose_output_acc: 0.6298 - footwear_output_acc: 0.5895 - emotion_output_acc: 0.7086 - val_loss: 23.7552 - val_gender_output_loss: 0.5617 - val_image_quality_output_loss: 0.9205 - val_age_output_loss: 1.3768 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8823 - val_pose_output_loss: 0.7766 - val_footwear_output_loss: 0.8759 - val_emotion_output_loss: 0.8987 - val_gender_output_acc: 0.6949 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5848 - val_pose_output_acc: 0.6562 - val_footwear_output_acc: 0.6002 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00032: val_loss improved from 23.86140 to 23.75524, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.032.h5\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 128s 639ms/step - loss: 45.8023 - gender_output_loss: 0.6777 - image_quality_output_loss: 0.9244 - age_output_loss: 2.8538 - weight_output_loss: 2.4507 - bag_output_loss: 1.5146 - pose_output_loss: 1.5401 - footwear_output_loss: 1.2289 - emotion_output_loss: 2.5451 - gender_output_acc: 0.6850 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3958 - weight_output_acc: 0.6438 - bag_output_acc: 0.5898 - pose_output_acc: 0.6328 - footwear_output_acc: 0.5803 - emotion_output_acc: 0.7166 - val_loss: 23.7848 - val_gender_output_loss: 0.5642 - val_image_quality_output_loss: 0.9329 - val_age_output_loss: 1.3773 - val_weight_output_loss: 0.9737 - val_bag_output_loss: 0.8852 - val_pose_output_loss: 0.7742 - val_footwear_output_loss: 0.8802 - val_emotion_output_loss: 0.8984 - val_gender_output_acc: 0.6935 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5833 - val_pose_output_acc: 0.6617 - val_footwear_output_acc: 0.5967 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 23.75524\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 127s 636ms/step - loss: 46.4647 - gender_output_loss: 0.6557 - image_quality_output_loss: 0.9351 - age_output_loss: 2.8036 - weight_output_loss: 2.6318 - bag_output_loss: 1.4869 - pose_output_loss: 1.5857 - footwear_output_loss: 1.2143 - emotion_output_loss: 2.6249 - gender_output_acc: 0.6962 - image_quality_output_acc: 0.5458 - age_output_acc: 0.4019 - weight_output_acc: 0.6286 - bag_output_acc: 0.5962 - pose_output_acc: 0.6248 - footwear_output_acc: 0.5806 - emotion_output_acc: 0.7109 - val_loss: 23.8946 - val_gender_output_loss: 0.5822 - val_image_quality_output_loss: 0.9158 - val_age_output_loss: 1.3790 - val_weight_output_loss: 0.9717 - val_bag_output_loss: 0.8859 - val_pose_output_loss: 0.8092 - val_footwear_output_loss: 0.8719 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.6716 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5804 - val_pose_output_acc: 0.6572 - val_footwear_output_acc: 0.6047 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 23.75524\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 128s 641ms/step - loss: 46.5547 - gender_output_loss: 0.6620 - image_quality_output_loss: 0.9324 - age_output_loss: 2.8067 - weight_output_loss: 2.5591 - bag_output_loss: 1.4928 - pose_output_loss: 1.5673 - footwear_output_loss: 1.2029 - emotion_output_loss: 2.7127 - gender_output_acc: 0.6877 - image_quality_output_acc: 0.5597 - age_output_acc: 0.4067 - weight_output_acc: 0.6320 - bag_output_acc: 0.5839 - pose_output_acc: 0.6281 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7063 - val_loss: 24.0753 - val_gender_output_loss: 0.5822 - val_image_quality_output_loss: 0.9492 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.9044 - val_pose_output_loss: 0.7789 - val_footwear_output_loss: 0.8891 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.6825 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.5704 - val_pose_output_acc: 0.6548 - val_footwear_output_acc: 0.5997 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 23.75524\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 128s 642ms/step - loss: 45.6051 - gender_output_loss: 0.6782 - image_quality_output_loss: 0.9286 - age_output_loss: 2.7746 - weight_output_loss: 2.4554 - bag_output_loss: 1.5278 - pose_output_loss: 1.5619 - footwear_output_loss: 1.2280 - emotion_output_loss: 2.5450 - gender_output_acc: 0.6831 - image_quality_output_acc: 0.5606 - age_output_acc: 0.3998 - weight_output_acc: 0.6441 - bag_output_acc: 0.5884 - pose_output_acc: 0.6278 - footwear_output_acc: 0.5808 - emotion_output_acc: 0.7177 - val_loss: 24.2098 - val_gender_output_loss: 0.5765 - val_image_quality_output_loss: 0.9628 - val_age_output_loss: 1.3972 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8976 - val_pose_output_loss: 0.8037 - val_footwear_output_loss: 0.8966 - val_emotion_output_loss: 0.9175 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5228 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6438 - val_footwear_output_acc: 0.5789 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 23.75524\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 127s 635ms/step - loss: 46.1016 - gender_output_loss: 0.6687 - image_quality_output_loss: 0.9350 - age_output_loss: 2.7994 - weight_output_loss: 2.5491 - bag_output_loss: 1.5121 - pose_output_loss: 1.5400 - footwear_output_loss: 1.2181 - emotion_output_loss: 2.6097 - gender_output_acc: 0.6736 - image_quality_output_acc: 0.5567 - age_output_acc: 0.3958 - weight_output_acc: 0.6333 - bag_output_acc: 0.5920 - pose_output_acc: 0.6384 - footwear_output_acc: 0.5847 - emotion_output_acc: 0.7125 - val_loss: 23.8469 - val_gender_output_loss: 0.5587 - val_image_quality_output_loss: 0.9257 - val_age_output_loss: 1.3804 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8905 - val_pose_output_loss: 0.7958 - val_footwear_output_loss: 0.8731 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.7093 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5868 - val_pose_output_acc: 0.6572 - val_footwear_output_acc: 0.6081 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 23.75524\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 128s 640ms/step - loss: 46.3564 - gender_output_loss: 0.6685 - image_quality_output_loss: 0.9313 - age_output_loss: 2.8100 - weight_output_loss: 2.5706 - bag_output_loss: 1.5116 - pose_output_loss: 1.5411 - footwear_output_loss: 1.2188 - emotion_output_loss: 2.6488 - gender_output_acc: 0.6845 - image_quality_output_acc: 0.5627 - age_output_acc: 0.3973 - weight_output_acc: 0.6402 - bag_output_acc: 0.5917 - pose_output_acc: 0.6311 - footwear_output_acc: 0.5787 - emotion_output_acc: 0.7098 - val_loss: 24.0838 - val_gender_output_loss: 0.5481 - val_image_quality_output_loss: 0.9911 - val_age_output_loss: 1.3947 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.8837 - val_pose_output_loss: 0.7972 - val_footwear_output_loss: 0.8758 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.7088 - val_image_quality_output_acc: 0.5079 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5947 - val_pose_output_acc: 0.6518 - val_footwear_output_acc: 0.5977 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 23.75524\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 127s 637ms/step - loss: 45.4602 - gender_output_loss: 0.6508 - image_quality_output_loss: 0.9287 - age_output_loss: 2.7738 - weight_output_loss: 2.4595 - bag_output_loss: 1.4812 - pose_output_loss: 1.5098 - footwear_output_loss: 1.2078 - emotion_output_loss: 2.6066 - gender_output_acc: 0.7058 - image_quality_output_acc: 0.5586 - age_output_acc: 0.4006 - weight_output_acc: 0.6423 - bag_output_acc: 0.6003 - pose_output_acc: 0.6400 - footwear_output_acc: 0.5814 - emotion_output_acc: 0.7111 - val_loss: 23.7616 - val_gender_output_loss: 0.5460 - val_image_quality_output_loss: 0.9252 - val_age_output_loss: 1.3841 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.8815 - val_pose_output_loss: 0.7728 - val_footwear_output_loss: 0.8662 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.7158 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5962 - val_pose_output_acc: 0.6632 - val_footwear_output_acc: 0.6052 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 23.75524\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 128s 642ms/step - loss: 46.0068 - gender_output_loss: 0.6558 - image_quality_output_loss: 0.9170 - age_output_loss: 2.7581 - weight_output_loss: 2.6024 - bag_output_loss: 1.5208 - pose_output_loss: 1.4976 - footwear_output_loss: 1.2198 - emotion_output_loss: 2.6289 - gender_output_acc: 0.6898 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4003 - weight_output_acc: 0.6283 - bag_output_acc: 0.5969 - pose_output_acc: 0.6448 - footwear_output_acc: 0.5802 - emotion_output_acc: 0.7103 - val_loss: 23.6866 - val_gender_output_loss: 0.5469 - val_image_quality_output_loss: 0.9271 - val_age_output_loss: 1.3778 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8830 - val_pose_output_loss: 0.7576 - val_footwear_output_loss: 0.8687 - val_emotion_output_loss: 0.9027 - val_gender_output_acc: 0.7118 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.6637 - val_footwear_output_acc: 0.6126 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00040: val_loss improved from 23.75524 to 23.68665, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.040.h5\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 130s 650ms/step - loss: 45.5661 - gender_output_loss: 0.6562 - image_quality_output_loss: 0.9287 - age_output_loss: 2.7808 - weight_output_loss: 2.5088 - bag_output_loss: 1.4881 - pose_output_loss: 1.5128 - footwear_output_loss: 1.2058 - emotion_output_loss: 2.5804 - gender_output_acc: 0.6928 - image_quality_output_acc: 0.5595 - age_output_acc: 0.3986 - weight_output_acc: 0.6370 - bag_output_acc: 0.5938 - pose_output_acc: 0.6384 - footwear_output_acc: 0.5867 - emotion_output_acc: 0.7158 - val_loss: 23.7352 - val_gender_output_loss: 0.5520 - val_image_quality_output_loss: 0.9218 - val_age_output_loss: 1.3824 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.8938 - val_pose_output_loss: 0.7630 - val_footwear_output_loss: 0.8646 - val_emotion_output_loss: 0.8986 - val_gender_output_acc: 0.7103 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.5833 - val_pose_output_acc: 0.6716 - val_footwear_output_acc: 0.6071 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00040: val_loss improved from 23.75524 to 23.68665, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.040.h5\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 23.68665\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 135s 675ms/step - loss: 45.6508 - gender_output_loss: 0.6556 - image_quality_output_loss: 0.9344 - age_output_loss: 2.8255 - weight_output_loss: 2.4722 - bag_output_loss: 1.4862 - pose_output_loss: 1.5152 - footwear_output_loss: 1.2007 - emotion_output_loss: 2.5842 - gender_output_acc: 0.7020 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4042 - weight_output_acc: 0.6453 - bag_output_acc: 0.6014 - pose_output_acc: 0.6345 - footwear_output_acc: 0.5867 - emotion_output_acc: 0.7144 - val_loss: 24.0769 - val_gender_output_loss: 0.5635 - val_image_quality_output_loss: 0.9158 - val_age_output_loss: 1.3831 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.8880 - val_pose_output_loss: 0.8524 - val_footwear_output_loss: 0.8769 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.6979 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5818 - val_pose_output_acc: 0.6200 - val_footwear_output_acc: 0.6062 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 23.68665\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 132s 658ms/step - loss: 45.7190 - gender_output_loss: 0.6498 - image_quality_output_loss: 0.9263 - age_output_loss: 2.7474 - weight_output_loss: 2.5466 - bag_output_loss: 1.5035 - pose_output_loss: 1.5351 - footwear_output_loss: 1.2142 - emotion_output_loss: 2.5964 - gender_output_acc: 0.6978 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3973 - weight_output_acc: 0.6373 - bag_output_acc: 0.5925 - pose_output_acc: 0.6402 - footwear_output_acc: 0.5830 - emotion_output_acc: 0.7159 - val_loss: 24.0205 - val_gender_output_loss: 0.5758 - val_image_quality_output_loss: 0.9391 - val_age_output_loss: 1.4015 - val_weight_output_loss: 1.0047 - val_bag_output_loss: 0.8932 - val_pose_output_loss: 0.7512 - val_footwear_output_loss: 0.9017 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.6964 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.5813 - val_pose_output_acc: 0.6706 - val_footwear_output_acc: 0.5928 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 23.68665\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 133s 664ms/step - loss: 45.6665 - gender_output_loss: 0.6649 - image_quality_output_loss: 0.9293 - age_output_loss: 2.8038 - weight_output_loss: 2.5357 - bag_output_loss: 1.4756 - pose_output_loss: 1.5104 - footwear_output_loss: 1.2234 - emotion_output_loss: 2.5618 - gender_output_acc: 0.6855 - image_quality_output_acc: 0.5586 - age_output_acc: 0.3961 - weight_output_acc: 0.6366 - bag_output_acc: 0.5962 - pose_output_acc: 0.6366 - footwear_output_acc: 0.5800 - emotion_output_acc: 0.7170 - val_loss: 24.0806 - val_gender_output_loss: 0.5879 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.3802 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8871 - val_pose_output_loss: 0.7917 - val_footwear_output_loss: 0.9161 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.6959 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5828 - val_pose_output_acc: 0.6434 - val_footwear_output_acc: 0.5848 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 23.68665\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 132s 661ms/step - loss: 46.2608 - gender_output_loss: 0.6615 - image_quality_output_loss: 0.9401 - age_output_loss: 2.7921 - weight_output_loss: 2.5080 - bag_output_loss: 1.5505 - pose_output_loss: 1.4920 - footwear_output_loss: 1.2208 - emotion_output_loss: 2.6991 - gender_output_acc: 0.6889 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4028 - weight_output_acc: 0.6367 - bag_output_acc: 0.5800 - pose_output_acc: 0.6486 - footwear_output_acc: 0.5816 - emotion_output_acc: 0.7042 - val_loss: 24.5210 - val_gender_output_loss: 0.5875 - val_image_quality_output_loss: 0.9932 - val_age_output_loss: 1.4174 - val_weight_output_loss: 1.0123 - val_bag_output_loss: 0.9065 - val_pose_output_loss: 0.7845 - val_footwear_output_loss: 0.9029 - val_emotion_output_loss: 0.9441 - val_gender_output_acc: 0.6652 - val_image_quality_output_acc: 0.5144 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.5923 - val_bag_output_acc: 0.5764 - val_pose_output_acc: 0.6647 - val_footwear_output_acc: 0.5888 - val_emotion_output_acc: 0.6791\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 23.68665\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 131s 656ms/step - loss: 46.0166 - gender_output_loss: 0.6574 - image_quality_output_loss: 0.9292 - age_output_loss: 2.8064 - weight_output_loss: 2.5857 - bag_output_loss: 1.5569 - pose_output_loss: 1.4769 - footwear_output_loss: 1.2094 - emotion_output_loss: 2.5859 - gender_output_acc: 0.6869 - image_quality_output_acc: 0.5589 - age_output_acc: 0.3980 - weight_output_acc: 0.6316 - bag_output_acc: 0.5797 - pose_output_acc: 0.6497 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7144 - val_loss: 24.0205 - val_gender_output_loss: 0.5789 - val_image_quality_output_loss: 0.9357 - val_age_output_loss: 1.4025 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.8949 - val_pose_output_loss: 0.7746 - val_footwear_output_loss: 0.8732 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.5952 - val_bag_output_acc: 0.5833 - val_pose_output_acc: 0.6622 - val_footwear_output_acc: 0.6066 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 23.68665\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 133s 664ms/step - loss: 45.3762 - gender_output_loss: 0.6573 - image_quality_output_loss: 0.9277 - age_output_loss: 2.7616 - weight_output_loss: 2.4239 - bag_output_loss: 1.4432 - pose_output_loss: 1.4850 - footwear_output_loss: 1.2158 - emotion_output_loss: 2.6693 - gender_output_acc: 0.6858 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4048 - weight_output_acc: 0.6472 - bag_output_acc: 0.5975 - pose_output_acc: 0.6494 - footwear_output_acc: 0.5784 - emotion_output_acc: 0.7053 - val_loss: 23.7145 - val_gender_output_loss: 0.5436 - val_image_quality_output_loss: 0.9351 - val_age_output_loss: 1.3822 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.8868 - val_pose_output_loss: 0.7455 - val_footwear_output_loss: 0.8934 - val_emotion_output_loss: 0.9023 - val_gender_output_acc: 0.7217 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.6701 - val_footwear_output_acc: 0.5938 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 23.68665\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 132s 659ms/step - loss: 45.4673 - gender_output_loss: 0.6407 - image_quality_output_loss: 0.9214 - age_output_loss: 2.7555 - weight_output_loss: 2.6037 - bag_output_loss: 1.5194 - pose_output_loss: 1.4922 - footwear_output_loss: 1.2077 - emotion_output_loss: 2.5166 - gender_output_acc: 0.7006 - image_quality_output_acc: 0.5655 - age_output_acc: 0.4030 - weight_output_acc: 0.6328 - bag_output_acc: 0.5962 - pose_output_acc: 0.6517 - footwear_output_acc: 0.5886 - emotion_output_acc: 0.7178 - val_loss: 23.6800 - val_gender_output_loss: 0.5411 - val_image_quality_output_loss: 0.9402 - val_age_output_loss: 1.3807 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8773 - val_pose_output_loss: 0.7440 - val_footwear_output_loss: 0.8745 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.7257 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5918 - val_pose_output_acc: 0.6716 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00048: val_loss improved from 23.68665 to 23.68002, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.048.h5\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 23.68665Epoch 49/50\n",
            "200/200 [==============================] - 131s 656ms/step - loss: 45.0653 - gender_output_loss: 0.6438 - image_quality_output_loss: 0.9388 - age_output_loss: 2.8004 - weight_output_loss: 2.4221 - bag_output_loss: 1.4761 - pose_output_loss: 1.4095 - footwear_output_loss: 1.2030 - emotion_output_loss: 2.5941 - gender_output_acc: 0.7041 - image_quality_output_acc: 0.5492 - age_output_acc: 0.4041 - weight_output_acc: 0.6439 - bag_output_acc: 0.5950 - pose_output_acc: 0.6650 - footwear_output_acc: 0.5872 - emotion_output_acc: 0.7136 - val_loss: 23.5866 - val_gender_output_loss: 0.5449 - val_image_quality_output_loss: 0.9337 - val_age_output_loss: 1.3780 - val_weight_output_loss: 0.9730 - val_bag_output_loss: 0.8795 - val_pose_output_loss: 0.7405 - val_footwear_output_loss: 0.8665 - val_emotion_output_loss: 0.9032 - val_gender_output_acc: 0.7138 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5893 - val_pose_output_acc: 0.6756 - val_footwear_output_acc: 0.6086 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00049: val_loss improved from 23.68002 to 23.58658, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.049.h5\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 130s 648ms/step - loss: 45.6067 - gender_output_loss: 0.6411 - image_quality_output_loss: 0.9238 - age_output_loss: 2.8266 - weight_output_loss: 2.5498 - bag_output_loss: 1.5037 - pose_output_loss: 1.4549 - footwear_output_loss: 1.2083 - emotion_output_loss: 2.5592 - gender_output_acc: 0.7067 - image_quality_output_acc: 0.5608 - age_output_acc: 0.3988 - weight_output_acc: 0.6325 - bag_output_acc: 0.5927 - pose_output_acc: 0.6583 - footwear_output_acc: 0.5903 - emotion_output_acc: 0.7148 - val_loss: 23.7329 - val_gender_output_loss: 0.5571 - val_image_quality_output_loss: 0.9254 - val_age_output_loss: 1.3800 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.8841 - val_pose_output_loss: 0.7638 - val_footwear_output_loss: 0.8659 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.7078 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.5933 - val_pose_output_acc: 0.6582 - val_footwear_output_acc: 0.6047 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 23.58658\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577472697.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff03ff98c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5umiyWoNL91y",
        "colab_type": "code",
        "outputId": "aa1fefbe-d807-4fff-b3d4-df8111e8b120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_df.shape[0]//BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzs8k2MswOUC",
        "outputId": "f2f7278d-231f-4e87-a931-123e0568e5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=50\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577472697rd2_model.049.h5')\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    #steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577514347.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "360/360 [==============================] - 235s 653ms/step - loss: 45.3164 - gender_output_loss: 0.6390 - image_quality_output_loss: 0.9254 - age_output_loss: 2.7757 - weight_output_loss: 2.5116 - bag_output_loss: 1.4884 - pose_output_loss: 1.4378 - footwear_output_loss: 1.2043 - emotion_output_loss: 2.5927 - gender_output_acc: 0.7056 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4026 - weight_output_acc: 0.6379 - bag_output_acc: 0.5951 - pose_output_acc: 0.6609 - footwear_output_acc: 0.5877 - emotion_output_acc: 0.7123 - val_loss: 23.6241 - val_gender_output_loss: 0.5493 - val_image_quality_output_loss: 0.9352 - val_age_output_loss: 1.3759 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.8851 - val_pose_output_loss: 0.7361 - val_footwear_output_loss: 0.8728 - val_emotion_output_loss: 0.9020 - val_gender_output_acc: 0.7138 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.5898 - val_pose_output_acc: 0.6741 - val_footwear_output_acc: 0.6076 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 23.62410, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.001.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 211s 587ms/step - loss: 45.5469 - gender_output_loss: 0.6397 - image_quality_output_loss: 0.9290 - age_output_loss: 2.7848 - weight_output_loss: 2.5195 - bag_output_loss: 1.4963 - pose_output_loss: 1.4633 - footwear_output_loss: 1.2040 - emotion_output_loss: 2.6086 - gender_output_acc: 0.7050 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4031 - weight_output_acc: 0.6381 - bag_output_acc: 0.5951 - pose_output_acc: 0.6530 - footwear_output_acc: 0.5895 - emotion_output_acc: 0.7121 - val_loss: 23.5129 - val_gender_output_loss: 0.5461 - val_image_quality_output_loss: 0.9108 - val_age_output_loss: 1.3750 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.8779 - val_pose_output_loss: 0.7441 - val_footwear_output_loss: 0.8560 - val_emotion_output_loss: 0.8993 - val_gender_output_acc: 0.7163 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.5858 - val_pose_output_acc: 0.6721 - val_footwear_output_acc: 0.6190 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00002: val_loss improved from 23.62410 to 23.51295, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.002.h5\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 45.6130 - gender_output_loss: 0.6501 - image_quality_output_loss: 0.9310 - age_output_loss: 2.7871 - weight_output_loss: 2.5223 - bag_output_loss: 1.5061 - pose_output_loss: 1.4659 - footwear_output_loss: 1.2103 - emotion_output_loss: 2.6033 - gender_output_acc: 0.6982 - image_quality_output_acc: 0.5582 - age_output_acc: 0.4027 - weight_output_acc: 0.6376 - bag_output_acc: 0.5918 - pose_output_acc: 0.6557 - footwear_output_acc: 0.5835 - emotion_output_acc: 0.7122 - val_loss: 23.5946 - val_gender_output_loss: 0.5474 - val_image_quality_output_loss: 0.9457 - val_age_output_loss: 1.3762 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.8852 - val_pose_output_loss: 0.7250 - val_footwear_output_loss: 0.8557 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.7138 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.5883 - val_pose_output_acc: 0.6791 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 23.51295\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 45.7042 - gender_output_loss: 0.6523 - image_quality_output_loss: 0.9303 - age_output_loss: 2.7893 - weight_output_loss: 2.5356 - bag_output_loss: 1.4965 - pose_output_loss: 1.4761 - footwear_output_loss: 1.2111 - emotion_output_loss: 2.6136 - gender_output_acc: 0.6957 - image_quality_output_acc: 0.5589 - age_output_acc: 0.4023 - weight_output_acc: 0.6396 - bag_output_acc: 0.5968 - pose_output_acc: 0.6555 - footwear_output_acc: 0.5855 - emotion_output_acc: 0.7115 - val_loss: 24.2977 - val_gender_output_loss: 0.6360 - val_image_quality_output_loss: 0.9244 - val_age_output_loss: 1.3951 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.9485 - val_pose_output_loss: 0.7902 - val_footwear_output_loss: 0.8849 - val_emotion_output_loss: 0.9225 - val_gender_output_acc: 0.6577 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5660 - val_pose_output_acc: 0.6453 - val_footwear_output_acc: 0.6062 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 23.51295\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 216s 599ms/step - loss: 45.6279 - gender_output_loss: 0.6464 - image_quality_output_loss: 0.9319 - age_output_loss: 2.7912 - weight_output_loss: 2.5246 - bag_output_loss: 1.5037 - pose_output_loss: 1.4590 - footwear_output_loss: 1.2110 - emotion_output_loss: 2.6121 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.5600 - age_output_acc: 0.3944 - weight_output_acc: 0.6367 - bag_output_acc: 0.5906 - pose_output_acc: 0.6585 - footwear_output_acc: 0.5848 - emotion_output_acc: 0.7128 - val_loss: 23.7865 - val_gender_output_loss: 0.5405 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.3844 - val_weight_output_loss: 1.0052 - val_bag_output_loss: 0.8799 - val_pose_output_loss: 0.7266 - val_footwear_output_loss: 0.8741 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.7098 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.6880 - val_footwear_output_acc: 0.6032 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 23.51295\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 45.3710 - gender_output_loss: 0.6388 - image_quality_output_loss: 0.9262 - age_output_loss: 2.7811 - weight_output_loss: 2.5255 - bag_output_loss: 1.4955 - pose_output_loss: 1.4180 - footwear_output_loss: 1.2062 - emotion_output_loss: 2.6046 - gender_output_acc: 0.7041 - image_quality_output_acc: 0.5595 - age_output_acc: 0.4005 - weight_output_acc: 0.6372 - bag_output_acc: 0.5937 - pose_output_acc: 0.6707 - footwear_output_acc: 0.5845 - emotion_output_acc: 0.7123 - val_loss: 23.8368 - val_gender_output_loss: 0.5572 - val_image_quality_output_loss: 0.9985 - val_age_output_loss: 1.3823 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8962 - val_pose_output_loss: 0.7142 - val_footwear_output_loss: 0.8972 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.7063 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.5863 - val_pose_output_acc: 0.6880 - val_footwear_output_acc: 0.5942 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 23.51295\n",
            "Epoch 7/50\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 45.1322 - gender_output_loss: 0.6322 - image_quality_output_loss: 0.9205 - age_output_loss: 2.7753 - weight_output_loss: 2.5203 - bag_output_loss: 1.4877 - pose_output_loss: 1.3981 - footwear_output_loss: 1.1926 - emotion_output_loss: 2.5893 - gender_output_acc: 0.7092 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4021 - weight_output_acc: 0.6383 - bag_output_acc: 0.5987 - pose_output_acc: 0.6765 - footwear_output_acc: 0.5923 - emotion_output_acc: 0.7120 - val_loss: 23.5364 - val_gender_output_loss: 0.5381 - val_image_quality_output_loss: 0.9476 - val_age_output_loss: 1.3817 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.8794 - val_pose_output_loss: 0.6996 - val_footwear_output_loss: 0.8670 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.7168 - val_image_quality_output_acc: 0.5352 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.5947 - val_pose_output_acc: 0.6989 - val_footwear_output_acc: 0.6176 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 23.51295\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 45.0058 - gender_output_loss: 0.6241 - image_quality_output_loss: 0.9189 - age_output_loss: 2.7733 - weight_output_loss: 2.5215 - bag_output_loss: 1.4903 - pose_output_loss: 1.3733 - footwear_output_loss: 1.1903 - emotion_output_loss: 2.5820 - gender_output_acc: 0.7123 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4038 - weight_output_acc: 0.6380 - bag_output_acc: 0.5984 - pose_output_acc: 0.6791 - footwear_output_acc: 0.5905 - emotion_output_acc: 0.7112 - val_loss: 23.2681 - val_gender_output_loss: 0.5243 - val_image_quality_output_loss: 0.9362 - val_age_output_loss: 1.3653 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8775 - val_pose_output_loss: 0.6786 - val_footwear_output_loss: 0.8599 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7316 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5947 - val_pose_output_acc: 0.7044 - val_footwear_output_acc: 0.6116 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 23.51295\n",
            "Epoch 00008: val_loss improved from 23.51295 to 23.26807, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.008.h5\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 44.9159 - gender_output_loss: 0.6207 - image_quality_output_loss: 0.9176 - age_output_loss: 2.7603 - weight_output_loss: 2.5120 - bag_output_loss: 1.4869 - pose_output_loss: 1.3750 - footwear_output_loss: 1.1961 - emotion_output_loss: 2.5806 - gender_output_acc: 0.7178 - image_quality_output_acc: 0.5612 - age_output_acc: 0.4022 - weight_output_acc: 0.6378 - bag_output_acc: 0.5954 - pose_output_acc: 0.6826 - footwear_output_acc: 0.5905 - emotion_output_acc: 0.7125 - val_loss: 23.5855 - val_gender_output_loss: 0.5281 - val_image_quality_output_loss: 0.9618 - val_age_output_loss: 1.3695 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8836 - val_pose_output_loss: 0.7162 - val_footwear_output_loss: 0.8635 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.7257 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.6979 - val_footwear_output_acc: 0.6116 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 23.26807\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 45.0424 - gender_output_loss: 0.6304 - image_quality_output_loss: 0.9214 - age_output_loss: 2.7698 - weight_output_loss: 2.5025 - bag_output_loss: 1.4870 - pose_output_loss: 1.3934 - footwear_output_loss: 1.1961 - emotion_output_loss: 2.5898 - gender_output_acc: 0.7110 - image_quality_output_acc: 0.5618 - age_output_acc: 0.4020 - weight_output_acc: 0.6372 - bag_output_acc: 0.5995 - pose_output_acc: 0.6793 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7120 - val_loss: 23.4923 - val_gender_output_loss: 0.5218 - val_image_quality_output_loss: 0.9473 - val_age_output_loss: 1.3720 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8790 - val_pose_output_loss: 0.7222 - val_footwear_output_loss: 0.8545 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.7421 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5967 - val_pose_output_acc: 0.6969 - val_footwear_output_acc: 0.6136 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 23.26807\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 217s 602ms/step - loss: 45.1590 - gender_output_loss: 0.6333 - image_quality_output_loss: 0.9201 - age_output_loss: 2.7704 - weight_output_loss: 2.5341 - bag_output_loss: 1.4919 - pose_output_loss: 1.3921 - footwear_output_loss: 1.1953 - emotion_output_loss: 2.5924 - gender_output_acc: 0.7085 - image_quality_output_acc: 0.5645 - age_output_acc: 0.4044 - weight_output_acc: 0.6385 - bag_output_acc: 0.5961 - pose_output_acc: 0.6747 - footwear_output_acc: 0.5924 - emotion_output_acc: 0.7111 - val_loss: 23.4659 - val_gender_output_loss: 0.5385 - val_image_quality_output_loss: 0.9493 - val_age_output_loss: 1.3704 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.8795 - val_pose_output_loss: 0.7031 - val_footwear_output_loss: 0.8734 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.7168 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5873 - val_pose_output_acc: 0.6930 - val_footwear_output_acc: 0.6042 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 23.26807\n",
            "Epoch 12/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 45.3402 - gender_output_loss: 0.6322 - image_quality_output_loss: 0.9240 - age_output_loss: 2.7862 - weight_output_loss: 2.5325 - bag_output_loss: 1.4992 - pose_output_loss: 1.4014 - footwear_output_loss: 1.2069 - emotion_output_loss: 2.6048 - gender_output_acc: 0.7103 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3995 - weight_output_acc: 0.6368 - bag_output_acc: 0.5995 - pose_output_acc: 0.6762 - footwear_output_acc: 0.5849 - emotion_output_acc: 0.7115\n",
            "Epoch 00011: val_loss did not improve from 23.26807\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 45.3369 - gender_output_loss: 0.6323 - image_quality_output_loss: 0.9242 - age_output_loss: 2.7845 - weight_output_loss: 2.5313 - bag_output_loss: 1.4991 - pose_output_loss: 1.4003 - footwear_output_loss: 1.2078 - emotion_output_loss: 2.6069 - gender_output_acc: 0.7102 - image_quality_output_acc: 0.5634 - age_output_acc: 0.4000 - weight_output_acc: 0.6370 - bag_output_acc: 0.5993 - pose_output_acc: 0.6764 - footwear_output_acc: 0.5846 - emotion_output_acc: 0.7115 - val_loss: 23.5452 - val_gender_output_loss: 0.5418 - val_image_quality_output_loss: 0.9236 - val_age_output_loss: 1.3878 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8953 - val_pose_output_loss: 0.7261 - val_footwear_output_loss: 0.8663 - val_emotion_output_loss: 0.8964 - val_gender_output_acc: 0.7272 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5833 - val_pose_output_acc: 0.7024 - val_footwear_output_acc: 0.6071 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 23.26807\n",
            "Epoch 13/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 45.1637 - gender_output_loss: 0.6269 - image_quality_output_loss: 0.9241 - age_output_loss: 2.7814 - weight_output_loss: 2.5291 - bag_output_loss: 1.4946 - pose_output_loss: 1.3999 - footwear_output_loss: 1.2014 - emotion_output_loss: 2.5795 - gender_output_acc: 0.7093 - image_quality_output_acc: 0.5574 - age_output_acc: 0.4027 - weight_output_acc: 0.6381 - bag_output_acc: 0.6019 - pose_output_acc: 0.6740 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7118\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 45.1784 - gender_output_loss: 0.6275 - image_quality_output_loss: 0.9242 - age_output_loss: 2.7836 - weight_output_loss: 2.5291 - bag_output_loss: 1.4943 - pose_output_loss: 1.3997 - footwear_output_loss: 1.2008 - emotion_output_loss: 2.5813 - gender_output_acc: 0.7091 - image_quality_output_acc: 0.5572 - age_output_acc: 0.4027 - weight_output_acc: 0.6379 - bag_output_acc: 0.6022 - pose_output_acc: 0.6741 - footwear_output_acc: 0.5863 - emotion_output_acc: 0.7117 - val_loss: 23.5505 - val_gender_output_loss: 0.5306 - val_image_quality_output_loss: 0.9145 - val_age_output_loss: 1.3844 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8755 - val_pose_output_loss: 0.7748 - val_footwear_output_loss: 0.8419 - val_emotion_output_loss: 0.9046 - val_gender_output_acc: 0.7192 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5962 - val_pose_output_acc: 0.6691 - val_footwear_output_acc: 0.6295 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 23.26807\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 45.0106 - gender_output_loss: 0.6188 - image_quality_output_loss: 0.9218 - age_output_loss: 2.7759 - weight_output_loss: 2.5180 - bag_output_loss: 1.4846 - pose_output_loss: 1.3637 - footwear_output_loss: 1.1990 - emotion_output_loss: 2.5975 - gender_output_acc: 0.7214 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3974 - weight_output_acc: 0.6380 - bag_output_acc: 0.6061 - pose_output_acc: 0.6855 - footwear_output_acc: 0.5918 - emotion_output_acc: 0.7118 - val_loss: 23.8275 - val_gender_output_loss: 0.5241 - val_image_quality_output_loss: 0.9932 - val_age_output_loss: 1.3800 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.8769 - val_pose_output_loss: 0.7731 - val_footwear_output_loss: 0.8945 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.7321 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.6597 - val_footwear_output_acc: 0.5873 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 23.26807\n",
            "Epoch 15/50\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 44.7494 - gender_output_loss: 0.6135 - image_quality_output_loss: 0.9184 - age_output_loss: 2.7666 - weight_output_loss: 2.5091 - bag_output_loss: 1.4815 - pose_output_loss: 1.3431 - footwear_output_loss: 1.1869 - emotion_output_loss: 2.5772 - gender_output_acc: 0.7224 - image_quality_output_acc: 0.5627 - age_output_acc: 0.4045 - weight_output_acc: 0.6384 - bag_output_acc: 0.6025 - pose_output_acc: 0.6870 - footwear_output_acc: 0.5943 - emotion_output_acc: 0.7123 - val_loss: 23.7397 - val_gender_output_loss: 0.5284 - val_image_quality_output_loss: 1.0287 - val_age_output_loss: 1.3713 - val_weight_output_loss: 0.9885 - val_bag_output_loss: 0.8939 - val_pose_output_loss: 0.6885 - val_footwear_output_loss: 0.9167 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.7292 - val_image_quality_output_acc: 0.5069 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.7029 - val_footwear_output_acc: 0.5853 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 23.26807\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 217s 602ms/step - loss: 44.6253 - gender_output_loss: 0.6100 - image_quality_output_loss: 0.9133 - age_output_loss: 2.7732 - weight_output_loss: 2.5095 - bag_output_loss: 1.4759 - pose_output_loss: 1.3168 - footwear_output_loss: 1.1897 - emotion_output_loss: 2.5666 - gender_output_acc: 0.7247 - image_quality_output_acc: 0.5638 - age_output_acc: 0.4031 - weight_output_acc: 0.6385 - bag_output_acc: 0.6039 - pose_output_acc: 0.6974 - footwear_output_acc: 0.5922 - emotion_output_acc: 0.7127 - val_loss: 23.2255 - val_gender_output_loss: 0.5114 - val_image_quality_output_loss: 0.9375 - val_age_output_loss: 1.3683 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8880 - val_pose_output_loss: 0.6672 - val_footwear_output_loss: 0.8485 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.7366 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6126 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7098 - val_footwear_output_acc: 0.6151 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00016: val_loss improved from 23.26807 to 23.22548, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.016.h5\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 217s 603ms/step - loss: 44.4472 - gender_output_loss: 0.6035 - image_quality_output_loss: 0.9166 - age_output_loss: 2.7539 - weight_output_loss: 2.5103 - bag_output_loss: 1.4777 - pose_output_loss: 1.3007 - footwear_output_loss: 1.1830 - emotion_output_loss: 2.5567 - gender_output_acc: 0.7343 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4043 - weight_output_acc: 0.6387 - bag_output_acc: 0.6040 - pose_output_acc: 0.7045 - footwear_output_acc: 0.5977 - emotion_output_acc: 0.7122 - val_loss: 23.3269 - val_gender_output_loss: 0.5246 - val_image_quality_output_loss: 0.9349 - val_age_output_loss: 1.3826 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.8930 - val_pose_output_loss: 0.6610 - val_footwear_output_loss: 0.8443 - val_emotion_output_loss: 0.9043 - val_gender_output_acc: 0.7237 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.7202 - val_footwear_output_acc: 0.6225 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 23.22548\n",
            "Epoch 18/50\n",
            "360/360 [==============================] - 219s 607ms/step - loss: 44.7188 - gender_output_loss: 0.6092 - image_quality_output_loss: 0.9174 - age_output_loss: 2.7681 - weight_output_loss: 2.5072 - bag_output_loss: 1.4864 - pose_output_loss: 1.3228 - footwear_output_loss: 1.1843 - emotion_output_loss: 2.5862 - gender_output_acc: 0.7292 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4020 - weight_output_acc: 0.6384 - bag_output_acc: 0.6033 - pose_output_acc: 0.6929 - footwear_output_acc: 0.5946 - emotion_output_acc: 0.7117 - val_loss: 23.9334 - val_gender_output_loss: 0.5247 - val_image_quality_output_loss: 1.0544 - val_age_output_loss: 1.3788 - val_weight_output_loss: 1.0003 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.7147 - val_footwear_output_loss: 0.9420 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.7237 - val_image_quality_output_acc: 0.4782 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.5933 - val_bag_output_acc: 0.5933 - val_pose_output_acc: 0.6835 - val_footwear_output_acc: 0.5709 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 23.22548\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 218s 607ms/step - loss: 44.8473 - gender_output_loss: 0.6104 - image_quality_output_loss: 0.9181 - age_output_loss: 2.7697 - weight_output_loss: 2.5254 - bag_output_loss: 1.4877 - pose_output_loss: 1.3438 - footwear_output_loss: 1.1972 - emotion_output_loss: 2.5799 - gender_output_acc: 0.7218 - image_quality_output_acc: 0.5629 - age_output_acc: 0.4042 - weight_output_acc: 0.6378 - bag_output_acc: 0.6026 - pose_output_acc: 0.6864 - footwear_output_acc: 0.5871 - emotion_output_acc: 0.7115 - val_loss: 23.7599 - val_gender_output_loss: 0.5859 - val_image_quality_output_loss: 0.9390 - val_age_output_loss: 1.4028 - val_weight_output_loss: 0.9997 - val_bag_output_loss: 0.9063 - val_pose_output_loss: 0.6813 - val_footwear_output_loss: 0.8483 - val_emotion_output_loss: 0.9289 - val_gender_output_acc: 0.6925 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.5838 - val_pose_output_acc: 0.7118 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 23.22548\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 217s 604ms/step - loss: 44.9754 - gender_output_loss: 0.6190 - image_quality_output_loss: 0.9213 - age_output_loss: 2.7789 - weight_output_loss: 2.5238 - bag_output_loss: 1.4907 - pose_output_loss: 1.3459 - footwear_output_loss: 1.1990 - emotion_output_loss: 2.5945 - gender_output_acc: 0.7193 - image_quality_output_acc: 0.5607 - age_output_acc: 0.4033 - weight_output_acc: 0.6372 - bag_output_acc: 0.6004 - pose_output_acc: 0.6863 - footwear_output_acc: 0.5909 - emotion_output_acc: 0.7113 - val_loss: 23.4509 - val_gender_output_loss: 0.5232 - val_image_quality_output_loss: 0.9261 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.8796 - val_pose_output_loss: 0.6768 - val_footwear_output_loss: 0.8723 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.7336 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5933 - val_pose_output_acc: 0.7073 - val_footwear_output_acc: 0.6101 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 23.22548\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 217s 603ms/step - loss: 44.9727 - gender_output_loss: 0.6169 - image_quality_output_loss: 0.9212 - age_output_loss: 2.7795 - weight_output_loss: 2.5214 - bag_output_loss: 1.4946 - pose_output_loss: 1.3475 - footwear_output_loss: 1.1920 - emotion_output_loss: 2.5970 - gender_output_acc: 0.7210 - image_quality_output_acc: 0.5629 - age_output_acc: 0.4009 - weight_output_acc: 0.6371 - bag_output_acc: 0.5944 - pose_output_acc: 0.6912 - footwear_output_acc: 0.5977 - emotion_output_acc: 0.7117 - val_loss: 25.3048 - val_gender_output_loss: 0.7736 - val_image_quality_output_loss: 1.1170 - val_age_output_loss: 1.4789 - val_weight_output_loss: 1.0228 - val_bag_output_loss: 0.9968 - val_pose_output_loss: 0.6937 - val_footwear_output_loss: 0.8754 - val_emotion_output_loss: 0.9511 - val_gender_output_acc: 0.6652 - val_image_quality_output_acc: 0.4702 - val_age_output_acc: 0.3234 - val_weight_output_acc: 0.5689 - val_bag_output_acc: 0.5734 - val_pose_output_acc: 0.7088 - val_footwear_output_acc: 0.6151 - val_emotion_output_acc: 0.6691\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 23.22548\n",
            "Epoch 22/50\n",
            "360/360 [==============================] - 217s 604ms/step - loss: 44.6180 - gender_output_loss: 0.6085 - image_quality_output_loss: 0.9139 - age_output_loss: 2.7658 - weight_output_loss: 2.5115 - bag_output_loss: 1.4870 - pose_output_loss: 1.3113 - footwear_output_loss: 1.1901 - emotion_output_loss: 2.5723 - gender_output_acc: 0.7253 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4009 - weight_output_acc: 0.6365 - bag_output_acc: 0.6040 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5927 - emotion_output_acc: 0.7126 - val_loss: 23.0405 - val_gender_output_loss: 0.5164 - val_image_quality_output_loss: 0.9002 - val_age_output_loss: 1.3803 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8808 - val_pose_output_loss: 0.6526 - val_footwear_output_loss: 0.8400 - val_emotion_output_loss: 0.8922 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.7192 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00022: val_loss improved from 23.22548 to 23.04050, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.022.h5\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 217s 604ms/step - loss: 44.3626 - gender_output_loss: 0.6018 - image_quality_output_loss: 0.9116 - age_output_loss: 2.7610 - weight_output_loss: 2.4963 - bag_output_loss: 1.4778 - pose_output_loss: 1.2755 - footwear_output_loss: 1.1825 - emotion_output_loss: 2.5678 - gender_output_acc: 0.7282 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4023 - weight_output_acc: 0.6379 - bag_output_acc: 0.6048 - pose_output_acc: 0.7033 - footwear_output_acc: 0.5933 - emotion_output_acc: 0.7116 - val_loss: 23.0469 - val_gender_output_loss: 0.5064 - val_image_quality_output_loss: 0.9439 - val_age_output_loss: 1.3677 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8778 - val_pose_output_loss: 0.6365 - val_footwear_output_loss: 0.8405 - val_emotion_output_loss: 0.8992 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6081 - val_pose_output_acc: 0.7262 - val_footwear_output_acc: 0.6225 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 23.04050\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 44.3414 - gender_output_loss: 0.5935 - image_quality_output_loss: 0.9132 - age_output_loss: 2.7552 - weight_output_loss: 2.5159 - bag_output_loss: 1.4756 - pose_output_loss: 1.2673 - footwear_output_loss: 1.1818 - emotion_output_loss: 2.5655 - gender_output_acc: 0.7359 - image_quality_output_acc: 0.5654 - age_output_acc: 0.4033 - weight_output_acc: 0.6391 - bag_output_acc: 0.6074 - pose_output_acc: 0.7075 - footwear_output_acc: 0.5965 - emotion_output_acc: 0.7113 - val_loss: 23.0012 - val_gender_output_loss: 0.5014 - val_image_quality_output_loss: 0.9453 - val_age_output_loss: 1.3707 - val_weight_output_loss: 0.9895 - val_bag_output_loss: 0.8764 - val_pose_output_loss: 0.6193 - val_footwear_output_loss: 0.8389 - val_emotion_output_loss: 0.8967 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7371 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00024: val_loss improved from 23.04050 to 23.00124, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.024.h5\n",
            "Epoch 25/50\n",
            "360/360 [==============================] - 222s 616ms/step - loss: 44.1962 - gender_output_loss: 0.5918 - image_quality_output_loss: 0.9132 - age_output_loss: 2.7501 - weight_output_loss: 2.5034 - bag_output_loss: 1.4703 - pose_output_loss: 1.2521 - footwear_output_loss: 1.1746 - emotion_output_loss: 2.5637 - gender_output_acc: 0.7391 - image_quality_output_acc: 0.5658 - age_output_acc: 0.4063 - weight_output_acc: 0.6378 - bag_output_acc: 0.6118 - pose_output_acc: 0.7118 - footwear_output_acc: 0.6036 - emotion_output_acc: 0.7116 - val_loss: 23.3652 - val_gender_output_loss: 0.5180 - val_image_quality_output_loss: 0.9464 - val_age_output_loss: 1.3736 - val_weight_output_loss: 0.9858 - val_bag_output_loss: 0.8892 - val_pose_output_loss: 0.7063 - val_footwear_output_loss: 0.8472 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.7406 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.6915 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 23.00124\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 44.3555 - gender_output_loss: 0.5918 - image_quality_output_loss: 0.9125 - age_output_loss: 2.7529 - weight_output_loss: 2.5130 - bag_output_loss: 1.4789 - pose_output_loss: 1.2723 - footwear_output_loss: 1.1693 - emotion_output_loss: 2.5754 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5664 - age_output_acc: 0.4055 - weight_output_acc: 0.6388 - bag_output_acc: 0.6036 - pose_output_acc: 0.7073 - footwear_output_acc: 0.6001 - emotion_output_acc: 0.7121 - val_loss: 23.2334 - val_gender_output_loss: 0.5064 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.3732 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.6658 - val_footwear_output_loss: 0.8650 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7465 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7049 - val_footwear_output_acc: 0.6126 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 23.00124\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 44.5431 - gender_output_loss: 0.5940 - image_quality_output_loss: 0.9164 - age_output_loss: 2.7621 - weight_output_loss: 2.5197 - bag_output_loss: 1.4802 - pose_output_loss: 1.3015 - footwear_output_loss: 1.1786 - emotion_output_loss: 2.5784 - gender_output_acc: 0.7385 - image_quality_output_acc: 0.5640 - age_output_acc: 0.4013 - weight_output_acc: 0.6370 - bag_output_acc: 0.6022 - pose_output_acc: 0.6994 - footwear_output_acc: 0.6016 - emotion_output_acc: 0.7118 - val_loss: 24.6208 - val_gender_output_loss: 0.5611 - val_image_quality_output_loss: 0.9638 - val_age_output_loss: 1.3989 - val_weight_output_loss: 0.9933 - val_bag_output_loss: 0.9285 - val_pose_output_loss: 0.9492 - val_footwear_output_loss: 0.8425 - val_emotion_output_loss: 0.9453 - val_gender_output_acc: 0.7188 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.5754 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.6190 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 23.00124\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 44.5823 - gender_output_loss: 0.6051 - image_quality_output_loss: 0.9144 - age_output_loss: 2.7721 - weight_output_loss: 2.5225 - bag_output_loss: 1.4883 - pose_output_loss: 1.2994 - footwear_output_loss: 1.1814 - emotion_output_loss: 2.5669 - gender_output_acc: 0.7293 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4048 - weight_output_acc: 0.6388 - bag_output_acc: 0.6036 - pose_output_acc: 0.7039 - footwear_output_acc: 0.5982 - emotion_output_acc: 0.7116 - val_loss: 23.1164 - val_gender_output_loss: 0.5139 - val_image_quality_output_loss: 0.9559 - val_age_output_loss: 1.3785 - val_weight_output_loss: 0.9736 - val_bag_output_loss: 0.8771 - val_pose_output_loss: 0.6524 - val_footwear_output_loss: 0.8387 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.7292 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5938 - val_pose_output_acc: 0.7153 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 23.00124\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 44.6387 - gender_output_loss: 0.6003 - image_quality_output_loss: 0.9164 - age_output_loss: 2.7681 - weight_output_loss: 2.5155 - bag_output_loss: 1.4831 - pose_output_loss: 1.2991 - footwear_output_loss: 1.1899 - emotion_output_loss: 2.5930 - gender_output_acc: 0.7359 - image_quality_output_acc: 0.5620 - age_output_acc: 0.4031 - weight_output_acc: 0.6380 - bag_output_acc: 0.6117 - pose_output_acc: 0.6987 - footwear_output_acc: 0.5947 - emotion_output_acc: 0.7121 - val_loss: 23.6453 - val_gender_output_loss: 0.6233 - val_image_quality_output_loss: 0.9219 - val_age_output_loss: 1.3898 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.9503 - val_pose_output_loss: 0.6435 - val_footwear_output_loss: 0.8439 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.6974 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5764 - val_pose_output_acc: 0.7307 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 23.00124\n",
            "Epoch 30/50\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 44.3832 - gender_output_loss: 0.5918 - image_quality_output_loss: 0.9132 - age_output_loss: 2.7612 - weight_output_loss: 2.5319 - bag_output_loss: 1.4741 - pose_output_loss: 1.2593 - footwear_output_loss: 1.1756 - emotion_output_loss: 2.5745 - gender_output_acc: 0.7372 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4034 - weight_output_acc: 0.6380 - bag_output_acc: 0.6099 - pose_output_acc: 0.7177 - footwear_output_acc: 0.6022 - emotion_output_acc: 0.7118 - val_loss: 23.0554 - val_gender_output_loss: 0.5104 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3732 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 0.8849 - val_pose_output_loss: 0.6218 - val_footwear_output_loss: 0.8344 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.7436 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7331 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 23.00124\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 44.1011 - gender_output_loss: 0.5747 - image_quality_output_loss: 0.9106 - age_output_loss: 2.7411 - weight_output_loss: 2.4977 - bag_output_loss: 1.4750 - pose_output_loss: 1.2458 - footwear_output_loss: 1.1776 - emotion_output_loss: 2.5690 - gender_output_acc: 0.7491 - image_quality_output_acc: 0.5663 - age_output_acc: 0.4069 - weight_output_acc: 0.6373 - bag_output_acc: 0.6107 - pose_output_acc: 0.7171 - footwear_output_acc: 0.6011 - emotion_output_acc: 0.7115 - val_loss: 22.8992 - val_gender_output_loss: 0.5073 - val_image_quality_output_loss: 0.9269 - val_age_output_loss: 1.3638 - val_weight_output_loss: 0.9923 - val_bag_output_loss: 0.8803 - val_pose_output_loss: 0.6117 - val_footwear_output_loss: 0.8303 - val_emotion_output_loss: 0.8960 - val_gender_output_acc: 0.7426 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.7346 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00031: val_loss improved from 23.00124 to 22.89924, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.031.h5\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 227s 629ms/step - loss: 43.9145 - gender_output_loss: 0.5740 - image_quality_output_loss: 0.9094 - age_output_loss: 2.7458 - weight_output_loss: 2.5012 - bag_output_loss: 1.4695 - pose_output_loss: 1.2116 - footwear_output_loss: 1.1694 - emotion_output_loss: 2.5503 - gender_output_acc: 0.7510 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4060 - weight_output_acc: 0.6379 - bag_output_acc: 0.6089 - pose_output_acc: 0.7244 - footwear_output_acc: 0.6016 - emotion_output_acc: 0.7111 - val_loss: 23.0607 - val_gender_output_loss: 0.5108 - val_image_quality_output_loss: 0.9662 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.8880 - val_pose_output_loss: 0.6313 - val_footwear_output_loss: 0.8300 - val_emotion_output_loss: 0.8982 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7232 - val_footwear_output_acc: 0.6314 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 22.89924\n",
            "Epoch 33/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.9435 - gender_output_loss: 0.5717 - image_quality_output_loss: 0.9062 - age_output_loss: 2.7494 - weight_output_loss: 2.4889 - bag_output_loss: 1.4665 - pose_output_loss: 1.2246 - footwear_output_loss: 1.1725 - emotion_output_loss: 2.5571 - gender_output_acc: 0.7501 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4058 - weight_output_acc: 0.6378 - bag_output_acc: 0.6140 - pose_output_acc: 0.7268 - footwear_output_acc: 0.6025 - emotion_output_acc: 0.7123\n",
            "Epoch 00032: val_loss did not improve from 22.89924\n",
            "\n",
            "360/360 [==============================] - 228s 632ms/step - loss: 43.9472 - gender_output_loss: 0.5716 - image_quality_output_loss: 0.9062 - age_output_loss: 2.7480 - weight_output_loss: 2.4904 - bag_output_loss: 1.4685 - pose_output_loss: 1.2230 - footwear_output_loss: 1.1717 - emotion_output_loss: 2.5583 - gender_output_acc: 0.7498 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4059 - weight_output_acc: 0.6378 - bag_output_acc: 0.6139 - pose_output_acc: 0.7270 - footwear_output_acc: 0.6027 - emotion_output_acc: 0.7122 - val_loss: 22.6992 - val_gender_output_loss: 0.4790 - val_image_quality_output_loss: 0.9360 - val_age_output_loss: 1.3564 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.8690 - val_pose_output_loss: 0.6013 - val_footwear_output_loss: 0.8241 - val_emotion_output_loss: 0.8939 - val_gender_output_acc: 0.7629 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6126 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7426 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00033: val_loss improved from 22.89924 to 22.69916, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.033.h5\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 226s 629ms/step - loss: 44.0284 - gender_output_loss: 0.5780 - image_quality_output_loss: 0.9082 - age_output_loss: 2.7438 - weight_output_loss: 2.5023 - bag_output_loss: 1.4673 - pose_output_loss: 1.2245 - footwear_output_loss: 1.1720 - emotion_output_loss: 2.5699 - gender_output_acc: 0.7438 - image_quality_output_acc: 0.5653 - age_output_acc: 0.4055 - weight_output_acc: 0.6398 - bag_output_acc: 0.6089 - pose_output_acc: 0.7243 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7110 - val_loss: 23.1612 - val_gender_output_loss: 0.4955 - val_image_quality_output_loss: 0.9276 - val_age_output_loss: 1.3891 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.6846 - val_footwear_output_loss: 0.8284 - val_emotion_output_loss: 0.8949 - val_gender_output_acc: 0.7510 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.6989 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 22.69916\n",
            "Epoch 35/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 44.2163 - gender_output_loss: 0.5855 - image_quality_output_loss: 0.9103 - age_output_loss: 2.7529 - weight_output_loss: 2.4993 - bag_output_loss: 1.4676 - pose_output_loss: 1.2501 - footwear_output_loss: 1.1746 - emotion_output_loss: 2.5854 - gender_output_acc: 0.7393 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4037 - weight_output_acc: 0.6373 - bag_output_acc: 0.6096 - pose_output_acc: 0.7098 - footwear_output_acc: 0.6019 - emotion_output_acc: 0.7115Epoch 35/50\n",
            "\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 44.2330 - gender_output_loss: 0.5853 - image_quality_output_loss: 0.9105 - age_output_loss: 2.7537 - weight_output_loss: 2.5028 - bag_output_loss: 1.4666 - pose_output_loss: 1.2500 - footwear_output_loss: 1.1749 - emotion_output_loss: 2.5869 - gender_output_acc: 0.7394 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4035 - weight_output_acc: 0.6368 - bag_output_acc: 0.6100 - pose_output_acc: 0.7098 - footwear_output_acc: 0.6016 - emotion_output_acc: 0.7113 - val_loss: 23.4243 - val_gender_output_loss: 0.5017 - val_image_quality_output_loss: 1.0322 - val_age_output_loss: 1.3684 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.8828 - val_pose_output_loss: 0.6841 - val_footwear_output_loss: 0.8422 - val_emotion_output_loss: 0.9264 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.7173 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6776\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 22.69916\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 44.3019 - gender_output_loss: 0.5963 - image_quality_output_loss: 0.9129 - age_output_loss: 2.7615 - weight_output_loss: 2.5122 - bag_output_loss: 1.4732 - pose_output_loss: 1.2592 - footwear_output_loss: 1.1781 - emotion_output_loss: 2.5702 - gender_output_acc: 0.7352 - image_quality_output_acc: 0.5660 - age_output_acc: 0.4056 - weight_output_acc: 0.6365 - bag_output_acc: 0.6078 - pose_output_acc: 0.7125 - footwear_output_acc: 0.5970 - emotion_output_acc: 0.7116 - val_loss: 23.6600 - val_gender_output_loss: 0.4974 - val_image_quality_output_loss: 0.9356 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.9008 - val_pose_output_loss: 0.7967 - val_footwear_output_loss: 0.8618 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.7560 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5843 - val_pose_output_acc: 0.6409 - val_footwear_output_acc: 0.6101 - val_emotion_output_acc: 0.7078\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 22.69916\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 44.2330 - gender_output_loss: 0.5853 - image_quality_output_loss: 0.9105 - age_output_loss: 2.7537 - weight_output_loss: 2.5028 - bag_output_loss: 1.4666 - pose_output_loss: 1.2500 - footwear_output_loss: 1.1749 - emotion_output_loss: 2.5869 - gender_output_acc: 0.7394 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4035 - weight_output_acc: 0.6368 - bag_output_acc: 0.6100 - pose_output_acc: 0.7098 - footwear_output_acc: 0.6016 - emotion_output_acc: 0.7113 - val_loss: 23.4243 - val_gender_output_loss: 0.5017 - val_image_quality_output_loss: 1.0322 - val_age_output_loss: 1.3684 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.8828 - val_pose_output_loss: 0.6841 - val_footwear_output_loss: 0.8422 - val_emotion_output_loss: 0.9264 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.7173 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6776\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 44.2783 - gender_output_loss: 0.5920 - image_quality_output_loss: 0.9161 - age_output_loss: 2.7585 - weight_output_loss: 2.5242 - bag_output_loss: 1.4720 - pose_output_loss: 1.2518 - footwear_output_loss: 1.1779 - emotion_output_loss: 2.5668 - gender_output_acc: 0.7393 - image_quality_output_acc: 0.5649 - age_output_acc: 0.4000 - weight_output_acc: 0.6375 - bag_output_acc: 0.6115 - pose_output_acc: 0.7174 - footwear_output_acc: 0.5965 - emotion_output_acc: 0.7118 - val_loss: 23.3678 - val_gender_output_loss: 0.5204 - val_image_quality_output_loss: 1.0064 - val_age_output_loss: 1.3715 - val_weight_output_loss: 0.9754 - val_bag_output_loss: 0.8994 - val_pose_output_loss: 0.6737 - val_footwear_output_loss: 0.8290 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.7569 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7188 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.7034\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 22.69916\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 226s 629ms/step - loss: 44.2103 - gender_output_loss: 0.5822 - image_quality_output_loss: 0.9115 - age_output_loss: 2.7541 - weight_output_loss: 2.5178 - bag_output_loss: 1.4765 - pose_output_loss: 1.2451 - footwear_output_loss: 1.1818 - emotion_output_loss: 2.5672 - gender_output_acc: 0.7405 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4036 - weight_output_acc: 0.6388 - bag_output_acc: 0.6089 - pose_output_acc: 0.7165 - footwear_output_acc: 0.5945 - emotion_output_acc: 0.7104 - val_loss: 23.0503 - val_gender_output_loss: 0.5019 - val_image_quality_output_loss: 0.9513 - val_age_output_loss: 1.3668 - val_weight_output_loss: 0.9933 - val_bag_output_loss: 0.8831 - val_pose_output_loss: 0.6019 - val_footwear_output_loss: 0.8249 - val_emotion_output_loss: 0.9345 - val_gender_output_acc: 0.7440 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.5938 - val_pose_output_acc: 0.7490 - val_footwear_output_acc: 0.6409 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 22.69916\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 222s 616ms/step - loss: 43.8719 - gender_output_loss: 0.5724 - image_quality_output_loss: 0.9096 - age_output_loss: 2.7402 - weight_output_loss: 2.4986 - bag_output_loss: 1.4638 - pose_output_loss: 1.2142 - footwear_output_loss: 1.1718 - emotion_output_loss: 2.5553 - gender_output_acc: 0.7516 - image_quality_output_acc: 0.5680 - age_output_acc: 0.4035 - weight_output_acc: 0.6385 - bag_output_acc: 0.6116 - pose_output_acc: 0.7252 - footwear_output_acc: 0.5986 - emotion_output_acc: 0.7122 - val_loss: 22.9109 - val_gender_output_loss: 0.4841 - val_image_quality_output_loss: 0.9869 - val_age_output_loss: 1.3549 - val_weight_output_loss: 0.9912 - val_bag_output_loss: 0.8928 - val_pose_output_loss: 0.6012 - val_footwear_output_loss: 0.8333 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.7723 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.5947 - val_pose_output_acc: 0.7386 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 22.69916\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 43.7524 - gender_output_loss: 0.5674 - image_quality_output_loss: 0.9064 - age_output_loss: 2.7384 - weight_output_loss: 2.5001 - bag_output_loss: 1.4623 - pose_output_loss: 1.1961 - footwear_output_loss: 1.1673 - emotion_output_loss: 2.5476 - gender_output_acc: 0.7539 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4061 - weight_output_acc: 0.6367 - bag_output_acc: 0.6117 - pose_output_acc: 0.7325 - footwear_output_acc: 0.6018 - emotion_output_acc: 0.7121 - val_loss: 22.9192 - val_gender_output_loss: 0.4965 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.3608 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8814 - val_pose_output_loss: 0.5988 - val_footwear_output_loss: 0.8226 - val_emotion_output_loss: 0.9024 - val_gender_output_acc: 0.7574 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6126 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7391 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 22.69916\n",
            "Epoch 41/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.6424 - gender_output_loss: 0.5628 - image_quality_output_loss: 0.9061 - age_output_loss: 2.7394 - weight_output_loss: 2.4971 - bag_output_loss: 1.4580 - pose_output_loss: 1.1731 - footwear_output_loss: 1.1679 - emotion_output_loss: 2.5441 - gender_output_acc: 0.7553 - image_quality_output_acc: 0.5690 - age_output_acc: 0.4080 - weight_output_acc: 0.6389 - bag_output_acc: 0.6130 - pose_output_acc: 0.7359 - footwear_output_acc: 0.6021 - emotion_output_acc: 0.7118\n",
            "Epoch 00040: val_loss did not improve from 22.69916\n",
            "360/360 [==============================] - 225s 626ms/step - loss: 43.6398 - gender_output_loss: 0.5627 - image_quality_output_loss: 0.9058 - age_output_loss: 2.7377 - weight_output_loss: 2.4974 - bag_output_loss: 1.4579 - pose_output_loss: 1.1727 - footwear_output_loss: 1.1675 - emotion_output_loss: 2.5458 - gender_output_acc: 0.7551 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4080 - weight_output_acc: 0.6390 - bag_output_acc: 0.6130 - pose_output_acc: 0.7360 - footwear_output_acc: 0.6025 - emotion_output_acc: 0.7115 - val_loss: 22.6534 - val_gender_output_loss: 0.4825 - val_image_quality_output_loss: 0.9629 - val_age_output_loss: 1.3570 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.8683 - val_pose_output_loss: 0.5734 - val_footwear_output_loss: 0.8163 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.7560 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6042 - val_pose_output_acc: 0.7569 - val_footwear_output_acc: 0.6424 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00041: val_loss improved from 22.69916 to 22.65342, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.041.h5\n",
            "Epoch 42/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.6135 - gender_output_loss: 0.5614 - image_quality_output_loss: 0.9072 - age_output_loss: 2.7339 - weight_output_loss: 2.4925 - bag_output_loss: 1.4685 - pose_output_loss: 1.1702 - footwear_output_loss: 1.1643 - emotion_output_loss: 2.5427 - gender_output_acc: 0.7579 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4073 - weight_output_acc: 0.6374 - bag_output_acc: 0.6072 - pose_output_acc: 0.7347 - footwear_output_acc: 0.6033 - emotion_output_acc: 0.7120\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 43.6223 - gender_output_loss: 0.5625 - image_quality_output_loss: 0.9073 - age_output_loss: 2.7357 - weight_output_loss: 2.4947 - bag_output_loss: 1.4682 - pose_output_loss: 1.1708 - footwear_output_loss: 1.1638 - emotion_output_loss: 2.5408 - gender_output_acc: 0.7572 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4071 - weight_output_acc: 0.6374 - bag_output_acc: 0.6073 - pose_output_acc: 0.7345 - footwear_output_acc: 0.6035 - emotion_output_acc: 0.7122 - val_loss: 22.6194 - val_gender_output_loss: 0.4803 - val_image_quality_output_loss: 0.9460 - val_age_output_loss: 1.3577 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8688 - val_pose_output_loss: 0.5914 - val_footwear_output_loss: 0.8234 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7361 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00042: val_loss improved from 22.65342 to 22.61942, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.042.h5\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 43.9215 - gender_output_loss: 0.5748 - image_quality_output_loss: 0.9126 - age_output_loss: 2.7409 - weight_output_loss: 2.5118 - bag_output_loss: 1.4694 - pose_output_loss: 1.2019 - footwear_output_loss: 1.1705 - emotion_output_loss: 2.5620 - gender_output_acc: 0.7467 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4063 - weight_output_acc: 0.6362 - bag_output_acc: 0.6077 - pose_output_acc: 0.7279 - footwear_output_acc: 0.5977 - emotion_output_acc: 0.7105 - val_loss: 22.9374 - val_gender_output_loss: 0.4861 - val_image_quality_output_loss: 0.9282 - val_age_output_loss: 1.3598 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 0.8696 - val_pose_output_loss: 0.6413 - val_footwear_output_loss: 0.8296 - val_emotion_output_loss: 0.9146 - val_gender_output_acc: 0.7579 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7351 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.6746\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 22.61942\n",
            "Epoch 44/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.9881 - gender_output_loss: 0.5845 - image_quality_output_loss: 0.9102 - age_output_loss: 2.7563 - weight_output_loss: 2.4940 - bag_output_loss: 1.4717 - pose_output_loss: 1.2183 - footwear_output_loss: 1.1792 - emotion_output_loss: 2.5557 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5704 - age_output_acc: 0.4042 - weight_output_acc: 0.6379 - bag_output_acc: 0.6068 - pose_output_acc: 0.7232 - footwear_output_acc: 0.5995 - emotion_output_acc: 0.7105Epoch 44/50\n",
            "360/360 [==============================] - 225s 626ms/step - loss: 43.9863 - gender_output_loss: 0.5842 - image_quality_output_loss: 0.9101 - age_output_loss: 2.7547 - weight_output_loss: 2.4946 - bag_output_loss: 1.4711 - pose_output_loss: 1.2180 - footwear_output_loss: 1.1796 - emotion_output_loss: 2.5571 - gender_output_acc: 0.7398 - image_quality_output_acc: 0.5705 - age_output_acc: 0.4043 - weight_output_acc: 0.6378 - bag_output_acc: 0.6071 - pose_output_acc: 0.7233 - footwear_output_acc: 0.5992 - emotion_output_acc: 0.7102 - val_loss: 22.9251 - val_gender_output_loss: 0.5054 - val_image_quality_output_loss: 0.9570 - val_age_output_loss: 1.3839 - val_weight_output_loss: 0.9680 - val_bag_output_loss: 0.8755 - val_pose_output_loss: 0.6124 - val_footwear_output_loss: 0.8312 - val_emotion_output_loss: 0.8993 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.7371 - val_footwear_output_acc: 0.6295 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 22.61942\n",
            "Epoch 45/50\n",
            "360/360 [==============================] - 226s 627ms/step - loss: 44.0076 - gender_output_loss: 0.5772 - image_quality_output_loss: 0.9116 - age_output_loss: 2.7538 - weight_output_loss: 2.5082 - bag_output_loss: 1.4689 - pose_output_loss: 1.2279 - footwear_output_loss: 1.1766 - emotion_output_loss: 2.5530 - gender_output_acc: 0.7455 - image_quality_output_acc: 0.5616 - age_output_acc: 0.3984 - weight_output_acc: 0.6390 - bag_output_acc: 0.6073 - pose_output_acc: 0.7219 - footwear_output_acc: 0.5973 - emotion_output_acc: 0.7109 - val_loss: 22.6685 - val_gender_output_loss: 0.4717 - val_image_quality_output_loss: 0.9267 - val_age_output_loss: 1.3649 - val_weight_output_loss: 0.9974 - val_bag_output_loss: 0.8692 - val_pose_output_loss: 0.5929 - val_footwear_output_loss: 0.8180 - val_emotion_output_loss: 0.8914 - val_gender_output_acc: 0.7654 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7530 - val_footwear_output_acc: 0.6429 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 22.61942\n",
            "Epoch 46/50\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 43.8266 - gender_output_loss: 0.5668 - image_quality_output_loss: 0.9090 - age_output_loss: 2.7409 - weight_output_loss: 2.4991 - bag_output_loss: 1.4657 - pose_output_loss: 1.1949 - footwear_output_loss: 1.1659 - emotion_output_loss: 2.5676 - gender_output_acc: 0.7524 - image_quality_output_acc: 0.5685 - age_output_acc: 0.4064 - weight_output_acc: 0.6379 - bag_output_acc: 0.6149 - pose_output_acc: 0.7339 - footwear_output_acc: 0.6076 - emotion_output_acc: 0.7113 - val_loss: 23.5505 - val_gender_output_loss: 0.4980 - val_image_quality_output_loss: 1.0080 - val_age_output_loss: 1.3733 - val_weight_output_loss: 1.0144 - val_bag_output_loss: 0.8899 - val_pose_output_loss: 0.7367 - val_footwear_output_loss: 0.8323 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.7684 - val_image_quality_output_acc: 0.5198 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.5898 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.6989 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 22.61942\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 43.5763 - gender_output_loss: 0.5604 - image_quality_output_loss: 0.9080 - age_output_loss: 2.7268 - weight_output_loss: 2.4918 - bag_output_loss: 1.4564 - pose_output_loss: 1.1887 - footwear_output_loss: 1.1657 - emotion_output_loss: 2.5408 - gender_output_acc: 0.7617 - image_quality_output_acc: 0.5688 - age_output_acc: 0.4080 - weight_output_acc: 0.6376 - bag_output_acc: 0.6137 - pose_output_acc: 0.7293 - footwear_output_acc: 0.6019 - emotion_output_acc: 0.7127 - val_loss: 22.8167 - val_gender_output_loss: 0.4901 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.3670 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8804 - val_pose_output_loss: 0.5911 - val_footwear_output_loss: 0.8282 - val_emotion_output_loss: 0.8897 - val_gender_output_acc: 0.7604 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7401 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 22.61942\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 225s 626ms/step - loss: 43.3928 - gender_output_loss: 0.5555 - image_quality_output_loss: 0.9066 - age_output_loss: 2.7185 - weight_output_loss: 2.4881 - bag_output_loss: 1.4545 - pose_output_loss: 1.1567 - footwear_output_loss: 1.1602 - emotion_output_loss: 2.5379 - gender_output_acc: 0.7647 - image_quality_output_acc: 0.5703 - age_output_acc: 0.4131 - weight_output_acc: 0.6379 - bag_output_acc: 0.6138 - pose_output_acc: 0.7408 - footwear_output_acc: 0.6060 - emotion_output_acc: 0.7120 - val_loss: 22.5998 - val_gender_output_loss: 0.4686 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.3588 - val_weight_output_loss: 0.9853 - val_bag_output_loss: 0.8683 - val_pose_output_loss: 0.5609 - val_footwear_output_loss: 0.8256 - val_emotion_output_loss: 0.8844 - val_gender_output_acc: 0.7773 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00048: val_loss improved from 22.61942 to 22.59984, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.048.h5\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 43.2451 - gender_output_loss: 0.5467 - image_quality_output_loss: 0.9070 - age_output_loss: 2.7143 - weight_output_loss: 2.4833 - bag_output_loss: 1.4495 - pose_output_loss: 1.1499 - footwear_output_loss: 1.1624 - emotion_output_loss: 2.5209 - gender_output_acc: 0.7668 - image_quality_output_acc: 0.5697 - age_output_acc: 0.4108 - weight_output_acc: 0.6396 - bag_output_acc: 0.6188 - pose_output_acc: 0.7420 - footwear_output_acc: 0.6049 - emotion_output_acc: 0.7128 - val_loss: 22.7376 - val_gender_output_loss: 0.4636 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.3553 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.8633 - val_pose_output_loss: 0.5991 - val_footwear_output_loss: 0.8268 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6076 - val_pose_output_acc: 0.7445 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 22.59984\n",
            "Epoch 50/50\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 43.5493 - gender_output_loss: 0.5557 - image_quality_output_loss: 0.9080 - age_output_loss: 2.7222 - weight_output_loss: 2.5001 - bag_output_loss: 1.4608 - pose_output_loss: 1.1671 - footwear_output_loss: 1.1617 - emotion_output_loss: 2.5508 - gender_output_acc: 0.7580 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4089 - weight_output_acc: 0.6395 - bag_output_acc: 0.6106 - pose_output_acc: 0.7413 - footwear_output_acc: 0.5986 - emotion_output_acc: 0.7110 - val_loss: 24.2378 - val_gender_output_loss: 0.4893 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.3584 - val_weight_output_loss: 1.0006 - val_bag_output_loss: 0.9020 - val_pose_output_loss: 0.9869 - val_footwear_output_loss: 0.8357 - val_emotion_output_loss: 0.9212 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.6047 - val_footwear_output_acc: 0.6364 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 22.59984\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577514347.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1dc6555e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZRJsa_c2qlf",
        "colab_type": "code",
        "outputId": "63beaa53-930f-447c-9fc1-21433d32b13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of run with EPOCHS= 50 STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYSGZUstwQBm",
        "outputId": "1b52feb5-e7e9-474f-ee66-c107a7fc00a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=train_df.shape[0]\n",
        "EPOCHS=20\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.048.h5')\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    #steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577514347.json\n",
            "Returning new callback array with steps_per_epoch= 11537 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 20\n",
            "Epoch 1/20\n",
            "360/360 [==============================] - 231s 641ms/step - loss: 43.2748 - gender_output_loss: 0.5494 - image_quality_output_loss: 0.9041 - age_output_loss: 2.7054 - weight_output_loss: 2.4936 - bag_output_loss: 1.4515 - pose_output_loss: 1.1386 - footwear_output_loss: 1.1529 - emotion_output_loss: 2.5414 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4122 - weight_output_acc: 0.6387 - bag_output_acc: 0.6175 - pose_output_acc: 0.7459 - footwear_output_acc: 0.6083 - emotion_output_acc: 0.7119 - val_loss: 22.4716 - val_gender_output_loss: 0.4729 - val_image_quality_output_loss: 0.9360 - val_age_output_loss: 1.3573 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8694 - val_pose_output_loss: 0.5540 - val_footwear_output_loss: 0.8131 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.7684 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7659 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.47158, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 43.3210 - gender_output_loss: 0.5514 - image_quality_output_loss: 0.9032 - age_output_loss: 2.7194 - weight_output_loss: 2.4939 - bag_output_loss: 1.4540 - pose_output_loss: 1.1302 - footwear_output_loss: 1.1623 - emotion_output_loss: 2.5377 - gender_output_acc: 0.7640 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4089 - weight_output_acc: 0.6391 - bag_output_acc: 0.6126 - pose_output_acc: 0.7461 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7116 - val_loss: 22.6494 - val_gender_output_loss: 0.4829 - val_image_quality_output_loss: 0.9451 - val_age_output_loss: 1.3608 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8756 - val_pose_output_loss: 0.5916 - val_footwear_output_loss: 0.8151 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7480 - val_footwear_output_acc: 0.6483 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 22.47158\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 221s 615ms/step - loss: 43.3256 - gender_output_loss: 0.5431 - image_quality_output_loss: 0.9036 - age_output_loss: 2.7156 - weight_output_loss: 2.4874 - bag_output_loss: 1.4480 - pose_output_loss: 1.1428 - footwear_output_loss: 1.1549 - emotion_output_loss: 2.5504 - gender_output_acc: 0.7653 - image_quality_output_acc: 0.5715 - age_output_acc: 0.4113 - weight_output_acc: 0.6386 - bag_output_acc: 0.6186 - pose_output_acc: 0.7457 - footwear_output_acc: 0.6121 - emotion_output_acc: 0.7120 - val_loss: 22.6132 - val_gender_output_loss: 0.4701 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.3587 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.8652 - val_pose_output_loss: 0.5790 - val_footwear_output_loss: 0.8236 - val_emotion_output_loss: 0.8829 - val_gender_output_acc: 0.7763 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.6404 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 22.47158\n",
            "Epoch 4/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.1989 - gender_output_loss: 0.5462 - image_quality_output_loss: 0.9047 - age_output_loss: 2.7094 - weight_output_loss: 2.4815 - bag_output_loss: 1.4559 - pose_output_loss: 1.1384 - footwear_output_loss: 1.1537 - emotion_output_loss: 2.5252 - gender_output_acc: 0.7669 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4104 - weight_output_acc: 0.6382 - bag_output_acc: 0.6155 - pose_output_acc: 0.7494 - footwear_output_acc: 0.6135 - emotion_output_acc: 0.7119\n",
            "Epoch 00003: val_loss did not improve from 22.47158\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 43.1922 - gender_output_loss: 0.5460 - image_quality_output_loss: 0.9047 - age_output_loss: 2.7077 - weight_output_loss: 2.4801 - bag_output_loss: 1.4549 - pose_output_loss: 1.1385 - footwear_output_loss: 1.1533 - emotion_output_loss: 2.5273 - gender_output_acc: 0.7670 - image_quality_output_acc: 0.5701 - age_output_acc: 0.4106 - weight_output_acc: 0.6382 - bag_output_acc: 0.6156 - pose_output_acc: 0.7494 - footwear_output_acc: 0.6136 - emotion_output_acc: 0.7118 - val_loss: 22.5617 - val_gender_output_loss: 0.4788 - val_image_quality_output_loss: 0.9628 - val_age_output_loss: 1.3620 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8707 - val_pose_output_loss: 0.5544 - val_footwear_output_loss: 0.8151 - val_emotion_output_loss: 0.8872 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 22.47158\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 43.3258 - gender_output_loss: 0.5475 - image_quality_output_loss: 0.9023 - age_output_loss: 2.7124 - weight_output_loss: 2.4859 - bag_output_loss: 1.4494 - pose_output_loss: 1.1456 - footwear_output_loss: 1.1587 - emotion_output_loss: 2.5483 - gender_output_acc: 0.7676 - image_quality_output_acc: 0.5682 - age_output_acc: 0.4103 - weight_output_acc: 0.6390 - bag_output_acc: 0.6162 - pose_output_acc: 0.7443 - footwear_output_acc: 0.6073 - emotion_output_acc: 0.7115 - val_loss: 22.8263 - val_gender_output_loss: 0.4901 - val_image_quality_output_loss: 1.0108 - val_age_output_loss: 1.3655 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.8809 - val_pose_output_loss: 0.5555 - val_footwear_output_loss: 0.8164 - val_emotion_output_loss: 0.9046 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7698 - val_footwear_output_acc: 0.6434 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 22.47158\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 223s 618ms/step - loss: 43.2775 - gender_output_loss: 0.5467 - image_quality_output_loss: 0.9027 - age_output_loss: 2.7155 - weight_output_loss: 2.4929 - bag_output_loss: 1.4463 - pose_output_loss: 1.1388 - footwear_output_loss: 1.1559 - emotion_output_loss: 2.5370 - gender_output_acc: 0.7673 - image_quality_output_acc: 0.5711 - age_output_acc: 0.4076 - weight_output_acc: 0.6390 - bag_output_acc: 0.6206 - pose_output_acc: 0.7475 - footwear_output_acc: 0.6082 - emotion_output_acc: 0.7122 - val_loss: 22.7845 - val_gender_output_loss: 0.5060 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.3711 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.8799 - val_pose_output_loss: 0.5528 - val_footwear_output_loss: 0.8124 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.7649 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7688 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 22.47158\n",
            "Epoch 7/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.2483 - gender_output_loss: 0.5478 - image_quality_output_loss: 0.9025 - age_output_loss: 2.7064 - weight_output_loss: 2.4801 - bag_output_loss: 1.4603 - pose_output_loss: 1.1271 - footwear_output_loss: 1.1583 - emotion_output_loss: 2.5451 - gender_output_acc: 0.7621 - image_quality_output_acc: 0.5724 - age_output_acc: 0.4117 - weight_output_acc: 0.6387 - bag_output_acc: 0.6136 - pose_output_acc: 0.7503 - footwear_output_acc: 0.6059 - emotion_output_acc: 0.7117Epoch 7/20\n",
            "360/360 [==============================] - 222s 618ms/step - loss: 43.2354 - gender_output_loss: 0.5481 - image_quality_output_loss: 0.9026 - age_output_loss: 2.7063 - weight_output_loss: 2.4801 - bag_output_loss: 1.4590 - pose_output_loss: 1.1272 - footwear_output_loss: 1.1578 - emotion_output_loss: 2.5429 - gender_output_acc: 0.7618 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4115 - weight_output_acc: 0.6387 - bag_output_acc: 0.6139 - pose_output_acc: 0.7502 - footwear_output_acc: 0.6060 - emotion_output_acc: 0.7118 - val_loss: 22.7967 - val_gender_output_loss: 0.4748 - val_image_quality_output_loss: 1.0109 - val_age_output_loss: 1.3582 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.8780 - val_pose_output_loss: 0.5816 - val_footwear_output_loss: 0.8247 - val_emotion_output_loss: 0.8928 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.7525 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 22.47158\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 43.1639 - gender_output_loss: 0.5446 - image_quality_output_loss: 0.9076 - age_output_loss: 2.7043 - weight_output_loss: 2.4786 - bag_output_loss: 1.4498 - pose_output_loss: 1.1297 - footwear_output_loss: 1.1600 - emotion_output_loss: 2.5315 - gender_output_acc: 0.7655 - image_quality_output_acc: 0.5707 - age_output_acc: 0.4133 - weight_output_acc: 0.6382 - bag_output_acc: 0.6145 - pose_output_acc: 0.7471 - footwear_output_acc: 0.6064 - emotion_output_acc: 0.7121 - val_loss: 22.7463 - val_gender_output_loss: 0.4746 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3600 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8614 - val_pose_output_loss: 0.6140 - val_footwear_output_loss: 0.8175 - val_emotion_output_loss: 0.8899 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7366 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 22.47158\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 43.2134 - gender_output_loss: 0.5478 - image_quality_output_loss: 0.9015 - age_output_loss: 2.7084 - weight_output_loss: 2.4882 - bag_output_loss: 1.4463 - pose_output_loss: 1.1285 - footwear_output_loss: 1.1571 - emotion_output_loss: 2.5391 - gender_output_acc: 0.7638 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4086 - weight_output_acc: 0.6375 - bag_output_acc: 0.6206 - pose_output_acc: 0.7491 - footwear_output_acc: 0.6064 - emotion_output_acc: 0.7122 - val_loss: 22.7014 - val_gender_output_loss: 0.4850 - val_image_quality_output_loss: 1.0013 - val_age_output_loss: 1.3596 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8771 - val_pose_output_loss: 0.5579 - val_footwear_output_loss: 0.8186 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.7773 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6399 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 22.47158\n",
            "Epoch 00009: val_loss did not improve from 22.47158\n",
            "Epoch 10/20\n",
            "360/360 [==============================] - 222s 617ms/step - loss: 43.3049 - gender_output_loss: 0.5433 - image_quality_output_loss: 0.9016 - age_output_loss: 2.7160 - weight_output_loss: 2.4934 - bag_output_loss: 1.4487 - pose_output_loss: 1.1429 - footwear_output_loss: 1.1619 - emotion_output_loss: 2.5378 - gender_output_acc: 0.7701 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4128 - weight_output_acc: 0.6395 - bag_output_acc: 0.6180 - pose_output_acc: 0.7494 - footwear_output_acc: 0.6009 - emotion_output_acc: 0.7117 - val_loss: 22.4772 - val_gender_output_loss: 0.4612 - val_image_quality_output_loss: 0.9541 - val_age_output_loss: 1.3567 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8640 - val_pose_output_loss: 0.5566 - val_footwear_output_loss: 0.8220 - val_emotion_output_loss: 0.8848 - val_gender_output_acc: 0.7817 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.6091 - val_pose_output_acc: 0.7624 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 22.47158\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 222s 618ms/step - loss: 43.1828 - gender_output_loss: 0.5407 - image_quality_output_loss: 0.9028 - age_output_loss: 2.7155 - weight_output_loss: 2.4835 - bag_output_loss: 1.4493 - pose_output_loss: 1.1266 - footwear_output_loss: 1.1573 - emotion_output_loss: 2.5301 - gender_output_acc: 0.7737 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4102 - weight_output_acc: 0.6383 - bag_output_acc: 0.6181 - pose_output_acc: 0.7477 - footwear_output_acc: 0.6122 - emotion_output_acc: 0.7107 - val_loss: 22.7525 - val_gender_output_loss: 0.4697 - val_image_quality_output_loss: 1.0087 - val_age_output_loss: 1.3632 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.8745 - val_pose_output_loss: 0.5685 - val_footwear_output_loss: 0.8216 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5228 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6091 - val_pose_output_acc: 0.7614 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.47158\n",
            "Epoch 12/20\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 43.1645 - gender_output_loss: 0.5472 - image_quality_output_loss: 0.8999 - age_output_loss: 2.7121 - weight_output_loss: 2.4836 - bag_output_loss: 1.4552 - pose_output_loss: 1.1367 - footwear_output_loss: 1.1550 - emotion_output_loss: 2.5163 - gender_output_acc: 0.7651 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4120 - weight_output_acc: 0.6379 - bag_output_acc: 0.6156 - pose_output_acc: 0.7452 - footwear_output_acc: 0.6097 - emotion_output_acc: 0.7123 - val_loss: 22.6850 - val_gender_output_loss: 0.4668 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.3569 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.8668 - val_pose_output_loss: 0.5774 - val_footwear_output_loss: 0.8197 - val_emotion_output_loss: 0.8934 - val_gender_output_acc: 0.7798 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6101 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.47158\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 223s 619ms/step - loss: 43.1618 - gender_output_loss: 0.5474 - image_quality_output_loss: 0.9077 - age_output_loss: 2.7026 - weight_output_loss: 2.4762 - bag_output_loss: 1.4500 - pose_output_loss: 1.1329 - footwear_output_loss: 1.1567 - emotion_output_loss: 2.5330 - gender_output_acc: 0.7709 - image_quality_output_acc: 0.5655 - age_output_acc: 0.4122 - weight_output_acc: 0.6393 - bag_output_acc: 0.6164 - pose_output_acc: 0.7497 - footwear_output_acc: 0.6053 - emotion_output_acc: 0.7112 - val_loss: 22.4789 - val_gender_output_loss: 0.4748 - val_image_quality_output_loss: 0.9382 - val_age_output_loss: 1.3562 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8639 - val_pose_output_loss: 0.5546 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.8924 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6091 - val_pose_output_acc: 0.7609 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.47158\n",
            "Epoch 00013: val_loss did not improve from 22.47158\n",
            "Epoch 14/20\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 43.1991 - gender_output_loss: 0.5438 - image_quality_output_loss: 0.9043 - age_output_loss: 2.7070 - weight_output_loss: 2.4846 - bag_output_loss: 1.4442 - pose_output_loss: 1.1311 - footwear_output_loss: 1.1651 - emotion_output_loss: 2.5368 - gender_output_acc: 0.7700 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4112 - weight_output_acc: 0.6396 - bag_output_acc: 0.6224 - pose_output_acc: 0.7503 - footwear_output_acc: 0.6018 - emotion_output_acc: 0.7118 - val_loss: 22.6771 - val_gender_output_loss: 0.4797 - val_image_quality_output_loss: 1.0250 - val_age_output_loss: 1.3552 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8718 - val_pose_output_loss: 0.5465 - val_footwear_output_loss: 0.8189 - val_emotion_output_loss: 0.8932 - val_gender_output_acc: 0.7783 - val_image_quality_output_acc: 0.5144 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 22.47158\n",
            "Epoch 15/20\n",
            "360/360 [==============================] - 219s 609ms/step - loss: 43.2870 - gender_output_loss: 0.5469 - image_quality_output_loss: 0.9034 - age_output_loss: 2.7214 - weight_output_loss: 2.4823 - bag_output_loss: 1.4512 - pose_output_loss: 1.1251 - footwear_output_loss: 1.1570 - emotion_output_loss: 2.5485 - gender_output_acc: 0.7648 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4137 - weight_output_acc: 0.6379 - bag_output_acc: 0.6197 - pose_output_acc: 0.7523 - footwear_output_acc: 0.6052 - emotion_output_acc: 0.7124 - val_loss: 22.5492 - val_gender_output_loss: 0.4591 - val_image_quality_output_loss: 0.9582 - val_age_output_loss: 1.3648 - val_weight_output_loss: 0.9935 - val_bag_output_loss: 0.8698 - val_pose_output_loss: 0.5440 - val_footwear_output_loss: 0.8215 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7902 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6111 - val_pose_output_acc: 0.7788 - val_footwear_output_acc: 0.6399 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 22.47158\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 219s 608ms/step - loss: 43.2337 - gender_output_loss: 0.5413 - image_quality_output_loss: 0.9029 - age_output_loss: 2.7096 - weight_output_loss: 2.4844 - bag_output_loss: 1.4541 - pose_output_loss: 1.1278 - footwear_output_loss: 1.1579 - emotion_output_loss: 2.5441 - gender_output_acc: 0.7692 - image_quality_output_acc: 0.5703 - age_output_acc: 0.4102 - weight_output_acc: 0.6391 - bag_output_acc: 0.6141 - pose_output_acc: 0.7477 - footwear_output_acc: 0.6061 - emotion_output_acc: 0.7118 - val_loss: 22.4982 - val_gender_output_loss: 0.4523 - val_image_quality_output_loss: 0.9677 - val_age_output_loss: 1.3530 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.8656 - val_pose_output_loss: 0.5645 - val_footwear_output_loss: 0.8198 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.7827 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7589 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 22.47158\n",
            "Epoch 17/20\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 43.2038 - gender_output_loss: 0.5411 - image_quality_output_loss: 0.9045 - age_output_loss: 2.7098 - weight_output_loss: 2.4881 - bag_output_loss: 1.4466 - pose_output_loss: 1.1297 - footwear_output_loss: 1.1584 - emotion_output_loss: 2.5371 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5673 - age_output_acc: 0.4103 - weight_output_acc: 0.6391 - bag_output_acc: 0.6179 - pose_output_acc: 0.7481 - footwear_output_acc: 0.6081 - emotion_output_acc: 0.7116 - val_loss: 22.6538 - val_gender_output_loss: 0.4684 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.3599 - val_weight_output_loss: 0.9970 - val_bag_output_loss: 0.8704 - val_pose_output_loss: 0.5515 - val_footwear_output_loss: 0.8167 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.7897 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7783 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 22.47158\n",
            "Epoch 18/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.1249 - gender_output_loss: 0.5444 - image_quality_output_loss: 0.9020 - age_output_loss: 2.7172 - weight_output_loss: 2.4833 - bag_output_loss: 1.4500 - pose_output_loss: 1.1044 - footwear_output_loss: 1.1551 - emotion_output_loss: 2.5315 - gender_output_acc: 0.7684 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4108 - weight_output_acc: 0.6382 - bag_output_acc: 0.6175 - pose_output_acc: 0.7518 - footwear_output_acc: 0.6089 - emotion_output_acc: 0.7108Epoch 18/20\n",
            "360/360 [==============================] - 218s 605ms/step - loss: 43.1382 - gender_output_loss: 0.5445 - image_quality_output_loss: 0.9015 - age_output_loss: 2.7187 - weight_output_loss: 2.4879 - bag_output_loss: 1.4487 - pose_output_loss: 1.1037 - footwear_output_loss: 1.1551 - emotion_output_loss: 2.5316 - gender_output_acc: 0.7682 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4106 - weight_output_acc: 0.6379 - bag_output_acc: 0.6177 - pose_output_acc: 0.7517 - footwear_output_acc: 0.6088 - emotion_output_acc: 0.7108 - val_loss: 22.5031 - val_gender_output_loss: 0.4641 - val_image_quality_output_loss: 0.9500 - val_age_output_loss: 1.3542 - val_weight_output_loss: 0.9853 - val_bag_output_loss: 0.8628 - val_pose_output_loss: 0.5607 - val_footwear_output_loss: 0.8190 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7649 - val_footwear_output_acc: 0.6295 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 22.47158\n",
            "Epoch 19/20\n",
            "360/360 [==============================] - 219s 610ms/step - loss: 43.1860 - gender_output_loss: 0.5441 - image_quality_output_loss: 0.9046 - age_output_loss: 2.7145 - weight_output_loss: 2.4887 - bag_output_loss: 1.4536 - pose_output_loss: 1.1203 - footwear_output_loss: 1.1505 - emotion_output_loss: 2.5323 - gender_output_acc: 0.7663 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4111 - weight_output_acc: 0.6405 - bag_output_acc: 0.6181 - pose_output_acc: 0.7521 - footwear_output_acc: 0.6141 - emotion_output_acc: 0.7114 - val_loss: 22.5982 - val_gender_output_loss: 0.4612 - val_image_quality_output_loss: 0.9780 - val_age_output_loss: 1.3565 - val_weight_output_loss: 0.9990 - val_bag_output_loss: 0.8641 - val_pose_output_loss: 0.5721 - val_footwear_output_loss: 0.8132 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.7808 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7550 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 22.47158\n",
            "Epoch 20/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 43.0994 - gender_output_loss: 0.5406 - image_quality_output_loss: 0.9000 - age_output_loss: 2.7041 - weight_output_loss: 2.4827 - bag_output_loss: 1.4565 - pose_output_loss: 1.1210 - footwear_output_loss: 1.1532 - emotion_output_loss: 2.5257 - gender_output_acc: 0.7686 - image_quality_output_acc: 0.5712 - age_output_acc: 0.4091 - weight_output_acc: 0.6394 - bag_output_acc: 0.6179 - pose_output_acc: 0.7503 - footwear_output_acc: 0.6107 - emotion_output_acc: 0.7119Epoch 20/20\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 43.1166 - gender_output_loss: 0.5406 - image_quality_output_loss: 0.8999 - age_output_loss: 2.7071 - weight_output_loss: 2.4841 - bag_output_loss: 1.4553 - pose_output_loss: 1.1195 - footwear_output_loss: 1.1552 - emotion_output_loss: 2.5271 - gender_output_acc: 0.7686 - image_quality_output_acc: 0.5710 - age_output_acc: 0.4092 - weight_output_acc: 0.6391 - bag_output_acc: 0.6181 - pose_output_acc: 0.7506 - footwear_output_acc: 0.6102 - emotion_output_acc: 0.7117 - val_loss: 22.8017 - val_gender_output_loss: 0.4668 - val_image_quality_output_loss: 1.0344 - val_age_output_loss: 1.3642 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.8680 - val_pose_output_loss: 0.5575 - val_footwear_output_loss: 0.8250 - val_emotion_output_loss: 0.9056 - val_gender_output_acc: 0.7852 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6007 - val_pose_output_acc: 0.7649 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6811\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 22.47158\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577514347.json\n",
            "End of run with EPOCHS= 20 STEPS_PER_EPOCH= 11537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hcXpwJsNsAn",
        "colab_type": "code",
        "outputId": "e0667274-817d-4c44-c51c-51c0c927313d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"Try multiple augmentations\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Try multiple augmentations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lsZxVEwBTWe",
        "colab_type": "code",
        "outputId": "85093511-b2f0-4133-c14e-8246fd1832da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "29/30 [============================>.] - ETA: 1s - loss: 45.9765 - gender_output_loss: 0.5712 - image_quality_output_loss: 0.9318 - age_output_loss: 2.8523 - weight_output_loss: 2.6926 - bag_output_loss: 1.4085 - pose_output_loss: 1.2380 - footwear_output_loss: 1.1824 - emotion_output_loss: 2.8392 - gender_output_acc: 0.7565 - image_quality_output_acc: 0.5582 - age_output_acc: 0.4106 - weight_output_acc: 0.6272 - bag_output_acc: 0.6131 - pose_output_acc: 0.7231 - footwear_output_acc: 0.5797 - emotion_output_acc: 0.6800\n",
            "30/30 [==============================] - 47s 2s/step - loss: 45.8594 - gender_output_loss: 0.5667 - image_quality_output_loss: 0.9281 - age_output_loss: 2.8584 - weight_output_loss: 2.6536 - bag_output_loss: 1.4112 - pose_output_loss: 1.2222 - footwear_output_loss: 1.1822 - emotion_output_loss: 2.8472 - gender_output_acc: 0.7594 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4115 - weight_output_acc: 0.6323 - bag_output_acc: 0.6094 - pose_output_acc: 0.7271 - footwear_output_acc: 0.5781 - emotion_output_acc: 0.6792 - val_loss: 22.9172 - val_gender_output_loss: 0.4853 - val_image_quality_output_loss: 1.0051 - val_age_output_loss: 1.3660 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8787 - val_pose_output_loss: 0.5957 - val_footwear_output_loss: 0.8235 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6066 - val_pose_output_acc: 0.7515 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.91717, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.001.h5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 43.8799 - gender_output_loss: 0.5523 - image_quality_output_loss: 0.9171 - age_output_loss: 2.7881 - weight_output_loss: 2.7801 - bag_output_loss: 1.5149 - pose_output_loss: 1.2294 - footwear_output_loss: 1.1899 - emotion_output_loss: 2.2534 - gender_output_acc: 0.7854 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4031 - weight_output_acc: 0.5990 - bag_output_acc: 0.6052 - pose_output_acc: 0.7333 - footwear_output_acc: 0.5948 - emotion_output_acc: 0.7365 - val_loss: 22.7980 - val_gender_output_loss: 0.4878 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.3654 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8699 - val_pose_output_loss: 0.5938 - val_footwear_output_loss: 0.8253 - val_emotion_output_loss: 0.8943 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6042 - val_pose_output_acc: 0.7460 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00002: val_loss improved from 22.91717 to 22.79799, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.002.h5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 41.7070 - gender_output_loss: 0.5820 - image_quality_output_loss: 0.9221 - age_output_loss: 2.5956 - weight_output_loss: 2.2578 - bag_output_loss: 1.4618 - pose_output_loss: 1.2146 - footwear_output_loss: 1.1660 - emotion_output_loss: 2.3400 - gender_output_acc: 0.7448 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4354 - weight_output_acc: 0.6646 - bag_output_acc: 0.6177 - pose_output_acc: 0.7260 - footwear_output_acc: 0.6073 - emotion_output_acc: 0.7375 - val_loss: 24.1625 - val_gender_output_loss: 0.4986 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.3731 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9091 - val_pose_output_loss: 0.9286 - val_footwear_output_loss: 0.8916 - val_emotion_output_loss: 0.9011 - val_gender_output_acc: 0.7584 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.5903 - val_pose_output_acc: 0.6086 - val_footwear_output_acc: 0.5977 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 22.79799\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.7780 - gender_output_loss: 0.6004 - image_quality_output_loss: 0.9308 - age_output_loss: 2.7356 - weight_output_loss: 2.5297 - bag_output_loss: 1.4926 - pose_output_loss: 1.2607 - footwear_output_loss: 1.2160 - emotion_output_loss: 2.4176 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5615 - age_output_acc: 0.3990 - weight_output_acc: 0.6146 - bag_output_acc: 0.6062 - pose_output_acc: 0.7146 - footwear_output_acc: 0.5854 - emotion_output_acc: 0.7302 - val_loss: 22.5760 - val_gender_output_loss: 0.4794 - val_image_quality_output_loss: 0.9171 - val_age_output_loss: 1.3725 - val_weight_output_loss: 0.9717 - val_bag_output_loss: 0.8702 - val_pose_output_loss: 0.5856 - val_footwear_output_loss: 0.8152 - val_emotion_output_loss: 0.8897 - val_gender_output_acc: 0.7669 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.7555 - val_footwear_output_acc: 0.6493 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00004: val_loss improved from 22.79799 to 22.57604, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.004.h5\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 44.5780 - gender_output_loss: 0.5755 - image_quality_output_loss: 0.9101 - age_output_loss: 2.6889 - weight_output_loss: 2.7433 - bag_output_loss: 1.5117 - pose_output_loss: 1.2304 - footwear_output_loss: 1.2063 - emotion_output_loss: 2.5404 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4052 - weight_output_acc: 0.6344 - bag_output_acc: 0.6021 - pose_output_acc: 0.7146 - footwear_output_acc: 0.5813 - emotion_output_acc: 0.7042 - val_loss: 23.0505 - val_gender_output_loss: 0.4788 - val_image_quality_output_loss: 0.9660 - val_age_output_loss: 1.3755 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8841 - val_pose_output_loss: 0.6547 - val_footwear_output_loss: 0.8386 - val_emotion_output_loss: 0.9010 - val_gender_output_acc: 0.7718 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5908 - val_pose_output_acc: 0.7361 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6944\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00005: val_loss did not improve from 22.57604\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.9614 - gender_output_loss: 0.5826 - image_quality_output_loss: 0.9064 - age_output_loss: 2.6007 - weight_output_loss: 2.2505 - bag_output_loss: 1.3818 - pose_output_loss: 1.2952 - footwear_output_loss: 1.1982 - emotion_output_loss: 2.6453 - gender_output_acc: 0.7365 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4302 - weight_output_acc: 0.6677 - bag_output_acc: 0.6198 - pose_output_acc: 0.7031 - footwear_output_acc: 0.5969 - emotion_output_acc: 0.7156 - val_loss: 22.7879 - val_gender_output_loss: 0.4936 - val_image_quality_output_loss: 0.9696 - val_age_output_loss: 1.3709 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.8702 - val_pose_output_loss: 0.6016 - val_footwear_output_loss: 0.8282 - val_emotion_output_loss: 0.8852 - val_gender_output_acc: 0.7604 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7431 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 22.57604\n",
            "Epoch 7/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.1439 - gender_output_loss: 0.5578 - image_quality_output_loss: 0.9106 - age_output_loss: 2.6711 - weight_output_loss: 2.3629 - bag_output_loss: 1.5104 - pose_output_loss: 1.1378 - footwear_output_loss: 1.1318 - emotion_output_loss: 2.6014 - gender_output_acc: 0.7500 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4116 - weight_output_acc: 0.6519 - bag_output_acc: 0.6045 - pose_output_acc: 0.7371 - footwear_output_acc: 0.6196 - emotion_output_acc: 0.7134Epoch 7/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 43.4986 - gender_output_loss: 0.5573 - image_quality_output_loss: 0.9107 - age_output_loss: 2.6841 - weight_output_loss: 2.3477 - bag_output_loss: 1.5153 - pose_output_loss: 1.1355 - footwear_output_loss: 1.1379 - emotion_output_loss: 2.6836 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4083 - weight_output_acc: 0.6521 - bag_output_acc: 0.6073 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6156 - emotion_output_acc: 0.7073 - val_loss: 23.3083 - val_gender_output_loss: 0.4998 - val_image_quality_output_loss: 1.0739 - val_age_output_loss: 1.3808 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.8769 - val_pose_output_loss: 0.6213 - val_footwear_output_loss: 0.8597 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5104 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6106 - val_pose_output_acc: 0.7465 - val_footwear_output_acc: 0.6022 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 22.57604\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.9850 - gender_output_loss: 0.6102 - image_quality_output_loss: 0.9217 - age_output_loss: 2.8876 - weight_output_loss: 2.6382 - bag_output_loss: 1.5303 - pose_output_loss: 1.2330 - footwear_output_loss: 1.1649 - emotion_output_loss: 2.5040 - gender_output_acc: 0.7208 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4062 - weight_output_acc: 0.6354 - bag_output_acc: 0.6052 - pose_output_acc: 0.7063 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7125 - val_loss: 23.1264 - val_gender_output_loss: 0.4803 - val_image_quality_output_loss: 1.0108 - val_age_output_loss: 1.3703 - val_weight_output_loss: 1.0017 - val_bag_output_loss: 0.8753 - val_pose_output_loss: 0.6519 - val_footwear_output_loss: 0.8446 - val_emotion_output_loss: 0.8919 - val_gender_output_acc: 0.7713 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6111 - val_pose_output_acc: 0.7257 - val_footwear_output_acc: 0.6220 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 22.57604\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 43.1600 - gender_output_loss: 0.5720 - image_quality_output_loss: 0.9112 - age_output_loss: 2.8193 - weight_output_loss: 2.4741 - bag_output_loss: 1.4833 - pose_output_loss: 1.1393 - footwear_output_loss: 1.2048 - emotion_output_loss: 2.3492 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4073 - weight_output_acc: 0.6354 - bag_output_acc: 0.5938 - pose_output_acc: 0.7323 - footwear_output_acc: 0.6052 - emotion_output_acc: 0.7292 - val_loss: 22.7242 - val_gender_output_loss: 0.4839 - val_image_quality_output_loss: 0.9648 - val_age_output_loss: 1.3664 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8654 - val_pose_output_loss: 0.5799 - val_footwear_output_loss: 0.8240 - val_emotion_output_loss: 0.8941 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7550 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 22.57604\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 37s 1s/step - loss: 43.8757 - gender_output_loss: 0.5517 - image_quality_output_loss: 0.9315 - age_output_loss: 2.8512 - weight_output_loss: 2.2054 - bag_output_loss: 1.4093 - pose_output_loss: 1.1599 - footwear_output_loss: 1.1195 - emotion_output_loss: 2.7804 - gender_output_acc: 0.7698 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3719 - weight_output_acc: 0.6594 - bag_output_acc: 0.6062 - pose_output_acc: 0.7333 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.6958 - val_loss: 23.1496 - val_gender_output_loss: 0.5541 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.3821 - val_weight_output_loss: 1.0065 - val_bag_output_loss: 0.9179 - val_pose_output_loss: 0.5818 - val_footwear_output_loss: 0.8319 - val_emotion_output_loss: 0.8911 - val_gender_output_acc: 0.7560 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 22.57604\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 35s 1s/step - loss: 45.0644 - gender_output_loss: 0.5712 - image_quality_output_loss: 0.9171 - age_output_loss: 2.7412 - weight_output_loss: 2.5741 - bag_output_loss: 1.4838 - pose_output_loss: 1.2268 - footwear_output_loss: 1.2038 - emotion_output_loss: 2.7606 - gender_output_acc: 0.7385 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4094 - weight_output_acc: 0.6333 - bag_output_acc: 0.5990 - pose_output_acc: 0.7271 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.6938 - val_loss: 22.8752 - val_gender_output_loss: 0.4900 - val_image_quality_output_loss: 0.9321 - val_age_output_loss: 1.3665 - val_weight_output_loss: 1.0059 - val_bag_output_loss: 0.8842 - val_pose_output_loss: 0.5972 - val_footwear_output_loss: 0.8642 - val_emotion_output_loss: 0.8889 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.7569 - val_footwear_output_acc: 0.6017 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.57604\n",
            "Epoch 12/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 44.4129 - gender_output_loss: 0.5579 - image_quality_output_loss: 0.8935 - age_output_loss: 2.6841 - weight_output_loss: 2.6686 - bag_output_loss: 1.4709 - pose_output_loss: 1.1870 - footwear_output_loss: 1.1462 - emotion_output_loss: 2.6706 - gender_output_acc: 0.7565 - image_quality_output_acc: 0.5841 - age_output_acc: 0.4321 - weight_output_acc: 0.6272 - bag_output_acc: 0.6261 - pose_output_acc: 0.7457 - footwear_output_acc: 0.6067 - emotion_output_acc: 0.7069\n",
            "30/30 [==============================] - 34s 1s/step - loss: 44.2512 - gender_output_loss: 0.5593 - image_quality_output_loss: 0.8929 - age_output_loss: 2.6695 - weight_output_loss: 2.6593 - bag_output_loss: 1.4696 - pose_output_loss: 1.1944 - footwear_output_loss: 1.1312 - emotion_output_loss: 2.6543 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5844 - age_output_acc: 0.4281 - weight_output_acc: 0.6271 - bag_output_acc: 0.6271 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.7073 - val_loss: 22.6831 - val_gender_output_loss: 0.5035 - val_image_quality_output_loss: 0.9049 - val_age_output_loss: 1.3612 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8690 - val_pose_output_loss: 0.5980 - val_footwear_output_loss: 0.8325 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.7589 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7465 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.57604\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 44.2512 - gender_output_loss: 0.5593 - image_quality_output_loss: 0.8929 - age_output_loss: 2.6695 - weight_output_loss: 2.6593 - bag_output_loss: 1.4696 - pose_output_loss: 1.1944 - footwear_output_loss: 1.1312 - emotion_output_loss: 2.6543 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5844 - age_output_acc: 0.4281 - weight_output_acc: 0.6271 - bag_output_acc: 0.6271 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.7073 - val_loss: 22.6831 - val_gender_output_loss: 0.5035 - val_image_quality_output_loss: 0.9049 - val_age_output_loss: 1.3612 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8690 - val_pose_output_loss: 0.5980 - val_footwear_output_loss: 0.8325 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.7589 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7465 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6974\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.4250 - gender_output_loss: 0.5924 - image_quality_output_loss: 0.9177 - age_output_loss: 2.7700 - weight_output_loss: 2.6114 - bag_output_loss: 1.3650 - pose_output_loss: 1.2069 - footwear_output_loss: 1.1620 - emotion_output_loss: 2.4079 - gender_output_acc: 0.7156 - image_quality_output_acc: 0.5594 - age_output_acc: 0.4167 - weight_output_acc: 0.6167 - bag_output_acc: 0.6302 - pose_output_acc: 0.7260 - footwear_output_acc: 0.6052 - emotion_output_acc: 0.7198 - val_loss: 23.2996 - val_gender_output_loss: 0.6252 - val_image_quality_output_loss: 0.8994 - val_age_output_loss: 1.3790 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.9025 - val_pose_output_loss: 0.6523 - val_footwear_output_loss: 0.8432 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7192 - val_image_quality_output_acc: 0.5709 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6042 - val_pose_output_acc: 0.7440 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 22.57604\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 45.3817 - gender_output_loss: 0.5861 - image_quality_output_loss: 0.9214 - age_output_loss: 2.6764 - weight_output_loss: 2.5120 - bag_output_loss: 1.5626 - pose_output_loss: 1.2591 - footwear_output_loss: 1.2190 - emotion_output_loss: 2.8509 - gender_output_acc: 0.7437 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3927 - weight_output_acc: 0.6344 - bag_output_acc: 0.5958 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.6927 - val_loss: 23.2596 - val_gender_output_loss: 0.5021 - val_image_quality_output_loss: 0.9912 - val_age_output_loss: 1.3663 - val_weight_output_loss: 0.9915 - val_bag_output_loss: 0.8715 - val_pose_output_loss: 0.6358 - val_footwear_output_loss: 0.8325 - val_emotion_output_loss: 0.9571 - val_gender_output_acc: 0.7540 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6126 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7480 - val_footwear_output_acc: 0.6295 - val_emotion_output_acc: 0.6394\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 22.57604\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.8820 - gender_output_loss: 0.5553 - image_quality_output_loss: 0.9252 - age_output_loss: 2.7398 - weight_output_loss: 2.2087 - bag_output_loss: 1.4825 - pose_output_loss: 1.2409 - footwear_output_loss: 1.2158 - emotion_output_loss: 2.9790 - gender_output_acc: 0.7469 - image_quality_output_acc: 0.5396 - age_output_acc: 0.3958 - weight_output_acc: 0.6615 - bag_output_acc: 0.6062 - pose_output_acc: 0.7302 - footwear_output_acc: 0.5865 - emotion_output_acc: 0.6854 - val_loss: 22.7694 - val_gender_output_loss: 0.4875 - val_image_quality_output_loss: 0.9616 - val_age_output_loss: 1.3650 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.8734 - val_pose_output_loss: 0.5851 - val_footwear_output_loss: 0.8313 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7500 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 22.57604\n",
            "Epoch 16/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.1529 - gender_output_loss: 0.5357 - image_quality_output_loss: 0.9218 - age_output_loss: 2.8528 - weight_output_loss: 2.5548 - bag_output_loss: 1.4635 - pose_output_loss: 1.2238 - footwear_output_loss: 1.1936 - emotion_output_loss: 2.7238 - gender_output_acc: 0.7802 - image_quality_output_acc: 0.5603 - age_output_acc: 0.4149 - weight_output_acc: 0.6369 - bag_output_acc: 0.5959 - pose_output_acc: 0.7284 - footwear_output_acc: 0.5970 - emotion_output_acc: 0.6994\n",
            "Epoch 00015: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 32s 1s/step - loss: 45.0841 - gender_output_loss: 0.5429 - image_quality_output_loss: 0.9221 - age_output_loss: 2.8462 - weight_output_loss: 2.5263 - bag_output_loss: 1.4909 - pose_output_loss: 1.2167 - footwear_output_loss: 1.1877 - emotion_output_loss: 2.7184 - gender_output_acc: 0.7760 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4177 - weight_output_acc: 0.6406 - bag_output_acc: 0.5938 - pose_output_acc: 0.7271 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.6990 - val_loss: 22.7808 - val_gender_output_loss: 0.4810 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.3634 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8725 - val_pose_output_loss: 0.6025 - val_footwear_output_loss: 0.8379 - val_emotion_output_loss: 0.8966 - val_gender_output_acc: 0.7664 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 22.57604\n",
            "Epoch 17/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 42.1620 - gender_output_loss: 0.5665 - image_quality_output_loss: 0.9090 - age_output_loss: 2.6144 - weight_output_loss: 2.3570 - bag_output_loss: 1.3996 - pose_output_loss: 1.1936 - footwear_output_loss: 1.1300 - emotion_output_loss: 2.4562 - gender_output_acc: 0.7586 - image_quality_output_acc: 0.5636 - age_output_acc: 0.4030 - weight_output_acc: 0.6562 - bag_output_acc: 0.6067 - pose_output_acc: 0.7349 - footwear_output_acc: 0.6142 - emotion_output_acc: 0.7155\n",
            "Epoch 00016: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.0117 - gender_output_loss: 0.5689 - image_quality_output_loss: 0.9143 - age_output_loss: 2.6135 - weight_output_loss: 2.3539 - bag_output_loss: 1.4036 - pose_output_loss: 1.1888 - footwear_output_loss: 1.1407 - emotion_output_loss: 2.4132 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4021 - weight_output_acc: 0.6573 - bag_output_acc: 0.6052 - pose_output_acc: 0.7354 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.7198 - val_loss: 23.1235 - val_gender_output_loss: 0.5210 - val_image_quality_output_loss: 1.0186 - val_age_output_loss: 1.3666 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8966 - val_pose_output_loss: 0.6142 - val_footwear_output_loss: 0.8326 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.7560 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7391 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 22.57604\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.6173 - gender_output_loss: 0.5695 - image_quality_output_loss: 0.9172 - age_output_loss: 2.8862 - weight_output_loss: 2.4959 - bag_output_loss: 1.5384 - pose_output_loss: 1.1151 - footwear_output_loss: 1.1779 - emotion_output_loss: 2.6193 - gender_output_acc: 0.7594 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4021 - weight_output_acc: 0.6385 - bag_output_acc: 0.6115 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7083 - val_loss: 22.9072 - val_gender_output_loss: 0.4926 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9971 - val_bag_output_loss: 0.8729 - val_pose_output_loss: 0.5750 - val_footwear_output_loss: 0.8497 - val_emotion_output_loss: 0.8898 - val_gender_output_acc: 0.7589 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7614 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 22.57604\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.8144 - gender_output_loss: 0.5472 - image_quality_output_loss: 0.9207 - age_output_loss: 2.7107 - weight_output_loss: 2.4901 - bag_output_loss: 1.4224 - pose_output_loss: 1.1660 - footwear_output_loss: 1.1281 - emotion_output_loss: 2.4317 - gender_output_acc: 0.7792 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4073 - weight_output_acc: 0.6417 - bag_output_acc: 0.6177 - pose_output_acc: 0.7323 - footwear_output_acc: 0.6156 - emotion_output_acc: 0.7260 - val_loss: 23.3607 - val_gender_output_loss: 0.5480 - val_image_quality_output_loss: 1.0035 - val_age_output_loss: 1.3857 - val_weight_output_loss: 0.9997 - val_bag_output_loss: 0.9126 - val_pose_output_loss: 0.5975 - val_footwear_output_loss: 0.8595 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.7619 - val_image_quality_output_acc: 0.5198 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7550 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 22.57604\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 22.57604\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.1176 - gender_output_loss: 0.5807 - image_quality_output_loss: 0.8771 - age_output_loss: 2.6711 - weight_output_loss: 2.4894 - bag_output_loss: 1.4579 - pose_output_loss: 1.2383 - footwear_output_loss: 1.1454 - emotion_output_loss: 2.2132 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4042 - weight_output_acc: 0.6292 - bag_output_acc: 0.6156 - pose_output_acc: 0.7104 - footwear_output_acc: 0.6115 - emotion_output_acc: 0.7406 - val_loss: 24.2600 - val_gender_output_loss: 0.4947 - val_image_quality_output_loss: 1.2046 - val_age_output_loss: 1.3766 - val_weight_output_loss: 0.9995 - val_bag_output_loss: 0.8860 - val_pose_output_loss: 0.8105 - val_footwear_output_loss: 0.8733 - val_emotion_output_loss: 0.9259 - val_gender_output_acc: 0.7679 - val_image_quality_output_acc: 0.4653 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.6781 - val_footwear_output_acc: 0.6037 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 22.57604\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.7714 - gender_output_loss: 0.5591 - image_quality_output_loss: 0.9113 - age_output_loss: 2.7872 - weight_output_loss: 2.4536 - bag_output_loss: 1.4992 - pose_output_loss: 1.3352 - footwear_output_loss: 1.1792 - emotion_output_loss: 2.6608 - gender_output_acc: 0.7927 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4115 - weight_output_acc: 0.6510 - bag_output_acc: 0.6240 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5813 - emotion_output_acc: 0.7031 - val_loss: 23.0992 - val_gender_output_loss: 0.5120 - val_image_quality_output_loss: 0.9574 - val_age_output_loss: 1.3780 - val_weight_output_loss: 1.0012 - val_bag_output_loss: 0.8960 - val_pose_output_loss: 0.6146 - val_footwear_output_loss: 0.8315 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.7659 - val_image_quality_output_acc: 0.5332 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.7406 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 22.57604\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.2446 - gender_output_loss: 0.5912 - image_quality_output_loss: 0.8959 - age_output_loss: 2.7669 - weight_output_loss: 2.6152 - bag_output_loss: 1.4677 - pose_output_loss: 1.2532 - footwear_output_loss: 1.1880 - emotion_output_loss: 2.2505 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3938 - weight_output_acc: 0.6198 - bag_output_acc: 0.6073 - pose_output_acc: 0.7281 - footwear_output_acc: 0.6021 - emotion_output_acc: 0.7500 - val_loss: 23.2029 - val_gender_output_loss: 0.5313 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.3811 - val_weight_output_loss: 1.0028 - val_bag_output_loss: 0.9018 - val_pose_output_loss: 0.5943 - val_footwear_output_loss: 0.8371 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.7436 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7579 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 22.57604\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.3382 - gender_output_loss: 0.5721 - image_quality_output_loss: 0.9306 - age_output_loss: 2.7141 - weight_output_loss: 2.7720 - bag_output_loss: 1.3345 - pose_output_loss: 1.2007 - footwear_output_loss: 1.1959 - emotion_output_loss: 2.5868 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5365 - age_output_acc: 0.4104 - weight_output_acc: 0.6062 - bag_output_acc: 0.6292 - pose_output_acc: 0.7437 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7063 - val_loss: 23.0346 - val_gender_output_loss: 0.4905 - val_image_quality_output_loss: 0.9470 - val_age_output_loss: 1.3751 - val_weight_output_loss: 1.0027 - val_bag_output_loss: 0.8756 - val_pose_output_loss: 0.6494 - val_footwear_output_loss: 0.8369 - val_emotion_output_loss: 0.8966 - val_gender_output_acc: 0.7634 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.7282 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 22.57604\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.7467 - gender_output_loss: 0.5965 - image_quality_output_loss: 0.9181 - age_output_loss: 2.7373 - weight_output_loss: 2.4817 - bag_output_loss: 1.5127 - pose_output_loss: 1.0882 - footwear_output_loss: 1.1349 - emotion_output_loss: 2.6088 - gender_output_acc: 0.7375 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4187 - weight_output_acc: 0.6521 - bag_output_acc: 0.6073 - pose_output_acc: 0.7604 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7042 - val_loss: 22.7567 - val_gender_output_loss: 0.4910 - val_image_quality_output_loss: 0.9418 - val_age_output_loss: 1.3734 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.8707 - val_pose_output_loss: 0.5861 - val_footwear_output_loss: 0.8211 - val_emotion_output_loss: 0.9005 - val_gender_output_acc: 0.7649 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7535 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 22.57604\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 41.6337 - gender_output_loss: 0.5281 - image_quality_output_loss: 0.9064 - age_output_loss: 2.3936 - weight_output_loss: 2.4589 - bag_output_loss: 1.5033 - pose_output_loss: 1.1004 - footwear_output_loss: 1.1110 - emotion_output_loss: 2.4911 - gender_output_acc: 0.7719 - image_quality_output_acc: 0.5865 - age_output_acc: 0.4260 - weight_output_acc: 0.6562 - bag_output_acc: 0.6260 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.7156 - val_loss: 22.6111 - val_gender_output_loss: 0.4835 - val_image_quality_output_loss: 0.9229 - val_age_output_loss: 1.3646 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8656 - val_pose_output_loss: 0.5840 - val_footwear_output_loss: 0.8193 - val_emotion_output_loss: 0.8960 - val_gender_output_acc: 0.7664 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6409 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 22.57604\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 45.0481 - gender_output_loss: 0.6015 - image_quality_output_loss: 0.8834 - age_output_loss: 2.7920 - weight_output_loss: 2.6095 - bag_output_loss: 1.4815 - pose_output_loss: 1.3352 - footwear_output_loss: 1.1674 - emotion_output_loss: 2.6203 - gender_output_acc: 0.7271 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4042 - weight_output_acc: 0.6187 - bag_output_acc: 0.6135 - pose_output_acc: 0.6969 - footwear_output_acc: 0.6177 - emotion_output_acc: 0.7073 - val_loss: 22.8871 - val_gender_output_loss: 0.4787 - val_image_quality_output_loss: 1.0154 - val_age_output_loss: 1.3690 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8738 - val_pose_output_loss: 0.5812 - val_footwear_output_loss: 0.8414 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.7783 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7574 - val_footwear_output_acc: 0.6136 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 22.57604\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.5188 - gender_output_loss: 0.5462 - image_quality_output_loss: 0.8874 - age_output_loss: 2.8184 - weight_output_loss: 2.4926 - bag_output_loss: 1.4443 - pose_output_loss: 1.1100 - footwear_output_loss: 1.1461 - emotion_output_loss: 2.2825 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5844 - age_output_acc: 0.3979 - weight_output_acc: 0.6500 - bag_output_acc: 0.6250 - pose_output_acc: 0.7437 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.7385 - val_loss: 24.0876 - val_gender_output_loss: 0.6618 - val_image_quality_output_loss: 1.0754 - val_age_output_loss: 1.3896 - val_weight_output_loss: 1.0072 - val_bag_output_loss: 0.9502 - val_pose_output_loss: 0.6469 - val_footwear_output_loss: 0.8545 - val_emotion_output_loss: 0.9294 - val_gender_output_acc: 0.7242 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7321 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 22.57604\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.7766 - gender_output_loss: 0.5608 - image_quality_output_loss: 0.9039 - age_output_loss: 2.7980 - weight_output_loss: 2.3988 - bag_output_loss: 1.5372 - pose_output_loss: 1.2058 - footwear_output_loss: 1.1826 - emotion_output_loss: 2.7623 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5906 - age_output_acc: 0.4052 - weight_output_acc: 0.6469 - bag_output_acc: 0.6021 - pose_output_acc: 0.7365 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.6906 - val_loss: 22.9400 - val_gender_output_loss: 0.4833 - val_image_quality_output_loss: 0.9682 - val_age_output_loss: 1.3700 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.6495 - val_footwear_output_loss: 0.8288 - val_emotion_output_loss: 0.8854 - val_gender_output_acc: 0.7654 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5903 - val_pose_output_acc: 0.7257 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 22.57604\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.5891 - gender_output_loss: 0.5853 - image_quality_output_loss: 0.9003 - age_output_loss: 2.6561 - weight_output_loss: 2.5011 - bag_output_loss: 1.5119 - pose_output_loss: 1.3436 - footwear_output_loss: 1.2336 - emotion_output_loss: 2.6604 - gender_output_acc: 0.7469 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4177 - weight_output_acc: 0.6500 - bag_output_acc: 0.6250 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.7063 - val_loss: 22.9624 - val_gender_output_loss: 0.5008 - val_image_quality_output_loss: 0.9376 - val_age_output_loss: 1.3647 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.9311 - val_pose_output_loss: 0.6129 - val_footwear_output_loss: 0.8357 - val_emotion_output_loss: 0.8862 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5526 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5660 - val_pose_output_acc: 0.7386 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 22.57604\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.0872 - gender_output_loss: 0.5571 - image_quality_output_loss: 0.9386 - age_output_loss: 2.8289 - weight_output_loss: 2.4442 - bag_output_loss: 1.5135 - pose_output_loss: 1.2737 - footwear_output_loss: 1.2236 - emotion_output_loss: 2.4561 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.5281 - age_output_acc: 0.4104 - weight_output_acc: 0.6323 - bag_output_acc: 0.6010 - pose_output_acc: 0.7240 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7188 - val_loss: 22.6831 - val_gender_output_loss: 0.4812 - val_image_quality_output_loss: 0.9225 - val_age_output_loss: 1.3609 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8816 - val_pose_output_loss: 0.6105 - val_footwear_output_loss: 0.8328 - val_emotion_output_loss: 0.8831 - val_gender_output_acc: 0.7693 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5913 - val_pose_output_acc: 0.7326 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 22.57604\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.2489 - gender_output_loss: 0.5670 - image_quality_output_loss: 0.9173 - age_output_loss: 2.6227 - weight_output_loss: 2.4121 - bag_output_loss: 1.5336 - pose_output_loss: 1.1538 - footwear_output_loss: 1.1333 - emotion_output_loss: 2.6026 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4208 - weight_output_acc: 0.6427 - bag_output_acc: 0.6385 - pose_output_acc: 0.7406 - footwear_output_acc: 0.6135 - emotion_output_acc: 0.7198 - val_loss: 22.6918 - val_gender_output_loss: 0.4854 - val_image_quality_output_loss: 0.9510 - val_age_output_loss: 1.3672 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.8695 - val_pose_output_loss: 0.5849 - val_footwear_output_loss: 0.8325 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7455 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 22.57604\n",
            "Epoch 32/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 42.4362 - gender_output_loss: 0.5513 - image_quality_output_loss: 0.9405 - age_output_loss: 2.7227 - weight_output_loss: 2.4331 - bag_output_loss: 1.3215 - pose_output_loss: 1.1333 - footwear_output_loss: 1.1574 - emotion_output_loss: 2.4423 - gender_output_acc: 0.7694 - image_quality_output_acc: 0.5409 - age_output_acc: 0.4149 - weight_output_acc: 0.6325 - bag_output_acc: 0.6207 - pose_output_acc: 0.7500 - footwear_output_acc: 0.5916 - emotion_output_acc: 0.7274\n",
            "Epoch 00031: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.3731 - gender_output_loss: 0.5512 - image_quality_output_loss: 0.9401 - age_output_loss: 2.7349 - weight_output_loss: 2.4397 - bag_output_loss: 1.3108 - pose_output_loss: 1.1235 - footwear_output_loss: 1.1572 - emotion_output_loss: 2.4252 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4135 - weight_output_acc: 0.6313 - bag_output_acc: 0.6271 - pose_output_acc: 0.7510 - footwear_output_acc: 0.5927 - emotion_output_acc: 0.7281 - val_loss: 22.9067 - val_gender_output_loss: 0.4827 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.3709 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8806 - val_pose_output_loss: 0.6139 - val_footwear_output_loss: 0.8469 - val_emotion_output_loss: 0.8898 - val_gender_output_acc: 0.7783 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.7371 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 22.57604\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 45.0695 - gender_output_loss: 0.5790 - image_quality_output_loss: 0.9238 - age_output_loss: 2.8581 - weight_output_loss: 2.7505 - bag_output_loss: 1.4033 - pose_output_loss: 1.1336 - footwear_output_loss: 1.1712 - emotion_output_loss: 2.6533 - gender_output_acc: 0.7344 - image_quality_output_acc: 0.5573 - age_output_acc: 0.3833 - weight_output_acc: 0.6125 - bag_output_acc: 0.6167 - pose_output_acc: 0.7406 - footwear_output_acc: 0.5969 - emotion_output_acc: 0.7042 - val_loss: 22.6036 - val_gender_output_loss: 0.4710 - val_image_quality_output_loss: 0.9177 - val_age_output_loss: 1.3625 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.8719 - val_pose_output_loss: 0.6024 - val_footwear_output_loss: 0.8228 - val_emotion_output_loss: 0.8815 - val_gender_output_acc: 0.7808 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7431 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 22.57604\n",
            "Epoch 34/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 42.9366 - gender_output_loss: 0.5662 - image_quality_output_loss: 0.9191 - age_output_loss: 2.6240 - weight_output_loss: 2.2932 - bag_output_loss: 1.4825 - pose_output_loss: 1.2895 - footwear_output_loss: 1.1229 - emotion_output_loss: 2.5538 - gender_output_acc: 0.7522 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4224 - weight_output_acc: 0.6562 - bag_output_acc: 0.5797 - pose_output_acc: 0.7177 - footwear_output_acc: 0.6099 - emotion_output_acc: 0.7112\n",
            "Epoch 00033: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.2210 - gender_output_loss: 0.5651 - image_quality_output_loss: 0.9213 - age_output_loss: 2.6265 - weight_output_loss: 2.3229 - bag_output_loss: 1.4692 - pose_output_loss: 1.2919 - footwear_output_loss: 1.1289 - emotion_output_loss: 2.6047 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5427 - age_output_acc: 0.4250 - weight_output_acc: 0.6521 - bag_output_acc: 0.5844 - pose_output_acc: 0.7177 - footwear_output_acc: 0.6083 - emotion_output_acc: 0.7073 - val_loss: 23.1585 - val_gender_output_loss: 0.5024 - val_image_quality_output_loss: 1.0698 - val_age_output_loss: 1.3773 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8752 - val_pose_output_loss: 0.5950 - val_footwear_output_loss: 0.8341 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.7713 - val_image_quality_output_acc: 0.4871 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.7495 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.6756\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 22.57604\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.5667 - gender_output_loss: 0.5764 - image_quality_output_loss: 0.9271 - age_output_loss: 2.9214 - weight_output_loss: 2.2510 - bag_output_loss: 1.5444 - pose_output_loss: 1.2805 - footwear_output_loss: 1.1846 - emotion_output_loss: 2.6159 - gender_output_acc: 0.7365 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3885 - weight_output_acc: 0.6573 - bag_output_acc: 0.5948 - pose_output_acc: 0.7198 - footwear_output_acc: 0.6021 - emotion_output_acc: 0.7063 - val_loss: 22.8682 - val_gender_output_loss: 0.4933 - val_image_quality_output_loss: 0.9558 - val_age_output_loss: 1.3790 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8777 - val_pose_output_loss: 0.5893 - val_footwear_output_loss: 0.8579 - val_emotion_output_loss: 0.8927 - val_gender_output_acc: 0.7693 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7530 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 22.57604\n",
            "Epoch 36/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.9270 - gender_output_loss: 0.5725 - image_quality_output_loss: 0.9195 - age_output_loss: 2.8786 - weight_output_loss: 2.9199 - bag_output_loss: 1.3708 - pose_output_loss: 1.3397 - footwear_output_loss: 1.1671 - emotion_output_loss: 2.5975 - gender_output_acc: 0.7522 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4030 - weight_output_acc: 0.6034 - bag_output_acc: 0.6282 - pose_output_acc: 0.6929 - footwear_output_acc: 0.5894 - emotion_output_acc: 0.7015\n",
            "Epoch 00035: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 34s 1s/step - loss: 45.7428 - gender_output_loss: 0.5734 - image_quality_output_loss: 0.9189 - age_output_loss: 2.8828 - weight_output_loss: 2.8861 - bag_output_loss: 1.3596 - pose_output_loss: 1.3350 - footwear_output_loss: 1.1759 - emotion_output_loss: 2.5801 - gender_output_acc: 0.7510 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3969 - weight_output_acc: 0.6104 - bag_output_acc: 0.6313 - pose_output_acc: 0.6917 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7021 - val_loss: 22.8794 - val_gender_output_loss: 0.4779 - val_image_quality_output_loss: 0.9582 - val_age_output_loss: 1.3846 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8706 - val_pose_output_loss: 0.6102 - val_footwear_output_loss: 0.8344 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.7669 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7480 - val_footwear_output_acc: 0.6314 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 22.57604\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.8238 - gender_output_loss: 0.6147 - image_quality_output_loss: 0.9225 - age_output_loss: 2.7413 - weight_output_loss: 2.7585 - bag_output_loss: 1.3545 - pose_output_loss: 1.1966 - footwear_output_loss: 1.1141 - emotion_output_loss: 2.7037 - gender_output_acc: 0.7271 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3938 - weight_output_acc: 0.6271 - bag_output_acc: 0.6313 - pose_output_acc: 0.7156 - footwear_output_acc: 0.6250 - emotion_output_acc: 0.7010 - val_loss: 22.6372 - val_gender_output_loss: 0.4755 - val_image_quality_output_loss: 0.9432 - val_age_output_loss: 1.3730 - val_weight_output_loss: 0.9887 - val_bag_output_loss: 0.8668 - val_pose_output_loss: 0.5849 - val_footwear_output_loss: 0.8315 - val_emotion_output_loss: 0.8778 - val_gender_output_acc: 0.7684 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7465 - val_footwear_output_acc: 0.6305 - val_emotion_output_acc: 0.7054\n",
            "30/30 [==============================]\n",
            "Epoch 00037: val_loss did not improve from 22.57604\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.2588 - gender_output_loss: 0.5880 - image_quality_output_loss: 0.8976 - age_output_loss: 2.7285 - weight_output_loss: 2.5208 - bag_output_loss: 1.4380 - pose_output_loss: 1.1515 - footwear_output_loss: 1.1102 - emotion_output_loss: 2.7526 - gender_output_acc: 0.7469 - image_quality_output_acc: 0.5802 - age_output_acc: 0.4083 - weight_output_acc: 0.6365 - bag_output_acc: 0.6167 - pose_output_acc: 0.7437 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.6979 - val_loss: 23.0283 - val_gender_output_loss: 0.4843 - val_image_quality_output_loss: 1.0541 - val_age_output_loss: 1.3629 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8809 - val_pose_output_loss: 0.6051 - val_footwear_output_loss: 0.8298 - val_emotion_output_loss: 0.9015 - val_gender_output_acc: 0.7684 - val_image_quality_output_acc: 0.5084 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.7436 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 22.57604\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.2334 - gender_output_loss: 0.5399 - image_quality_output_loss: 0.9217 - age_output_loss: 2.7546 - weight_output_loss: 2.2174 - bag_output_loss: 1.5177 - pose_output_loss: 1.2305 - footwear_output_loss: 1.1841 - emotion_output_loss: 2.8037 - gender_output_acc: 0.7719 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4052 - weight_output_acc: 0.6521 - bag_output_acc: 0.6219 - pose_output_acc: 0.7292 - footwear_output_acc: 0.6115 - emotion_output_acc: 0.6958 - val_loss: 22.7875 - val_gender_output_loss: 0.4706 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.3636 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8674 - val_pose_output_loss: 0.5935 - val_footwear_output_loss: 0.8534 - val_emotion_output_loss: 0.8922 - val_gender_output_acc: 0.7693 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6200 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 22.57604\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.6501 - gender_output_loss: 0.5491 - image_quality_output_loss: 0.9230 - age_output_loss: 2.7157 - weight_output_loss: 2.7660 - bag_output_loss: 1.3880 - pose_output_loss: 1.1527 - footwear_output_loss: 1.1687 - emotion_output_loss: 2.4435 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4094 - weight_output_acc: 0.6146 - bag_output_acc: 0.6250 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6115 - emotion_output_acc: 0.7094 - val_loss: 22.8364 - val_gender_output_loss: 0.4953 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3722 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8840 - val_pose_output_loss: 0.5628 - val_footwear_output_loss: 0.8357 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7584 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 22.57604\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.6486 - gender_output_loss: 0.5535 - image_quality_output_loss: 0.9196 - age_output_loss: 2.7181 - weight_output_loss: 2.3799 - bag_output_loss: 1.4690 - pose_output_loss: 1.0978 - footwear_output_loss: 1.2127 - emotion_output_loss: 2.6883 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5510 - age_output_acc: 0.4083 - weight_output_acc: 0.6479 - bag_output_acc: 0.6260 - pose_output_acc: 0.7448 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7083 - val_loss: 22.8455 - val_gender_output_loss: 0.4975 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3736 - val_weight_output_loss: 0.9916 - val_bag_output_loss: 0.8833 - val_pose_output_loss: 0.5733 - val_footwear_output_loss: 0.8317 - val_emotion_output_loss: 0.8978 - val_gender_output_acc: 0.7669 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.7560 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.6501 - gender_output_loss: 0.5491 - image_quality_output_loss: 0.9230 - age_output_loss: 2.7157 - weight_output_loss: 2.7660 - bag_output_loss: 1.3880 - pose_output_loss: 1.1527 - footwear_output_loss: 1.1687 - emotion_output_loss: 2.4435 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4094 - weight_output_acc: 0.6146 - bag_output_acc: 0.6250 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6115 - emotion_output_acc: 0.7094 - val_loss: 22.8364 - val_gender_output_loss: 0.4953 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3722 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8840 - val_pose_output_loss: 0.5628 - val_footwear_output_loss: 0.8357 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.7614 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7584 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 22.57604Epoch 42/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.0816 - gender_output_loss: 0.5700 - image_quality_output_loss: 0.9062 - age_output_loss: 2.6940 - weight_output_loss: 2.3640 - bag_output_loss: 1.5494 - pose_output_loss: 1.1559 - footwear_output_loss: 1.1570 - emotion_output_loss: 2.5049 - gender_output_acc: 0.7489 - image_quality_output_acc: 0.5819 - age_output_acc: 0.4149 - weight_output_acc: 0.6649 - bag_output_acc: 0.5938 - pose_output_acc: 0.7231 - footwear_output_acc: 0.6088 - emotion_output_acc: 0.7155\n",
            "Epoch 00041: val_loss did not improve from 22.57604\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.1764 - gender_output_loss: 0.5801 - image_quality_output_loss: 0.9082 - age_output_loss: 2.6837 - weight_output_loss: 2.3252 - bag_output_loss: 1.5503 - pose_output_loss: 1.1437 - footwear_output_loss: 1.1564 - emotion_output_loss: 2.5708 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4146 - weight_output_acc: 0.6687 - bag_output_acc: 0.5938 - pose_output_acc: 0.7271 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.7125 - val_loss: 22.4935 - val_gender_output_loss: 0.4763 - val_image_quality_output_loss: 0.9075 - val_age_output_loss: 1.3622 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8733 - val_pose_output_loss: 0.5740 - val_footwear_output_loss: 0.8309 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.7783 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7515 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.7049\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss improved from 22.57604 to 22.49350, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.042.h5\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.4676 - gender_output_loss: 0.5359 - image_quality_output_loss: 0.9149 - age_output_loss: 2.9578 - weight_output_loss: 2.5570 - bag_output_loss: 1.3861 - pose_output_loss: 1.1447 - footwear_output_loss: 1.1629 - emotion_output_loss: 2.5836 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3802 - weight_output_acc: 0.6438 - bag_output_acc: 0.6396 - pose_output_acc: 0.7365 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.7031 - val_loss: 23.5202 - val_gender_output_loss: 0.5067 - val_image_quality_output_loss: 1.0772 - val_age_output_loss: 1.3832 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8899 - val_pose_output_loss: 0.6900 - val_footwear_output_loss: 0.8332 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.4965 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.5947 - val_pose_output_acc: 0.7143 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 22.49350\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 45.6047 - gender_output_loss: 0.6009 - image_quality_output_loss: 0.9100 - age_output_loss: 2.8113 - weight_output_loss: 2.9389 - bag_output_loss: 1.5907 - pose_output_loss: 1.2995 - footwear_output_loss: 1.2093 - emotion_output_loss: 2.4053 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4062 - weight_output_acc: 0.5979 - bag_output_acc: 0.5708 - pose_output_acc: 0.6865 - footwear_output_acc: 0.5813 - emotion_output_acc: 0.7333 - val_loss: 23.0697 - val_gender_output_loss: 0.5047 - val_image_quality_output_loss: 0.9031 - val_age_output_loss: 1.4016 - val_weight_output_loss: 1.0302 - val_bag_output_loss: 0.8701 - val_pose_output_loss: 0.6424 - val_footwear_output_loss: 0.8379 - val_emotion_output_loss: 0.8833 - val_gender_output_acc: 0.7634 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5853 - val_bag_output_acc: 0.6076 - val_pose_output_acc: 0.7331 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 22.49350\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 45.8696 - gender_output_loss: 0.6131 - image_quality_output_loss: 0.9281 - age_output_loss: 2.7572 - weight_output_loss: 2.5960 - bag_output_loss: 1.5353 - pose_output_loss: 1.3378 - footwear_output_loss: 1.2284 - emotion_output_loss: 2.7711 - gender_output_acc: 0.7260 - image_quality_output_acc: 0.5406 - age_output_acc: 0.3906 - weight_output_acc: 0.6146 - bag_output_acc: 0.5854 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5885 - emotion_output_acc: 0.6948 - val_loss: 23.0775 - val_gender_output_loss: 0.4820 - val_image_quality_output_loss: 1.0468 - val_age_output_loss: 1.3741 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.8637 - val_pose_output_loss: 0.5965 - val_footwear_output_loss: 0.8603 - val_emotion_output_loss: 0.9183 - val_gender_output_acc: 0.7763 - val_image_quality_output_acc: 0.4866 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7530 - val_footwear_output_acc: 0.6156 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 22.49350\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.1726 - gender_output_loss: 0.5926 - image_quality_output_loss: 0.9392 - age_output_loss: 2.7450 - weight_output_loss: 2.4344 - bag_output_loss: 1.5844 - pose_output_loss: 1.2886 - footwear_output_loss: 1.1437 - emotion_output_loss: 2.2774 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5365 - age_output_acc: 0.4135 - weight_output_acc: 0.6417 - bag_output_acc: 0.5792 - pose_output_acc: 0.7229 - footwear_output_acc: 0.6073 - emotion_output_acc: 0.7365 - val_loss: 22.6870 - val_gender_output_loss: 0.5167 - val_image_quality_output_loss: 0.9307 - val_age_output_loss: 1.3593 - val_weight_output_loss: 0.9723 - val_bag_output_loss: 0.8703 - val_pose_output_loss: 0.5944 - val_footwear_output_loss: 0.8194 - val_emotion_output_loss: 0.8989 - val_gender_output_acc: 0.7639 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7515 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 22.49350\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.0788 - gender_output_loss: 0.5640 - image_quality_output_loss: 0.9152 - age_output_loss: 2.7013 - weight_output_loss: 2.2958 - bag_output_loss: 1.4289 - pose_output_loss: 1.1312 - footwear_output_loss: 1.2220 - emotion_output_loss: 2.3734 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4229 - weight_output_acc: 0.6583 - bag_output_acc: 0.6271 - pose_output_acc: 0.7396 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7219 - val_loss: 22.5917 - val_gender_output_loss: 0.4686 - val_image_quality_output_loss: 0.9611 - val_age_output_loss: 1.3626 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8631 - val_pose_output_loss: 0.5795 - val_footwear_output_loss: 0.8253 - val_emotion_output_loss: 0.8909 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7609 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 22.49350\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 42.3288 - gender_output_loss: 0.5685 - image_quality_output_loss: 0.9055 - age_output_loss: 2.6710 - weight_output_loss: 2.3463 - bag_output_loss: 1.3712 - pose_output_loss: 1.2277 - footwear_output_loss: 1.0901 - emotion_output_loss: 2.4679 - gender_output_acc: 0.7552 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4208 - weight_output_acc: 0.6448 - bag_output_acc: 0.6396 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6625 - emotion_output_acc: 0.7250 - val_loss: 22.5617 - val_gender_output_loss: 0.4734 - val_image_quality_output_loss: 0.9526 - val_age_output_loss: 1.3600 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8656 - val_pose_output_loss: 0.5788 - val_footwear_output_loss: 0.8173 - val_emotion_output_loss: 0.8904 - val_gender_output_acc: 0.7778 - val_image_quality_output_acc: 0.5352 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7574 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 22.49350\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.4895 - gender_output_loss: 0.5483 - image_quality_output_loss: 0.9072 - age_output_loss: 2.7486 - weight_output_loss: 2.3185 - bag_output_loss: 1.4908 - pose_output_loss: 1.1980 - footwear_output_loss: 1.1927 - emotion_output_loss: 2.8419 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4104 - weight_output_acc: 0.6656 - bag_output_acc: 0.6031 - pose_output_acc: 0.7302 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.6833 - val_loss: 22.8099 - val_gender_output_loss: 0.4686 - val_image_quality_output_loss: 1.0233 - val_age_output_loss: 1.3635 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.8620 - val_pose_output_loss: 0.5803 - val_footwear_output_loss: 0.8469 - val_emotion_output_loss: 0.8998 - val_gender_output_acc: 0.7798 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 22.49350\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 45.3044 - gender_output_loss: 0.5824 - image_quality_output_loss: 0.9403 - age_output_loss: 2.7595 - weight_output_loss: 2.6494 - bag_output_loss: 1.5085 - pose_output_loss: 1.1899 - footwear_output_loss: 1.2082 - emotion_output_loss: 2.7379 - gender_output_acc: 0.7594 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4042 - weight_output_acc: 0.6240 - bag_output_acc: 0.5948 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5719 - emotion_output_acc: 0.7010 - val_loss: 22.6875 - val_gender_output_loss: 0.4652 - val_image_quality_output_loss: 0.9590 - val_age_output_loss: 1.3586 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8644 - val_pose_output_loss: 0.5846 - val_footwear_output_loss: 0.8429 - val_emotion_output_loss: 0.9052 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7560 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.6930\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 22.49350\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.9062 - gender_output_loss: 0.5540 - image_quality_output_loss: 0.8981 - age_output_loss: 2.7189 - weight_output_loss: 2.6374 - bag_output_loss: 1.3833 - pose_output_loss: 1.1224 - footwear_output_loss: 1.0919 - emotion_output_loss: 2.4260 - gender_output_acc: 0.7615 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4177 - weight_output_acc: 0.6229 - bag_output_acc: 0.6354 - pose_output_acc: 0.7531 - footwear_output_acc: 0.6469 - emotion_output_acc: 0.7208 - val_loss: 22.7280 - val_gender_output_loss: 0.4832 - val_image_quality_output_loss: 0.9395 - val_age_output_loss: 1.3620 - val_weight_output_loss: 1.0067 - val_bag_output_loss: 0.8786 - val_pose_output_loss: 0.5814 - val_footwear_output_loss: 0.8272 - val_emotion_output_loss: 0.8927 - val_gender_output_acc: 0.7768 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.5655 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7624 - val_footwear_output_acc: 0.6404 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 22.49350\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.5419 - gender_output_loss: 0.5729 - image_quality_output_loss: 0.9090 - age_output_loss: 2.6783 - weight_output_loss: 2.5268 - bag_output_loss: 1.4400 - pose_output_loss: 1.2712 - footwear_output_loss: 1.1523 - emotion_output_loss: 2.5094 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.5740 - age_output_acc: 0.4313 - weight_output_acc: 0.6448 - bag_output_acc: 0.6177 - pose_output_acc: 0.7156 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7146 - val_loss: 24.7260 - val_gender_output_loss: 0.5786 - val_image_quality_output_loss: 1.1362 - val_age_output_loss: 1.4162 - val_weight_output_loss: 1.0148 - val_bag_output_loss: 0.9210 - val_pose_output_loss: 0.9038 - val_footwear_output_loss: 0.8633 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.7460 - val_image_quality_output_acc: 0.4871 - val_age_output_acc: 0.3552 - val_weight_output_acc: 0.5883 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.5967 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 22.49350\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.5442 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.8948 - age_output_loss: 2.6570 - weight_output_loss: 2.3813 - bag_output_loss: 1.3454 - pose_output_loss: 1.2098 - footwear_output_loss: 1.1241 - emotion_output_loss: 2.5425 - gender_output_acc: 0.7677 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4104 - weight_output_acc: 0.6427 - bag_output_acc: 0.6323 - pose_output_acc: 0.7260 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.7260 - val_loss: 23.4611 - val_gender_output_loss: 0.4787 - val_image_quality_output_loss: 1.1398 - val_age_output_loss: 1.3773 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8839 - val_pose_output_loss: 0.6380 - val_footwear_output_loss: 0.8466 - val_emotion_output_loss: 0.9253 - val_gender_output_acc: 0.7822 - val_image_quality_output_acc: 0.4782 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.7391 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 22.49350\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 42.5442 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.8948 - age_output_loss: 2.6570 - weight_output_loss: 2.3813 - bag_output_loss: 1.3454 - pose_output_loss: 1.2098 - footwear_output_loss: 1.1241 - emotion_output_loss: 2.5425 - gender_output_acc: 0.7677 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4104 - weight_output_acc: 0.6427 - bag_output_acc: 0.6323 - pose_output_acc: 0.7260 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.7260 - val_loss: 23.4611 - val_gender_output_loss: 0.4787 - val_image_quality_output_loss: 1.1398 - val_age_output_loss: 1.3773 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8839 - val_pose_output_loss: 0.6380 - val_footwear_output_loss: 0.8466 - val_emotion_output_loss: 0.9253 - val_gender_output_acc: 0.7822 - val_image_quality_output_acc: 0.4782 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.5888 - val_pose_output_acc: 0.7391 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6915\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.2521 - gender_output_loss: 0.5646 - image_quality_output_loss: 0.9200 - age_output_loss: 2.8887 - weight_output_loss: 2.4524 - bag_output_loss: 1.5067 - pose_output_loss: 1.2187 - footwear_output_loss: 1.1598 - emotion_output_loss: 2.2668 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3729 - weight_output_acc: 0.6323 - bag_output_acc: 0.6021 - pose_output_acc: 0.7177 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7271 - val_loss: 23.3405 - val_gender_output_loss: 0.5535 - val_image_quality_output_loss: 0.9540 - val_age_output_loss: 1.3760 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9041 - val_pose_output_loss: 0.6937 - val_footwear_output_loss: 0.8385 - val_emotion_output_loss: 0.8978 - val_gender_output_acc: 0.7490 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.5977 - val_pose_output_acc: 0.7044 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 22.49350\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.4172 - gender_output_loss: 0.5618 - image_quality_output_loss: 0.9074 - age_output_loss: 2.7315 - weight_output_loss: 2.6117 - bag_output_loss: 1.4419 - pose_output_loss: 1.2436 - footwear_output_loss: 1.1842 - emotion_output_loss: 2.6214 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5740 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.6177 - pose_output_acc: 0.7344 - footwear_output_acc: 0.5927 - emotion_output_acc: 0.7052 - val_loss: 22.5657 - val_gender_output_loss: 0.4642 - val_image_quality_output_loss: 0.9615 - val_age_output_loss: 1.3642 - val_weight_output_loss: 0.9740 - val_bag_output_loss: 0.8622 - val_pose_output_loss: 0.5752 - val_footwear_output_loss: 0.8195 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.7753 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7555 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 22.49350\n",
            "Epoch 56/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.4904 - gender_output_loss: 0.5708 - image_quality_output_loss: 0.9183 - age_output_loss: 2.7396 - weight_output_loss: 2.6596 - bag_output_loss: 1.4663 - pose_output_loss: 1.1426 - footwear_output_loss: 1.1622 - emotion_output_loss: 2.4040 - gender_output_acc: 0.7295 - image_quality_output_acc: 0.5733 - age_output_acc: 0.4041 - weight_output_acc: 0.6401 - bag_output_acc: 0.6185 - pose_output_acc: 0.7360 - footwear_output_acc: 0.6272 - emotion_output_acc: 0.7231Epoch 56/100\n",
            "\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.4818 - gender_output_loss: 0.5675 - image_quality_output_loss: 0.9113 - age_output_loss: 2.7371 - weight_output_loss: 2.6700 - bag_output_loss: 1.4774 - pose_output_loss: 1.1338 - footwear_output_loss: 1.1536 - emotion_output_loss: 2.4044 - gender_output_acc: 0.7312 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4031 - weight_output_acc: 0.6354 - bag_output_acc: 0.6104 - pose_output_acc: 0.7375 - footwear_output_acc: 0.6323 - emotion_output_acc: 0.7219 - val_loss: 22.7227 - val_gender_output_loss: 0.4772 - val_image_quality_output_loss: 0.9877 - val_age_output_loss: 1.3652 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.8713 - val_pose_output_loss: 0.5833 - val_footwear_output_loss: 0.8325 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 22.49350\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.0952 - gender_output_loss: 0.5413 - image_quality_output_loss: 0.9189 - age_output_loss: 2.7459 - weight_output_loss: 2.3487 - bag_output_loss: 1.5339 - pose_output_loss: 1.1789 - footwear_output_loss: 1.1424 - emotion_output_loss: 2.4786 - gender_output_acc: 0.7708 - image_quality_output_acc: 0.5719 - age_output_acc: 0.3979 - weight_output_acc: 0.6417 - bag_output_acc: 0.5917 - pose_output_acc: 0.7292 - footwear_output_acc: 0.6052 - emotion_output_acc: 0.7177 - val_loss: 22.5492 - val_gender_output_loss: 0.4669 - val_image_quality_output_loss: 0.9652 - val_age_output_loss: 1.3608 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8677 - val_pose_output_loss: 0.5680 - val_footwear_output_loss: 0.8249 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6066 - val_pose_output_acc: 0.7579 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 22.49350\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.7219 - gender_output_loss: 0.5384 - image_quality_output_loss: 0.9095 - age_output_loss: 2.8057 - weight_output_loss: 2.1748 - bag_output_loss: 1.5004 - pose_output_loss: 1.1639 - footwear_output_loss: 1.1483 - emotion_output_loss: 2.7456 - gender_output_acc: 0.7802 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4292 - weight_output_acc: 0.6594 - bag_output_acc: 0.6333 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.6979 - val_loss: 23.0944 - val_gender_output_loss: 0.5115 - val_image_quality_output_loss: 1.0482 - val_age_output_loss: 1.3742 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8912 - val_pose_output_loss: 0.5877 - val_footwear_output_loss: 0.8302 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5094 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7555 - val_footwear_output_acc: 0.6409 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 22.49350\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.5234 - gender_output_loss: 0.5806 - image_quality_output_loss: 0.9263 - age_output_loss: 2.5895 - weight_output_loss: 2.5383 - bag_output_loss: 1.4682 - pose_output_loss: 1.1877 - footwear_output_loss: 1.2033 - emotion_output_loss: 2.5889 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4208 - weight_output_acc: 0.6250 - bag_output_acc: 0.5990 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7146 - val_loss: 23.6770 - val_gender_output_loss: 0.5043 - val_image_quality_output_loss: 1.0103 - val_age_output_loss: 1.3659 - val_weight_output_loss: 0.9923 - val_bag_output_loss: 0.8778 - val_pose_output_loss: 0.8463 - val_footwear_output_loss: 0.8287 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.6453 - val_footwear_output_acc: 0.6295 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 22.49350\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 34s 1s/step - loss: 44.2422 - gender_output_loss: 0.5892 - image_quality_output_loss: 0.9105 - age_output_loss: 2.7720 - weight_output_loss: 2.5833 - bag_output_loss: 1.4775 - pose_output_loss: 1.1821 - footwear_output_loss: 1.1647 - emotion_output_loss: 2.5726 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3865 - weight_output_acc: 0.6229 - bag_output_acc: 0.5906 - pose_output_acc: 0.7208 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.7104 - val_loss: 25.0370 - val_gender_output_loss: 0.7501 - val_image_quality_output_loss: 0.9378 - val_age_output_loss: 1.4926 - val_weight_output_loss: 1.0552 - val_bag_output_loss: 1.0095 - val_pose_output_loss: 0.6842 - val_footwear_output_loss: 0.9034 - val_emotion_output_loss: 0.9577 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5188 - val_pose_output_acc: 0.7490 - val_footwear_output_acc: 0.5794 - val_emotion_output_acc: 0.6523\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 22.49350\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.0588 - gender_output_loss: 0.5600 - image_quality_output_loss: 0.8842 - age_output_loss: 2.7094 - weight_output_loss: 2.5851 - bag_output_loss: 1.4308 - pose_output_loss: 1.2813 - footwear_output_loss: 1.1938 - emotion_output_loss: 2.5618 - gender_output_acc: 0.7510 - image_quality_output_acc: 0.5979 - age_output_acc: 0.4146 - weight_output_acc: 0.6406 - bag_output_acc: 0.6146 - pose_output_acc: 0.7260 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7188 - val_loss: 23.1611 - val_gender_output_loss: 0.4888 - val_image_quality_output_loss: 0.9319 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8717 - val_pose_output_loss: 0.7231 - val_footwear_output_loss: 0.8330 - val_emotion_output_loss: 0.8898 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5962 - val_pose_output_acc: 0.6944 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 22.49350\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.9050 - gender_output_loss: 0.5779 - image_quality_output_loss: 0.9076 - age_output_loss: 2.5728 - weight_output_loss: 2.3182 - bag_output_loss: 1.4973 - pose_output_loss: 1.2317 - footwear_output_loss: 1.2043 - emotion_output_loss: 2.8216 - gender_output_acc: 0.7417 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3958 - weight_output_acc: 0.6687 - bag_output_acc: 0.5969 - pose_output_acc: 0.7385 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.6896 - val_loss: 22.6139 - val_gender_output_loss: 0.4710 - val_image_quality_output_loss: 0.9183 - val_age_output_loss: 1.3648 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8660 - val_pose_output_loss: 0.5958 - val_footwear_output_loss: 0.8248 - val_emotion_output_loss: 0.9002 - val_gender_output_acc: 0.7783 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7540 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 22.49350\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.3792 - gender_output_loss: 0.5858 - image_quality_output_loss: 0.9385 - age_output_loss: 2.9096 - weight_output_loss: 2.4866 - bag_output_loss: 1.3653 - pose_output_loss: 1.1701 - footwear_output_loss: 1.1910 - emotion_output_loss: 2.3597 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4083 - weight_output_acc: 0.6469 - bag_output_acc: 0.6281 - pose_output_acc: 0.7344 - footwear_output_acc: 0.5729 - emotion_output_acc: 0.7177 - val_loss: 22.6990 - val_gender_output_loss: 0.4885 - val_image_quality_output_loss: 0.9287 - val_age_output_loss: 1.3691 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8890 - val_pose_output_loss: 0.5811 - val_footwear_output_loss: 0.8214 - val_emotion_output_loss: 0.8945 - val_gender_output_acc: 0.7693 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.5938 - val_pose_output_acc: 0.7698 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 22.49350\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.9160 - gender_output_loss: 0.5936 - image_quality_output_loss: 0.9143 - age_output_loss: 2.8518 - weight_output_loss: 2.5278 - bag_output_loss: 1.4481 - pose_output_loss: 1.1053 - footwear_output_loss: 1.1426 - emotion_output_loss: 2.5396 - gender_output_acc: 0.7229 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4073 - weight_output_acc: 0.6427 - bag_output_acc: 0.6292 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6052 - emotion_output_acc: 0.7063 - val_loss: 22.8617 - val_gender_output_loss: 0.4885 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3706 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8905 - val_pose_output_loss: 0.5783 - val_footwear_output_loss: 0.8316 - val_emotion_output_loss: 0.8992 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7589 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 22.49350\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.1329 - gender_output_loss: 0.5520 - image_quality_output_loss: 0.9082 - age_output_loss: 2.8204 - weight_output_loss: 2.6440 - bag_output_loss: 1.3773 - pose_output_loss: 1.1666 - footwear_output_loss: 1.2342 - emotion_output_loss: 2.5234 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3896 - weight_output_acc: 0.6333 - bag_output_acc: 0.6365 - pose_output_acc: 0.7396 - footwear_output_acc: 0.5854 - emotion_output_acc: 0.7167 - val_loss: 22.7816 - val_gender_output_loss: 0.4787 - val_image_quality_output_loss: 0.9618 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8816 - val_pose_output_loss: 0.5945 - val_footwear_output_loss: 0.8311 - val_emotion_output_loss: 0.8924 - val_gender_output_acc: 0.7793 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7450 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 22.49350\n",
            "Epoch 66/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.4505 - gender_output_loss: 0.5673 - image_quality_output_loss: 0.9272 - age_output_loss: 2.7484 - weight_output_loss: 2.5170 - bag_output_loss: 1.4398 - pose_output_loss: 1.2013 - footwear_output_loss: 1.1362 - emotion_output_loss: 2.4791 - gender_output_acc: 0.7522 - image_quality_output_acc: 0.5474 - age_output_acc: 0.4224 - weight_output_acc: 0.6315 - bag_output_acc: 0.5948 - pose_output_acc: 0.7295 - footwear_output_acc: 0.6369 - emotion_output_acc: 0.7198\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.2862 - gender_output_loss: 0.5649 - image_quality_output_loss: 0.9217 - age_output_loss: 2.7360 - weight_output_loss: 2.5300 - bag_output_loss: 1.4386 - pose_output_loss: 1.1972 - footwear_output_loss: 1.1469 - emotion_output_loss: 2.4432 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4260 - weight_output_acc: 0.6323 - bag_output_acc: 0.5917 - pose_output_acc: 0.7323 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7229 - val_loss: 22.7586 - val_gender_output_loss: 0.4797 - val_image_quality_output_loss: 0.9644 - val_age_output_loss: 1.3755 - val_weight_output_loss: 0.9957 - val_bag_output_loss: 0.8776 - val_pose_output_loss: 0.5675 - val_footwear_output_loss: 0.8260 - val_emotion_output_loss: 0.8972 - val_gender_output_acc: 0.7644 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.5923 - val_bag_output_acc: 0.5967 - val_pose_output_acc: 0.7664 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 22.49350\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.0819 - gender_output_loss: 0.5866 - image_quality_output_loss: 0.9181 - age_output_loss: 2.7350 - weight_output_loss: 2.4258 - bag_output_loss: 1.6043 - pose_output_loss: 1.2982 - footwear_output_loss: 1.1518 - emotion_output_loss: 2.5099 - gender_output_acc: 0.7271 - image_quality_output_acc: 0.5469 - age_output_acc: 0.4156 - weight_output_acc: 0.6292 - bag_output_acc: 0.5750 - pose_output_acc: 0.7063 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7094 - val_loss: 23.0217 - val_gender_output_loss: 0.4774 - val_image_quality_output_loss: 0.9231 - val_age_output_loss: 1.3659 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8719 - val_pose_output_loss: 0.7259 - val_footwear_output_loss: 0.8340 - val_emotion_output_loss: 0.8893 - val_gender_output_acc: 0.7609 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.6786 - val_footwear_output_acc: 0.6280 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 22.49350\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.2917 - gender_output_loss: 0.5466 - image_quality_output_loss: 0.9221 - age_output_loss: 2.7775 - weight_output_loss: 2.4522 - bag_output_loss: 1.5904 - pose_output_loss: 1.1648 - footwear_output_loss: 1.1144 - emotion_output_loss: 2.3971 - gender_output_acc: 0.7740 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4021 - weight_output_acc: 0.6479 - bag_output_acc: 0.5927 - pose_output_acc: 0.7365 - footwear_output_acc: 0.6260 - emotion_output_acc: 0.7375 - val_loss: 23.6064 - val_gender_output_loss: 0.4823 - val_image_quality_output_loss: 1.1528 - val_age_output_loss: 1.3869 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8764 - val_pose_output_loss: 0.6247 - val_footwear_output_loss: 0.8929 - val_emotion_output_loss: 0.9379 - val_gender_output_acc: 0.7634 - val_image_quality_output_acc: 0.4598 - val_age_output_acc: 0.3651 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.7485 - val_footwear_output_acc: 0.5967 - val_emotion_output_acc: 0.6572\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 22.49350\n",
            "Epoch 68/100Epoch 69/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.5759 - gender_output_loss: 0.5710 - image_quality_output_loss: 0.9064 - age_output_loss: 2.6660 - weight_output_loss: 2.4205 - bag_output_loss: 1.5868 - pose_output_loss: 1.1940 - footwear_output_loss: 1.2082 - emotion_output_loss: 2.7831 - gender_output_acc: 0.7635 - image_quality_output_acc: 0.5750 - age_output_acc: 0.3938 - weight_output_acc: 0.6365 - bag_output_acc: 0.5917 - pose_output_acc: 0.7375 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.6958 - val_loss: 22.9596 - val_gender_output_loss: 0.4826 - val_image_quality_output_loss: 1.0361 - val_age_output_loss: 1.3675 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8717 - val_pose_output_loss: 0.5888 - val_footwear_output_loss: 0.8331 - val_emotion_output_loss: 0.9108 - val_gender_output_acc: 0.7753 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7530 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6850\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 22.49350\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.5756 - gender_output_loss: 0.5612 - image_quality_output_loss: 0.9098 - age_output_loss: 2.6012 - weight_output_loss: 2.3404 - bag_output_loss: 1.4977 - pose_output_loss: 1.1380 - footwear_output_loss: 1.1616 - emotion_output_loss: 2.5435 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4000 - weight_output_acc: 0.6385 - bag_output_acc: 0.6156 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7104 - val_loss: 22.9375 - val_gender_output_loss: 0.4647 - val_image_quality_output_loss: 1.0250 - val_age_output_loss: 1.3873 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8618 - val_pose_output_loss: 0.5873 - val_footwear_output_loss: 0.8493 - val_emotion_output_loss: 0.9034 - val_gender_output_acc: 0.7877 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7525 - val_footwear_output_acc: 0.6106 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 22.49350\n",
            "Epoch 71/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 44.4754 - gender_output_loss: 0.5647 - image_quality_output_loss: 0.8873 - age_output_loss: 2.7440 - weight_output_loss: 2.5174 - bag_output_loss: 1.4323 - pose_output_loss: 1.2144 - footwear_output_loss: 1.2053 - emotion_output_loss: 2.7222 - gender_output_acc: 0.7608 - image_quality_output_acc: 0.5830 - age_output_acc: 0.3998 - weight_output_acc: 0.6261 - bag_output_acc: 0.6099 - pose_output_acc: 0.7263 - footwear_output_acc: 0.5948 - emotion_output_acc: 0.6950\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.1896 - gender_output_loss: 0.5623 - image_quality_output_loss: 0.8916 - age_output_loss: 2.7212 - weight_output_loss: 2.4910 - bag_output_loss: 1.4360 - pose_output_loss: 1.2106 - footwear_output_loss: 1.2053 - emotion_output_loss: 2.6926 - gender_output_acc: 0.7635 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4031 - weight_output_acc: 0.6271 - bag_output_acc: 0.6062 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5948 - emotion_output_acc: 0.6979 - val_loss: 22.6244 - val_gender_output_loss: 0.4799 - val_image_quality_output_loss: 0.9157 - val_age_output_loss: 1.3636 - val_weight_output_loss: 0.9903 - val_bag_output_loss: 0.8680 - val_pose_output_loss: 0.5892 - val_footwear_output_loss: 0.8247 - val_emotion_output_loss: 0.8958 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.5947 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 22.49350\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.6912 - gender_output_loss: 0.5549 - image_quality_output_loss: 0.9135 - age_output_loss: 2.7314 - weight_output_loss: 2.7390 - bag_output_loss: 1.3941 - pose_output_loss: 1.1197 - footwear_output_loss: 1.1252 - emotion_output_loss: 2.5042 - gender_output_acc: 0.7573 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3979 - weight_output_acc: 0.6250 - bag_output_acc: 0.6104 - pose_output_acc: 0.7552 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7177 - val_loss: 22.8629 - val_gender_output_loss: 0.4809 - val_image_quality_output_loss: 0.9521 - val_age_output_loss: 1.3691 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8841 - val_pose_output_loss: 0.6171 - val_footwear_output_loss: 0.8423 - val_emotion_output_loss: 0.8927 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.5982 - val_pose_output_acc: 0.7361 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 22.49350\n",
            "Epoch 00072: val_loss did not improve from 22.49350\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.7609 - gender_output_loss: 0.5646 - image_quality_output_loss: 0.8795 - age_output_loss: 2.8874 - weight_output_loss: 2.6408 - bag_output_loss: 1.5079 - pose_output_loss: 1.1639 - footwear_output_loss: 1.1875 - emotion_output_loss: 2.5518 - gender_output_acc: 0.7385 - image_quality_output_acc: 0.5990 - age_output_acc: 0.4062 - weight_output_acc: 0.6042 - bag_output_acc: 0.6042 - pose_output_acc: 0.7344 - footwear_output_acc: 0.5729 - emotion_output_acc: 0.7115 - val_loss: 22.8896 - val_gender_output_loss: 0.4531 - val_image_quality_output_loss: 1.0062 - val_age_output_loss: 1.3686 - val_weight_output_loss: 0.9917 - val_bag_output_loss: 0.8611 - val_pose_output_loss: 0.6043 - val_footwear_output_loss: 0.8718 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.7892 - val_image_quality_output_acc: 0.5193 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.5962 - val_pose_output_acc: 0.7361 - val_footwear_output_acc: 0.6141 - val_emotion_output_acc: 0.6964\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 22.49350\n",
            "Epoch 74/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.1044 - gender_output_loss: 0.5528 - image_quality_output_loss: 0.9250 - age_output_loss: 2.8908 - weight_output_loss: 2.7336 - bag_output_loss: 1.4806 - pose_output_loss: 1.2742 - footwear_output_loss: 1.2046 - emotion_output_loss: 2.4770 - gender_output_acc: 0.7586 - image_quality_output_acc: 0.5496 - age_output_acc: 0.4159 - weight_output_acc: 0.6207 - bag_output_acc: 0.6121 - pose_output_acc: 0.7134 - footwear_output_acc: 0.5884 - emotion_output_acc: 0.7166\n",
            "Epoch 00073: val_loss did not improve from 22.49350\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.9093 - gender_output_loss: 0.5573 - image_quality_output_loss: 0.9215 - age_output_loss: 2.8818 - weight_output_loss: 2.7214 - bag_output_loss: 1.4699 - pose_output_loss: 1.2661 - footwear_output_loss: 1.2035 - emotion_output_loss: 2.4605 - gender_output_acc: 0.7531 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4146 - weight_output_acc: 0.6187 - bag_output_acc: 0.6135 - pose_output_acc: 0.7177 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7177 - val_loss: 22.6223 - val_gender_output_loss: 0.4743 - val_image_quality_output_loss: 0.9502 - val_age_output_loss: 1.3703 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.8604 - val_pose_output_loss: 0.5756 - val_footwear_output_loss: 0.8155 - val_emotion_output_loss: 0.8912 - val_gender_output_acc: 0.7679 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.7530 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 22.49350\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 45.3548 - gender_output_loss: 0.5646 - image_quality_output_loss: 0.9415 - age_output_loss: 2.7201 - weight_output_loss: 2.4591 - bag_output_loss: 1.5598 - pose_output_loss: 1.1941 - footwear_output_loss: 1.1907 - emotion_output_loss: 2.9098 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5188 - age_output_acc: 0.4010 - weight_output_acc: 0.6427 - bag_output_acc: 0.5875 - pose_output_acc: 0.7156 - footwear_output_acc: 0.6083 - emotion_output_acc: 0.6875 - val_loss: 23.5528 - val_gender_output_loss: 0.4856 - val_image_quality_output_loss: 1.0699 - val_age_output_loss: 1.4330 - val_weight_output_loss: 1.0071 - val_bag_output_loss: 0.8906 - val_pose_output_loss: 0.6313 - val_footwear_output_loss: 0.8541 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.4712 - val_age_output_acc: 0.3557 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.5918 - val_pose_output_acc: 0.7321 - val_footwear_output_acc: 0.6195 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 22.49350\n",
            "Epoch 76/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 44.1097 - gender_output_loss: 0.5760 - image_quality_output_loss: 0.9272 - age_output_loss: 2.6021 - weight_output_loss: 2.7477 - bag_output_loss: 1.4686 - pose_output_loss: 1.2578 - footwear_output_loss: 1.1950 - emotion_output_loss: 2.5201 - gender_output_acc: 0.7468 - image_quality_output_acc: 0.5593 - age_output_acc: 0.4256 - weight_output_acc: 0.6239 - bag_output_acc: 0.5927 - pose_output_acc: 0.7037 - footwear_output_acc: 0.5884 - emotion_output_acc: 0.7198Epoch 76/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.0170 - gender_output_loss: 0.5737 - image_quality_output_loss: 0.9214 - age_output_loss: 2.6039 - weight_output_loss: 2.7342 - bag_output_loss: 1.4602 - pose_output_loss: 1.2589 - footwear_output_loss: 1.2009 - emotion_output_loss: 2.5118 - gender_output_acc: 0.7531 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4271 - weight_output_acc: 0.6250 - bag_output_acc: 0.5927 - pose_output_acc: 0.7031 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7198 - val_loss: 23.0153 - val_gender_output_loss: 0.4612 - val_image_quality_output_loss: 0.8918 - val_age_output_loss: 1.3686 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.8584 - val_pose_output_loss: 0.7573 - val_footwear_output_loss: 0.8277 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.7748 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.6920 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 22.49350\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.3589 - gender_output_loss: 0.5807 - image_quality_output_loss: 0.9091 - age_output_loss: 2.6978 - weight_output_loss: 2.4266 - bag_output_loss: 1.5371 - pose_output_loss: 1.2413 - footwear_output_loss: 1.1508 - emotion_output_loss: 2.7174 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5740 - age_output_acc: 0.4115 - weight_output_acc: 0.6687 - bag_output_acc: 0.6031 - pose_output_acc: 0.7094 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7031 - val_loss: 24.2942 - val_gender_output_loss: 0.4788 - val_image_quality_output_loss: 1.0143 - val_age_output_loss: 1.3865 - val_weight_output_loss: 0.9947 - val_bag_output_loss: 0.9015 - val_pose_output_loss: 0.9680 - val_footwear_output_loss: 0.8589 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.7718 - val_image_quality_output_acc: 0.5188 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5918 - val_pose_output_acc: 0.5878 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 22.49350\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.2249 - gender_output_loss: 0.5273 - image_quality_output_loss: 0.9019 - age_output_loss: 2.6578 - weight_output_loss: 2.5159 - bag_output_loss: 1.3171 - pose_output_loss: 1.2263 - footwear_output_loss: 1.1585 - emotion_output_loss: 2.6097 - gender_output_acc: 0.7792 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4083 - weight_output_acc: 0.6281 - bag_output_acc: 0.6562 - pose_output_acc: 0.7167 - footwear_output_acc: 0.6198 - emotion_output_acc: 0.7063 - val_loss: 23.2008 - val_gender_output_loss: 0.5096 - val_image_quality_output_loss: 1.0160 - val_age_output_loss: 1.3827 - val_weight_output_loss: 0.9722 - val_bag_output_loss: 0.8983 - val_pose_output_loss: 0.6463 - val_footwear_output_loss: 0.8461 - val_emotion_output_loss: 0.8938 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.5203 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7222 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 22.49350\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.7752 - gender_output_loss: 0.5529 - image_quality_output_loss: 0.9108 - age_output_loss: 2.6978 - weight_output_loss: 2.6481 - bag_output_loss: 1.4831 - pose_output_loss: 1.2291 - footwear_output_loss: 1.1570 - emotion_output_loss: 2.7151 - gender_output_acc: 0.7656 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4156 - weight_output_acc: 0.6333 - bag_output_acc: 0.6083 - pose_output_acc: 0.7198 - footwear_output_acc: 0.6219 - emotion_output_acc: 0.7052 - val_loss: 22.6783 - val_gender_output_loss: 0.4785 - val_image_quality_output_loss: 0.9823 - val_age_output_loss: 1.3572 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8678 - val_pose_output_loss: 0.5892 - val_footwear_output_loss: 0.8242 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7520 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 22.49350\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.1419 - gender_output_loss: 0.5390 - image_quality_output_loss: 0.9088 - age_output_loss: 2.7687 - weight_output_loss: 2.4499 - bag_output_loss: 1.4317 - pose_output_loss: 1.1094 - footwear_output_loss: 1.2273 - emotion_output_loss: 2.4857 - gender_output_acc: 0.7760 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4031 - weight_output_acc: 0.6458 - bag_output_acc: 0.6031 - pose_output_acc: 0.7656 - footwear_output_acc: 0.5688 - emotion_output_acc: 0.7292 - val_loss: 22.5640 - val_gender_output_loss: 0.4778 - val_image_quality_output_loss: 0.9666 - val_age_output_loss: 1.3606 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.8706 - val_pose_output_loss: 0.5570 - val_footwear_output_loss: 0.8238 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7753 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7634 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 22.49350\n",
            "Epoch 81/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 40.8906 - gender_output_loss: 0.6196 - image_quality_output_loss: 0.9308 - age_output_loss: 2.5211 - weight_output_loss: 2.1816 - bag_output_loss: 1.4409 - pose_output_loss: 1.2629 - footwear_output_loss: 1.1974 - emotion_output_loss: 2.2133 - gender_output_acc: 0.7220 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4203 - weight_output_acc: 0.6703 - bag_output_acc: 0.5894 - pose_output_acc: 0.7231 - footwear_output_acc: 0.6013 - emotion_output_acc: 0.7349\n",
            "30/30 [==============================] - 32s 1s/step - loss: 41.0393 - gender_output_loss: 0.6201 - image_quality_output_loss: 0.9328 - age_output_loss: 2.5178 - weight_output_loss: 2.1931 - bag_output_loss: 1.4314 - pose_output_loss: 1.2451 - footwear_output_loss: 1.2007 - emotion_output_loss: 2.2628 - gender_output_acc: 0.7208 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4208 - weight_output_acc: 0.6687 - bag_output_acc: 0.5896 - pose_output_acc: 0.7302 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7302 - val_loss: 22.4412 - val_gender_output_loss: 0.4748 - val_image_quality_output_loss: 0.9329 - val_age_output_loss: 1.3550 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8673 - val_pose_output_loss: 0.5673 - val_footwear_output_loss: 0.8172 - val_emotion_output_loss: 0.8884 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.7609 - val_footwear_output_acc: 0.6429 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00081: val_loss improved from 22.49350 to 22.44117, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.081.h5\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 41.6251 - gender_output_loss: 0.5578 - image_quality_output_loss: 0.9100 - age_output_loss: 2.6504 - weight_output_loss: 2.3074 - bag_output_loss: 1.4297 - pose_output_loss: 1.0891 - footwear_output_loss: 1.1042 - emotion_output_loss: 2.4000 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3938 - weight_output_acc: 0.6458 - bag_output_acc: 0.6156 - pose_output_acc: 0.7417 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7260 - val_loss: 23.0551 - val_gender_output_loss: 0.4640 - val_image_quality_output_loss: 1.0437 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8675 - val_pose_output_loss: 0.6390 - val_footwear_output_loss: 0.8722 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7272 - val_footwear_output_acc: 0.6176 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 22.44117\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 41.6251 - gender_output_loss: 0.5578 - image_quality_output_loss: 0.9100 - age_output_loss: 2.6504 - weight_output_loss: 2.3074 - bag_output_loss: 1.4297 - pose_output_loss: 1.0891 - footwear_output_loss: 1.1042 - emotion_output_loss: 2.4000 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3938 - weight_output_acc: 0.6458 - bag_output_acc: 0.6156 - pose_output_acc: 0.7417 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7260 - val_loss: 23.0551 - val_gender_output_loss: 0.4640 - val_image_quality_output_loss: 1.0437 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8675 - val_pose_output_loss: 0.6390 - val_footwear_output_loss: 0.8722 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7272 - val_footwear_output_acc: 0.6176 - val_emotion_output_acc: 0.7014\n",
            "30/30 [==============================] - 32s 1s/step - loss: 43.4352 - gender_output_loss: 0.5472 - image_quality_output_loss: 0.9011 - age_output_loss: 2.8158 - weight_output_loss: 2.5185 - bag_output_loss: 1.4097 - pose_output_loss: 1.2000 - footwear_output_loss: 1.2023 - emotion_output_loss: 2.4214 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5875 - age_output_acc: 0.3896 - weight_output_acc: 0.6292 - bag_output_acc: 0.6385 - pose_output_acc: 0.7500 - footwear_output_acc: 0.5948 - emotion_output_acc: 0.7250 - val_loss: 22.8262 - val_gender_output_loss: 0.4724 - val_image_quality_output_loss: 0.9261 - val_age_output_loss: 1.3673 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8743 - val_pose_output_loss: 0.6516 - val_footwear_output_loss: 0.8297 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.7723 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7123 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 22.44117\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 45.5019 - gender_output_loss: 0.5789 - image_quality_output_loss: 0.9086 - age_output_loss: 2.8587 - weight_output_loss: 2.4689 - bag_output_loss: 1.5278 - pose_output_loss: 1.2619 - footwear_output_loss: 1.1294 - emotion_output_loss: 2.8143 - gender_output_acc: 0.7510 - image_quality_output_acc: 0.5698 - age_output_acc: 0.3927 - weight_output_acc: 0.6583 - bag_output_acc: 0.6021 - pose_output_acc: 0.7083 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.6854 - val_loss: 23.0464 - val_gender_output_loss: 0.5089 - val_image_quality_output_loss: 0.9754 - val_age_output_loss: 1.3710 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.8793 - val_pose_output_loss: 0.6333 - val_footwear_output_loss: 0.8152 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.7485 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7460 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6627\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 22.44117\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 43.3260 - gender_output_loss: 0.5883 - image_quality_output_loss: 0.9146 - age_output_loss: 2.6942 - weight_output_loss: 2.4058 - bag_output_loss: 1.4419 - pose_output_loss: 1.1600 - footwear_output_loss: 1.1745 - emotion_output_loss: 2.5929 - gender_output_acc: 0.7344 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4052 - weight_output_acc: 0.6500 - bag_output_acc: 0.6000 - pose_output_acc: 0.7354 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.7042 - val_loss: 23.7213 - val_gender_output_loss: 0.5165 - val_image_quality_output_loss: 1.1099 - val_age_output_loss: 1.3889 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.8844 - val_pose_output_loss: 0.6699 - val_footwear_output_loss: 0.8298 - val_emotion_output_loss: 0.9575 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3557 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7108 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.6334\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 22.44117\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.1264 - gender_output_loss: 0.5333 - image_quality_output_loss: 0.9257 - age_output_loss: 2.7994 - weight_output_loss: 2.3787 - bag_output_loss: 1.4170 - pose_output_loss: 1.1652 - footwear_output_loss: 1.1966 - emotion_output_loss: 2.7339 - gender_output_acc: 0.7833 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4104 - weight_output_acc: 0.6469 - bag_output_acc: 0.6250 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.7000 - val_loss: 22.6003 - val_gender_output_loss: 0.4722 - val_image_quality_output_loss: 0.9229 - val_age_output_loss: 1.3569 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.8736 - val_pose_output_loss: 0.5944 - val_footwear_output_loss: 0.8211 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.5938 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 22.44117\n",
            "\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 44.4086 - gender_output_loss: 0.5575 - image_quality_output_loss: 0.8863 - age_output_loss: 2.8799 - weight_output_loss: 2.3528 - bag_output_loss: 1.5366 - pose_output_loss: 1.1476 - footwear_output_loss: 1.1763 - emotion_output_loss: 2.6845 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5844 - age_output_acc: 0.3844 - weight_output_acc: 0.6562 - bag_output_acc: 0.5938 - pose_output_acc: 0.7292 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.6990 - val_loss: 22.4730 - val_gender_output_loss: 0.4689 - val_image_quality_output_loss: 0.9222 - val_age_output_loss: 1.3625 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8572 - val_pose_output_loss: 0.5775 - val_footwear_output_loss: 0.8138 - val_emotion_output_loss: 0.8954 - val_gender_output_acc: 0.7688 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7644 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 22.44117\n",
            "Epoch 88/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 42.7882 - gender_output_loss: 0.5876 - image_quality_output_loss: 0.8933 - age_output_loss: 2.5023 - weight_output_loss: 2.7479 - bag_output_loss: 1.4341 - pose_output_loss: 1.2346 - footwear_output_loss: 1.1151 - emotion_output_loss: 2.3845 - gender_output_acc: 0.7468 - image_quality_output_acc: 0.5873 - age_output_acc: 0.4138 - weight_output_acc: 0.6412 - bag_output_acc: 0.6153 - pose_output_acc: 0.7144 - footwear_output_acc: 0.6293 - emotion_output_acc: 0.7209\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00087: val_loss did not improve from 22.44117\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.7962 - gender_output_loss: 0.5849 - image_quality_output_loss: 0.8964 - age_output_loss: 2.5128 - weight_output_loss: 2.7424 - bag_output_loss: 1.4222 - pose_output_loss: 1.2240 - footwear_output_loss: 1.1188 - emotion_output_loss: 2.3949 - gender_output_acc: 0.7490 - image_quality_output_acc: 0.5917 - age_output_acc: 0.4156 - weight_output_acc: 0.6385 - bag_output_acc: 0.6156 - pose_output_acc: 0.7167 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7198 - val_loss: 22.8621 - val_gender_output_loss: 0.4813 - val_image_quality_output_loss: 1.0256 - val_age_output_loss: 1.3668 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8747 - val_pose_output_loss: 0.5820 - val_footwear_output_loss: 0.8278 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7450 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 22.44117\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.4220 - gender_output_loss: 0.5477 - image_quality_output_loss: 0.9019 - age_output_loss: 2.7812 - weight_output_loss: 2.3769 - bag_output_loss: 1.3920 - pose_output_loss: 1.1967 - footwear_output_loss: 1.1764 - emotion_output_loss: 2.3372 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4156 - weight_output_acc: 0.6313 - bag_output_acc: 0.6417 - pose_output_acc: 0.7563 - footwear_output_acc: 0.5906 - emotion_output_acc: 0.7281 - val_loss: 22.8009 - val_gender_output_loss: 0.4707 - val_image_quality_output_loss: 0.9796 - val_age_output_loss: 1.3622 - val_weight_output_loss: 0.9868 - val_bag_output_loss: 0.8744 - val_pose_output_loss: 0.6256 - val_footwear_output_loss: 0.8195 - val_emotion_output_loss: 0.8882 - val_gender_output_acc: 0.7793 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.7336 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 22.44117\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 44.5653 - gender_output_loss: 0.5635 - image_quality_output_loss: 0.8935 - age_output_loss: 2.8429 - weight_output_loss: 2.4963 - bag_output_loss: 1.5412 - pose_output_loss: 1.2848 - footwear_output_loss: 1.1649 - emotion_output_loss: 2.5461 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4000 - weight_output_acc: 0.6354 - bag_output_acc: 0.6146 - pose_output_acc: 0.7188 - footwear_output_acc: 0.6156 - emotion_output_acc: 0.7115 - val_loss: 22.7695 - val_gender_output_loss: 0.4649 - val_image_quality_output_loss: 1.0026 - val_age_output_loss: 1.3624 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8664 - val_pose_output_loss: 0.6083 - val_footwear_output_loss: 0.8256 - val_emotion_output_loss: 0.8896 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7490 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 22.44117\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 44.4444 - gender_output_loss: 0.5620 - image_quality_output_loss: 0.9167 - age_output_loss: 2.6670 - weight_output_loss: 2.5562 - bag_output_loss: 1.5332 - pose_output_loss: 1.1557 - footwear_output_loss: 1.1753 - emotion_output_loss: 2.7336 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4083 - weight_output_acc: 0.6448 - bag_output_acc: 0.5969 - pose_output_acc: 0.7385 - footwear_output_acc: 0.5958 - emotion_output_acc: 0.7000 - val_loss: 22.6047 - val_gender_output_loss: 0.4632 - val_image_quality_output_loss: 0.9685 - val_age_output_loss: 1.3575 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8684 - val_pose_output_loss: 0.5798 - val_footwear_output_loss: 0.8254 - val_emotion_output_loss: 0.8903 - val_gender_output_acc: 0.7847 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7644 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 22.44117\n",
            "Epoch 92/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.2402 - gender_output_loss: 0.5754 - image_quality_output_loss: 0.9305 - age_output_loss: 2.6529 - weight_output_loss: 2.6193 - bag_output_loss: 1.4212 - pose_output_loss: 1.1754 - footwear_output_loss: 1.1916 - emotion_output_loss: 2.4468 - gender_output_acc: 0.7446 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4181 - weight_output_acc: 0.6131 - bag_output_acc: 0.6142 - pose_output_acc: 0.7231 - footwear_output_acc: 0.5959 - emotion_output_acc: 0.7177\n",
            "Epoch 00091: val_loss did not improve from 22.44117\n",
            "30/30 [==============================] - 31s 1s/step - loss: 43.2386 - gender_output_loss: 0.5787 - image_quality_output_loss: 0.9263 - age_output_loss: 2.6719 - weight_output_loss: 2.5763 - bag_output_loss: 1.4177 - pose_output_loss: 1.1891 - footwear_output_loss: 1.1903 - emotion_output_loss: 2.4532 - gender_output_acc: 0.7448 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4177 - weight_output_acc: 0.6177 - bag_output_acc: 0.6125 - pose_output_acc: 0.7250 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.7167 - val_loss: 24.2128 - val_gender_output_loss: 0.6002 - val_image_quality_output_loss: 1.1079 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.9121 - val_pose_output_loss: 0.7289 - val_footwear_output_loss: 0.8681 - val_emotion_output_loss: 0.9371 - val_gender_output_acc: 0.7287 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3631 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7044 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 22.44117\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 43.7860 - gender_output_loss: 0.5916 - image_quality_output_loss: 0.9151 - age_output_loss: 2.6763 - weight_output_loss: 2.5475 - bag_output_loss: 1.5272 - pose_output_loss: 1.2124 - footwear_output_loss: 1.1726 - emotion_output_loss: 2.5158 - gender_output_acc: 0.7312 - image_quality_output_acc: 0.5781 - age_output_acc: 0.3990 - weight_output_acc: 0.6354 - bag_output_acc: 0.6000 - pose_output_acc: 0.7437 - footwear_output_acc: 0.5865 - emotion_output_acc: 0.7177 - val_loss: 24.4139 - val_gender_output_loss: 0.5263 - val_image_quality_output_loss: 1.0020 - val_age_output_loss: 1.3764 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9042 - val_pose_output_loss: 0.9143 - val_footwear_output_loss: 0.8720 - val_emotion_output_loss: 0.9760 - val_gender_output_acc: 0.7455 - val_image_quality_output_acc: 0.5298 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.5838 - val_pose_output_acc: 0.7346 - val_footwear_output_acc: 0.5947 - val_emotion_output_acc: 0.6285\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 22.44117\n",
            "Epoch 94/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.1059 - gender_output_loss: 0.5652 - image_quality_output_loss: 0.9535 - age_output_loss: 2.6508 - weight_output_loss: 2.7600 - bag_output_loss: 1.5259 - pose_output_loss: 1.2334 - footwear_output_loss: 1.1642 - emotion_output_loss: 2.6954 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5506 - age_output_acc: 0.4095 - weight_output_acc: 0.6164 - bag_output_acc: 0.5873 - pose_output_acc: 0.7069 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7004\n",
            "Epoch 00093: val_loss did not improve from 22.44117\n",
            "30/30 [==============================] - 31s 1s/step - loss: 44.9245 - gender_output_loss: 0.5629 - image_quality_output_loss: 0.9501 - age_output_loss: 2.6591 - weight_output_loss: 2.7448 - bag_output_loss: 1.5222 - pose_output_loss: 1.2337 - footwear_output_loss: 1.1537 - emotion_output_loss: 2.6638 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4104 - weight_output_acc: 0.6198 - bag_output_acc: 0.5906 - pose_output_acc: 0.7073 - footwear_output_acc: 0.6115 - emotion_output_acc: 0.7021 - val_loss: 23.0645 - val_gender_output_loss: 0.4890 - val_image_quality_output_loss: 0.9314 - val_age_output_loss: 1.3765 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8744 - val_pose_output_loss: 0.6941 - val_footwear_output_loss: 0.8341 - val_emotion_output_loss: 0.9032 - val_gender_output_acc: 0.7649 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7118 - val_footwear_output_acc: 0.6176 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 22.44117\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 44.7707 - gender_output_loss: 0.5545 - image_quality_output_loss: 0.8952 - age_output_loss: 2.9289 - weight_output_loss: 2.5502 - bag_output_loss: 1.5165 - pose_output_loss: 1.2097 - footwear_output_loss: 1.1811 - emotion_output_loss: 2.5418 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.6062 - age_output_acc: 0.4125 - weight_output_acc: 0.6417 - bag_output_acc: 0.6104 - pose_output_acc: 0.7240 - footwear_output_acc: 0.5927 - emotion_output_acc: 0.7188 - val_loss: 22.6016 - val_gender_output_loss: 0.4787 - val_image_quality_output_loss: 0.9007 - val_age_output_loss: 1.3620 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8895 - val_pose_output_loss: 0.5830 - val_footwear_output_loss: 0.8286 - val_emotion_output_loss: 0.8938 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.5942 - val_pose_output_acc: 0.7535 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 22.44117\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 42.2173 - gender_output_loss: 0.5575 - image_quality_output_loss: 0.9274 - age_output_loss: 2.7061 - weight_output_loss: 2.4539 - bag_output_loss: 1.3114 - pose_output_loss: 1.0995 - footwear_output_loss: 1.1059 - emotion_output_loss: 2.4550 - gender_output_acc: 0.7635 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4094 - weight_output_acc: 0.6354 - bag_output_acc: 0.6427 - pose_output_acc: 0.7781 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7302 - val_loss: 22.7049 - val_gender_output_loss: 0.4776 - val_image_quality_output_loss: 0.9564 - val_age_output_loss: 1.3618 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8761 - val_pose_output_loss: 0.5815 - val_footwear_output_loss: 0.8316 - val_emotion_output_loss: 0.9028 - val_gender_output_acc: 0.7763 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7609 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 22.44117\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 42.5279 - gender_output_loss: 0.5326 - image_quality_output_loss: 0.9060 - age_output_loss: 2.7146 - weight_output_loss: 2.5044 - bag_output_loss: 1.4220 - pose_output_loss: 1.1784 - footwear_output_loss: 1.1157 - emotion_output_loss: 2.3624 - gender_output_acc: 0.7677 - image_quality_output_acc: 0.5458 - age_output_acc: 0.4042 - weight_output_acc: 0.6365 - bag_output_acc: 0.6260 - pose_output_acc: 0.7323 - footwear_output_acc: 0.6135 - emotion_output_acc: 0.7281 - val_loss: 23.0911 - val_gender_output_loss: 0.4907 - val_image_quality_output_loss: 1.0266 - val_age_output_loss: 1.3749 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.8961 - val_pose_output_loss: 0.5902 - val_footwear_output_loss: 0.8492 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.7659 - val_image_quality_output_acc: 0.5074 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.5923 - val_pose_output_acc: 0.7510 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 22.44117\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 42.4154 - gender_output_loss: 0.5718 - image_quality_output_loss: 0.9045 - age_output_loss: 2.5660 - weight_output_loss: 2.3773 - bag_output_loss: 1.5064 - pose_output_loss: 1.1726 - footwear_output_loss: 1.1127 - emotion_output_loss: 2.5019 - gender_output_acc: 0.7500 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4240 - weight_output_acc: 0.6469 - bag_output_acc: 0.5896 - pose_output_acc: 0.7312 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7094 - val_loss: 22.5845 - val_gender_output_loss: 0.4636 - val_image_quality_output_loss: 0.9466 - val_age_output_loss: 1.3589 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8653 - val_pose_output_loss: 0.5824 - val_footwear_output_loss: 0.8233 - val_emotion_output_loss: 0.8996 - val_gender_output_acc: 0.7698 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7604 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.6910\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 22.44117\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 42.5482 - gender_output_loss: 0.5708 - image_quality_output_loss: 0.8880 - age_output_loss: 2.6239 - weight_output_loss: 2.4891 - bag_output_loss: 1.4952 - pose_output_loss: 1.1991 - footwear_output_loss: 1.1198 - emotion_output_loss: 2.3870 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4375 - weight_output_acc: 0.6302 - bag_output_acc: 0.6198 - pose_output_acc: 0.7479 - footwear_output_acc: 0.6177 - emotion_output_acc: 0.7323 - val_loss: 22.8389 - val_gender_output_loss: 0.4948 - val_image_quality_output_loss: 0.9705 - val_age_output_loss: 1.3595 - val_weight_output_loss: 0.9956 - val_bag_output_loss: 0.8865 - val_pose_output_loss: 0.5828 - val_footwear_output_loss: 0.8312 - val_emotion_output_loss: 0.9042 - val_gender_output_acc: 0.7698 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.6007 - val_pose_output_acc: 0.7664 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 22.44117\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 44.7476 - gender_output_loss: 0.5909 - image_quality_output_loss: 0.9065 - age_output_loss: 2.7026 - weight_output_loss: 2.6320 - bag_output_loss: 1.4715 - pose_output_loss: 1.1270 - footwear_output_loss: 1.2112 - emotion_output_loss: 2.7581 - gender_output_acc: 0.7406 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4292 - weight_output_acc: 0.6240 - bag_output_acc: 0.6229 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6010 - emotion_output_acc: 0.6958 - val_loss: 23.4061 - val_gender_output_loss: 0.4900 - val_image_quality_output_loss: 1.0512 - val_age_output_loss: 1.3917 - val_weight_output_loss: 1.0217 - val_bag_output_loss: 0.8741 - val_pose_output_loss: 0.6345 - val_footwear_output_loss: 0.8694 - val_emotion_output_loss: 0.9079 - val_gender_output_acc: 0.7540 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7277 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 22.44117\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "End of run with EPOCHS= 100 STEPS_PER_EPOCH= 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlMUoi70Q5ld",
        "colab_type": "code",
        "outputId": "1042fbeb-d691-48d9-93c5-69b446827024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577545001_model.081.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "Returning new callback array with steps_per_epoch= 100 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json  to: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577549602_backup\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 83s 826ms/step - loss: 44.3853 - gender_output_loss: 0.5839 - image_quality_output_loss: 0.9158 - age_output_loss: 2.7567 - weight_output_loss: 2.6501 - bag_output_loss: 1.4445 - pose_output_loss: 1.2089 - footwear_output_loss: 1.1807 - emotion_output_loss: 2.5715 - gender_output_acc: 0.7441 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4078 - weight_output_acc: 0.6328 - bag_output_acc: 0.6244 - pose_output_acc: 0.7338 - footwear_output_acc: 0.6019 - emotion_output_acc: 0.7119 - val_loss: 22.8830 - val_gender_output_loss: 0.4661 - val_image_quality_output_loss: 0.9888 - val_age_output_loss: 1.3634 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.8714 - val_pose_output_loss: 0.6333 - val_footwear_output_loss: 0.8267 - val_emotion_output_loss: 0.8931 - val_gender_output_acc: 0.7798 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7307 - val_footwear_output_acc: 0.6314 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.88303, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.001.h5\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 71s 707ms/step - loss: 43.4643 - gender_output_loss: 0.5527 - image_quality_output_loss: 0.9075 - age_output_loss: 2.7201 - weight_output_loss: 2.4594 - bag_output_loss: 1.4465 - pose_output_loss: 1.1998 - footwear_output_loss: 1.1711 - emotion_output_loss: 2.5509 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5716 - age_output_acc: 0.4066 - weight_output_acc: 0.6325 - bag_output_acc: 0.6153 - pose_output_acc: 0.7303 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7097 - val_loss: 22.6913 - val_gender_output_loss: 0.5038 - val_image_quality_output_loss: 0.9510 - val_age_output_loss: 1.3684 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8803 - val_pose_output_loss: 0.5538 - val_footwear_output_loss: 0.8214 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7679 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7639 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00002: val_loss improved from 22.88303 to 22.69128, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.002.h5\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 73s 731ms/step - loss: 43.2543 - gender_output_loss: 0.5452 - image_quality_output_loss: 0.9029 - age_output_loss: 2.7287 - weight_output_loss: 2.4735 - bag_output_loss: 1.4544 - pose_output_loss: 1.1557 - footwear_output_loss: 1.1693 - emotion_output_loss: 2.5136 - gender_output_acc: 0.7616 - image_quality_output_acc: 0.5816 - age_output_acc: 0.4116 - weight_output_acc: 0.6403 - bag_output_acc: 0.6091 - pose_output_acc: 0.7459 - footwear_output_acc: 0.5928 - emotion_output_acc: 0.7147 - val_loss: 23.7764 - val_gender_output_loss: 0.6224 - val_image_quality_output_loss: 0.9988 - val_age_output_loss: 1.4069 - val_weight_output_loss: 1.0091 - val_bag_output_loss: 0.9769 - val_pose_output_loss: 0.6386 - val_footwear_output_loss: 0.8320 - val_emotion_output_loss: 0.8924 - val_gender_output_acc: 0.7247 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.3616 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.5769 - val_pose_output_acc: 0.7232 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 22.69128\n",
            "Epoch 4/100\n",
            " 59/100 [================>.............] - ETA: 22s - loss: 43.5036 - gender_output_loss: 0.5846 - image_quality_output_loss: 0.9302 - age_output_loss: 2.7651 - weight_output_loss: 2.3397 - bag_output_loss: 1.5143 - pose_output_loss: 1.1408 - footwear_output_loss: 1.1742 - emotion_output_loss: 2.5706 - gender_output_acc: 0.7415 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4010 - weight_output_acc: 0.6472 - bag_output_acc: 0.5985 - pose_output_acc: 0.7410 - footwear_output_acc: 0.6043 - emotion_output_acc: 0.7092Epoch 1/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.7620 - gender_output_loss: 0.5857 - image_quality_output_loss: 0.9213 - age_output_loss: 2.7582 - weight_output_loss: 2.4855 - bag_output_loss: 1.4452 - pose_output_loss: 1.1752 - footwear_output_loss: 1.1635 - emotion_output_loss: 2.5680 - gender_output_acc: 0.7402 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3946 - weight_output_acc: 0.6383 - bag_output_acc: 0.6133 - pose_output_acc: 0.7342 - footwear_output_acc: 0.6070 - emotion_output_acc: 0.7093Epoch 4/100\n",
            "100/100 [==============================] - 74s 744ms/step - loss: 43.6968 - gender_output_loss: 0.5848 - image_quality_output_loss: 0.9219 - age_output_loss: 2.7546 - weight_output_loss: 2.4740 - bag_output_loss: 1.4458 - pose_output_loss: 1.1805 - footwear_output_loss: 1.1649 - emotion_output_loss: 2.5590 - gender_output_acc: 0.7409 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3947 - weight_output_acc: 0.6391 - bag_output_acc: 0.6128 - pose_output_acc: 0.7341 - footwear_output_acc: 0.6069 - emotion_output_acc: 0.7103 - val_loss: 24.0906 - val_gender_output_loss: 0.4962 - val_image_quality_output_loss: 0.9414 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8728 - val_pose_output_loss: 0.9692 - val_footwear_output_loss: 0.8166 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.7535 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.5779 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 22.69128\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 73s 735ms/step - loss: 43.7704 - gender_output_loss: 0.5821 - image_quality_output_loss: 0.9290 - age_output_loss: 2.6782 - weight_output_loss: 2.4678 - bag_output_loss: 1.5510 - pose_output_loss: 1.2333 - footwear_output_loss: 1.1674 - emotion_output_loss: 2.5369 - gender_output_acc: 0.7512 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4125 - weight_output_acc: 0.6391 - bag_output_acc: 0.6041 - pose_output_acc: 0.7219 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7153 - val_loss: 22.8433 - val_gender_output_loss: 0.4955 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.3714 - val_weight_output_loss: 0.9775 - val_bag_output_loss: 0.8877 - val_pose_output_loss: 0.5924 - val_footwear_output_loss: 0.8284 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.7659 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5972 - val_pose_output_acc: 0.7574 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 22.69128\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 43.4908 - gender_output_loss: 0.5708 - image_quality_output_loss: 0.9171 - age_output_loss: 2.7089 - weight_output_loss: 2.3869 - bag_output_loss: 1.4342 - pose_output_loss: 1.2022 - footwear_output_loss: 1.1734 - emotion_output_loss: 2.6167 - gender_output_acc: 0.7572 - image_quality_output_acc: 0.5597 - age_output_acc: 0.4113 - weight_output_acc: 0.6431 - bag_output_acc: 0.6222 - pose_output_acc: 0.7353 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7069 - val_loss: 22.5819 - val_gender_output_loss: 0.4742 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8736 - val_pose_output_loss: 0.5577 - val_footwear_output_loss: 0.8178 - val_emotion_output_loss: 0.8880 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7674 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00006: val_loss improved from 22.69128 to 22.58189, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.006.h5\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 43.4908 - gender_output_loss: 0.5708 - image_quality_output_loss: 0.9171 - age_output_loss: 2.7089 - weight_output_loss: 2.3869 - bag_output_loss: 1.4342 - pose_output_loss: 1.2022 - footwear_output_loss: 1.1734 - emotion_output_loss: 2.6167 - gender_output_acc: 0.7572 - image_quality_output_acc: 0.5597 - age_output_acc: 0.4113 - weight_output_acc: 0.6431 - bag_output_acc: 0.6222 - pose_output_acc: 0.7353 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7069 - val_loss: 22.5819 - val_gender_output_loss: 0.4742 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8736 - val_pose_output_loss: 0.5577 - val_footwear_output_loss: 0.8178 - val_emotion_output_loss: 0.8880 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7674 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6984\n",
            "100/100 [==============================] - 73s 735ms/step - loss: 43.7156 - gender_output_loss: 0.5572 - image_quality_output_loss: 0.9023 - age_output_loss: 2.7730 - weight_output_loss: 2.4777 - bag_output_loss: 1.4329 - pose_output_loss: 1.1859 - footwear_output_loss: 1.1652 - emotion_output_loss: 2.5725 - gender_output_acc: 0.7541 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4000 - weight_output_acc: 0.6444 - bag_output_acc: 0.6166 - pose_output_acc: 0.7309 - footwear_output_acc: 0.6044 - emotion_output_acc: 0.7131 - val_loss: 22.7835 - val_gender_output_loss: 0.4708 - val_image_quality_output_loss: 0.9994 - val_age_output_loss: 1.3756 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.8781 - val_pose_output_loss: 0.5592 - val_footwear_output_loss: 0.8529 - val_emotion_output_loss: 0.8914 - val_gender_output_acc: 0.7778 - val_image_quality_output_acc: 0.5188 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6007 - val_pose_output_acc: 0.7604 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 22.58189\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 75s 749ms/step - loss: 43.3615 - gender_output_loss: 0.5540 - image_quality_output_loss: 0.9196 - age_output_loss: 2.7479 - weight_output_loss: 2.5298 - bag_output_loss: 1.5113 - pose_output_loss: 1.1565 - footwear_output_loss: 1.1926 - emotion_output_loss: 2.4125 - gender_output_acc: 0.7619 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4059 - weight_output_acc: 0.6369 - bag_output_acc: 0.6153 - pose_output_acc: 0.7425 - footwear_output_acc: 0.5913 - emotion_output_acc: 0.7194 - val_loss: 22.5336 - val_gender_output_loss: 0.4665 - val_image_quality_output_loss: 0.9568 - val_age_output_loss: 1.3599 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8707 - val_pose_output_loss: 0.5672 - val_footwear_output_loss: 0.8177 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7669 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00008: val_loss improved from 22.58189 to 22.53361, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.008.h5\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 74s 739ms/step - loss: 43.5644 - gender_output_loss: 0.5606 - image_quality_output_loss: 0.9108 - age_output_loss: 2.7391 - weight_output_loss: 2.4592 - bag_output_loss: 1.4937 - pose_output_loss: 1.1592 - footwear_output_loss: 1.1889 - emotion_output_loss: 2.5392 - gender_output_acc: 0.7547 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4181 - weight_output_acc: 0.6378 - bag_output_acc: 0.6081 - pose_output_acc: 0.7463 - footwear_output_acc: 0.5962 - emotion_output_acc: 0.7106 - val_loss: 23.0687 - val_gender_output_loss: 0.4728 - val_image_quality_output_loss: 1.0739 - val_age_output_loss: 1.3782 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8774 - val_pose_output_loss: 0.5757 - val_footwear_output_loss: 0.8640 - val_emotion_output_loss: 0.9001 - val_gender_output_acc: 0.7763 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.5987 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6171 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 22.53361\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 74s 745ms/step - loss: 43.8142 - gender_output_loss: 0.5500 - image_quality_output_loss: 0.9150 - age_output_loss: 2.7163 - weight_output_loss: 2.5487 - bag_output_loss: 1.4316 - pose_output_loss: 1.1463 - footwear_output_loss: 1.1551 - emotion_output_loss: 2.6339 - gender_output_acc: 0.7603 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4012 - weight_output_acc: 0.6325 - bag_output_acc: 0.6141 - pose_output_acc: 0.7294 - footwear_output_acc: 0.6091 - emotion_output_acc: 0.7044 - val_loss: 22.4539 - val_gender_output_loss: 0.4599 - val_image_quality_output_loss: 0.9178 - val_age_output_loss: 1.3545 - val_weight_output_loss: 0.9860 - val_bag_output_loss: 0.8678 - val_pose_output_loss: 0.5781 - val_footwear_output_loss: 0.8259 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.7837 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6106 - val_pose_output_acc: 0.7634 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00010: val_loss improved from 22.53361 to 22.45390, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.010.h5\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 75s 747ms/step - loss: 43.7815 - gender_output_loss: 0.5644 - image_quality_output_loss: 0.9142 - age_output_loss: 2.7374 - weight_output_loss: 2.5521 - bag_output_loss: 1.4020 - pose_output_loss: 1.1984 - footwear_output_loss: 1.1744 - emotion_output_loss: 2.5689 - gender_output_acc: 0.7497 - image_quality_output_acc: 0.5678 - age_output_acc: 0.3934 - weight_output_acc: 0.6356 - bag_output_acc: 0.6225 - pose_output_acc: 0.7291 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7109 - val_loss: 23.8876 - val_gender_output_loss: 0.5781 - val_image_quality_output_loss: 1.0736 - val_age_output_loss: 1.4037 - val_weight_output_loss: 1.0181 - val_bag_output_loss: 0.9473 - val_pose_output_loss: 0.6111 - val_footwear_output_loss: 0.8266 - val_emotion_output_loss: 0.9486 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.4881 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.5952 - val_bag_output_acc: 0.5898 - val_pose_output_acc: 0.7564 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6543\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.45390\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 44.2340 - gender_output_loss: 0.5713 - image_quality_output_loss: 0.9191 - age_output_loss: 2.6943 - weight_output_loss: 2.5994 - bag_output_loss: 1.4913 - pose_output_loss: 1.2198 - footwear_output_loss: 1.1742 - emotion_output_loss: 2.6011 - gender_output_acc: 0.7494 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4141 - weight_output_acc: 0.6338 - bag_output_acc: 0.6069 - pose_output_acc: 0.7278 - footwear_output_acc: 0.6112 - emotion_output_acc: 0.7109 - val_loss: 23.6047 - val_gender_output_loss: 0.4974 - val_image_quality_output_loss: 0.9471 - val_age_output_loss: 1.3715 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.9005 - val_pose_output_loss: 0.8159 - val_footwear_output_loss: 0.8444 - val_emotion_output_loss: 0.9223 - val_gender_output_acc: 0.7688 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5799 - val_pose_output_acc: 0.7083 - val_footwear_output_acc: 0.6136 - val_emotion_output_acc: 0.6701\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.45390\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 75s 747ms/step - loss: 44.0259 - gender_output_loss: 0.5668 - image_quality_output_loss: 0.9073 - age_output_loss: 2.7518 - weight_output_loss: 2.5047 - bag_output_loss: 1.4391 - pose_output_loss: 1.2463 - footwear_output_loss: 1.1744 - emotion_output_loss: 2.5904 - gender_output_acc: 0.7481 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4003 - weight_output_acc: 0.6388 - bag_output_acc: 0.6131 - pose_output_acc: 0.7175 - footwear_output_acc: 0.6041 - emotion_output_acc: 0.7106 - val_loss: 23.1353 - val_gender_output_loss: 0.5027 - val_image_quality_output_loss: 0.9671 - val_age_output_loss: 1.3760 - val_weight_output_loss: 0.9932 - val_bag_output_loss: 0.8904 - val_pose_output_loss: 0.6662 - val_footwear_output_loss: 0.8472 - val_emotion_output_loss: 0.8895 - val_gender_output_acc: 0.7639 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5913 - val_pose_output_acc: 0.7153 - val_footwear_output_acc: 0.6111 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 22.45390\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 75s 750ms/step - loss: 43.1685 - gender_output_loss: 0.5824 - image_quality_output_loss: 0.9067 - age_output_loss: 2.7640 - weight_output_loss: 2.3808 - bag_output_loss: 1.4649 - pose_output_loss: 1.1722 - footwear_output_loss: 1.1804 - emotion_output_loss: 2.4828 - gender_output_acc: 0.7325 - image_quality_output_acc: 0.5719 - age_output_acc: 0.3953 - weight_output_acc: 0.6453 - bag_output_acc: 0.6116 - pose_output_acc: 0.7322 - footwear_output_acc: 0.5947 - emotion_output_acc: 0.7134 - val_loss: 22.6415 - val_gender_output_loss: 0.4828 - val_image_quality_output_loss: 0.9510 - val_age_output_loss: 1.3706 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.8624 - val_pose_output_loss: 0.5810 - val_footwear_output_loss: 0.8433 - val_emotion_output_loss: 0.8881 - val_gender_output_acc: 0.7728 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6066 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.6195 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 22.45390\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 75s 751ms/step - loss: 43.7401 - gender_output_loss: 0.5623 - image_quality_output_loss: 0.9087 - age_output_loss: 2.7219 - weight_output_loss: 2.5378 - bag_output_loss: 1.4819 - pose_output_loss: 1.1798 - footwear_output_loss: 1.1330 - emotion_output_loss: 2.5645 - gender_output_acc: 0.7525 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3953 - weight_output_acc: 0.6434 - bag_output_acc: 0.6109 - pose_output_acc: 0.7416 - footwear_output_acc: 0.6134 - emotion_output_acc: 0.7109 - val_loss: 22.3530 - val_gender_output_loss: 0.4597 - val_image_quality_output_loss: 0.9163 - val_age_output_loss: 1.3539 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8577 - val_pose_output_loss: 0.5665 - val_footwear_output_loss: 0.8248 - val_emotion_output_loss: 0.8831 - val_gender_output_acc: 0.7827 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7639 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.45390 to 22.35298, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.015.h5\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 74s 740ms/step - loss: 43.8999 - gender_output_loss: 0.5574 - image_quality_output_loss: 0.9136 - age_output_loss: 2.7106 - weight_output_loss: 2.4854 - bag_output_loss: 1.4709 - pose_output_loss: 1.1409 - footwear_output_loss: 1.1783 - emotion_output_loss: 2.6701 - gender_output_acc: 0.7516 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4188 - weight_output_acc: 0.6409 - bag_output_acc: 0.6053 - pose_output_acc: 0.7419 - footwear_output_acc: 0.6028 - emotion_output_acc: 0.6978 - val_loss: 22.3923 - val_gender_output_loss: 0.4545 - val_image_quality_output_loss: 0.9300 - val_age_output_loss: 1.3541 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8637 - val_pose_output_loss: 0.5682 - val_footwear_output_loss: 0.8277 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.7882 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6071 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 22.35298\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 75s 749ms/step - loss: 42.5727 - gender_output_loss: 0.5543 - image_quality_output_loss: 0.9187 - age_output_loss: 2.7396 - weight_output_loss: 2.3981 - bag_output_loss: 1.4688 - pose_output_loss: 1.1682 - footwear_output_loss: 1.1603 - emotion_output_loss: 2.3639 - gender_output_acc: 0.7622 - image_quality_output_acc: 0.5703 - age_output_acc: 0.4125 - weight_output_acc: 0.6503 - bag_output_acc: 0.6178 - pose_output_acc: 0.7409 - footwear_output_acc: 0.6009 - emotion_output_acc: 0.7266 - val_loss: 22.9147 - val_gender_output_loss: 0.4969 - val_image_quality_output_loss: 0.9487 - val_age_output_loss: 1.3767 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.8942 - val_pose_output_loss: 0.6087 - val_footwear_output_loss: 0.8287 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.5933 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 22.35298\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 76s 757ms/step - loss: 43.6621 - gender_output_loss: 0.5386 - image_quality_output_loss: 0.9082 - age_output_loss: 2.7404 - weight_output_loss: 2.5528 - bag_output_loss: 1.4389 - pose_output_loss: 1.1526 - footwear_output_loss: 1.1676 - emotion_output_loss: 2.5631 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4037 - weight_output_acc: 0.6203 - bag_output_acc: 0.6275 - pose_output_acc: 0.7356 - footwear_output_acc: 0.6106 - emotion_output_acc: 0.7131 - val_loss: 22.7465 - val_gender_output_loss: 0.5254 - val_image_quality_output_loss: 0.9243 - val_age_output_loss: 1.3631 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8977 - val_pose_output_loss: 0.5759 - val_footwear_output_loss: 0.8299 - val_emotion_output_loss: 0.8977 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6151 - val_bag_output_acc: 0.5967 - val_pose_output_acc: 0.7490 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 22.35298\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 75s 753ms/step - loss: 43.4331 - gender_output_loss: 0.5607 - image_quality_output_loss: 0.9190 - age_output_loss: 2.6636 - weight_output_loss: 2.5167 - bag_output_loss: 1.4708 - pose_output_loss: 1.1635 - footwear_output_loss: 1.1748 - emotion_output_loss: 2.5578 - gender_output_acc: 0.7569 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4125 - weight_output_acc: 0.6469 - bag_output_acc: 0.6153 - pose_output_acc: 0.7466 - footwear_output_acc: 0.5975 - emotion_output_acc: 0.7069 - val_loss: 23.0099 - val_gender_output_loss: 0.5111 - val_image_quality_output_loss: 0.9939 - val_age_output_loss: 1.3690 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8852 - val_pose_output_loss: 0.6412 - val_footwear_output_loss: 0.8236 - val_emotion_output_loss: 0.8913 - val_gender_output_acc: 0.7634 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7346 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.7034\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00019: val_loss did not improve from 22.35298\n",
            "Epoch 20/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 44.0227 - gender_output_loss: 0.5754 - image_quality_output_loss: 0.8854 - age_output_loss: 2.7273 - weight_output_loss: 2.4631 - bag_output_loss: 1.4192 - pose_output_loss: 1.2429 - footwear_output_loss: 1.1706 - emotion_output_loss: 2.6727 - gender_output_acc: 0.7554 - image_quality_output_acc: 0.5748 - age_output_acc: 0.4031 - weight_output_acc: 0.6395 - bag_output_acc: 0.6237 - pose_output_acc: 0.7307 - footwear_output_acc: 0.6039 - emotion_output_acc: 0.7011Epoch 20/100\n",
            "100/100 [==============================] - 75s 748ms/step - loss: 44.0062 - gender_output_loss: 0.5757 - image_quality_output_loss: 0.8841 - age_output_loss: 2.7317 - weight_output_loss: 2.4651 - bag_output_loss: 1.4207 - pose_output_loss: 1.2412 - footwear_output_loss: 1.1689 - emotion_output_loss: 2.6641 - gender_output_acc: 0.7556 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4031 - weight_output_acc: 0.6394 - bag_output_acc: 0.6231 - pose_output_acc: 0.7313 - footwear_output_acc: 0.6044 - emotion_output_acc: 0.7016 - val_loss: 22.8363 - val_gender_output_loss: 0.5238 - val_image_quality_output_loss: 0.9355 - val_age_output_loss: 1.3834 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.8663 - val_pose_output_loss: 0.5922 - val_footwear_output_loss: 0.8153 - val_emotion_output_loss: 0.9034 - val_gender_output_acc: 0.7550 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7560 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 22.35298\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 75s 746ms/step - loss: 43.9490 - gender_output_loss: 0.5474 - image_quality_output_loss: 0.9202 - age_output_loss: 2.7773 - weight_output_loss: 2.5427 - bag_output_loss: 1.4263 - pose_output_loss: 1.1669 - footwear_output_loss: 1.1557 - emotion_output_loss: 2.6006 - gender_output_acc: 0.7656 - image_quality_output_acc: 0.5619 - age_output_acc: 0.4012 - weight_output_acc: 0.6306 - bag_output_acc: 0.6166 - pose_output_acc: 0.7362 - footwear_output_acc: 0.6109 - emotion_output_acc: 0.7150 - val_loss: 23.0235 - val_gender_output_loss: 0.5131 - val_image_quality_output_loss: 1.0093 - val_age_output_loss: 1.3690 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8950 - val_pose_output_loss: 0.6084 - val_footwear_output_loss: 0.8300 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7406 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 22.35298\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 74s 744ms/step - loss: 43.3642 - gender_output_loss: 0.5597 - image_quality_output_loss: 0.9149 - age_output_loss: 2.7692 - weight_output_loss: 2.4592 - bag_output_loss: 1.5233 - pose_output_loss: 1.1929 - footwear_output_loss: 1.1831 - emotion_output_loss: 2.4160 - gender_output_acc: 0.7606 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4025 - weight_output_acc: 0.6434 - bag_output_acc: 0.6016 - pose_output_acc: 0.7331 - footwear_output_acc: 0.5966 - emotion_output_acc: 0.7244 - val_loss: 22.6148 - val_gender_output_loss: 0.4650 - val_image_quality_output_loss: 0.9206 - val_age_output_loss: 1.3618 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8751 - val_pose_output_loss: 0.6255 - val_footwear_output_loss: 0.8147 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.7793 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7406 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 22.35298\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 74s 739ms/step - loss: 43.7310 - gender_output_loss: 0.5429 - image_quality_output_loss: 0.9096 - age_output_loss: 2.7742 - weight_output_loss: 2.4628 - bag_output_loss: 1.4233 - pose_output_loss: 1.1337 - footwear_output_loss: 1.1910 - emotion_output_loss: 2.6267 - gender_output_acc: 0.7703 - image_quality_output_acc: 0.5584 - age_output_acc: 0.4116 - weight_output_acc: 0.6475 - bag_output_acc: 0.6300 - pose_output_acc: 0.7425 - footwear_output_acc: 0.5953 - emotion_output_acc: 0.7022 - val_loss: 22.5144 - val_gender_output_loss: 0.4665 - val_image_quality_output_loss: 0.9428 - val_age_output_loss: 1.3655 - val_weight_output_loss: 0.9903 - val_bag_output_loss: 0.8705 - val_pose_output_loss: 0.5490 - val_footwear_output_loss: 0.8279 - val_emotion_output_loss: 0.8921 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6071 - val_pose_output_acc: 0.7644 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 22.35298\n",
            "Epoch 24/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.4855 - gender_output_loss: 0.5550 - image_quality_output_loss: 0.9112 - age_output_loss: 2.6778 - weight_output_loss: 2.6170 - bag_output_loss: 1.4207 - pose_output_loss: 1.1475 - footwear_output_loss: 1.1702 - emotion_output_loss: 2.5414 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5666 - age_output_acc: 0.4141 - weight_output_acc: 0.6247 - bag_output_acc: 0.6329 - pose_output_acc: 0.7421 - footwear_output_acc: 0.5966 - emotion_output_acc: 0.7159\n",
            "100/100 [==============================] - 74s 740ms/step - loss: 43.4888 - gender_output_loss: 0.5555 - image_quality_output_loss: 0.9120 - age_output_loss: 2.6823 - weight_output_loss: 2.6135 - bag_output_loss: 1.4278 - pose_output_loss: 1.1451 - footwear_output_loss: 1.1714 - emotion_output_loss: 2.5355 - gender_output_acc: 0.7556 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4138 - weight_output_acc: 0.6247 - bag_output_acc: 0.6322 - pose_output_acc: 0.7425 - footwear_output_acc: 0.5959 - emotion_output_acc: 0.7163 - val_loss: 22.6610 - val_gender_output_loss: 0.4532 - val_image_quality_output_loss: 1.0079 - val_age_output_loss: 1.3600 - val_weight_output_loss: 1.0128 - val_bag_output_loss: 0.8757 - val_pose_output_loss: 0.5486 - val_footwear_output_loss: 0.8269 - val_emotion_output_loss: 0.8883 - val_gender_output_acc: 0.7887 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.7684 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 22.35298\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 74s 743ms/step - loss: 43.0715 - gender_output_loss: 0.5600 - image_quality_output_loss: 0.8966 - age_output_loss: 2.7212 - weight_output_loss: 2.5133 - bag_output_loss: 1.5135 - pose_output_loss: 1.1404 - footwear_output_loss: 1.1266 - emotion_output_loss: 2.4347 - gender_output_acc: 0.7588 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4109 - weight_output_acc: 0.6406 - bag_output_acc: 0.6072 - pose_output_acc: 0.7519 - footwear_output_acc: 0.6209 - emotion_output_acc: 0.7234 - val_loss: 22.3310 - val_gender_output_loss: 0.4546 - val_image_quality_output_loss: 0.9171 - val_age_output_loss: 1.3539 - val_weight_output_loss: 0.9885 - val_bag_output_loss: 0.8696 - val_pose_output_loss: 0.5573 - val_footwear_output_loss: 0.8177 - val_emotion_output_loss: 0.8777 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5511 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7560 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00025: val_loss improved from 22.35298 to 22.33097, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.025.h5\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 76s 757ms/step - loss: 43.2919 - gender_output_loss: 0.5405 - image_quality_output_loss: 0.8873 - age_output_loss: 2.7110 - weight_output_loss: 2.4262 - bag_output_loss: 1.4405 - pose_output_loss: 1.1489 - footwear_output_loss: 1.1628 - emotion_output_loss: 2.6100 - gender_output_acc: 0.7822 - image_quality_output_acc: 0.5837 - age_output_acc: 0.4134 - weight_output_acc: 0.6416 - bag_output_acc: 0.6138 - pose_output_acc: 0.7472 - footwear_output_acc: 0.6091 - emotion_output_acc: 0.7050 - val_loss: 22.7980 - val_gender_output_loss: 0.4744 - val_image_quality_output_loss: 0.9882 - val_age_output_loss: 1.3601 - val_weight_output_loss: 0.9935 - val_bag_output_loss: 0.8846 - val_pose_output_loss: 0.5882 - val_footwear_output_loss: 0.8311 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5322 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 22.33097\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 75s 745ms/step - loss: 44.8188 - gender_output_loss: 0.5434 - image_quality_output_loss: 0.9124 - age_output_loss: 2.8211 - weight_output_loss: 2.5669 - bag_output_loss: 1.5109 - pose_output_loss: 1.2040 - footwear_output_loss: 1.1512 - emotion_output_loss: 2.6741 - gender_output_acc: 0.7575 - image_quality_output_acc: 0.5628 - age_output_acc: 0.3991 - weight_output_acc: 0.6353 - bag_output_acc: 0.5988 - pose_output_acc: 0.7300 - footwear_output_acc: 0.6128 - emotion_output_acc: 0.7009 - val_loss: 22.5016 - val_gender_output_loss: 0.4621 - val_image_quality_output_loss: 0.9457 - val_age_output_loss: 1.3613 - val_weight_output_loss: 1.0035 - val_bag_output_loss: 0.8652 - val_pose_output_loss: 0.5618 - val_footwear_output_loss: 0.8172 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.7808 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7644 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 22.33097\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 75s 747ms/step - loss: 43.2305 - gender_output_loss: 0.5724 - image_quality_output_loss: 0.9190 - age_output_loss: 2.6194 - weight_output_loss: 2.5001 - bag_output_loss: 1.4434 - pose_output_loss: 1.1935 - footwear_output_loss: 1.1638 - emotion_output_loss: 2.5634 - gender_output_acc: 0.7541 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4022 - weight_output_acc: 0.6381 - bag_output_acc: 0.6222 - pose_output_acc: 0.7359 - footwear_output_acc: 0.6059 - emotion_output_acc: 0.7119 - val_loss: 23.6531 - val_gender_output_loss: 0.4671 - val_image_quality_output_loss: 1.0668 - val_age_output_loss: 1.3775 - val_weight_output_loss: 1.0040 - val_bag_output_loss: 0.8667 - val_pose_output_loss: 0.8271 - val_footwear_output_loss: 0.8390 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.4816 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6007 - val_pose_output_acc: 0.6384 - val_footwear_output_acc: 0.6200 - val_emotion_output_acc: 0.7044\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 22.33097\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 76s 761ms/step - loss: 43.5529 - gender_output_loss: 0.5693 - image_quality_output_loss: 0.9149 - age_output_loss: 2.7669 - weight_output_loss: 2.5038 - bag_output_loss: 1.4735 - pose_output_loss: 1.1809 - footwear_output_loss: 1.1709 - emotion_output_loss: 2.4811 - gender_output_acc: 0.7497 - image_quality_output_acc: 0.5616 - age_output_acc: 0.3972 - weight_output_acc: 0.6334 - bag_output_acc: 0.6075 - pose_output_acc: 0.7250 - footwear_output_acc: 0.5931 - emotion_output_acc: 0.7216 - val_loss: 23.1605 - val_gender_output_loss: 0.4776 - val_image_quality_output_loss: 1.0234 - val_age_output_loss: 1.3942 - val_weight_output_loss: 0.9928 - val_bag_output_loss: 0.8728 - val_pose_output_loss: 0.6613 - val_footwear_output_loss: 0.8176 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.7698 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.3636 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7282 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 22.33097\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 76s 756ms/step - loss: 43.3604 - gender_output_loss: 0.5485 - image_quality_output_loss: 0.9054 - age_output_loss: 2.6815 - weight_output_loss: 2.5802 - bag_output_loss: 1.4853 - pose_output_loss: 1.1749 - footwear_output_loss: 1.1371 - emotion_output_loss: 2.4891 - gender_output_acc: 0.7609 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4131 - weight_output_acc: 0.6344 - bag_output_acc: 0.6069 - pose_output_acc: 0.7322 - footwear_output_acc: 0.6259 - emotion_output_acc: 0.7178 - val_loss: 22.7830 - val_gender_output_loss: 0.4576 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.3696 - val_weight_output_loss: 1.0232 - val_bag_output_loss: 0.8728 - val_pose_output_loss: 0.5674 - val_footwear_output_loss: 0.8340 - val_emotion_output_loss: 0.8962 - val_gender_output_acc: 0.7808 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.5774 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.7708 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 22.33097\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 76s 756ms/step - loss: 43.6519 - gender_output_loss: 0.5589 - image_quality_output_loss: 0.9016 - age_output_loss: 2.7627 - weight_output_loss: 2.3278 - bag_output_loss: 1.4855 - pose_output_loss: 1.1954 - footwear_output_loss: 1.1694 - emotion_output_loss: 2.6352 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5791 - age_output_acc: 0.3884 - weight_output_acc: 0.6469 - bag_output_acc: 0.6278 - pose_output_acc: 0.7306 - footwear_output_acc: 0.6041 - emotion_output_acc: 0.7069 - val_loss: 23.1937 - val_gender_output_loss: 0.5115 - val_image_quality_output_loss: 1.0583 - val_age_output_loss: 1.3902 - val_weight_output_loss: 1.0148 - val_bag_output_loss: 0.8953 - val_pose_output_loss: 0.5547 - val_footwear_output_loss: 0.8198 - val_emotion_output_loss: 0.9213 - val_gender_output_acc: 0.7674 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.5764 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7718 - val_footwear_output_acc: 0.6404 - val_emotion_output_acc: 0.6756\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 22.33097\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 75s 751ms/step - loss: 43.1037 - gender_output_loss: 0.5426 - image_quality_output_loss: 0.9147 - age_output_loss: 2.6896 - weight_output_loss: 2.5600 - bag_output_loss: 1.4212 - pose_output_loss: 1.0903 - footwear_output_loss: 1.1655 - emotion_output_loss: 2.5279 - gender_output_acc: 0.7728 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4194 - weight_output_acc: 0.6291 - bag_output_acc: 0.6194 - pose_output_acc: 0.7613 - footwear_output_acc: 0.6084 - emotion_output_acc: 0.7125 - val_loss: 22.5525 - val_gender_output_loss: 0.4800 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.3559 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.8821 - val_pose_output_loss: 0.5436 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7718 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 22.33097\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 75s 751ms/step - loss: 43.1037 - gender_output_loss: 0.5426 - image_quality_output_loss: 0.9147 - age_output_loss: 2.6896 - weight_output_loss: 2.5600 - bag_output_loss: 1.4212 - pose_output_loss: 1.0903 - footwear_output_loss: 1.1655 - emotion_output_loss: 2.5279 - gender_output_acc: 0.7728 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4194 - weight_output_acc: 0.6291 - bag_output_acc: 0.6194 - pose_output_acc: 0.7613 - footwear_output_acc: 0.6084 - emotion_output_acc: 0.7125 - val_loss: 22.5525 - val_gender_output_loss: 0.4800 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.3559 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.8821 - val_pose_output_loss: 0.5436 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7718 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.6944\n",
            "100/100 [==============================] - 77s 770ms/step - loss: 42.7514 - gender_output_loss: 0.5380 - image_quality_output_loss: 0.9073 - age_output_loss: 2.7351 - weight_output_loss: 2.4852 - bag_output_loss: 1.3931 - pose_output_loss: 1.1196 - footwear_output_loss: 1.1419 - emotion_output_loss: 2.4674 - gender_output_acc: 0.7703 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4075 - weight_output_acc: 0.6353 - bag_output_acc: 0.6209 - pose_output_acc: 0.7481 - footwear_output_acc: 0.6091 - emotion_output_acc: 0.7200 - val_loss: 22.7588 - val_gender_output_loss: 0.5139 - val_image_quality_output_loss: 0.9387 - val_age_output_loss: 1.3712 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8977 - val_pose_output_loss: 0.5547 - val_footwear_output_loss: 0.8157 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7609 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5908 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.7649 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 22.33097\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 75s 749ms/step - loss: 43.8904 - gender_output_loss: 0.5481 - image_quality_output_loss: 0.9129 - age_output_loss: 2.7031 - weight_output_loss: 2.6081 - bag_output_loss: 1.4403 - pose_output_loss: 1.1233 - footwear_output_loss: 1.1411 - emotion_output_loss: 2.6465 - gender_output_acc: 0.7672 - image_quality_output_acc: 0.5619 - age_output_acc: 0.4194 - weight_output_acc: 0.6272 - bag_output_acc: 0.6172 - pose_output_acc: 0.7444 - footwear_output_acc: 0.6112 - emotion_output_acc: 0.7016 - val_loss: 22.5820 - val_gender_output_loss: 0.4729 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.3604 - val_weight_output_loss: 0.9996 - val_bag_output_loss: 0.8714 - val_pose_output_loss: 0.5617 - val_footwear_output_loss: 0.8164 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7679 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 22.33097\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 75s 748ms/step - loss: 42.9429 - gender_output_loss: 0.5508 - image_quality_output_loss: 0.9131 - age_output_loss: 2.6694 - weight_output_loss: 2.3663 - bag_output_loss: 1.4608 - pose_output_loss: 1.1979 - footwear_output_loss: 1.1522 - emotion_output_loss: 2.5466 - gender_output_acc: 0.7628 - image_quality_output_acc: 0.5603 - age_output_acc: 0.4169 - weight_output_acc: 0.6553 - bag_output_acc: 0.6112 - pose_output_acc: 0.7325 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7094 - val_loss: 23.5724 - val_gender_output_loss: 0.5498 - val_image_quality_output_loss: 1.0821 - val_age_output_loss: 1.3747 - val_weight_output_loss: 1.0007 - val_bag_output_loss: 0.8869 - val_pose_output_loss: 0.6964 - val_footwear_output_loss: 0.8204 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.7564 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6116 - val_pose_output_acc: 0.7083 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 22.33097\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 76s 758ms/step - loss: 44.0027 - gender_output_loss: 0.5669 - image_quality_output_loss: 0.9090 - age_output_loss: 2.7816 - weight_output_loss: 2.4471 - bag_output_loss: 1.5137 - pose_output_loss: 1.2187 - footwear_output_loss: 1.2021 - emotion_output_loss: 2.5527 - gender_output_acc: 0.7531 - image_quality_output_acc: 0.5834 - age_output_acc: 0.3975 - weight_output_acc: 0.6384 - bag_output_acc: 0.6006 - pose_output_acc: 0.7278 - footwear_output_acc: 0.5797 - emotion_output_acc: 0.7159 - val_loss: 23.9483 - val_gender_output_loss: 0.5783 - val_image_quality_output_loss: 1.0061 - val_age_output_loss: 1.4542 - val_weight_output_loss: 1.0380 - val_bag_output_loss: 0.9058 - val_pose_output_loss: 0.7056 - val_footwear_output_loss: 0.8117 - val_emotion_output_loss: 0.9051 - val_gender_output_acc: 0.7287 - val_image_quality_output_acc: 0.5317 - val_age_output_acc: 0.3581 - val_weight_output_acc: 0.5694 - val_bag_output_acc: 0.5878 - val_pose_output_acc: 0.6820 - val_footwear_output_acc: 0.6468 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 22.33097\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 75s 747ms/step - loss: 43.6991 - gender_output_loss: 0.5733 - image_quality_output_loss: 0.9006 - age_output_loss: 2.7577 - weight_output_loss: 2.4983 - bag_output_loss: 1.4445 - pose_output_loss: 1.2440 - footwear_output_loss: 1.1714 - emotion_output_loss: 2.5120 - gender_output_acc: 0.7547 - image_quality_output_acc: 0.5722 - age_output_acc: 0.3975 - weight_output_acc: 0.6406 - bag_output_acc: 0.6134 - pose_output_acc: 0.7181 - footwear_output_acc: 0.6028 - emotion_output_acc: 0.7191 - val_loss: 23.6453 - val_gender_output_loss: 0.6444 - val_image_quality_output_loss: 0.9580 - val_age_output_loss: 1.4043 - val_weight_output_loss: 1.0074 - val_bag_output_loss: 0.9677 - val_pose_output_loss: 0.5869 - val_footwear_output_loss: 0.8300 - val_emotion_output_loss: 0.9272 - val_gender_output_acc: 0.7242 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3616 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.5804 - val_pose_output_acc: 0.7525 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6791\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 22.33097\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 75s 748ms/step - loss: 43.4627 - gender_output_loss: 0.5552 - image_quality_output_loss: 0.9112 - age_output_loss: 2.6983 - weight_output_loss: 2.5097 - bag_output_loss: 1.4174 - pose_output_loss: 1.1271 - footwear_output_loss: 1.1426 - emotion_output_loss: 2.6302 - gender_output_acc: 0.7572 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4188 - weight_output_acc: 0.6328 - bag_output_acc: 0.6222 - pose_output_acc: 0.7484 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7041 - val_loss: 22.5041 - val_gender_output_loss: 0.4581 - val_image_quality_output_loss: 0.9597 - val_age_output_loss: 1.3532 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.8627 - val_pose_output_loss: 0.5762 - val_footwear_output_loss: 0.8095 - val_emotion_output_loss: 0.8967 - val_gender_output_acc: 0.7907 - val_image_quality_output_acc: 0.5352 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7688 - val_footwear_output_acc: 0.6493 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 22.33097\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 75s 752ms/step - loss: 43.4979 - gender_output_loss: 0.5563 - image_quality_output_loss: 0.9063 - age_output_loss: 2.7693 - weight_output_loss: 2.4275 - bag_output_loss: 1.4872 - pose_output_loss: 1.1427 - footwear_output_loss: 1.1844 - emotion_output_loss: 2.5467 - gender_output_acc: 0.7644 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4097 - weight_output_acc: 0.6431 - bag_output_acc: 0.6144 - pose_output_acc: 0.7441 - footwear_output_acc: 0.6066 - emotion_output_acc: 0.7084 - val_loss: 22.2337 - val_gender_output_loss: 0.4637 - val_image_quality_output_loss: 0.9224 - val_age_output_loss: 1.3445 - val_weight_output_loss: 0.9751 - val_bag_output_loss: 0.8619 - val_pose_output_loss: 0.5409 - val_footwear_output_loss: 0.8094 - val_emotion_output_loss: 0.8910 - val_gender_output_acc: 0.7793 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6116 - val_pose_output_acc: 0.7748 - val_footwear_output_acc: 0.6458 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00039: val_loss improved from 22.33097 to 22.23373, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.039.h5\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 76s 765ms/step - loss: 43.1888 - gender_output_loss: 0.5354 - image_quality_output_loss: 0.9101 - age_output_loss: 2.6866 - weight_output_loss: 2.5329 - bag_output_loss: 1.4965 - pose_output_loss: 1.1213 - footwear_output_loss: 1.1399 - emotion_output_loss: 2.5131 - gender_output_acc: 0.7753 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4144 - weight_output_acc: 0.6256 - bag_output_acc: 0.6066 - pose_output_acc: 0.7541 - footwear_output_acc: 0.6213 - emotion_output_acc: 0.7181 - val_loss: 22.5366 - val_gender_output_loss: 0.4686 - val_image_quality_output_loss: 0.9682 - val_age_output_loss: 1.3583 - val_weight_output_loss: 0.9977 - val_bag_output_loss: 0.8722 - val_pose_output_loss: 0.5448 - val_footwear_output_loss: 0.8148 - val_emotion_output_loss: 0.8973 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7698 - val_footwear_output_acc: 0.6399 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 22.23373\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 75s 753ms/step - loss: 43.7401 - gender_output_loss: 0.5603 - image_quality_output_loss: 0.9062 - age_output_loss: 2.7927 - weight_output_loss: 2.5043 - bag_output_loss: 1.4407 - pose_output_loss: 1.1536 - footwear_output_loss: 1.1837 - emotion_output_loss: 2.5515 - gender_output_acc: 0.7659 - image_quality_output_acc: 0.5741 - age_output_acc: 0.3959 - weight_output_acc: 0.6384 - bag_output_acc: 0.6134 - pose_output_acc: 0.7428 - footwear_output_acc: 0.5950 - emotion_output_acc: 0.7125 - val_loss: 22.4357 - val_gender_output_loss: 0.4732 - val_image_quality_output_loss: 0.9455 - val_age_output_loss: 1.3546 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8699 - val_pose_output_loss: 0.5591 - val_footwear_output_loss: 0.8137 - val_emotion_output_loss: 0.8866 - val_gender_output_acc: 0.7788 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.5997 - val_pose_output_acc: 0.7669 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 22.23373\n",
            "Epoch 42/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.4904 - gender_output_loss: 0.5652 - image_quality_output_loss: 0.9065 - age_output_loss: 2.7515 - weight_output_loss: 2.4091 - bag_output_loss: 1.4225 - pose_output_loss: 1.1596 - footwear_output_loss: 1.1261 - emotion_output_loss: 2.6372 - gender_output_acc: 0.7497 - image_quality_output_acc: 0.5698 - age_output_acc: 0.4075 - weight_output_acc: 0.6528 - bag_output_acc: 0.6275 - pose_output_acc: 0.7431 - footwear_output_acc: 0.6136 - emotion_output_acc: 0.7074\n",
            "Epoch 00041: val_loss did not improve from 22.23373\n",
            "100/100 [==============================] - 75s 753ms/step - loss: 43.4638 - gender_output_loss: 0.5655 - image_quality_output_loss: 0.9067 - age_output_loss: 2.7478 - weight_output_loss: 2.4104 - bag_output_loss: 1.4220 - pose_output_loss: 1.1636 - footwear_output_loss: 1.1260 - emotion_output_loss: 2.6305 - gender_output_acc: 0.7491 - image_quality_output_acc: 0.5691 - age_output_acc: 0.4075 - weight_output_acc: 0.6522 - bag_output_acc: 0.6275 - pose_output_acc: 0.7438 - footwear_output_acc: 0.6138 - emotion_output_acc: 0.7078 - val_loss: 22.4309 - val_gender_output_loss: 0.4922 - val_image_quality_output_loss: 0.8988 - val_age_output_loss: 1.3459 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8618 - val_pose_output_loss: 0.5963 - val_footwear_output_loss: 0.8151 - val_emotion_output_loss: 0.8928 - val_gender_output_acc: 0.7688 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6066 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 22.23373\n",
            "Epoch 43/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.6452 - gender_output_loss: 0.5517 - image_quality_output_loss: 0.9147 - age_output_loss: 2.6989 - weight_output_loss: 2.5754 - bag_output_loss: 1.4881 - pose_output_loss: 1.1879 - footwear_output_loss: 1.1810 - emotion_output_loss: 2.5088 - gender_output_acc: 0.7582 - image_quality_output_acc: 0.5565 - age_output_acc: 0.4129 - weight_output_acc: 0.6259 - bag_output_acc: 0.6190 - pose_output_acc: 0.7307 - footwear_output_acc: 0.6070 - emotion_output_acc: 0.7115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "100/100 [==============================] - 75s 752ms/step - loss: 43.6130 - gender_output_loss: 0.5533 - image_quality_output_loss: 0.9135 - age_output_loss: 2.6960 - weight_output_loss: 2.5652 - bag_output_loss: 1.4942 - pose_output_loss: 1.1911 - footwear_output_loss: 1.1837 - emotion_output_loss: 2.5028 - gender_output_acc: 0.7575 - image_quality_output_acc: 0.5578 - age_output_acc: 0.4144 - weight_output_acc: 0.6272 - bag_output_acc: 0.6184 - pose_output_acc: 0.7294 - footwear_output_acc: 0.6059 - emotion_output_acc: 0.7122 - val_loss: 22.8172 - val_gender_output_loss: 0.5073 - val_image_quality_output_loss: 0.9548 - val_age_output_loss: 1.3713 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8745 - val_pose_output_loss: 0.5569 - val_footwear_output_loss: 0.8461 - val_emotion_output_loss: 0.9265 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5317 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.5967 - val_pose_output_acc: 0.7753 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6687\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 22.23373\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 74s 743ms/step - loss: 43.3999 - gender_output_loss: 0.5647 - image_quality_output_loss: 0.9144 - age_output_loss: 2.6888 - weight_output_loss: 2.4999 - bag_output_loss: 1.4876 - pose_output_loss: 1.1680 - footwear_output_loss: 1.1450 - emotion_output_loss: 2.5415 - gender_output_acc: 0.7537 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4075 - weight_output_acc: 0.6425 - bag_output_acc: 0.6103 - pose_output_acc: 0.7347 - footwear_output_acc: 0.6081 - emotion_output_acc: 0.7144 - val_loss: 23.2431 - val_gender_output_loss: 0.4782 - val_image_quality_output_loss: 1.0114 - val_age_output_loss: 1.3764 - val_weight_output_loss: 0.9938 - val_bag_output_loss: 0.8713 - val_pose_output_loss: 0.5915 - val_footwear_output_loss: 0.8407 - val_emotion_output_loss: 0.9858 - val_gender_output_acc: 0.7753 - val_image_quality_output_acc: 0.5114 - val_age_output_acc: 0.3621 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.6121\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 22.23373\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 72s 716ms/step - loss: 43.4659 - gender_output_loss: 0.5601 - image_quality_output_loss: 0.9141 - age_output_loss: 2.6986 - weight_output_loss: 2.5310 - bag_output_loss: 1.4200 - pose_output_loss: 1.2074 - footwear_output_loss: 1.1749 - emotion_output_loss: 2.5338 - gender_output_acc: 0.7613 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4072 - weight_output_acc: 0.6363 - bag_output_acc: 0.6138 - pose_output_acc: 0.7272 - footwear_output_acc: 0.6112 - emotion_output_acc: 0.7144 - val_loss: 22.4049 - val_gender_output_loss: 0.4385 - val_image_quality_output_loss: 0.9374 - val_age_output_loss: 1.3652 - val_weight_output_loss: 0.9742 - val_bag_output_loss: 0.8574 - val_pose_output_loss: 0.5778 - val_footwear_output_loss: 0.8184 - val_emotion_output_loss: 0.8913 - val_gender_output_acc: 0.7981 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6136 - val_pose_output_acc: 0.7584 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 22.23373\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 71s 710ms/step - loss: 44.3118 - gender_output_loss: 0.5525 - image_quality_output_loss: 0.9215 - age_output_loss: 2.7890 - weight_output_loss: 2.6068 - bag_output_loss: 1.4672 - pose_output_loss: 1.1517 - footwear_output_loss: 1.1781 - emotion_output_loss: 2.6031 - gender_output_acc: 0.7700 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4022 - weight_output_acc: 0.6309 - bag_output_acc: 0.6228 - pose_output_acc: 0.7409 - footwear_output_acc: 0.5953 - emotion_output_acc: 0.7081 - val_loss: 22.5507 - val_gender_output_loss: 0.4583 - val_image_quality_output_loss: 0.9704 - val_age_output_loss: 1.3622 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8652 - val_pose_output_loss: 0.5739 - val_footwear_output_loss: 0.8129 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.7882 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7564 - val_footwear_output_acc: 0.6434 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 22.23373\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 72s 722ms/step - loss: 43.0265 - gender_output_loss: 0.5523 - image_quality_output_loss: 0.9104 - age_output_loss: 2.6632 - weight_output_loss: 2.3807 - bag_output_loss: 1.4885 - pose_output_loss: 1.1502 - footwear_output_loss: 1.1435 - emotion_output_loss: 2.5857 - gender_output_acc: 0.7613 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4131 - weight_output_acc: 0.6484 - bag_output_acc: 0.6116 - pose_output_acc: 0.7456 - footwear_output_acc: 0.6069 - emotion_output_acc: 0.7097 - val_loss: 22.5689 - val_gender_output_loss: 0.4883 - val_image_quality_output_loss: 0.9626 - val_age_output_loss: 1.3598 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8824 - val_pose_output_loss: 0.5549 - val_footwear_output_loss: 0.8021 - val_emotion_output_loss: 0.8988 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.7664 - val_footwear_output_acc: 0.6543 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 22.23373\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 70s 702ms/step - loss: 43.3655 - gender_output_loss: 0.5497 - image_quality_output_loss: 0.9214 - age_output_loss: 2.7206 - weight_output_loss: 2.5239 - bag_output_loss: 1.4167 - pose_output_loss: 1.1667 - footwear_output_loss: 1.1573 - emotion_output_loss: 2.5360 - gender_output_acc: 0.7628 - image_quality_output_acc: 0.5453 - age_output_acc: 0.4150 - weight_output_acc: 0.6363 - bag_output_acc: 0.6297 - pose_output_acc: 0.7434 - footwear_output_acc: 0.6103 - emotion_output_acc: 0.7103 - val_loss: 22.6846 - val_gender_output_loss: 0.4671 - val_image_quality_output_loss: 0.9976 - val_age_output_loss: 1.3658 - val_weight_output_loss: 0.9953 - val_bag_output_loss: 0.8743 - val_pose_output_loss: 0.5621 - val_footwear_output_loss: 0.8151 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.7897 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6062 - val_pose_output_acc: 0.7654 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 22.23373\n",
            "Epoch 49/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 42.9157 - gender_output_loss: 0.5488 - image_quality_output_loss: 0.9175 - age_output_loss: 2.6552 - weight_output_loss: 2.4927 - bag_output_loss: 1.5238 - pose_output_loss: 1.1042 - footwear_output_loss: 1.1687 - emotion_output_loss: 2.4757 - gender_output_acc: 0.7664 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4208 - weight_output_acc: 0.6414 - bag_output_acc: 0.6016 - pose_output_acc: 0.7582 - footwear_output_acc: 0.6045 - emotion_output_acc: 0.7156\n",
            "100/100 [==============================] - 70s 699ms/step - loss: 42.8966 - gender_output_loss: 0.5487 - image_quality_output_loss: 0.9195 - age_output_loss: 2.6593 - weight_output_loss: 2.4943 - bag_output_loss: 1.5179 - pose_output_loss: 1.1022 - footwear_output_loss: 1.1662 - emotion_output_loss: 2.4718 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5606 - age_output_acc: 0.4188 - weight_output_acc: 0.6412 - bag_output_acc: 0.6028 - pose_output_acc: 0.7591 - footwear_output_acc: 0.6050 - emotion_output_acc: 0.7159 - val_loss: 22.4151 - val_gender_output_loss: 0.4452 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3575 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8528 - val_pose_output_loss: 0.5433 - val_footwear_output_loss: 0.8054 - val_emotion_output_loss: 0.8967 - val_gender_output_acc: 0.7887 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6156 - val_pose_output_acc: 0.7738 - val_footwear_output_acc: 0.6443 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 22.23373\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 70s 699ms/step - loss: 42.5056 - gender_output_loss: 0.5320 - image_quality_output_loss: 0.8875 - age_output_loss: 2.7372 - weight_output_loss: 2.3881 - bag_output_loss: 1.4062 - pose_output_loss: 1.1264 - footwear_output_loss: 1.1484 - emotion_output_loss: 2.4749 - gender_output_acc: 0.7788 - image_quality_output_acc: 0.5872 - age_output_acc: 0.4031 - weight_output_acc: 0.6469 - bag_output_acc: 0.6216 - pose_output_acc: 0.7525 - footwear_output_acc: 0.5984 - emotion_output_acc: 0.7166 - val_loss: 22.7510 - val_gender_output_loss: 0.4663 - val_image_quality_output_loss: 1.0093 - val_age_output_loss: 1.3634 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.8664 - val_pose_output_loss: 0.5865 - val_footwear_output_loss: 0.8140 - val_emotion_output_loss: 0.9162 - val_gender_output_acc: 0.7907 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7555 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6791\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 22.23373\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 71s 706ms/step - loss: 44.0086 - gender_output_loss: 0.5449 - image_quality_output_loss: 0.9127 - age_output_loss: 2.7900 - weight_output_loss: 2.4666 - bag_output_loss: 1.5105 - pose_output_loss: 1.1046 - footwear_output_loss: 1.1832 - emotion_output_loss: 2.6408 - gender_output_acc: 0.7709 - image_quality_output_acc: 0.5647 - age_output_acc: 0.4109 - weight_output_acc: 0.6384 - bag_output_acc: 0.6116 - pose_output_acc: 0.7572 - footwear_output_acc: 0.6078 - emotion_output_acc: 0.7062 - val_loss: 23.3959 - val_gender_output_loss: 0.4539 - val_image_quality_output_loss: 1.0188 - val_age_output_loss: 1.3649 - val_weight_output_loss: 1.0004 - val_bag_output_loss: 0.9043 - val_pose_output_loss: 0.7467 - val_footwear_output_loss: 0.8462 - val_emotion_output_loss: 0.8966 - val_gender_output_acc: 0.7937 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.5868 - val_pose_output_acc: 0.6801 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.7078\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 22.23373\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 70s 695ms/step - loss: 43.4894 - gender_output_loss: 0.5512 - image_quality_output_loss: 0.9029 - age_output_loss: 2.7971 - weight_output_loss: 2.4328 - bag_output_loss: 1.3626 - pose_output_loss: 1.2115 - footwear_output_loss: 1.1801 - emotion_output_loss: 2.5636 - gender_output_acc: 0.7634 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4100 - weight_output_acc: 0.6412 - bag_output_acc: 0.6319 - pose_output_acc: 0.7300 - footwear_output_acc: 0.6016 - emotion_output_acc: 0.7128 - val_loss: 22.9856 - val_gender_output_loss: 0.4598 - val_image_quality_output_loss: 1.0214 - val_age_output_loss: 1.3831 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.8685 - val_pose_output_loss: 0.5856 - val_footwear_output_loss: 0.8440 - val_emotion_output_loss: 0.9237 - val_gender_output_acc: 0.7902 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.3477 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6141 - val_pose_output_acc: 0.7599 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 22.23373\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 69s 693ms/step - loss: 43.4240 - gender_output_loss: 0.5759 - image_quality_output_loss: 0.9145 - age_output_loss: 2.6519 - weight_output_loss: 2.5483 - bag_output_loss: 1.4627 - pose_output_loss: 1.1754 - footwear_output_loss: 1.1596 - emotion_output_loss: 2.5503 - gender_output_acc: 0.7491 - image_quality_output_acc: 0.5634 - age_output_acc: 0.4134 - weight_output_acc: 0.6350 - bag_output_acc: 0.6022 - pose_output_acc: 0.7350 - footwear_output_acc: 0.6141 - emotion_output_acc: 0.7147 - val_loss: 23.3239 - val_gender_output_loss: 0.5397 - val_image_quality_output_loss: 0.9697 - val_age_output_loss: 1.3764 - val_weight_output_loss: 1.0038 - val_bag_output_loss: 0.9856 - val_pose_output_loss: 0.5946 - val_footwear_output_loss: 0.8385 - val_emotion_output_loss: 0.9038 - val_gender_output_acc: 0.7450 - val_image_quality_output_acc: 0.5337 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.5962 - val_bag_output_acc: 0.5749 - val_pose_output_acc: 0.7500 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 22.23373\n",
            "Epoch 54/100\n",
            "\n",
            "100/100 [==============================] - 71s 709ms/step - loss: 43.8091 - gender_output_loss: 0.5453 - image_quality_output_loss: 0.9069 - age_output_loss: 2.7503 - weight_output_loss: 2.5953 - bag_output_loss: 1.4730 - pose_output_loss: 1.1532 - footwear_output_loss: 1.1361 - emotion_output_loss: 2.5530 - gender_output_acc: 0.7775 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4003 - weight_output_acc: 0.6256 - bag_output_acc: 0.6162 - pose_output_acc: 0.7428 - footwear_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 22.5387 - val_gender_output_loss: 0.4564 - val_image_quality_output_loss: 0.9924 - val_age_output_loss: 1.3601 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8627 - val_pose_output_loss: 0.5722 - val_footwear_output_loss: 0.8111 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.7808 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6131 - val_pose_output_acc: 0.7550 - val_footwear_output_acc: 0.6394 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 22.23373\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 43.3871 - gender_output_loss: 0.5534 - image_quality_output_loss: 0.9049 - age_output_loss: 2.7088 - weight_output_loss: 2.4290 - bag_output_loss: 1.4153 - pose_output_loss: 1.1521 - footwear_output_loss: 1.1565 - emotion_output_loss: 2.6448 - gender_output_acc: 0.7606 - image_quality_output_acc: 0.5697 - age_output_acc: 0.4113 - weight_output_acc: 0.6503 - bag_output_acc: 0.6228 - pose_output_acc: 0.7400 - footwear_output_acc: 0.6100 - emotion_output_acc: 0.7059 - val_loss: 22.4635 - val_gender_output_loss: 0.4598 - val_image_quality_output_loss: 0.9669 - val_age_output_loss: 1.3571 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8686 - val_pose_output_loss: 0.5441 - val_footwear_output_loss: 0.8098 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.7951 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 22.23373\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 42.6578 - gender_output_loss: 0.5283 - image_quality_output_loss: 0.9094 - age_output_loss: 2.6978 - weight_output_loss: 2.5473 - bag_output_loss: 1.4443 - pose_output_loss: 1.0663 - footwear_output_loss: 1.1563 - emotion_output_loss: 2.4378 - gender_output_acc: 0.7784 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4203 - weight_output_acc: 0.6331 - bag_output_acc: 0.6388 - pose_output_acc: 0.7659 - footwear_output_acc: 0.6075 - emotion_output_acc: 0.7194 - val_loss: 22.2592 - val_gender_output_loss: 0.4429 - val_image_quality_output_loss: 0.9366 - val_age_output_loss: 1.3550 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8599 - val_pose_output_loss: 0.5452 - val_footwear_output_loss: 0.8053 - val_emotion_output_loss: 0.8845 - val_gender_output_acc: 0.7951 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7738 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 22.23373\n",
            "Epoch 57/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.3127 - gender_output_loss: 0.5343 - image_quality_output_loss: 0.9168 - age_output_loss: 2.7138 - weight_output_loss: 2.5683 - bag_output_loss: 1.4746 - pose_output_loss: 1.1407 - footwear_output_loss: 1.1128 - emotion_output_loss: 2.5064 - gender_output_acc: 0.7655 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4110 - weight_output_acc: 0.6332 - bag_output_acc: 0.6193 - pose_output_acc: 0.7497 - footwear_output_acc: 0.6225 - emotion_output_acc: 0.7140\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 22.23373\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 43.2404 - gender_output_loss: 0.5362 - image_quality_output_loss: 0.9161 - age_output_loss: 2.7106 - weight_output_loss: 2.5573 - bag_output_loss: 1.4809 - pose_output_loss: 1.1381 - footwear_output_loss: 1.1135 - emotion_output_loss: 2.4960 - gender_output_acc: 0.7644 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4103 - weight_output_acc: 0.6338 - bag_output_acc: 0.6188 - pose_output_acc: 0.7506 - footwear_output_acc: 0.6222 - emotion_output_acc: 0.7147 - val_loss: 22.5109 - val_gender_output_loss: 0.4685 - val_image_quality_output_loss: 0.9479 - val_age_output_loss: 1.3650 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8709 - val_pose_output_loss: 0.5649 - val_footwear_output_loss: 0.8110 - val_emotion_output_loss: 0.8928 - val_gender_output_acc: 0.7907 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6146 - val_pose_output_acc: 0.7669 - val_footwear_output_acc: 0.6429 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 22.23373\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 71s 707ms/step - loss: 42.5957 - gender_output_loss: 0.5477 - image_quality_output_loss: 0.8934 - age_output_loss: 2.6792 - weight_output_loss: 2.3736 - bag_output_loss: 1.4334 - pose_output_loss: 1.1468 - footwear_output_loss: 1.1735 - emotion_output_loss: 2.5089 - gender_output_acc: 0.7638 - image_quality_output_acc: 0.5834 - age_output_acc: 0.4109 - weight_output_acc: 0.6469 - bag_output_acc: 0.6297 - pose_output_acc: 0.7391 - footwear_output_acc: 0.6091 - emotion_output_acc: 0.7125 - val_loss: 22.2559 - val_gender_output_loss: 0.4706 - val_image_quality_output_loss: 0.8995 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8607 - val_pose_output_loss: 0.5546 - val_footwear_output_loss: 0.8159 - val_emotion_output_loss: 0.8828 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.6071 - val_pose_output_acc: 0.7693 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 22.23373\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 42.4294 - gender_output_loss: 0.5549 - image_quality_output_loss: 0.9074 - age_output_loss: 2.7078 - weight_output_loss: 2.3886 - bag_output_loss: 1.4286 - pose_output_loss: 1.1871 - footwear_output_loss: 1.1727 - emotion_output_loss: 2.3908 - gender_output_acc: 0.7628 - image_quality_output_acc: 0.5572 - age_output_acc: 0.4172 - weight_output_acc: 0.6503 - bag_output_acc: 0.6262 - pose_output_acc: 0.7378 - footwear_output_acc: 0.5975 - emotion_output_acc: 0.7284 - val_loss: 22.7903 - val_gender_output_loss: 0.4599 - val_image_quality_output_loss: 1.0331 - val_age_output_loss: 1.3751 - val_weight_output_loss: 0.9925 - val_bag_output_loss: 0.8689 - val_pose_output_loss: 0.5779 - val_footwear_output_loss: 0.8063 - val_emotion_output_loss: 0.9055 - val_gender_output_acc: 0.7842 - val_image_quality_output_acc: 0.5164 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7515 - val_footwear_output_acc: 0.6443 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 22.23373\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 44.1305 - gender_output_loss: 0.5576 - image_quality_output_loss: 0.9126 - age_output_loss: 2.7227 - weight_output_loss: 2.6384 - bag_output_loss: 1.5170 - pose_output_loss: 1.1726 - footwear_output_loss: 1.1676 - emotion_output_loss: 2.5572 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5634 - age_output_acc: 0.3950 - weight_output_acc: 0.6338 - bag_output_acc: 0.6112 - pose_output_acc: 0.7328 - footwear_output_acc: 0.6097 - emotion_output_acc: 0.7103 - val_loss: 23.6132 - val_gender_output_loss: 0.5702 - val_image_quality_output_loss: 0.9752 - val_age_output_loss: 1.4313 - val_weight_output_loss: 1.0185 - val_bag_output_loss: 0.9138 - val_pose_output_loss: 0.6179 - val_footwear_output_loss: 0.8172 - val_emotion_output_loss: 0.9405 - val_gender_output_acc: 0.7361 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3601 - val_weight_output_acc: 0.5933 - val_bag_output_acc: 0.5918 - val_pose_output_acc: 0.7485 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6642\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 22.23373\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 69s 693ms/step - loss: 44.5743 - gender_output_loss: 0.5531 - image_quality_output_loss: 0.9147 - age_output_loss: 2.7508 - weight_output_loss: 2.5716 - bag_output_loss: 1.4251 - pose_output_loss: 1.2011 - footwear_output_loss: 1.1792 - emotion_output_loss: 2.7336 - gender_output_acc: 0.7603 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4006 - weight_output_acc: 0.6319 - bag_output_acc: 0.6122 - pose_output_acc: 0.7275 - footwear_output_acc: 0.5959 - emotion_output_acc: 0.6956 - val_loss: 22.6280 - val_gender_output_loss: 0.4431 - val_image_quality_output_loss: 0.9492 - val_age_output_loss: 1.3769 - val_weight_output_loss: 1.0272 - val_bag_output_loss: 0.8667 - val_pose_output_loss: 0.5610 - val_footwear_output_loss: 0.8103 - val_emotion_output_loss: 0.9004 - val_gender_output_acc: 0.7882 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.5823 - val_bag_output_acc: 0.6106 - val_pose_output_acc: 0.7649 - val_footwear_output_acc: 0.6305 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 22.23373\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 43.3123 - gender_output_loss: 0.5364 - image_quality_output_loss: 0.9160 - age_output_loss: 2.7465 - weight_output_loss: 2.5565 - bag_output_loss: 1.3986 - pose_output_loss: 1.1011 - footwear_output_loss: 1.1399 - emotion_output_loss: 2.5562 - gender_output_acc: 0.7809 - image_quality_output_acc: 0.5572 - age_output_acc: 0.4109 - weight_output_acc: 0.6338 - bag_output_acc: 0.6316 - pose_output_acc: 0.7447 - footwear_output_acc: 0.6116 - emotion_output_acc: 0.7091 - val_loss: 22.7834 - val_gender_output_loss: 0.4415 - val_image_quality_output_loss: 1.0214 - val_age_output_loss: 1.3724 - val_weight_output_loss: 1.0415 - val_bag_output_loss: 0.8745 - val_pose_output_loss: 0.5460 - val_footwear_output_loss: 0.8115 - val_emotion_output_loss: 0.9028 - val_gender_output_acc: 0.7981 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5848 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.7798 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 22.23373\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 42.9031 - gender_output_loss: 0.5471 - image_quality_output_loss: 0.8980 - age_output_loss: 2.7184 - weight_output_loss: 2.3660 - bag_output_loss: 1.5237 - pose_output_loss: 1.1015 - footwear_output_loss: 1.1636 - emotion_output_loss: 2.5227 - gender_output_acc: 0.7681 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4075 - weight_output_acc: 0.6444 - bag_output_acc: 0.6112 - pose_output_acc: 0.7566 - footwear_output_acc: 0.6131 - emotion_output_acc: 0.7131 - val_loss: 22.6991 - val_gender_output_loss: 0.4544 - val_image_quality_output_loss: 0.9895 - val_age_output_loss: 1.3745 - val_weight_output_loss: 0.9868 - val_bag_output_loss: 0.8596 - val_pose_output_loss: 0.6188 - val_footwear_output_loss: 0.8138 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.7857 - val_image_quality_output_acc: 0.5208 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6096 - val_pose_output_acc: 0.7445 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 22.23373\n",
            "Epoch 64/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.6042 - gender_output_loss: 0.5240 - image_quality_output_loss: 0.9061 - age_output_loss: 2.6911 - weight_output_loss: 2.4762 - bag_output_loss: 1.4355 - pose_output_loss: 1.1835 - footwear_output_loss: 1.1845 - emotion_output_loss: 2.6445 - gender_output_acc: 0.7740 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4053 - weight_output_acc: 0.6357 - bag_output_acc: 0.6212 - pose_output_acc: 0.7402 - footwear_output_acc: 0.5903 - emotion_output_acc: 0.7039\n",
            "Epoch 00063: val_loss did not improve from 22.23373\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 43.5240 - gender_output_loss: 0.5241 - image_quality_output_loss: 0.9052 - age_output_loss: 2.6926 - weight_output_loss: 2.4641 - bag_output_loss: 1.4340 - pose_output_loss: 1.1816 - footwear_output_loss: 1.1858 - emotion_output_loss: 2.6344 - gender_output_acc: 0.7744 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4044 - weight_output_acc: 0.6366 - bag_output_acc: 0.6225 - pose_output_acc: 0.7403 - footwear_output_acc: 0.5900 - emotion_output_acc: 0.7044 - val_loss: 22.5293 - val_gender_output_loss: 0.4378 - val_image_quality_output_loss: 1.0113 - val_age_output_loss: 1.3644 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.8588 - val_pose_output_loss: 0.5254 - val_footwear_output_loss: 0.8117 - val_emotion_output_loss: 0.9169 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5164 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5928 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7827 - val_footwear_output_acc: 0.6434 - val_emotion_output_acc: 0.6741\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 22.23373\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 43.5234 - gender_output_loss: 0.5416 - image_quality_output_loss: 0.9017 - age_output_loss: 2.7427 - weight_output_loss: 2.6233 - bag_output_loss: 1.4666 - pose_output_loss: 1.1368 - footwear_output_loss: 1.1283 - emotion_output_loss: 2.4956 - gender_output_acc: 0.7650 - image_quality_output_acc: 0.5794 - age_output_acc: 0.4116 - weight_output_acc: 0.6363 - bag_output_acc: 0.6203 - pose_output_acc: 0.7494 - footwear_output_acc: 0.6097 - emotion_output_acc: 0.7188 - val_loss: 22.7601 - val_gender_output_loss: 0.4500 - val_image_quality_output_loss: 1.0014 - val_age_output_loss: 1.3759 - val_weight_output_loss: 1.0102 - val_bag_output_loss: 0.8657 - val_pose_output_loss: 0.5963 - val_footwear_output_loss: 0.8112 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7569 - val_footwear_output_acc: 0.6478 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 22.23373\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 42.9658 - gender_output_loss: 0.5469 - image_quality_output_loss: 0.9124 - age_output_loss: 2.6620 - weight_output_loss: 2.4424 - bag_output_loss: 1.4557 - pose_output_loss: 1.0987 - footwear_output_loss: 1.1854 - emotion_output_loss: 2.5730 - gender_output_acc: 0.7647 - image_quality_output_acc: 0.5659 - age_output_acc: 0.4069 - weight_output_acc: 0.6484 - bag_output_acc: 0.6225 - pose_output_acc: 0.7666 - footwear_output_acc: 0.5903 - emotion_output_acc: 0.7075 - val_loss: 22.7989 - val_gender_output_loss: 0.5491 - val_image_quality_output_loss: 0.9224 - val_age_output_loss: 1.3674 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.9097 - val_pose_output_loss: 0.5548 - val_footwear_output_loss: 0.8157 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.7644 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6022 - val_pose_output_acc: 0.7758 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 22.23373\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 43.2984 - gender_output_loss: 0.5436 - image_quality_output_loss: 0.9023 - age_output_loss: 2.7390 - weight_output_loss: 2.4829 - bag_output_loss: 1.4460 - pose_output_loss: 1.1358 - footwear_output_loss: 1.1446 - emotion_output_loss: 2.5554 - gender_output_acc: 0.7578 - image_quality_output_acc: 0.5634 - age_output_acc: 0.4034 - weight_output_acc: 0.6391 - bag_output_acc: 0.6022 - pose_output_acc: 0.7444 - footwear_output_acc: 0.6172 - emotion_output_acc: 0.7113 - val_loss: 22.5949 - val_gender_output_loss: 0.4646 - val_image_quality_output_loss: 1.0229 - val_age_output_loss: 1.3664 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8613 - val_pose_output_loss: 0.5537 - val_footwear_output_loss: 0.8052 - val_emotion_output_loss: 0.9018 - val_gender_output_acc: 0.7743 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7664 - val_footwear_output_acc: 0.6463 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 22.23373\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 70s 695ms/step - loss: 43.7940 - gender_output_loss: 0.5342 - image_quality_output_loss: 0.9081 - age_output_loss: 2.7970 - weight_output_loss: 2.5585 - bag_output_loss: 1.4767 - pose_output_loss: 1.1348 - footwear_output_loss: 1.1578 - emotion_output_loss: 2.5379 - gender_output_acc: 0.7712 - image_quality_output_acc: 0.5622 - age_output_acc: 0.4003 - weight_output_acc: 0.6297 - bag_output_acc: 0.6134 - pose_output_acc: 0.7444 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7141 - val_loss: 23.2683 - val_gender_output_loss: 0.5451 - val_image_quality_output_loss: 0.9408 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.9398 - val_pose_output_loss: 0.6060 - val_footwear_output_loss: 0.8638 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.7564 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.5913 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.6215 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 22.23373\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 43.2478 - gender_output_loss: 0.5428 - image_quality_output_loss: 0.9058 - age_output_loss: 2.6454 - weight_output_loss: 2.3774 - bag_output_loss: 1.4787 - pose_output_loss: 1.1995 - footwear_output_loss: 1.1546 - emotion_output_loss: 2.6375 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3969 - weight_output_acc: 0.6419 - bag_output_acc: 0.6100 - pose_output_acc: 0.7325 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7044 - val_loss: 23.1931 - val_gender_output_loss: 0.4898 - val_image_quality_output_loss: 1.1358 - val_age_output_loss: 1.3914 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8700 - val_pose_output_loss: 0.5841 - val_footwear_output_loss: 0.8307 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.7733 - val_image_quality_output_acc: 0.4787 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6205 - val_pose_output_acc: 0.7693 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 22.23373\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 69s 688ms/step - loss: 43.1686 - gender_output_loss: 0.5471 - image_quality_output_loss: 0.9147 - age_output_loss: 2.7392 - weight_output_loss: 2.4707 - bag_output_loss: 1.4400 - pose_output_loss: 1.1758 - footwear_output_loss: 1.1545 - emotion_output_loss: 2.4945 - gender_output_acc: 0.7684 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4109 - weight_output_acc: 0.6409 - bag_output_acc: 0.6100 - pose_output_acc: 0.7428 - footwear_output_acc: 0.6128 - emotion_output_acc: 0.7188 - val_loss: 22.5759 - val_gender_output_loss: 0.4640 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.3782 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8682 - val_pose_output_loss: 0.5671 - val_footwear_output_loss: 0.8271 - val_emotion_output_loss: 0.8830 - val_gender_output_acc: 0.7832 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6027 - val_pose_output_acc: 0.7604 - val_footwear_output_acc: 0.6349 - val_emotion_output_acc: 0.7083\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 22.23373\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 69s 686ms/step - loss: 43.3434 - gender_output_loss: 0.5403 - image_quality_output_loss: 0.9152 - age_output_loss: 2.7904 - weight_output_loss: 2.5159 - bag_output_loss: 1.3926 - pose_output_loss: 1.0920 - footwear_output_loss: 1.1364 - emotion_output_loss: 2.5639 - gender_output_acc: 0.7738 - image_quality_output_acc: 0.5616 - age_output_acc: 0.4053 - weight_output_acc: 0.6372 - bag_output_acc: 0.6353 - pose_output_acc: 0.7569 - footwear_output_acc: 0.6138 - emotion_output_acc: 0.7072 - val_loss: 22.4204 - val_gender_output_loss: 0.4447 - val_image_quality_output_loss: 0.9602 - val_age_output_loss: 1.3659 - val_weight_output_loss: 0.9978 - val_bag_output_loss: 0.8719 - val_pose_output_loss: 0.5345 - val_footwear_output_loss: 0.8098 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.8016 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.5952 - val_bag_output_acc: 0.6071 - val_pose_output_acc: 0.7763 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 22.23373\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 43.2395 - gender_output_loss: 0.5364 - image_quality_output_loss: 0.9062 - age_output_loss: 2.6497 - weight_output_loss: 2.5773 - bag_output_loss: 1.5063 - pose_output_loss: 1.1118 - footwear_output_loss: 1.1593 - emotion_output_loss: 2.5276 - gender_output_acc: 0.7769 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4119 - weight_output_acc: 0.6347 - bag_output_acc: 0.6097 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6109 - emotion_output_acc: 0.7141 - val_loss: 22.1748 - val_gender_output_loss: 0.4332 - val_image_quality_output_loss: 0.9304 - val_age_output_loss: 1.3551 - val_weight_output_loss: 0.9953 - val_bag_output_loss: 0.8556 - val_pose_output_loss: 0.5225 - val_footwear_output_loss: 0.8119 - val_emotion_output_loss: 0.8857 - val_gender_output_acc: 0.7917 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6131 - val_pose_output_acc: 0.7822 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "\n",
            "Epoch 00072: val_loss improved from 22.23373 to 22.17476, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.072.h5\n",
            "Epoch 73/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.3732 - gender_output_loss: 0.5302 - image_quality_output_loss: 0.8992 - age_output_loss: 2.6932 - weight_output_loss: 2.5245 - bag_output_loss: 1.4613 - pose_output_loss: 1.1193 - footwear_output_loss: 1.1492 - emotion_output_loss: 2.5968 - gender_output_acc: 0.7715 - image_quality_output_acc: 0.5742 - age_output_acc: 0.4154 - weight_output_acc: 0.6367 - bag_output_acc: 0.6218 - pose_output_acc: 0.7519 - footwear_output_acc: 0.6152 - emotion_output_acc: 0.7080\n",
            "Epoch 00072: val_loss improved from 22.23373 to 22.17476, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.072.h5\n",
            "100/100 [==============================] - 69s 689ms/step - loss: 43.3220 - gender_output_loss: 0.5309 - image_quality_output_loss: 0.8978 - age_output_loss: 2.6928 - weight_output_loss: 2.5201 - bag_output_loss: 1.4634 - pose_output_loss: 1.1183 - footwear_output_loss: 1.1510 - emotion_output_loss: 2.5864 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4156 - weight_output_acc: 0.6372 - bag_output_acc: 0.6209 - pose_output_acc: 0.7522 - footwear_output_acc: 0.6153 - emotion_output_acc: 0.7091 - val_loss: 22.5348 - val_gender_output_loss: 0.4634 - val_image_quality_output_loss: 0.9323 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8806 - val_pose_output_loss: 0.5519 - val_footwear_output_loss: 0.8373 - val_emotion_output_loss: 0.8959 - val_gender_output_acc: 0.7971 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6116 - val_pose_output_acc: 0.7753 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 22.17476\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 42.7868 - gender_output_loss: 0.5291 - image_quality_output_loss: 0.9141 - age_output_loss: 2.6192 - weight_output_loss: 2.3744 - bag_output_loss: 1.3756 - pose_output_loss: 1.1267 - footwear_output_loss: 1.1518 - emotion_output_loss: 2.6875 - gender_output_acc: 0.7762 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4197 - weight_output_acc: 0.6522 - bag_output_acc: 0.6381 - pose_output_acc: 0.7584 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7009 - val_loss: 22.2021 - val_gender_output_loss: 0.4496 - val_image_quality_output_loss: 0.9261 - val_age_output_loss: 1.3600 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.8534 - val_pose_output_loss: 0.5318 - val_footwear_output_loss: 0.8106 - val_emotion_output_loss: 0.8872 - val_gender_output_acc: 0.7753 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6156 - val_pose_output_acc: 0.7817 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 22.17476\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 69s 689ms/step - loss: 44.1837 - gender_output_loss: 0.5387 - image_quality_output_loss: 0.8996 - age_output_loss: 2.8045 - weight_output_loss: 2.6059 - bag_output_loss: 1.4837 - pose_output_loss: 1.1272 - footwear_output_loss: 1.1454 - emotion_output_loss: 2.6023 - gender_output_acc: 0.7772 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4050 - weight_output_acc: 0.6259 - bag_output_acc: 0.6109 - pose_output_acc: 0.7481 - footwear_output_acc: 0.6100 - emotion_output_acc: 0.7097 - val_loss: 22.5812 - val_gender_output_loss: 0.4828 - val_image_quality_output_loss: 0.9412 - val_age_output_loss: 1.3782 - val_weight_output_loss: 1.0001 - val_bag_output_loss: 0.8714 - val_pose_output_loss: 0.5558 - val_footwear_output_loss: 0.8118 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.5898 - val_bag_output_acc: 0.6066 - val_pose_output_acc: 0.7733 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 22.17476\n",
            "Epoch 76/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.1223 - gender_output_loss: 0.5579 - image_quality_output_loss: 0.9232 - age_output_loss: 2.7363 - weight_output_loss: 2.4728 - bag_output_loss: 1.4648 - pose_output_loss: 1.1856 - footwear_output_loss: 1.1445 - emotion_output_loss: 2.4546 - gender_output_acc: 0.7595 - image_quality_output_acc: 0.5590 - age_output_acc: 0.3955 - weight_output_acc: 0.6376 - bag_output_acc: 0.6080 - pose_output_acc: 0.7399 - footwear_output_acc: 0.6149 - emotion_output_acc: 0.7213\n",
            "Epoch 00075: val_loss did not improve from 22.17476\n",
            "100/100 [==============================] - 71s 707ms/step - loss: 43.1529 - gender_output_loss: 0.5582 - image_quality_output_loss: 0.9219 - age_output_loss: 2.7446 - weight_output_loss: 2.4749 - bag_output_loss: 1.4623 - pose_output_loss: 1.1898 - footwear_output_loss: 1.1467 - emotion_output_loss: 2.4506 - gender_output_acc: 0.7591 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3950 - weight_output_acc: 0.6384 - bag_output_acc: 0.6081 - pose_output_acc: 0.7397 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7219 - val_loss: 22.5465 - val_gender_output_loss: 0.4636 - val_image_quality_output_loss: 0.8855 - val_age_output_loss: 1.3973 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8642 - val_pose_output_loss: 0.5873 - val_footwear_output_loss: 0.8209 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.7698 - val_image_quality_output_acc: 0.5784 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6017 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.6364 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 22.17476\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 43.8019 - gender_output_loss: 0.5469 - image_quality_output_loss: 0.9105 - age_output_loss: 2.7733 - weight_output_loss: 2.6935 - bag_output_loss: 1.4082 - pose_output_loss: 1.0725 - footwear_output_loss: 1.1257 - emotion_output_loss: 2.5709 - gender_output_acc: 0.7659 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4019 - weight_output_acc: 0.6159 - bag_output_acc: 0.6225 - pose_output_acc: 0.7597 - footwear_output_acc: 0.6178 - emotion_output_acc: 0.7034 - val_loss: 22.4078 - val_gender_output_loss: 0.4572 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.3657 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.8586 - val_pose_output_loss: 0.5518 - val_footwear_output_loss: 0.8180 - val_emotion_output_loss: 0.8899 - val_gender_output_acc: 0.7946 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6146 - val_pose_output_acc: 0.7778 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 22.17476\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 42.7275 - gender_output_loss: 0.5465 - image_quality_output_loss: 0.9128 - age_output_loss: 2.6462 - weight_output_loss: 2.3465 - bag_output_loss: 1.4306 - pose_output_loss: 1.1605 - footwear_output_loss: 1.1657 - emotion_output_loss: 2.5862 - gender_output_acc: 0.7706 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4134 - weight_output_acc: 0.6484 - bag_output_acc: 0.6116 - pose_output_acc: 0.7334 - footwear_output_acc: 0.6116 - emotion_output_acc: 0.7078 - val_loss: 22.4821 - val_gender_output_loss: 0.4550 - val_image_quality_output_loss: 1.0132 - val_age_output_loss: 1.3586 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8580 - val_pose_output_loss: 0.5607 - val_footwear_output_loss: 0.8138 - val_emotion_output_loss: 0.8887 - val_gender_output_acc: 0.7817 - val_image_quality_output_acc: 0.5129 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7639 - val_footwear_output_acc: 0.6384 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 22.17476\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 42.9302 - gender_output_loss: 0.5358 - image_quality_output_loss: 0.9051 - age_output_loss: 2.7153 - weight_output_loss: 2.3700 - bag_output_loss: 1.5108 - pose_output_loss: 1.1467 - footwear_output_loss: 1.1677 - emotion_output_loss: 2.5087 - gender_output_acc: 0.7700 - image_quality_output_acc: 0.5606 - age_output_acc: 0.3997 - weight_output_acc: 0.6556 - bag_output_acc: 0.6144 - pose_output_acc: 0.7428 - footwear_output_acc: 0.5978 - emotion_output_acc: 0.7175 - val_loss: 22.1803 - val_gender_output_loss: 0.4332 - val_image_quality_output_loss: 0.9351 - val_age_output_loss: 1.3560 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8597 - val_pose_output_loss: 0.5236 - val_footwear_output_loss: 0.8088 - val_emotion_output_loss: 0.8837 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6131 - val_pose_output_acc: 0.7847 - val_footwear_output_acc: 0.6538 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 22.17476\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 70s 705ms/step - loss: 42.6228 - gender_output_loss: 0.5239 - image_quality_output_loss: 0.9025 - age_output_loss: 2.6779 - weight_output_loss: 2.6001 - bag_output_loss: 1.4896 - pose_output_loss: 1.1299 - footwear_output_loss: 1.1694 - emotion_output_loss: 2.3316 - gender_output_acc: 0.7759 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4200 - weight_output_acc: 0.6309 - bag_output_acc: 0.6119 - pose_output_acc: 0.7525 - footwear_output_acc: 0.6012 - emotion_output_acc: 0.7306 - val_loss: 22.4950 - val_gender_output_loss: 0.4569 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.3620 - val_weight_output_loss: 1.0060 - val_bag_output_loss: 0.8718 - val_pose_output_loss: 0.5564 - val_footwear_output_loss: 0.8058 - val_emotion_output_loss: 0.8863 - val_gender_output_acc: 0.7941 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.7679 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 22.17476\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 69s 693ms/step - loss: 42.9210 - gender_output_loss: 0.5181 - image_quality_output_loss: 0.9076 - age_output_loss: 2.7444 - weight_output_loss: 2.4233 - bag_output_loss: 1.4818 - pose_output_loss: 1.0826 - footwear_output_loss: 1.1892 - emotion_output_loss: 2.5042 - gender_output_acc: 0.7781 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4091 - weight_output_acc: 0.6450 - bag_output_acc: 0.6112 - pose_output_acc: 0.7547 - footwear_output_acc: 0.5969 - emotion_output_acc: 0.7116 - val_loss: 22.5137 - val_gender_output_loss: 0.4655 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.3664 - val_weight_output_loss: 0.9994 - val_bag_output_loss: 0.8776 - val_pose_output_loss: 0.5341 - val_footwear_output_loss: 0.8112 - val_emotion_output_loss: 0.8896 - val_gender_output_acc: 0.7857 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.7827 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 22.17476\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 42.5591 - gender_output_loss: 0.5684 - image_quality_output_loss: 0.9078 - age_output_loss: 2.6629 - weight_output_loss: 2.4704 - bag_output_loss: 1.4129 - pose_output_loss: 1.1059 - footwear_output_loss: 1.1278 - emotion_output_loss: 2.4997 - gender_output_acc: 0.7525 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4197 - weight_output_acc: 0.6475 - bag_output_acc: 0.6253 - pose_output_acc: 0.7566 - footwear_output_acc: 0.6222 - emotion_output_acc: 0.7144 - val_loss: 22.4741 - val_gender_output_loss: 0.4659 - val_image_quality_output_loss: 0.9353 - val_age_output_loss: 1.3785 - val_weight_output_loss: 1.0081 - val_bag_output_loss: 0.8840 - val_pose_output_loss: 0.5280 - val_footwear_output_loss: 0.8134 - val_emotion_output_loss: 0.8844 - val_gender_output_acc: 0.7788 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6012 - val_pose_output_acc: 0.7808 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 22.17476\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 71s 708ms/step - loss: 44.1061 - gender_output_loss: 0.5265 - image_quality_output_loss: 0.9130 - age_output_loss: 2.7602 - weight_output_loss: 2.4870 - bag_output_loss: 1.4165 - pose_output_loss: 1.0836 - footwear_output_loss: 1.1424 - emotion_output_loss: 2.8019 - gender_output_acc: 0.7822 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4081 - weight_output_acc: 0.6341 - bag_output_acc: 0.6262 - pose_output_acc: 0.7591 - footwear_output_acc: 0.6097 - emotion_output_acc: 0.6916 - val_loss: 23.4790 - val_gender_output_loss: 0.5032 - val_image_quality_output_loss: 0.9338 - val_age_output_loss: 1.3661 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.8586 - val_pose_output_loss: 0.7055 - val_footwear_output_loss: 0.8424 - val_emotion_output_loss: 1.0180 - val_gender_output_acc: 0.7669 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.5928 - val_bag_output_acc: 0.6076 - val_pose_output_acc: 0.7401 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6007\n",
            "\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 22.17476\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 70s 697ms/step - loss: 42.3606 - gender_output_loss: 0.5444 - image_quality_output_loss: 0.8978 - age_output_loss: 2.6671 - weight_output_loss: 2.3290 - bag_output_loss: 1.4763 - pose_output_loss: 1.1458 - footwear_output_loss: 1.1606 - emotion_output_loss: 2.4754 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4138 - weight_output_acc: 0.6538 - bag_output_acc: 0.6141 - pose_output_acc: 0.7469 - footwear_output_acc: 0.6012 - emotion_output_acc: 0.7141 - val_loss: 23.0468 - val_gender_output_loss: 0.4734 - val_image_quality_output_loss: 1.0438 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8680 - val_pose_output_loss: 0.6040 - val_footwear_output_loss: 0.8403 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.7817 - val_image_quality_output_acc: 0.4995 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6081 - val_pose_output_acc: 0.7564 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.7088\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 22.17476\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 69s 693ms/step - loss: 43.7241 - gender_output_loss: 0.5472 - image_quality_output_loss: 0.9042 - age_output_loss: 2.7948 - weight_output_loss: 2.5734 - bag_output_loss: 1.4703 - pose_output_loss: 1.2053 - footwear_output_loss: 1.1561 - emotion_output_loss: 2.4632 - gender_output_acc: 0.7616 - image_quality_output_acc: 0.5675 - age_output_acc: 0.3978 - weight_output_acc: 0.6341 - bag_output_acc: 0.6194 - pose_output_acc: 0.7275 - footwear_output_acc: 0.6066 - emotion_output_acc: 0.7222 - val_loss: 23.5447 - val_gender_output_loss: 0.5370 - val_image_quality_output_loss: 1.0027 - val_age_output_loss: 1.3865 - val_weight_output_loss: 1.0450 - val_bag_output_loss: 0.9131 - val_pose_output_loss: 0.6425 - val_footwear_output_loss: 0.8704 - val_emotion_output_loss: 0.9119 - val_gender_output_acc: 0.7550 - val_image_quality_output_acc: 0.5114 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.5878 - val_bag_output_acc: 0.5928 - val_pose_output_acc: 0.7421 - val_footwear_output_acc: 0.6161 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 22.17476\n",
            "Epoch 86/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 43.8150 - gender_output_loss: 0.5366 - image_quality_output_loss: 0.9124 - age_output_loss: 2.7087 - weight_output_loss: 2.5114 - bag_output_loss: 1.4588 - pose_output_loss: 1.1093 - footwear_output_loss: 1.1559 - emotion_output_loss: 2.7007 - gender_output_acc: 0.7702 - image_quality_output_acc: 0.5657 - age_output_acc: 0.4062 - weight_output_acc: 0.6373 - bag_output_acc: 0.6196 - pose_output_acc: 0.7462 - footwear_output_acc: 0.6080 - emotion_output_acc: 0.6985\n",
            "Epoch 00085: val_loss did not improve from 22.17476\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 43.8291 - gender_output_loss: 0.5375 - image_quality_output_loss: 0.9122 - age_output_loss: 2.7071 - weight_output_loss: 2.5121 - bag_output_loss: 1.4635 - pose_output_loss: 1.1070 - footwear_output_loss: 1.1564 - emotion_output_loss: 2.7029 - gender_output_acc: 0.7684 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4062 - weight_output_acc: 0.6363 - bag_output_acc: 0.6188 - pose_output_acc: 0.7469 - footwear_output_acc: 0.6078 - emotion_output_acc: 0.6984 - val_loss: 22.3409 - val_gender_output_loss: 0.4520 - val_image_quality_output_loss: 0.9657 - val_age_output_loss: 1.3525 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8638 - val_pose_output_loss: 0.5507 - val_footwear_output_loss: 0.8039 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.7956 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6136 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6513 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 22.17476\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 43.1551 - gender_output_loss: 0.5420 - image_quality_output_loss: 0.8998 - age_output_loss: 2.6955 - weight_output_loss: 2.5223 - bag_output_loss: 1.4591 - pose_output_loss: 1.1476 - footwear_output_loss: 1.1323 - emotion_output_loss: 2.5275 - gender_output_acc: 0.7647 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4062 - weight_output_acc: 0.6294 - bag_output_acc: 0.6269 - pose_output_acc: 0.7466 - footwear_output_acc: 0.6234 - emotion_output_acc: 0.7106 - val_loss: 22.1194 - val_gender_output_loss: 0.4483 - val_image_quality_output_loss: 0.9143 - val_age_output_loss: 1.3530 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.8557 - val_pose_output_loss: 0.5339 - val_footwear_output_loss: 0.8058 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.7932 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6111 - val_pose_output_acc: 0.7763 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00087: val_loss improved from 22.17476 to 22.11941, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.087.h5\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 42.4952 - gender_output_loss: 0.5249 - image_quality_output_loss: 0.8846 - age_output_loss: 2.6935 - weight_output_loss: 2.5300 - bag_output_loss: 1.4015 - pose_output_loss: 1.0955 - footwear_output_loss: 1.1577 - emotion_output_loss: 2.4445 - gender_output_acc: 0.7859 - image_quality_output_acc: 0.5825 - age_output_acc: 0.4088 - weight_output_acc: 0.6338 - bag_output_acc: 0.6200 - pose_output_acc: 0.7578 - footwear_output_acc: 0.6066 - emotion_output_acc: 0.7228 - val_loss: 22.5795 - val_gender_output_loss: 0.4924 - val_image_quality_output_loss: 0.9592 - val_age_output_loss: 1.3818 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8765 - val_pose_output_loss: 0.5268 - val_footwear_output_loss: 0.8193 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.7822 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6146 - val_pose_output_acc: 0.7857 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 22.11941\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 70s 702ms/step - loss: 43.1149 - gender_output_loss: 0.5213 - image_quality_output_loss: 0.9113 - age_output_loss: 2.7488 - weight_output_loss: 2.4395 - bag_output_loss: 1.4222 - pose_output_loss: 1.0979 - footwear_output_loss: 1.1453 - emotion_output_loss: 2.5895 - gender_output_acc: 0.7841 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4131 - weight_output_acc: 0.6416 - bag_output_acc: 0.6197 - pose_output_acc: 0.7509 - footwear_output_acc: 0.6141 - emotion_output_acc: 0.7034 - val_loss: 22.6474 - val_gender_output_loss: 0.5028 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.3780 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.8766 - val_pose_output_loss: 0.5287 - val_footwear_output_loss: 0.8130 - val_emotion_output_loss: 0.9048 - val_gender_output_acc: 0.7798 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6101 - val_pose_output_acc: 0.7793 - val_footwear_output_acc: 0.6434 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 22.11941\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 71s 707ms/step - loss: 43.4329 - gender_output_loss: 0.5373 - image_quality_output_loss: 0.9139 - age_output_loss: 2.6941 - weight_output_loss: 2.5340 - bag_output_loss: 1.4652 - pose_output_loss: 1.0858 - footwear_output_loss: 1.1447 - emotion_output_loss: 2.6208 - gender_output_acc: 0.7641 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4134 - weight_output_acc: 0.6328 - bag_output_acc: 0.6184 - pose_output_acc: 0.7534 - footwear_output_acc: 0.6106 - emotion_output_acc: 0.7078 - val_loss: 22.6148 - val_gender_output_loss: 0.5237 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3533 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8703 - val_pose_output_loss: 0.5848 - val_footwear_output_loss: 0.8282 - val_emotion_output_loss: 0.8955 - val_gender_output_acc: 0.7659 - val_image_quality_output_acc: 0.5521 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6131 - val_pose_output_acc: 0.7664 - val_footwear_output_acc: 0.6399 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 22.11941\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 69s 694ms/step - loss: 43.5640 - gender_output_loss: 0.5351 - image_quality_output_loss: 0.9105 - age_output_loss: 2.7652 - weight_output_loss: 2.5524 - bag_output_loss: 1.4648 - pose_output_loss: 1.0767 - footwear_output_loss: 1.1544 - emotion_output_loss: 2.5739 - gender_output_acc: 0.7681 - image_quality_output_acc: 0.5644 - age_output_acc: 0.4088 - weight_output_acc: 0.6331 - bag_output_acc: 0.6159 - pose_output_acc: 0.7541 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7097 - val_loss: 23.4019 - val_gender_output_loss: 0.6265 - val_image_quality_output_loss: 0.9666 - val_age_output_loss: 1.3772 - val_weight_output_loss: 0.9879 - val_bag_output_loss: 0.9874 - val_pose_output_loss: 0.5724 - val_footwear_output_loss: 0.8204 - val_emotion_output_loss: 0.9243 - val_gender_output_acc: 0.7401 - val_image_quality_output_acc: 0.5461 - val_age_output_acc: 0.3849 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.5868 - val_pose_output_acc: 0.7798 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6726\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 22.11941\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 70s 698ms/step - loss: 43.2131 - gender_output_loss: 0.5380 - image_quality_output_loss: 0.9199 - age_output_loss: 2.6657 - weight_output_loss: 2.4490 - bag_output_loss: 1.4962 - pose_output_loss: 1.2201 - footwear_output_loss: 1.1343 - emotion_output_loss: 2.5363 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5675 - age_output_acc: 0.4109 - weight_output_acc: 0.6456 - bag_output_acc: 0.6053 - pose_output_acc: 0.7356 - footwear_output_acc: 0.6134 - emotion_output_acc: 0.7137 - val_loss: 23.4651 - val_gender_output_loss: 0.5452 - val_image_quality_output_loss: 1.0140 - val_age_output_loss: 1.4078 - val_weight_output_loss: 1.0610 - val_bag_output_loss: 0.9398 - val_pose_output_loss: 0.5752 - val_footwear_output_loss: 0.8198 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.7569 - val_image_quality_output_acc: 0.5045 - val_age_output_acc: 0.3418 - val_weight_output_acc: 0.5332 - val_bag_output_acc: 0.5967 - val_pose_output_acc: 0.7684 - val_footwear_output_acc: 0.6379 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 22.11941\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 43.8562 - gender_output_loss: 0.5496 - image_quality_output_loss: 0.8994 - age_output_loss: 2.6897 - weight_output_loss: 2.4774 - bag_output_loss: 1.4357 - pose_output_loss: 1.1852 - footwear_output_loss: 1.1708 - emotion_output_loss: 2.7098 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4012 - weight_output_acc: 0.6406 - bag_output_acc: 0.6219 - pose_output_acc: 0.7316 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.6975 - val_loss: 22.6460 - val_gender_output_loss: 0.5080 - val_image_quality_output_loss: 0.9503 - val_age_output_loss: 1.3638 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.8903 - val_pose_output_loss: 0.5795 - val_footwear_output_loss: 0.8029 - val_emotion_output_loss: 0.8943 - val_gender_output_acc: 0.7778 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6042 - val_pose_output_acc: 0.7659 - val_footwear_output_acc: 0.6434 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 22.11941\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 42.4315 - gender_output_loss: 0.5546 - image_quality_output_loss: 0.9180 - age_output_loss: 2.7322 - weight_output_loss: 2.5193 - bag_output_loss: 1.4496 - pose_output_loss: 1.1446 - footwear_output_loss: 1.1491 - emotion_output_loss: 2.2990 - gender_output_acc: 0.7675 - image_quality_output_acc: 0.5653 - age_output_acc: 0.4081 - weight_output_acc: 0.6319 - bag_output_acc: 0.6156 - pose_output_acc: 0.7444 - footwear_output_acc: 0.6172 - emotion_output_acc: 0.7328 - val_loss: 22.2044 - val_gender_output_loss: 0.4407 - val_image_quality_output_loss: 0.9435 - val_age_output_loss: 1.3543 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8546 - val_pose_output_loss: 0.5562 - val_footwear_output_loss: 0.7984 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.7976 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.7684 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 22.11941\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 43.5180 - gender_output_loss: 0.5397 - image_quality_output_loss: 0.9003 - age_output_loss: 2.7228 - weight_output_loss: 2.5467 - bag_output_loss: 1.4544 - pose_output_loss: 1.1092 - footwear_output_loss: 1.1454 - emotion_output_loss: 2.6008 - gender_output_acc: 0.7753 - image_quality_output_acc: 0.5703 - age_output_acc: 0.3959 - weight_output_acc: 0.6359 - bag_output_acc: 0.6181 - pose_output_acc: 0.7478 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7125 - val_loss: 22.5588 - val_gender_output_loss: 0.5040 - val_image_quality_output_loss: 0.9969 - val_age_output_loss: 1.3714 - val_weight_output_loss: 0.9924 - val_bag_output_loss: 0.8722 - val_pose_output_loss: 0.5218 - val_footwear_output_loss: 0.8002 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.7842 - val_image_quality_output_acc: 0.5317 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6181 - val_pose_output_acc: 0.7907 - val_footwear_output_acc: 0.6558 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 22.11941\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 69s 692ms/step - loss: 43.1562 - gender_output_loss: 0.5113 - image_quality_output_loss: 0.9011 - age_output_loss: 2.7402 - weight_output_loss: 2.4557 - bag_output_loss: 1.5136 - pose_output_loss: 1.0978 - footwear_output_loss: 1.1494 - emotion_output_loss: 2.5373 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4169 - weight_output_acc: 0.6366 - bag_output_acc: 0.6125 - pose_output_acc: 0.7594 - footwear_output_acc: 0.6047 - emotion_output_acc: 0.7134 - val_loss: 22.3235 - val_gender_output_loss: 0.4571 - val_image_quality_output_loss: 0.9554 - val_age_output_loss: 1.3521 - val_weight_output_loss: 0.9934 - val_bag_output_loss: 0.8705 - val_pose_output_loss: 0.5300 - val_footwear_output_loss: 0.8149 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.7937 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6111 - val_pose_output_acc: 0.7803 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 22.11941\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 69s 690ms/step - loss: 42.3549 - gender_output_loss: 0.5159 - image_quality_output_loss: 0.9146 - age_output_loss: 2.6315 - weight_output_loss: 2.4403 - bag_output_loss: 1.3523 - pose_output_loss: 1.0571 - footwear_output_loss: 1.1470 - emotion_output_loss: 2.6009 - gender_output_acc: 0.7869 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4084 - weight_output_acc: 0.6422 - bag_output_acc: 0.6391 - pose_output_acc: 0.7650 - footwear_output_acc: 0.6084 - emotion_output_acc: 0.7031 - val_loss: 22.3193 - val_gender_output_loss: 0.4472 - val_image_quality_output_loss: 1.0072 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.8626 - val_pose_output_loss: 0.5327 - val_footwear_output_loss: 0.7970 - val_emotion_output_loss: 0.8911 - val_gender_output_acc: 0.8011 - val_image_quality_output_acc: 0.5198 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6136 - val_pose_output_acc: 0.7763 - val_footwear_output_acc: 0.6612 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 22.11941\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 42.2159 - gender_output_loss: 0.5251 - image_quality_output_loss: 0.8943 - age_output_loss: 2.6259 - weight_output_loss: 2.5628 - bag_output_loss: 1.4694 - pose_output_loss: 1.0378 - footwear_output_loss: 1.1496 - emotion_output_loss: 2.4109 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.5859 - age_output_acc: 0.4244 - weight_output_acc: 0.6372 - bag_output_acc: 0.6225 - pose_output_acc: 0.7694 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.7234 - val_loss: 22.4764 - val_gender_output_loss: 0.4695 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.3476 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.8757 - val_pose_output_loss: 0.5386 - val_footwear_output_loss: 0.8026 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.7882 - val_image_quality_output_acc: 0.5317 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7822 - val_footwear_output_acc: 0.6577 - val_emotion_output_acc: 0.6791\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 22.11941\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 69s 687ms/step - loss: 43.6208 - gender_output_loss: 0.5491 - image_quality_output_loss: 0.9153 - age_output_loss: 2.8029 - weight_output_loss: 2.4430 - bag_output_loss: 1.4054 - pose_output_loss: 1.1836 - footwear_output_loss: 1.1596 - emotion_output_loss: 2.5863 - gender_output_acc: 0.7669 - image_quality_output_acc: 0.5759 - age_output_acc: 0.4144 - weight_output_acc: 0.6347 - bag_output_acc: 0.6294 - pose_output_acc: 0.7406 - footwear_output_acc: 0.6003 - emotion_output_acc: 0.7053 - val_loss: 22.6424 - val_gender_output_loss: 0.4647 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3697 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.8596 - val_pose_output_loss: 0.6280 - val_footwear_output_loss: 0.7933 - val_emotion_output_loss: 0.8899 - val_gender_output_acc: 0.7822 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6190 - val_pose_output_acc: 0.7416 - val_footwear_output_acc: 0.6622 - val_emotion_output_acc: 0.7088\n",
            "100/100 [==============================]\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 22.11941\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 44.1424 - gender_output_loss: 0.5299 - image_quality_output_loss: 0.9152 - age_output_loss: 2.7294 - weight_output_loss: 2.4951 - bag_output_loss: 1.4690 - pose_output_loss: 1.1130 - footwear_output_loss: 1.1585 - emotion_output_loss: 2.7669 - gender_output_acc: 0.7712 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4072 - weight_output_acc: 0.6391 - bag_output_acc: 0.6012 - pose_output_acc: 0.7475 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.6941 - val_loss: 25.2355 - val_gender_output_loss: 0.4850 - val_image_quality_output_loss: 1.3139 - val_age_output_loss: 1.4351 - val_weight_output_loss: 1.0701 - val_bag_output_loss: 0.8944 - val_pose_output_loss: 0.9829 - val_footwear_output_loss: 0.8467 - val_emotion_output_loss: 0.9109 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.4390 - val_age_output_acc: 0.3532 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.6057 - val_pose_output_acc: 0.6176 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 22.11941\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "End of run with EPOCHS= 100 STEPS_PER_EPOCH= 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21RePjXTw5gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=50\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577549600_model.087.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2yhCS7MAiX8",
        "colab_type": "code",
        "outputId": "211208d0-5a18-405b-e7f2-934ede5465af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=50\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 6, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "Returning new callback array with steps_per_epoch= 50 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json  to: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577562063_backup\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 53s 1s/step - loss: 48.0036 - gender_output_loss: 0.5003 - image_quality_output_loss: 0.8874 - age_output_loss: 2.6350 - weight_output_loss: 2.5622 - bag_output_loss: 1.4127 - pose_output_loss: 1.1423 - footwear_output_loss: 1.1480 - emotion_output_loss: 2.3292 - gender_output_acc: 0.7831 - image_quality_output_acc: 0.5819 - age_output_acc: 0.4237 - weight_output_acc: 0.6400 - bag_output_acc: 0.6300 - pose_output_acc: 0.7450 - footwear_output_acc: 0.6106 - emotion_output_acc: 0.7300 - val_loss: 26.4230 - val_gender_output_loss: 0.4263 - val_image_quality_output_loss: 0.9344 - val_age_output_loss: 1.3431 - val_weight_output_loss: 0.9882 - val_bag_output_loss: 0.8489 - val_pose_output_loss: 0.5146 - val_footwear_output_loss: 0.8121 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.8051 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 26.42299, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.001.h5\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 42s 831ms/step - loss: 49.0936 - gender_output_loss: 0.4990 - image_quality_output_loss: 0.8951 - age_output_loss: 2.6224 - weight_output_loss: 2.6323 - bag_output_loss: 1.3744 - pose_output_loss: 1.0711 - footwear_output_loss: 1.1529 - emotion_output_loss: 2.6632 - gender_output_acc: 0.8006 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4194 - weight_output_acc: 0.6356 - bag_output_acc: 0.6388 - pose_output_acc: 0.7662 - footwear_output_acc: 0.6100 - emotion_output_acc: 0.7056 - val_loss: 27.2091 - val_gender_output_loss: 0.4330 - val_image_quality_output_loss: 1.0240 - val_age_output_loss: 1.3531 - val_weight_output_loss: 0.9968 - val_bag_output_loss: 0.8681 - val_pose_output_loss: 0.5673 - val_footwear_output_loss: 0.8254 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.4043 - val_weight_output_acc: 0.5873 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7644 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 26.42299\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 44s 876ms/step - loss: 49.2377 - gender_output_loss: 0.5172 - image_quality_output_loss: 0.8963 - age_output_loss: 2.7561 - weight_output_loss: 2.5169 - bag_output_loss: 1.5005 - pose_output_loss: 1.0595 - footwear_output_loss: 1.1720 - emotion_output_loss: 2.4517 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5894 - age_output_acc: 0.3987 - weight_output_acc: 0.6406 - bag_output_acc: 0.6212 - pose_output_acc: 0.7600 - footwear_output_acc: 0.6044 - emotion_output_acc: 0.7156 - val_loss: 26.8665 - val_gender_output_loss: 0.4330 - val_image_quality_output_loss: 0.9349 - val_age_output_loss: 1.3765 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8565 - val_pose_output_loss: 0.5585 - val_footwear_output_loss: 0.8420 - val_emotion_output_loss: 0.8876 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6235 - val_pose_output_acc: 0.7703 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 26.42299\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 44s 879ms/step - loss: 48.4070 - gender_output_loss: 0.5279 - image_quality_output_loss: 0.8929 - age_output_loss: 2.6657 - weight_output_loss: 2.1947 - bag_output_loss: 1.4849 - pose_output_loss: 1.1752 - footwear_output_loss: 1.1510 - emotion_output_loss: 2.5213 - gender_output_acc: 0.7806 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4062 - weight_output_acc: 0.6462 - bag_output_acc: 0.6119 - pose_output_acc: 0.7306 - footwear_output_acc: 0.6144 - emotion_output_acc: 0.7144 - val_loss: 28.7103 - val_gender_output_loss: 0.5518 - val_image_quality_output_loss: 1.1809 - val_age_output_loss: 1.3873 - val_weight_output_loss: 1.0134 - val_bag_output_loss: 0.8877 - val_pose_output_loss: 0.6073 - val_footwear_output_loss: 0.8575 - val_emotion_output_loss: 0.9301 - val_gender_output_acc: 0.7758 - val_image_quality_output_acc: 0.4742 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.5928 - val_bag_output_acc: 0.6141 - val_pose_output_acc: 0.7470 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6687\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 26.42299\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 44s 876ms/step - loss: 49.9255 - gender_output_loss: 0.5240 - image_quality_output_loss: 0.9196 - age_output_loss: 2.9247 - weight_output_loss: 2.6476 - bag_output_loss: 1.3762 - pose_output_loss: 1.1262 - footwear_output_loss: 1.1391 - emotion_output_loss: 2.4834 - gender_output_acc: 0.7744 - image_quality_output_acc: 0.5713 - age_output_acc: 0.3919 - weight_output_acc: 0.6169 - bag_output_acc: 0.6262 - pose_output_acc: 0.7463 - footwear_output_acc: 0.6175 - emotion_output_acc: 0.7181 - val_loss: 28.7188 - val_gender_output_loss: 0.5203 - val_image_quality_output_loss: 1.0306 - val_age_output_loss: 1.4299 - val_weight_output_loss: 1.0451 - val_bag_output_loss: 0.9813 - val_pose_output_loss: 0.6112 - val_footwear_output_loss: 0.8293 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.7763 - val_image_quality_output_acc: 0.5144 - val_age_output_acc: 0.3611 - val_weight_output_acc: 0.5660 - val_bag_output_acc: 0.5957 - val_pose_output_acc: 0.7569 - val_footwear_output_acc: 0.6374 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 26.42299\n",
            "Epoch 6/100\n",
            "49/50 [============================>.] - ETA: 0s - loss: 49.1836 - gender_output_loss: 0.4933 - image_quality_output_loss: 0.9181 - age_output_loss: 2.7557 - weight_output_loss: 2.5026 - bag_output_loss: 1.4477 - pose_output_loss: 1.0639 - footwear_output_loss: 1.1253 - emotion_output_loss: 2.5392 - gender_output_acc: 0.7966 - image_quality_output_acc: 0.5466 - age_output_acc: 0.4043 - weight_output_acc: 0.6371 - bag_output_acc: 0.6084 - pose_output_acc: 0.7615 - footwear_output_acc: 0.6065 - emotion_output_acc: 0.7136\n",
            "Epoch 00005: val_loss did not improve from 26.42299\n",
            "50/50 [==============================] - 44s 875ms/step - loss: 49.0845 - gender_output_loss: 0.4937 - image_quality_output_loss: 0.9165 - age_output_loss: 2.7486 - weight_output_loss: 2.4797 - bag_output_loss: 1.4484 - pose_output_loss: 1.0575 - footwear_output_loss: 1.1259 - emotion_output_loss: 2.5435 - gender_output_acc: 0.7963 - image_quality_output_acc: 0.5475 - age_output_acc: 0.4075 - weight_output_acc: 0.6394 - bag_output_acc: 0.6094 - pose_output_acc: 0.7619 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7144 - val_loss: 26.9855 - val_gender_output_loss: 0.4293 - val_image_quality_output_loss: 1.0276 - val_age_output_loss: 1.3484 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8616 - val_pose_output_loss: 0.5393 - val_footwear_output_loss: 0.8159 - val_emotion_output_loss: 0.8838 - val_gender_output_acc: 0.7986 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.6116 - val_pose_output_acc: 0.7793 - val_footwear_output_acc: 0.6409 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 26.42299\n",
            "Epoch 7/100\n",
            "49/50 [============================>.] - ETA: 0s - loss: 50.2429 - gender_output_loss: 0.5228 - image_quality_output_loss: 0.9278 - age_output_loss: 2.6512 - weight_output_loss: 2.3524 - bag_output_loss: 1.5431 - pose_output_loss: 1.1077 - footwear_output_loss: 1.2066 - emotion_output_loss: 2.7802 - gender_output_acc: 0.7653 - image_quality_output_acc: 0.5555 - age_output_acc: 0.4088 - weight_output_acc: 0.6467 - bag_output_acc: 0.6167 - pose_output_acc: 0.7672 - footwear_output_acc: 0.5874 - emotion_output_acc: 0.6869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "50/50 [==============================] - 43s 855ms/step - loss: 50.2355 - gender_output_loss: 0.5230 - image_quality_output_loss: 0.9284 - age_output_loss: 2.6507 - weight_output_loss: 2.3378 - bag_output_loss: 1.5493 - pose_output_loss: 1.1142 - footwear_output_loss: 1.2029 - emotion_output_loss: 2.7768 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4094 - weight_output_acc: 0.6488 - bag_output_acc: 0.6181 - pose_output_acc: 0.7675 - footwear_output_acc: 0.5894 - emotion_output_acc: 0.6881 - val_loss: 26.9793 - val_gender_output_loss: 0.4319 - val_image_quality_output_loss: 0.9912 - val_age_output_loss: 1.3481 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.8715 - val_pose_output_loss: 0.5387 - val_footwear_output_loss: 0.8125 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6086 - val_pose_output_acc: 0.7832 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 26.42299\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 45s 901ms/step - loss: 49.5129 - gender_output_loss: 0.4893 - image_quality_output_loss: 0.9134 - age_output_loss: 2.7186 - weight_output_loss: 2.5009 - bag_output_loss: 1.4622 - pose_output_loss: 1.0875 - footwear_output_loss: 1.1574 - emotion_output_loss: 2.6111 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.5700 - age_output_acc: 0.3944 - weight_output_acc: 0.6350 - bag_output_acc: 0.6294 - pose_output_acc: 0.7600 - footwear_output_acc: 0.6162 - emotion_output_acc: 0.7019 - val_loss: 26.7525 - val_gender_output_loss: 0.4364 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.3566 - val_weight_output_loss: 0.9945 - val_bag_output_loss: 0.8560 - val_pose_output_loss: 0.5215 - val_footwear_output_loss: 0.8056 - val_emotion_output_loss: 0.8899 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7872 - val_footwear_output_acc: 0.6463 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 26.42299\n",
            "Epoch 9/100\n",
            "49/50 [============================>.] - ETA: 0s - loss: 47.5338 - gender_output_loss: 0.5413 - image_quality_output_loss: 0.8849 - age_output_loss: 2.6424 - weight_output_loss: 2.3562 - bag_output_loss: 1.3991 - pose_output_loss: 1.1924 - footwear_output_loss: 1.1241 - emotion_output_loss: 2.3363 - gender_output_acc: 0.7723 - image_quality_output_acc: 0.5874 - age_output_acc: 0.4324 - weight_output_acc: 0.6524 - bag_output_acc: 0.6307 - pose_output_acc: 0.7570 - footwear_output_acc: 0.6218 - emotion_output_acc: 0.7258\n",
            "Epoch 00008: val_loss did not improve from 26.42299\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 44s 873ms/step - loss: 47.4128 - gender_output_loss: 0.5396 - image_quality_output_loss: 0.8837 - age_output_loss: 2.6356 - weight_output_loss: 2.3404 - bag_output_loss: 1.3995 - pose_output_loss: 1.1873 - footwear_output_loss: 1.1297 - emotion_output_loss: 2.3272 - gender_output_acc: 0.7738 - image_quality_output_acc: 0.5869 - age_output_acc: 0.4312 - weight_output_acc: 0.6538 - bag_output_acc: 0.6312 - pose_output_acc: 0.7569 - footwear_output_acc: 0.6200 - emotion_output_acc: 0.7262 - val_loss: 26.2621 - val_gender_output_loss: 0.4257 - val_image_quality_output_loss: 0.9044 - val_age_output_loss: 1.3431 - val_weight_output_loss: 0.9699 - val_bag_output_loss: 0.8481 - val_pose_output_loss: 0.5310 - val_footwear_output_loss: 0.7992 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.8046 - val_image_quality_output_acc: 0.5729 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7768 - val_footwear_output_acc: 0.6587 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00009: val_loss improved from 26.42299 to 26.26209, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 43s 864ms/step - loss: 49.3362 - gender_output_loss: 0.5227 - image_quality_output_loss: 0.9287 - age_output_loss: 2.7438 - weight_output_loss: 2.4712 - bag_output_loss: 1.4427 - pose_output_loss: 1.0647 - footwear_output_loss: 1.1914 - emotion_output_loss: 2.5615 - gender_output_acc: 0.7844 - image_quality_output_acc: 0.5456 - age_output_acc: 0.4106 - weight_output_acc: 0.6400 - bag_output_acc: 0.6231 - pose_output_acc: 0.7750 - footwear_output_acc: 0.5994 - emotion_output_acc: 0.7137 - val_loss: 26.8527 - val_gender_output_loss: 0.4266 - val_image_quality_output_loss: 0.9864 - val_age_output_loss: 1.3570 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8624 - val_pose_output_loss: 0.5467 - val_footwear_output_loss: 0.8198 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5129 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6131 - val_pose_output_acc: 0.7703 - val_footwear_output_acc: 0.6354 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 26.26209\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 44s 874ms/step - loss: 48.7470 - gender_output_loss: 0.5214 - image_quality_output_loss: 0.9155 - age_output_loss: 2.7067 - weight_output_loss: 2.6125 - bag_output_loss: 1.4312 - pose_output_loss: 1.0893 - footwear_output_loss: 1.1725 - emotion_output_loss: 2.3676 - gender_output_acc: 0.7794 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4113 - weight_output_acc: 0.6344 - bag_output_acc: 0.6050 - pose_output_acc: 0.7562 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7238 - val_loss: 27.3881 - val_gender_output_loss: 0.4477 - val_image_quality_output_loss: 0.9364 - val_age_output_loss: 1.3739 - val_weight_output_loss: 1.0478 - val_bag_output_loss: 0.9182 - val_pose_output_loss: 0.5504 - val_footwear_output_loss: 0.8398 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.7966 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5952 - val_pose_output_acc: 0.7768 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 26.26209\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 44s 874ms/step - loss: 49.4153 - gender_output_loss: 0.5190 - image_quality_output_loss: 0.9212 - age_output_loss: 2.8527 - weight_output_loss: 2.3189 - bag_output_loss: 1.4471 - pose_output_loss: 1.1084 - footwear_output_loss: 1.1192 - emotion_output_loss: 2.5929 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.5794 - age_output_acc: 0.4050 - weight_output_acc: 0.6613 - bag_output_acc: 0.6169 - pose_output_acc: 0.7575 - footwear_output_acc: 0.6162 - emotion_output_acc: 0.7044 - val_loss: 27.5192 - val_gender_output_loss: 0.5331 - val_image_quality_output_loss: 0.8881 - val_age_output_loss: 1.3881 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 0.9013 - val_pose_output_loss: 0.6694 - val_footwear_output_loss: 0.8353 - val_emotion_output_loss: 0.8964 - val_gender_output_acc: 0.7594 - val_image_quality_output_acc: 0.5779 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6032 - val_pose_output_acc: 0.7579 - val_footwear_output_acc: 0.6424 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 26.26209\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 43s 869ms/step - loss: 50.9053 - gender_output_loss: 0.5385 - image_quality_output_loss: 0.8908 - age_output_loss: 2.6817 - weight_output_loss: 2.6869 - bag_output_loss: 1.6067 - pose_output_loss: 1.2358 - footwear_output_loss: 1.2065 - emotion_output_loss: 2.5026 - gender_output_acc: 0.7762 - image_quality_output_acc: 0.5931 - age_output_acc: 0.4012 - weight_output_acc: 0.6200 - bag_output_acc: 0.5938 - pose_output_acc: 0.7244 - footwear_output_acc: 0.5819 - emotion_output_acc: 0.7131 - val_loss: 27.0339 - val_gender_output_loss: 0.4958 - val_image_quality_output_loss: 0.9054 - val_age_output_loss: 1.3614 - val_weight_output_loss: 1.0129 - val_bag_output_loss: 0.9007 - val_pose_output_loss: 0.5453 - val_footwear_output_loss: 0.8081 - val_emotion_output_loss: 0.8934 - val_gender_output_acc: 0.7788 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.5794 - val_bag_output_acc: 0.6002 - val_pose_output_acc: 0.7738 - val_footwear_output_acc: 0.6478 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 26.26209\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 43s 868ms/step - loss: 49.4603 - gender_output_loss: 0.5504 - image_quality_output_loss: 0.9051 - age_output_loss: 2.6361 - weight_output_loss: 2.5469 - bag_output_loss: 1.3998 - pose_output_loss: 1.1538 - footwear_output_loss: 1.1334 - emotion_output_loss: 2.6803 - gender_output_acc: 0.7606 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4062 - weight_output_acc: 0.6287 - bag_output_acc: 0.6294 - pose_output_acc: 0.7431 - footwear_output_acc: 0.6194 - emotion_output_acc: 0.7037 - val_loss: 27.4396 - val_gender_output_loss: 0.4543 - val_image_quality_output_loss: 1.0781 - val_age_output_loss: 1.3663 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8693 - val_pose_output_loss: 0.5428 - val_footwear_output_loss: 0.8126 - val_emotion_output_loss: 0.8988 - val_gender_output_acc: 0.8006 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.7852 - val_footwear_output_acc: 0.6419 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 26.26209\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 45s 895ms/step - loss: 49.4015 - gender_output_loss: 0.5364 - image_quality_output_loss: 0.9050 - age_output_loss: 2.7245 - weight_output_loss: 2.4147 - bag_output_loss: 1.4570 - pose_output_loss: 1.0867 - footwear_output_loss: 1.1527 - emotion_output_loss: 2.6385 - gender_output_acc: 0.7687 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4119 - weight_output_acc: 0.6519 - bag_output_acc: 0.6144 - pose_output_acc: 0.7681 - footwear_output_acc: 0.6200 - emotion_output_acc: 0.7056 - val_loss: 26.8865 - val_gender_output_loss: 0.4328 - val_image_quality_output_loss: 0.9781 - val_age_output_loss: 1.3525 - val_weight_output_loss: 0.9970 - val_bag_output_loss: 0.8678 - val_pose_output_loss: 0.5233 - val_footwear_output_loss: 0.8189 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.8051 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6126 - val_pose_output_acc: 0.7867 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 26.26209\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 26.26209\n",
            "50/50 [==============================] - 43s 856ms/step - loss: 48.9302 - gender_output_loss: 0.5036 - image_quality_output_loss: 0.9022 - age_output_loss: 2.6084 - weight_output_loss: 2.4742 - bag_output_loss: 1.5084 - pose_output_loss: 1.0747 - footwear_output_loss: 1.1404 - emotion_output_loss: 2.5494 - gender_output_acc: 0.7825 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4363 - weight_output_acc: 0.6350 - bag_output_acc: 0.6181 - pose_output_acc: 0.7594 - footwear_output_acc: 0.6281 - emotion_output_acc: 0.7125 - val_loss: 26.4527 - val_gender_output_loss: 0.4330 - val_image_quality_output_loss: 0.9232 - val_age_output_loss: 1.3489 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.8544 - val_pose_output_loss: 0.5168 - val_footwear_output_loss: 0.8002 - val_emotion_output_loss: 0.8890 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5511 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7877 - val_footwear_output_acc: 0.6488 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 26.26209\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 26.26209\n",
            "49/50 [============================>.] - ETA: 0s - loss: 48.1337 - gender_output_loss: 0.4876 - image_quality_output_loss: 0.8825 - age_output_loss: 2.7945 - weight_output_loss: 2.4208 - bag_output_loss: 1.3948 - pose_output_loss: 1.0529 - footwear_output_loss: 1.1306 - emotion_output_loss: 2.4236 - gender_output_acc: 0.8048 - image_quality_output_acc: 0.5816 - age_output_acc: 0.3980 - weight_output_acc: 0.6333 - bag_output_acc: 0.6244 - pose_output_acc: 0.7742 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7245Epoch 17/100\n",
            "50/50 [==============================] - 43s 857ms/step - loss: 48.3516 - gender_output_loss: 0.4889 - image_quality_output_loss: 0.8824 - age_output_loss: 2.8184 - weight_output_loss: 2.4340 - bag_output_loss: 1.4011 - pose_output_loss: 1.0522 - footwear_output_loss: 1.1285 - emotion_output_loss: 2.4357 - gender_output_acc: 0.8044 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3944 - weight_output_acc: 0.6331 - bag_output_acc: 0.6238 - pose_output_acc: 0.7731 - footwear_output_acc: 0.6238 - emotion_output_acc: 0.7238 - val_loss: 26.9582 - val_gender_output_loss: 0.4618 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.3614 - val_weight_output_loss: 0.9959 - val_bag_output_loss: 0.8650 - val_pose_output_loss: 0.5355 - val_footwear_output_loss: 0.8059 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.5422 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.6200 - val_pose_output_acc: 0.7812 - val_footwear_output_acc: 0.6548 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 26.26209\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 42s 847ms/step - loss: 47.8354 - gender_output_loss: 0.5281 - image_quality_output_loss: 0.9074 - age_output_loss: 2.6832 - weight_output_loss: 2.4229 - bag_output_loss: 1.3763 - pose_output_loss: 1.0099 - footwear_output_loss: 1.1327 - emotion_output_loss: 2.4725 - gender_output_acc: 0.7769 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4031 - weight_output_acc: 0.6544 - bag_output_acc: 0.6131 - pose_output_acc: 0.7756 - footwear_output_acc: 0.6156 - emotion_output_acc: 0.7125 - val_loss: 26.9297 - val_gender_output_loss: 0.4348 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.3698 - val_weight_output_loss: 1.0089 - val_bag_output_loss: 0.8571 - val_pose_output_loss: 0.5312 - val_footwear_output_loss: 0.8165 - val_emotion_output_loss: 0.8846 - val_gender_output_acc: 0.7976 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6205 - val_pose_output_acc: 0.7827 - val_footwear_output_acc: 0.6448 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 26.26209\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 43s 851ms/step - loss: 48.6135 - gender_output_loss: 0.5185 - image_quality_output_loss: 0.9055 - age_output_loss: 2.6796 - weight_output_loss: 2.5309 - bag_output_loss: 1.5026 - pose_output_loss: 1.1226 - footwear_output_loss: 1.1236 - emotion_output_loss: 2.3270 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4169 - weight_output_acc: 0.6262 - bag_output_acc: 0.6106 - pose_output_acc: 0.7537 - footwear_output_acc: 0.6200 - emotion_output_acc: 0.7300 - val_loss: 28.2312 - val_gender_output_loss: 0.4615 - val_image_quality_output_loss: 1.2004 - val_age_output_loss: 1.3953 - val_weight_output_loss: 1.0143 - val_bag_output_loss: 0.8882 - val_pose_output_loss: 0.5427 - val_footwear_output_loss: 0.8050 - val_emotion_output_loss: 0.9027 - val_gender_output_acc: 0.7912 - val_image_quality_output_acc: 0.4578 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.5813 - val_bag_output_acc: 0.6101 - val_pose_output_acc: 0.7937 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6855\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 26.26209\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "End of run with EPOCHS= 100 STEPS_PER_EPOCH= 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0zA_JinFohx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=50\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "# aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "#                              vertical_flip=False,\n",
        "#                              rotation_range=4,\n",
        "#                              width_shift_range=0.1,\n",
        "#                              height_shift_range=0.1,\n",
        "#                              zoom_range=[0.5,2.5],\n",
        "#                              shear_range=0.2,\n",
        "#                              #zca_whitening=True,\n",
        "#                              brightness_range=[0.5,3.5],\n",
        "#                              #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "#                              )\n",
        "\n",
        "# train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "# valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 6, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 6, \n",
        "                        'bag_output': 6, \n",
        "                        'pose_output': 2, \n",
        "                        'footwear_output': 6, \n",
        "                        'emotion_output': 2}\n",
        "\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmNo1p0-Jp8z",
        "colab_type": "code",
        "outputId": "d69673bf-7fa5-4993-b010-7749ab440668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=50\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "# aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "#                              vertical_flip=False,\n",
        "#                              rotation_range=4,\n",
        "#                              width_shift_range=0.1,\n",
        "#                              height_shift_range=0.1,\n",
        "#                              zoom_range=[0.5,2.5],\n",
        "#                              shear_range=0.2,\n",
        "#                              #zca_whitening=True,\n",
        "#                              brightness_range=[0.5,3.5],\n",
        "#                              #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "#                              )\n",
        "\n",
        "# train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "# valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "loss_weights_compile = {'gender_output': 3, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 4, \n",
        "                        'bag_output': 4, \n",
        "                        'pose_output': 2, \n",
        "                        'footwear_output': 4, \n",
        "                        'emotion_output': 2}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_1577544670_1577564488_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577564488_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "Returning new callback array with steps_per_epoch= 50 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json  to: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 53s 1s/step - loss: 40.1246 - gender_output_loss: 0.4914 - image_quality_output_loss: 0.9074 - age_output_loss: 2.7119 - weight_output_loss: 2.5136 - bag_output_loss: 1.4258 - pose_output_loss: 1.0790 - footwear_output_loss: 1.1497 - emotion_output_loss: 2.6873 - gender_output_acc: 0.7950 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4119 - weight_output_acc: 0.6312 - bag_output_acc: 0.6262 - pose_output_acc: 0.7438 - footwear_output_acc: 0.6031 - emotion_output_acc: 0.7006 - val_loss: 22.8510 - val_gender_output_loss: 0.4177 - val_image_quality_output_loss: 0.9426 - val_age_output_loss: 1.3545 - val_weight_output_loss: 0.9978 - val_bag_output_loss: 0.8465 - val_pose_output_loss: 0.5245 - val_footwear_output_loss: 0.8036 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6195 - val_pose_output_acc: 0.7857 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.85104, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577564488_model.001.h5\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 42s 831ms/step - loss: 38.9623 - gender_output_loss: 0.5141 - image_quality_output_loss: 0.9017 - age_output_loss: 2.6720 - weight_output_loss: 2.4452 - bag_output_loss: 1.3642 - pose_output_loss: 1.1572 - footwear_output_loss: 1.1363 - emotion_output_loss: 2.3321 - gender_output_acc: 0.7850 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4056 - weight_output_acc: 0.6444 - bag_output_acc: 0.6400 - pose_output_acc: 0.7506 - footwear_output_acc: 0.6225 - emotion_output_acc: 0.7212 - val_loss: 22.9434 - val_gender_output_loss: 0.4305 - val_image_quality_output_loss: 0.9607 - val_age_output_loss: 1.3502 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8588 - val_pose_output_loss: 0.5387 - val_footwear_output_loss: 0.8054 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.8046 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7793 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 22.85104\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 43s 863ms/step - loss: 39.9118 - gender_output_loss: 0.5025 - image_quality_output_loss: 0.9119 - age_output_loss: 2.6580 - weight_output_loss: 2.4165 - bag_output_loss: 1.4664 - pose_output_loss: 1.0732 - footwear_output_loss: 1.1466 - emotion_output_loss: 2.7344 - gender_output_acc: 0.7969 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4213 - weight_output_acc: 0.6394 - bag_output_acc: 0.6287 - pose_output_acc: 0.7650 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.6900 - val_loss: 22.6639 - val_gender_output_loss: 0.4226 - val_image_quality_output_loss: 0.9154 - val_age_output_loss: 1.3485 - val_weight_output_loss: 0.9757 - val_bag_output_loss: 0.8500 - val_pose_output_loss: 0.5153 - val_footwear_output_loss: 0.8073 - val_emotion_output_loss: 0.8838 - val_gender_output_acc: 0.8165 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6171 - val_pose_output_acc: 0.7966 - val_footwear_output_acc: 0.6448 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00003: val_loss improved from 22.85104 to 22.66386, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577564488_model.003.h5\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 42s 847ms/step - loss: 40.3500 - gender_output_loss: 0.5317 - image_quality_output_loss: 0.8980 - age_output_loss: 2.7690 - weight_output_loss: 2.5613 - bag_output_loss: 1.4612 - pose_output_loss: 1.1595 - footwear_output_loss: 1.1960 - emotion_output_loss: 2.3628 - gender_output_acc: 0.7662 - image_quality_output_acc: 0.5775 - age_output_acc: 0.3894 - weight_output_acc: 0.6319 - bag_output_acc: 0.6325 - pose_output_acc: 0.7350 - footwear_output_acc: 0.5925 - emotion_output_acc: 0.7269 - val_loss: 23.5752 - val_gender_output_loss: 0.4471 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3708 - val_weight_output_loss: 1.0024 - val_bag_output_loss: 0.8574 - val_pose_output_loss: 0.7153 - val_footwear_output_loss: 0.8087 - val_emotion_output_loss: 0.8939 - val_gender_output_acc: 0.7956 - val_image_quality_output_acc: 0.5516 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.5893 - val_bag_output_acc: 0.6215 - val_pose_output_acc: 0.6984 - val_footwear_output_acc: 0.6488 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00003: val_loss improved from 22.85104 to 22.66386, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577564488_model.003.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 22.66386\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 43s 859ms/step - loss: 40.2262 - gender_output_loss: 0.5488 - image_quality_output_loss: 0.9137 - age_output_loss: 2.6878 - weight_output_loss: 2.5346 - bag_output_loss: 1.4890 - pose_output_loss: 1.0894 - footwear_output_loss: 1.1624 - emotion_output_loss: 2.4603 - gender_output_acc: 0.7606 - image_quality_output_acc: 0.5700 - age_output_acc: 0.4062 - weight_output_acc: 0.6381 - bag_output_acc: 0.5938 - pose_output_acc: 0.7519 - footwear_output_acc: 0.6075 - emotion_output_acc: 0.7181 - val_loss: 24.1098 - val_gender_output_loss: 0.5382 - val_image_quality_output_loss: 0.9621 - val_age_output_loss: 1.4009 - val_weight_output_loss: 1.0415 - val_bag_output_loss: 0.9371 - val_pose_output_loss: 0.5953 - val_footwear_output_loss: 0.8058 - val_emotion_output_loss: 0.9054 - val_gender_output_acc: 0.7604 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5903 - val_pose_output_acc: 0.7500 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6845\n",
            "Epoch 5/100\n",
            "Epoch 00005: val_loss did not improve from 22.66386\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 43s 857ms/step - loss: 40.1485 - gender_output_loss: 0.5324 - image_quality_output_loss: 0.9096 - age_output_loss: 2.6863 - weight_output_loss: 2.5896 - bag_output_loss: 1.4145 - pose_output_loss: 1.1473 - footwear_output_loss: 1.1013 - emotion_output_loss: 2.5591 - gender_output_acc: 0.7756 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4175 - weight_output_acc: 0.6300 - bag_output_acc: 0.6287 - pose_output_acc: 0.7594 - footwear_output_acc: 0.6156 - emotion_output_acc: 0.7144 - val_loss: 23.6421 - val_gender_output_loss: 0.4495 - val_image_quality_output_loss: 1.0468 - val_age_output_loss: 1.3580 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.8814 - val_pose_output_loss: 0.5551 - val_footwear_output_loss: 0.8255 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8001 - val_image_quality_output_acc: 0.4921 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.5873 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 22.66386\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 43s 858ms/step - loss: 39.2634 - gender_output_loss: 0.5047 - image_quality_output_loss: 0.8769 - age_output_loss: 2.7243 - weight_output_loss: 2.4147 - bag_output_loss: 1.4666 - pose_output_loss: 1.0779 - footwear_output_loss: 1.1559 - emotion_output_loss: 2.3915 - gender_output_acc: 0.7925 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4144 - weight_output_acc: 0.6575 - bag_output_acc: 0.6138 - pose_output_acc: 0.7594 - footwear_output_acc: 0.6194 - emotion_output_acc: 0.7244 - val_loss: 22.9854 - val_gender_output_loss: 0.4401 - val_image_quality_output_loss: 0.9642 - val_age_output_loss: 1.3495 - val_weight_output_loss: 0.9734 - val_bag_output_loss: 0.8639 - val_pose_output_loss: 0.5348 - val_footwear_output_loss: 0.8015 - val_emotion_output_loss: 0.8898 - val_gender_output_acc: 0.8075 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7822 - val_footwear_output_acc: 0.6513 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 22.66386\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 44s 877ms/step - loss: 40.4276 - gender_output_loss: 0.5299 - image_quality_output_loss: 0.8965 - age_output_loss: 2.8307 - weight_output_loss: 2.5661 - bag_output_loss: 1.4260 - pose_output_loss: 1.0641 - footwear_output_loss: 1.1660 - emotion_output_loss: 2.5625 - gender_output_acc: 0.7844 - image_quality_output_acc: 0.5856 - age_output_acc: 0.3975 - weight_output_acc: 0.6388 - bag_output_acc: 0.6294 - pose_output_acc: 0.7581 - footwear_output_acc: 0.5906 - emotion_output_acc: 0.7137 - val_loss: 23.3271 - val_gender_output_loss: 0.4644 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.3573 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8703 - val_pose_output_loss: 0.5831 - val_footwear_output_loss: 0.8088 - val_emotion_output_loss: 0.8905 - val_gender_output_acc: 0.7986 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7679 - val_footwear_output_acc: 0.6478 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 22.66386\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 44s 874ms/step - loss: 39.9927 - gender_output_loss: 0.5169 - image_quality_output_loss: 0.8955 - age_output_loss: 2.6818 - weight_output_loss: 2.5100 - bag_output_loss: 1.4684 - pose_output_loss: 1.1279 - footwear_output_loss: 1.1379 - emotion_output_loss: 2.5352 - gender_output_acc: 0.7781 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4062 - weight_output_acc: 0.6269 - bag_output_acc: 0.6225 - pose_output_acc: 0.7562 - footwear_output_acc: 0.6331 - emotion_output_acc: 0.7094 - val_loss: 23.0536 - val_gender_output_loss: 0.4338 - val_image_quality_output_loss: 0.9411 - val_age_output_loss: 1.3567 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.8693 - val_pose_output_loss: 0.5440 - val_footwear_output_loss: 0.8167 - val_emotion_output_loss: 0.8849 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.7793 - val_footwear_output_acc: 0.6463 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 22.66386\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 43s 852ms/step - loss: 39.0981 - gender_output_loss: 0.4925 - image_quality_output_loss: 0.9097 - age_output_loss: 2.6392 - weight_output_loss: 2.3644 - bag_output_loss: 1.3729 - pose_output_loss: 1.0721 - footwear_output_loss: 1.1659 - emotion_output_loss: 2.6210 - gender_output_acc: 0.8037 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4075 - weight_output_acc: 0.6550 - bag_output_acc: 0.6431 - pose_output_acc: 0.7750 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.7050 - val_loss: 22.9093 - val_gender_output_loss: 0.4262 - val_image_quality_output_loss: 0.9538 - val_age_output_loss: 1.3451 - val_weight_output_loss: 0.9683 - val_bag_output_loss: 0.8594 - val_pose_output_loss: 0.5453 - val_footwear_output_loss: 0.8085 - val_emotion_output_loss: 0.8928 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7703 - val_footwear_output_acc: 0.6483 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 22.66386\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 43s 857ms/step - loss: 40.3101 - gender_output_loss: 0.5288 - image_quality_output_loss: 0.8990 - age_output_loss: 2.5663 - weight_output_loss: 2.5703 - bag_output_loss: 1.5133 - pose_output_loss: 1.1209 - footwear_output_loss: 1.1843 - emotion_output_loss: 2.4889 - gender_output_acc: 0.7825 - image_quality_output_acc: 0.5800 - age_output_acc: 0.4319 - weight_output_acc: 0.6238 - bag_output_acc: 0.6188 - pose_output_acc: 0.7519 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7106 - val_loss: 23.9920 - val_gender_output_loss: 0.5349 - val_image_quality_output_loss: 1.0865 - val_age_output_loss: 1.3504 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.8661 - val_pose_output_loss: 0.5331 - val_footwear_output_loss: 0.8369 - val_emotion_output_loss: 0.9013 - val_gender_output_acc: 0.7703 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.4077 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6156 - val_pose_output_acc: 0.7783 - val_footwear_output_acc: 0.6205 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.66386\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 43s 854ms/step - loss: 39.9064 - gender_output_loss: 0.5278 - image_quality_output_loss: 0.9223 - age_output_loss: 2.7551 - weight_output_loss: 2.4851 - bag_output_loss: 1.4208 - pose_output_loss: 1.1559 - footwear_output_loss: 1.1571 - emotion_output_loss: 2.4280 - gender_output_acc: 0.7762 - image_quality_output_acc: 0.5456 - age_output_acc: 0.4088 - weight_output_acc: 0.6406 - bag_output_acc: 0.6112 - pose_output_acc: 0.7525 - footwear_output_acc: 0.6069 - emotion_output_acc: 0.7231 - val_loss: 23.3344 - val_gender_output_loss: 0.4701 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.3475 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8702 - val_pose_output_loss: 0.5808 - val_footwear_output_loss: 0.8195 - val_emotion_output_loss: 0.8775 - val_gender_output_acc: 0.7837 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.7574 - val_footwear_output_acc: 0.6404 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.66386\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 43s 857ms/step - loss: 40.4183 - gender_output_loss: 0.5222 - image_quality_output_loss: 0.9001 - age_output_loss: 2.7038 - weight_output_loss: 2.6180 - bag_output_loss: 1.4369 - pose_output_loss: 1.1327 - footwear_output_loss: 1.1396 - emotion_output_loss: 2.5489 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5775 - age_output_acc: 0.4062 - weight_output_acc: 0.6269 - bag_output_acc: 0.6331 - pose_output_acc: 0.7419 - footwear_output_acc: 0.6175 - emotion_output_acc: 0.7200 - val_loss: 23.5677 - val_gender_output_loss: 0.4452 - val_image_quality_output_loss: 1.0942 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8593 - val_pose_output_loss: 0.5518 - val_footwear_output_loss: 0.8017 - val_emotion_output_loss: 0.8966 - val_gender_output_acc: 0.8006 - val_image_quality_output_acc: 0.4886 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7847 - val_footwear_output_acc: 0.6543 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 22.66386\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json\n",
            "End of run with EPOCHS= 100 STEPS_PER_EPOCH= 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-n0SvZ9fjTT",
        "colab_type": "code",
        "outputId": "999e1db8-47f2-44f1-ce51-43ed86a48163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy_1577568218.json\n",
            "Returning new callback array with steps_per_epoch= 100 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 93s 927ms/step - loss: 8.3909 - gender_output_loss: 0.4407 - image_quality_output_loss: 0.9011 - age_output_loss: 1.3313 - weight_output_loss: 0.9284 - bag_output_loss: 0.8369 - pose_output_loss: 0.5743 - footwear_output_loss: 0.8400 - emotion_output_loss: 0.8303 - gender_output_acc: 0.7919 - image_quality_output_acc: 0.5684 - age_output_acc: 0.4181 - weight_output_acc: 0.6500 - bag_output_acc: 0.6231 - pose_output_acc: 0.7575 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.7253 - val_loss: 8.5835 - val_gender_output_loss: 0.4285 - val_image_quality_output_loss: 1.0249 - val_age_output_loss: 1.3543 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.8573 - val_pose_output_loss: 0.5279 - val_footwear_output_loss: 0.8102 - val_emotion_output_loss: 0.8882 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5079 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7862 - val_footwear_output_acc: 0.6429 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.58353, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.001.h5\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 77s 768ms/step - loss: 8.4581 - gender_output_loss: 0.4498 - image_quality_output_loss: 0.8999 - age_output_loss: 1.3352 - weight_output_loss: 0.9523 - bag_output_loss: 0.8405 - pose_output_loss: 0.5641 - footwear_output_loss: 0.8519 - emotion_output_loss: 0.8568 - gender_output_acc: 0.7819 - image_quality_output_acc: 0.5662 - age_output_acc: 0.4072 - weight_output_acc: 0.6325 - bag_output_acc: 0.6247 - pose_output_acc: 0.7622 - footwear_output_acc: 0.6109 - emotion_output_acc: 0.7106 - val_loss: 8.5344 - val_gender_output_loss: 0.4292 - val_image_quality_output_loss: 0.9532 - val_age_output_loss: 1.3702 - val_weight_output_loss: 0.9921 - val_bag_output_loss: 0.8591 - val_pose_output_loss: 0.5209 - val_footwear_output_loss: 0.8167 - val_emotion_output_loss: 0.8859 - val_gender_output_acc: 0.8056 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7941 - val_footwear_output_acc: 0.6488 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.58353 to 8.53436, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.002.h5\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 81s 814ms/step - loss: 8.4947 - gender_output_loss: 0.4595 - image_quality_output_loss: 0.9048 - age_output_loss: 1.3512 - weight_output_loss: 0.9587 - bag_output_loss: 0.8223 - pose_output_loss: 0.5815 - footwear_output_loss: 0.8418 - emotion_output_loss: 0.8683 - gender_output_acc: 0.7741 - image_quality_output_acc: 0.5709 - age_output_acc: 0.4091 - weight_output_acc: 0.6378 - bag_output_acc: 0.6344 - pose_output_acc: 0.7700 - footwear_output_acc: 0.6131 - emotion_output_acc: 0.7109 - val_loss: 8.5336 - val_gender_output_loss: 0.4314 - val_image_quality_output_loss: 0.9658 - val_age_output_loss: 1.3555 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8602 - val_pose_output_loss: 0.5303 - val_footwear_output_loss: 0.8136 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7832 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.53436 to 8.53358, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.003.h5\n",
            "Epoch 4/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.4142 - gender_output_loss: 0.4161 - image_quality_output_loss: 0.8921 - age_output_loss: 1.3336 - weight_output_loss: 0.9465 - bag_output_loss: 0.8411 - pose_output_loss: 0.5744 - footwear_output_loss: 0.8493 - emotion_output_loss: 0.8561 - gender_output_acc: 0.8093 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4192 - weight_output_acc: 0.6414 - bag_output_acc: 0.6275 - pose_output_acc: 0.7718 - footwear_output_acc: 0.6143 - emotion_output_acc: 0.7172\n",
            "100/100 [==============================] - 81s 814ms/step - loss: 8.4127 - gender_output_loss: 0.4174 - image_quality_output_loss: 0.8925 - age_output_loss: 1.3331 - weight_output_loss: 0.9462 - bag_output_loss: 0.8396 - pose_output_loss: 0.5749 - footwear_output_loss: 0.8498 - emotion_output_loss: 0.8542 - gender_output_acc: 0.8078 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4194 - weight_output_acc: 0.6416 - bag_output_acc: 0.6287 - pose_output_acc: 0.7716 - footwear_output_acc: 0.6141 - emotion_output_acc: 0.7178 - val_loss: 8.4915 - val_gender_output_loss: 0.4169 - val_image_quality_output_loss: 0.9597 - val_age_output_loss: 1.3481 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8517 - val_pose_output_loss: 0.5413 - val_footwear_output_loss: 0.8010 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.4097 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6181 - val_pose_output_acc: 0.7723 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00004: val_loss improved from 8.53358 to 8.49153, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.004.h5\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 76s 759ms/step - loss: 8.4560 - gender_output_loss: 0.4456 - image_quality_output_loss: 0.9118 - age_output_loss: 1.3399 - weight_output_loss: 0.9595 - bag_output_loss: 0.8325 - pose_output_loss: 0.5514 - footwear_output_loss: 0.8519 - emotion_output_loss: 0.8602 - gender_output_acc: 0.7900 - image_quality_output_acc: 0.5659 - age_output_acc: 0.4134 - weight_output_acc: 0.6297 - bag_output_acc: 0.6309 - pose_output_acc: 0.7738 - footwear_output_acc: 0.6169 - emotion_output_acc: 0.7100 - val_loss: 8.5666 - val_gender_output_loss: 0.4355 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.3520 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.8585 - val_pose_output_loss: 0.5649 - val_footwear_output_loss: 0.7995 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.8061 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7684 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 8.49153\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 77s 766ms/step - loss: 8.3908 - gender_output_loss: 0.4500 - image_quality_output_loss: 0.8897 - age_output_loss: 1.3223 - weight_output_loss: 0.9310 - bag_output_loss: 0.8303 - pose_output_loss: 0.5469 - footwear_output_loss: 0.8435 - emotion_output_loss: 0.8755 - gender_output_acc: 0.7822 - image_quality_output_acc: 0.5759 - age_output_acc: 0.4297 - weight_output_acc: 0.6419 - bag_output_acc: 0.6269 - pose_output_acc: 0.7812 - footwear_output_acc: 0.6162 - emotion_output_acc: 0.7028 - val_loss: 8.5258 - val_gender_output_loss: 0.4121 - val_image_quality_output_loss: 0.9598 - val_age_output_loss: 1.3520 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8657 - val_pose_output_loss: 0.5327 - val_footwear_output_loss: 0.8242 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.7847 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 8.49153\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 77s 766ms/step - loss: 8.4349 - gender_output_loss: 0.4428 - image_quality_output_loss: 0.8796 - age_output_loss: 1.3543 - weight_output_loss: 0.9366 - bag_output_loss: 0.8246 - pose_output_loss: 0.5745 - footwear_output_loss: 0.8571 - emotion_output_loss: 0.8648 - gender_output_acc: 0.7900 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4088 - weight_output_acc: 0.6425 - bag_output_acc: 0.6303 - pose_output_acc: 0.7631 - footwear_output_acc: 0.6050 - emotion_output_acc: 0.7097 - val_loss: 8.4709 - val_gender_output_loss: 0.4145 - val_image_quality_output_loss: 0.9615 - val_age_output_loss: 1.3561 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8495 - val_pose_output_loss: 0.5224 - val_footwear_output_loss: 0.8024 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7852 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00007: val_loss improved from 8.49153 to 8.47090, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.007.h5\n",
            "Epoch 00006: val_loss did not improve from 8.49153\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 8/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.4175 - gender_output_loss: 0.4448 - image_quality_output_loss: 0.8930 - age_output_loss: 1.3374 - weight_output_loss: 0.9388 - bag_output_loss: 0.8298 - pose_output_loss: 0.5804 - footwear_output_loss: 0.8384 - emotion_output_loss: 0.8550 - gender_output_acc: 0.7923 - image_quality_output_acc: 0.5764 - age_output_acc: 0.4205 - weight_output_acc: 0.6408 - bag_output_acc: 0.6294 - pose_output_acc: 0.7645 - footwear_output_acc: 0.6121 - emotion_output_acc: 0.7162\n",
            "100/100 [==============================] - 77s 771ms/step - loss: 8.4217 - gender_output_loss: 0.4449 - image_quality_output_loss: 0.8935 - age_output_loss: 1.3369 - weight_output_loss: 0.9396 - bag_output_loss: 0.8287 - pose_output_loss: 0.5820 - footwear_output_loss: 0.8377 - emotion_output_loss: 0.8585 - gender_output_acc: 0.7916 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4209 - weight_output_acc: 0.6409 - bag_output_acc: 0.6294 - pose_output_acc: 0.7641 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7147 - val_loss: 8.4645 - val_gender_output_loss: 0.4151 - val_image_quality_output_loss: 0.9754 - val_age_output_loss: 1.3454 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8505 - val_pose_output_loss: 0.5205 - val_footwear_output_loss: 0.8004 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.8080 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6195 - val_pose_output_acc: 0.7897 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00008: val_loss improved from 8.47090 to 8.46453, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.008.h5\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 76s 760ms/step - loss: 8.4290 - gender_output_loss: 0.4374 - image_quality_output_loss: 0.9046 - age_output_loss: 1.3491 - weight_output_loss: 0.9534 - bag_output_loss: 0.8402 - pose_output_loss: 0.5512 - footwear_output_loss: 0.8409 - emotion_output_loss: 0.8525 - gender_output_acc: 0.7947 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4119 - weight_output_acc: 0.6359 - bag_output_acc: 0.6272 - pose_output_acc: 0.7822 - footwear_output_acc: 0.6206 - emotion_output_acc: 0.7131 - val_loss: 8.5305 - val_gender_output_loss: 0.4190 - val_image_quality_output_loss: 1.0077 - val_age_output_loss: 1.3491 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8563 - val_pose_output_loss: 0.5272 - val_footwear_output_loss: 0.8063 - val_emotion_output_loss: 0.8830 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7842 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 8.46453\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 76s 765ms/step - loss: 8.5033 - gender_output_loss: 0.4506 - image_quality_output_loss: 0.9037 - age_output_loss: 1.3357 - weight_output_loss: 0.9487 - bag_output_loss: 0.8435 - pose_output_loss: 0.5942 - footwear_output_loss: 0.8547 - emotion_output_loss: 0.8730 - gender_output_acc: 0.7919 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4103 - weight_output_acc: 0.6312 - bag_output_acc: 0.6131 - pose_output_acc: 0.7638 - footwear_output_acc: 0.6056 - emotion_output_acc: 0.7072 - val_loss: 8.4669 - val_gender_output_loss: 0.4197 - val_image_quality_output_loss: 0.9672 - val_age_output_loss: 1.3494 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.8541 - val_pose_output_loss: 0.5159 - val_footwear_output_loss: 0.7999 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.8110 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7946 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 8.46453\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 8.3529 - gender_output_loss: 0.4454 - image_quality_output_loss: 0.8966 - age_output_loss: 1.3330 - weight_output_loss: 0.9305 - bag_output_loss: 0.8151 - pose_output_loss: 0.5589 - footwear_output_loss: 0.8249 - emotion_output_loss: 0.8502 - gender_output_acc: 0.7934 - image_quality_output_acc: 0.5691 - age_output_acc: 0.4166 - weight_output_acc: 0.6506 - bag_output_acc: 0.6381 - pose_output_acc: 0.7616 - footwear_output_acc: 0.6287 - emotion_output_acc: 0.7116 - val_loss: 8.6677 - val_gender_output_loss: 0.4147 - val_image_quality_output_loss: 1.1160 - val_age_output_loss: 1.3549 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8561 - val_pose_output_loss: 0.5518 - val_footwear_output_loss: 0.8049 - val_emotion_output_loss: 0.8896 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.4861 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7718 - val_footwear_output_acc: 0.6533 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 8.46453\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 73s 735ms/step - loss: 8.4390 - gender_output_loss: 0.4430 - image_quality_output_loss: 0.8860 - age_output_loss: 1.3427 - weight_output_loss: 0.9432 - bag_output_loss: 0.8433 - pose_output_loss: 0.5586 - footwear_output_loss: 0.8585 - emotion_output_loss: 0.8669 - gender_output_acc: 0.7956 - image_quality_output_acc: 0.5759 - age_output_acc: 0.4066 - weight_output_acc: 0.6394 - bag_output_acc: 0.6281 - pose_output_acc: 0.7756 - footwear_output_acc: 0.6063 - emotion_output_acc: 0.7078 - val_loss: 8.6268 - val_gender_output_loss: 0.4376 - val_image_quality_output_loss: 1.0308 - val_age_output_loss: 1.3592 - val_weight_output_loss: 0.9928 - val_bag_output_loss: 0.8623 - val_pose_output_loss: 0.5385 - val_footwear_output_loss: 0.8083 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5164 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7803 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6806\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 8.46453\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 8.4169 - gender_output_loss: 0.4363 - image_quality_output_loss: 0.9145 - age_output_loss: 1.3496 - weight_output_loss: 0.9420 - bag_output_loss: 0.8219 - pose_output_loss: 0.5700 - footwear_output_loss: 0.8331 - emotion_output_loss: 0.8546 - gender_output_acc: 0.7959 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4069 - weight_output_acc: 0.6431 - bag_output_acc: 0.6325 - pose_output_acc: 0.7597 - footwear_output_acc: 0.6294 - emotion_output_acc: 0.7188 - val_loss: 8.5396 - val_gender_output_loss: 0.4577 - val_image_quality_output_loss: 0.9599 - val_age_output_loss: 1.3582 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.8730 - val_pose_output_loss: 0.5278 - val_footwear_output_loss: 0.8011 - val_emotion_output_loss: 0.8843 - val_gender_output_acc: 0.8051 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6181 - val_pose_output_acc: 0.7817 - val_footwear_output_acc: 0.6543 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 8.46453\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 73s 729ms/step - loss: 8.4183 - gender_output_loss: 0.4434 - image_quality_output_loss: 0.9053 - age_output_loss: 1.3195 - weight_output_loss: 0.9467 - bag_output_loss: 0.8399 - pose_output_loss: 0.5657 - footwear_output_loss: 0.8395 - emotion_output_loss: 0.8648 - gender_output_acc: 0.7878 - image_quality_output_acc: 0.5772 - age_output_acc: 0.4231 - weight_output_acc: 0.6344 - bag_output_acc: 0.6259 - pose_output_acc: 0.7725 - footwear_output_acc: 0.6184 - emotion_output_acc: 0.7122 - val_loss: 8.4832 - val_gender_output_loss: 0.4166 - val_image_quality_output_loss: 0.9780 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8584 - val_pose_output_loss: 0.5130 - val_footwear_output_loss: 0.8020 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.8194 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7912 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 8.46453\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 73s 732ms/step - loss: 8.3894 - gender_output_loss: 0.4339 - image_quality_output_loss: 0.8800 - age_output_loss: 1.3306 - weight_output_loss: 0.9438 - bag_output_loss: 0.8358 - pose_output_loss: 0.5625 - footwear_output_loss: 0.8552 - emotion_output_loss: 0.8552 - gender_output_acc: 0.7997 - image_quality_output_acc: 0.5900 - age_output_acc: 0.4109 - weight_output_acc: 0.6375 - bag_output_acc: 0.6275 - pose_output_acc: 0.7650 - footwear_output_acc: 0.6050 - emotion_output_acc: 0.7125 - val_loss: 8.4253 - val_gender_output_loss: 0.4271 - val_image_quality_output_loss: 0.9411 - val_age_output_loss: 1.3423 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.8586 - val_pose_output_loss: 0.5170 - val_footwear_output_loss: 0.7957 - val_emotion_output_loss: 0.8798 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6141 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7941 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00015: val_loss improved from 8.46453 to 8.42531, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.015.h5\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 71s 710ms/step - loss: 8.3762 - gender_output_loss: 0.4347 - image_quality_output_loss: 0.8950 - age_output_loss: 1.3423 - weight_output_loss: 0.9620 - bag_output_loss: 0.8123 - pose_output_loss: 0.5527 - footwear_output_loss: 0.8209 - emotion_output_loss: 0.8647 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4197 - weight_output_acc: 0.6287 - bag_output_acc: 0.6347 - pose_output_acc: 0.7762 - footwear_output_acc: 0.6256 - emotion_output_acc: 0.7081 - val_loss: 8.5108 - val_gender_output_loss: 0.4416 - val_image_quality_output_loss: 0.9574 - val_age_output_loss: 1.3520 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8664 - val_pose_output_loss: 0.5272 - val_footwear_output_loss: 0.8072 - val_emotion_output_loss: 0.8847 - val_gender_output_acc: 0.8056 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7867 - val_footwear_output_acc: 0.6513 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 8.42531\n",
            "Epoch 17/100\n",
            " 65/100 [==================>...........] - ETA: 19s - loss: 8.3606 - gender_output_loss: 0.4391 - image_quality_output_loss: 0.8900 - age_output_loss: 1.3167 - weight_output_loss: 0.9177 - bag_output_loss: 0.8271 - pose_output_loss: 0.5799 - footwear_output_loss: 0.8645 - emotion_output_loss: 0.8340 - gender_output_acc: 0.7937 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4284 - weight_output_acc: 0.6553 - bag_output_acc: 0.6466 - pose_output_acc: 0.7447 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.7231"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwrtFrcTAgRo",
        "colab_type": "code",
        "outputId": "1fa0d100-4756-4856-e180-a56b45f10f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577570428_1577570429_model.015.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json\n",
            "Returning new callback array with steps_per_epoch= 100 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 84s 840ms/step - loss: 8.3335 - gender_output_loss: 0.4359 - image_quality_output_loss: 0.9054 - age_output_loss: 1.3253 - weight_output_loss: 0.9365 - bag_output_loss: 0.8277 - pose_output_loss: 0.5708 - footwear_output_loss: 0.8272 - emotion_output_loss: 0.8128 - gender_output_acc: 0.7941 - image_quality_output_acc: 0.5691 - age_output_acc: 0.4191 - weight_output_acc: 0.6484 - bag_output_acc: 0.6238 - pose_output_acc: 0.7628 - footwear_output_acc: 0.6287 - emotion_output_acc: 0.7353 - val_loss: 8.5636 - val_gender_output_loss: 0.4351 - val_image_quality_output_loss: 1.0232 - val_age_output_loss: 1.3506 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8652 - val_pose_output_loss: 0.5268 - val_footwear_output_loss: 0.8023 - val_emotion_output_loss: 0.8885 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5273 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6106 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7927 - val_footwear_output_acc: 0.6548 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.56360, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.001.h5\n",
            "Epoch 2/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.3984 - gender_output_loss: 0.4283 - image_quality_output_loss: 0.8937 - age_output_loss: 1.3405 - weight_output_loss: 0.9499 - bag_output_loss: 0.8241 - pose_output_loss: 0.5420 - footwear_output_loss: 0.8505 - emotion_output_loss: 0.8780 - gender_output_acc: 0.8046 - image_quality_output_acc: 0.5735 - age_output_acc: 0.4062 - weight_output_acc: 0.6351 - bag_output_acc: 0.6316 - pose_output_acc: 0.7863 - footwear_output_acc: 0.6190 - emotion_output_acc: 0.7008\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 8.4036 - gender_output_loss: 0.4290 - image_quality_output_loss: 0.8946 - age_output_loss: 1.3409 - weight_output_loss: 0.9513 - bag_output_loss: 0.8239 - pose_output_loss: 0.5432 - footwear_output_loss: 0.8524 - emotion_output_loss: 0.8770 - gender_output_acc: 0.8041 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4062 - weight_output_acc: 0.6347 - bag_output_acc: 0.6316 - pose_output_acc: 0.7869 - footwear_output_acc: 0.6172 - emotion_output_acc: 0.7013 - val_loss: 8.5595 - val_gender_output_loss: 0.4121 - val_image_quality_output_loss: 1.0360 - val_age_output_loss: 1.3538 - val_weight_output_loss: 0.9946 - val_bag_output_loss: 0.8583 - val_pose_output_loss: 0.5222 - val_footwear_output_loss: 0.8048 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.8125 - val_image_quality_output_acc: 0.5114 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.5962 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7902 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.56360 to 8.55955, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.002.h5\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 73s 727ms/step - loss: 8.4298 - gender_output_loss: 0.4300 - image_quality_output_loss: 0.8852 - age_output_loss: 1.3512 - weight_output_loss: 0.9544 - bag_output_loss: 0.8364 - pose_output_loss: 0.5646 - footwear_output_loss: 0.8400 - emotion_output_loss: 0.8777 - gender_output_acc: 0.7984 - image_quality_output_acc: 0.5831 - age_output_acc: 0.4047 - weight_output_acc: 0.6328 - bag_output_acc: 0.6350 - pose_output_acc: 0.7756 - footwear_output_acc: 0.6275 - emotion_output_acc: 0.7022 - val_loss: 8.6109 - val_gender_output_loss: 0.4320 - val_image_quality_output_loss: 1.0386 - val_age_output_loss: 1.3600 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.8641 - val_pose_output_loss: 0.5458 - val_footwear_output_loss: 0.7980 - val_emotion_output_loss: 0.8960 - val_gender_output_acc: 0.8090 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5952 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.7798 - val_footwear_output_acc: 0.6558 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 8.55955\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 74s 743ms/step - loss: 8.3929 - gender_output_loss: 0.4370 - image_quality_output_loss: 0.9011 - age_output_loss: 1.3370 - weight_output_loss: 0.9248 - bag_output_loss: 0.8307 - pose_output_loss: 0.5642 - footwear_output_loss: 0.8437 - emotion_output_loss: 0.8657 - gender_output_acc: 0.7997 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4138 - weight_output_acc: 0.6434 - bag_output_acc: 0.6353 - pose_output_acc: 0.7709 - footwear_output_acc: 0.6209 - emotion_output_acc: 0.7113 - val_loss: 8.4884 - val_gender_output_loss: 0.4202 - val_image_quality_output_loss: 0.9987 - val_age_output_loss: 1.3500 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.8594 - val_pose_output_loss: 0.5158 - val_footwear_output_loss: 0.7946 - val_emotion_output_loss: 0.8815 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.5198 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7902 - val_footwear_output_acc: 0.6622 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00004: val_loss improved from 8.55955 to 8.48842, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.004.h5\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 8.3658 - gender_output_loss: 0.4270 - image_quality_output_loss: 0.8958 - age_output_loss: 1.3343 - weight_output_loss: 0.9556 - bag_output_loss: 0.8175 - pose_output_loss: 0.5688 - footwear_output_loss: 0.8357 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8028 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4288 - weight_output_acc: 0.6378 - bag_output_acc: 0.6366 - pose_output_acc: 0.7606 - footwear_output_acc: 0.6184 - emotion_output_acc: 0.7169 - val_loss: 8.4705 - val_gender_output_loss: 0.4196 - val_image_quality_output_loss: 0.9556 - val_age_output_loss: 1.3490 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8579 - val_pose_output_loss: 0.5436 - val_footwear_output_loss: 0.7965 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7808 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00005: val_loss improved from 8.48842 to 8.47047, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.005.h5\n",
            "Epoch 6/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.4183 - gender_output_loss: 0.4265 - image_quality_output_loss: 0.9092 - age_output_loss: 1.3517 - weight_output_loss: 0.9394 - bag_output_loss: 0.8168 - pose_output_loss: 0.5664 - footwear_output_loss: 0.8454 - emotion_output_loss: 0.8775 - gender_output_acc: 0.8052 - image_quality_output_acc: 0.5600 - age_output_acc: 0.3955 - weight_output_acc: 0.6449 - bag_output_acc: 0.6360 - pose_output_acc: 0.7705 - footwear_output_acc: 0.6237 - emotion_output_acc: 0.7042\n",
            "Epoch 00005: val_loss improved from 8.48842 to 8.47047, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.005.h5\n",
            "100/100 [==============================] - 69s 691ms/step - loss: 8.4175 - gender_output_loss: 0.4262 - image_quality_output_loss: 0.9099 - age_output_loss: 1.3507 - weight_output_loss: 0.9407 - bag_output_loss: 0.8178 - pose_output_loss: 0.5644 - footwear_output_loss: 0.8468 - emotion_output_loss: 0.8756 - gender_output_acc: 0.8053 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3966 - weight_output_acc: 0.6441 - bag_output_acc: 0.6359 - pose_output_acc: 0.7719 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7050 - val_loss: 8.5133 - val_gender_output_loss: 0.4129 - val_image_quality_output_loss: 0.9958 - val_age_output_loss: 1.3533 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.8599 - val_pose_output_loss: 0.5390 - val_footwear_output_loss: 0.8007 - val_emotion_output_loss: 0.8826 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5283 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7862 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 8.47047\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 69s 695ms/step - loss: 8.3729 - gender_output_loss: 0.4446 - image_quality_output_loss: 0.8974 - age_output_loss: 1.3265 - weight_output_loss: 0.9444 - bag_output_loss: 0.8338 - pose_output_loss: 0.5567 - footwear_output_loss: 0.8281 - emotion_output_loss: 0.8572 - gender_output_acc: 0.7919 - image_quality_output_acc: 0.5700 - age_output_acc: 0.4172 - weight_output_acc: 0.6338 - bag_output_acc: 0.6256 - pose_output_acc: 0.7694 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7119 - val_loss: 8.5608 - val_gender_output_loss: 0.4292 - val_image_quality_output_loss: 1.0233 - val_age_output_loss: 1.3555 - val_weight_output_loss: 0.9908 - val_bag_output_loss: 0.8665 - val_pose_output_loss: 0.5149 - val_footwear_output_loss: 0.7995 - val_emotion_output_loss: 0.8971 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6255 - val_pose_output_acc: 0.7981 - val_footwear_output_acc: 0.6647 - val_emotion_output_acc: 0.6820\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 8.47047\n",
            "Epoch 00007: val_loss did not improve from 8.47047\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 71s 706ms/step - loss: 8.3365 - gender_output_loss: 0.4340 - image_quality_output_loss: 0.8938 - age_output_loss: 1.3159 - weight_output_loss: 0.9369 - bag_output_loss: 0.8337 - pose_output_loss: 0.5523 - footwear_output_loss: 0.8359 - emotion_output_loss: 0.8501 - gender_output_acc: 0.7931 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4222 - weight_output_acc: 0.6444 - bag_output_acc: 0.6303 - pose_output_acc: 0.7791 - footwear_output_acc: 0.6159 - emotion_output_acc: 0.7144 - val_loss: 8.6095 - val_gender_output_loss: 0.4165 - val_image_quality_output_loss: 1.0406 - val_age_output_loss: 1.3543 - val_weight_output_loss: 0.9970 - val_bag_output_loss: 0.8642 - val_pose_output_loss: 0.5541 - val_footwear_output_loss: 0.8075 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.8189 - val_image_quality_output_acc: 0.5164 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.5947 - val_bag_output_acc: 0.6290 - val_pose_output_acc: 0.7803 - val_footwear_output_acc: 0.6488 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 8.47047\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 70s 697ms/step - loss: 8.3451 - gender_output_loss: 0.4407 - image_quality_output_loss: 0.8840 - age_output_loss: 1.3407 - weight_output_loss: 0.9342 - bag_output_loss: 0.8167 - pose_output_loss: 0.5569 - footwear_output_loss: 0.8242 - emotion_output_loss: 0.8640 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4234 - weight_output_acc: 0.6412 - bag_output_acc: 0.6278 - pose_output_acc: 0.7747 - footwear_output_acc: 0.6259 - emotion_output_acc: 0.7056 - val_loss: 8.5008 - val_gender_output_loss: 0.4428 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.3513 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.8633 - val_pose_output_loss: 0.5390 - val_footwear_output_loss: 0.7939 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6195 - val_pose_output_acc: 0.7837 - val_footwear_output_acc: 0.6582 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 8.47047\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 8.4126 - gender_output_loss: 0.4291 - image_quality_output_loss: 0.9009 - age_output_loss: 1.3500 - weight_output_loss: 0.9500 - bag_output_loss: 0.8361 - pose_output_loss: 0.5735 - footwear_output_loss: 0.8458 - emotion_output_loss: 0.8440 - gender_output_acc: 0.7987 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4062 - weight_output_acc: 0.6316 - bag_output_acc: 0.6322 - pose_output_acc: 0.7641 - footwear_output_acc: 0.6134 - emotion_output_acc: 0.7241 - val_loss: 8.5016 - val_gender_output_loss: 0.4228 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8664 - val_pose_output_loss: 0.5318 - val_footwear_output_loss: 0.7951 - val_emotion_output_loss: 0.8855 - val_gender_output_acc: 0.8204 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.7882 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 8.47047\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 70s 705ms/step - loss: 8.4310 - gender_output_loss: 0.4483 - image_quality_output_loss: 0.9180 - age_output_loss: 1.3326 - weight_output_loss: 0.9368 - bag_output_loss: 0.8366 - pose_output_loss: 0.5630 - footwear_output_loss: 0.8524 - emotion_output_loss: 0.8611 - gender_output_acc: 0.7906 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4131 - weight_output_acc: 0.6384 - bag_output_acc: 0.6344 - pose_output_acc: 0.7731 - footwear_output_acc: 0.6209 - emotion_output_acc: 0.7113 - val_loss: 8.4658 - val_gender_output_loss: 0.4167 - val_image_quality_output_loss: 0.9662 - val_age_output_loss: 1.3467 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8521 - val_pose_output_loss: 0.5359 - val_footwear_output_loss: 0.8022 - val_emotion_output_loss: 0.8815 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7847 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00011: val_loss improved from 8.47047 to 8.46579, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.011.h5\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 70s 697ms/step - loss: 8.3937 - gender_output_loss: 0.4424 - image_quality_output_loss: 0.9000 - age_output_loss: 1.3352 - weight_output_loss: 0.9291 - bag_output_loss: 0.8325 - pose_output_loss: 0.5762 - footwear_output_loss: 0.8415 - emotion_output_loss: 0.8562 - gender_output_acc: 0.7903 - image_quality_output_acc: 0.5684 - age_output_acc: 0.4084 - weight_output_acc: 0.6441 - bag_output_acc: 0.6319 - pose_output_acc: 0.7603 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7150 - val_loss: 8.6458 - val_gender_output_loss: 0.4499 - val_image_quality_output_loss: 1.0627 - val_age_output_loss: 1.3602 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8728 - val_pose_output_loss: 0.5277 - val_footwear_output_loss: 0.8034 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.8070 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5913 - val_bag_output_acc: 0.6171 - val_pose_output_acc: 0.7922 - val_footwear_output_acc: 0.6582 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 8.46579\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 70s 697ms/step - loss: 8.4355 - gender_output_loss: 0.4339 - image_quality_output_loss: 0.9126 - age_output_loss: 1.3503 - weight_output_loss: 0.9673 - bag_output_loss: 0.8448 - pose_output_loss: 0.5594 - footwear_output_loss: 0.8418 - emotion_output_loss: 0.8466 - gender_output_acc: 0.7919 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4034 - weight_output_acc: 0.6331 - bag_output_acc: 0.6222 - pose_output_acc: 0.7725 - footwear_output_acc: 0.6159 - emotion_output_acc: 0.7184 - val_loss: 8.5628 - val_gender_output_loss: 0.4028 - val_image_quality_output_loss: 1.0609 - val_age_output_loss: 1.3482 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.8486 - val_pose_output_loss: 0.5160 - val_footwear_output_loss: 0.8220 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4861 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7912 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 8.46579\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 70s 700ms/step - loss: 8.3944 - gender_output_loss: 0.4254 - image_quality_output_loss: 0.8855 - age_output_loss: 1.3561 - weight_output_loss: 0.9453 - bag_output_loss: 0.8196 - pose_output_loss: 0.5751 - footwear_output_loss: 0.8449 - emotion_output_loss: 0.8653 - gender_output_acc: 0.7956 - image_quality_output_acc: 0.5866 - age_output_acc: 0.4084 - weight_output_acc: 0.6384 - bag_output_acc: 0.6244 - pose_output_acc: 0.7641 - footwear_output_acc: 0.6094 - emotion_output_acc: 0.7103 - val_loss: 8.5579 - val_gender_output_loss: 0.4187 - val_image_quality_output_loss: 0.9881 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9914 - val_bag_output_loss: 0.8681 - val_pose_output_loss: 0.5755 - val_footwear_output_loss: 0.8023 - val_emotion_output_loss: 0.8855 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7669 - val_footwear_output_acc: 0.6548 - val_emotion_output_acc: 0.6915\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 8.46579\n",
            "Epoch 15/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.3208 - gender_output_loss: 0.4290 - image_quality_output_loss: 0.8968 - age_output_loss: 1.3216 - weight_output_loss: 0.9410 - bag_output_loss: 0.8192 - pose_output_loss: 0.5463 - footwear_output_loss: 0.8145 - emotion_output_loss: 0.8762 - gender_output_acc: 0.7964 - image_quality_output_acc: 0.5783 - age_output_acc: 0.4246 - weight_output_acc: 0.6338 - bag_output_acc: 0.6357 - pose_output_acc: 0.7730 - footwear_output_acc: 0.6307 - emotion_output_acc: 0.6976\n",
            "Epoch 00014: val_loss did not improve from 8.46579\n",
            "100/100 [==============================] - 71s 710ms/step - loss: 8.3218 - gender_output_loss: 0.4301 - image_quality_output_loss: 0.8976 - age_output_loss: 1.3227 - weight_output_loss: 0.9400 - bag_output_loss: 0.8179 - pose_output_loss: 0.5469 - footwear_output_loss: 0.8142 - emotion_output_loss: 0.8761 - gender_output_acc: 0.7956 - image_quality_output_acc: 0.5772 - age_output_acc: 0.4237 - weight_output_acc: 0.6350 - bag_output_acc: 0.6366 - pose_output_acc: 0.7728 - footwear_output_acc: 0.6303 - emotion_output_acc: 0.6975 - val_loss: 8.5618 - val_gender_output_loss: 0.4145 - val_image_quality_output_loss: 1.0347 - val_age_output_loss: 1.3592 - val_weight_output_loss: 0.9970 - val_bag_output_loss: 0.8641 - val_pose_output_loss: 0.5282 - val_footwear_output_loss: 0.7983 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.8194 - val_image_quality_output_acc: 0.5193 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6171 - val_pose_output_acc: 0.7867 - val_footwear_output_acc: 0.6582 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 8.46579\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 70s 702ms/step - loss: 8.4170 - gender_output_loss: 0.4467 - image_quality_output_loss: 0.8861 - age_output_loss: 1.3371 - weight_output_loss: 0.9326 - bag_output_loss: 0.8291 - pose_output_loss: 0.5828 - footwear_output_loss: 0.8548 - emotion_output_loss: 0.8722 - gender_output_acc: 0.7844 - image_quality_output_acc: 0.5831 - age_output_acc: 0.4138 - weight_output_acc: 0.6509 - bag_output_acc: 0.6347 - pose_output_acc: 0.7634 - footwear_output_acc: 0.6034 - emotion_output_acc: 0.7116 - val_loss: 8.5039 - val_gender_output_loss: 0.4102 - val_image_quality_output_loss: 1.0208 - val_age_output_loss: 1.3504 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.8581 - val_pose_output_loss: 0.5256 - val_footwear_output_loss: 0.7964 - val_emotion_output_loss: 0.8854 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5114 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6200 - val_pose_output_acc: 0.7877 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 8.46579\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 70s 704ms/step - loss: 8.3805 - gender_output_loss: 0.4400 - image_quality_output_loss: 0.8949 - age_output_loss: 1.3298 - weight_output_loss: 0.9691 - bag_output_loss: 0.8235 - pose_output_loss: 0.5696 - footwear_output_loss: 0.8408 - emotion_output_loss: 0.8374 - gender_output_acc: 0.7956 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4138 - weight_output_acc: 0.6181 - bag_output_acc: 0.6372 - pose_output_acc: 0.7744 - footwear_output_acc: 0.6128 - emotion_output_acc: 0.7153 - val_loss: 8.4607 - val_gender_output_loss: 0.4103 - val_image_quality_output_loss: 0.9719 - val_age_output_loss: 1.3458 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.8579 - val_pose_output_loss: 0.5331 - val_footwear_output_loss: 0.7996 - val_emotion_output_loss: 0.8827 - val_gender_output_acc: 0.8155 - val_image_quality_output_acc: 0.5476 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7837 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00017: val_loss improved from 8.46579 to 8.46071, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.017.h5\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 76s 759ms/step - loss: 8.3001 - gender_output_loss: 0.4327 - image_quality_output_loss: 0.8981 - age_output_loss: 1.3451 - weight_output_loss: 0.9219 - bag_output_loss: 0.8221 - pose_output_loss: 0.5296 - footwear_output_loss: 0.8321 - emotion_output_loss: 0.8435 - gender_output_acc: 0.7938 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4094 - weight_output_acc: 0.6503 - bag_output_acc: 0.6381 - pose_output_acc: 0.7837 - footwear_output_acc: 0.6278 - emotion_output_acc: 0.7172 - val_loss: 8.5122 - val_gender_output_loss: 0.4145 - val_image_quality_output_loss: 1.0189 - val_age_output_loss: 1.3484 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8632 - val_pose_output_loss: 0.5202 - val_footwear_output_loss: 0.8072 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5213 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7897 - val_footwear_output_acc: 0.6458 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 8.46071\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 70s 698ms/step - loss: 8.3650 - gender_output_loss: 0.4372 - image_quality_output_loss: 0.8915 - age_output_loss: 1.3342 - weight_output_loss: 0.9459 - bag_output_loss: 0.8132 - pose_output_loss: 0.5702 - footwear_output_loss: 0.8410 - emotion_output_loss: 0.8579 - gender_output_acc: 0.7944 - image_quality_output_acc: 0.5816 - age_output_acc: 0.4184 - weight_output_acc: 0.6388 - bag_output_acc: 0.6437 - pose_output_acc: 0.7681 - footwear_output_acc: 0.6119 - emotion_output_acc: 0.7159 - val_loss: 8.4982 - val_gender_output_loss: 0.4183 - val_image_quality_output_loss: 1.0037 - val_age_output_loss: 1.3483 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8562 - val_pose_output_loss: 0.5402 - val_footwear_output_loss: 0.7969 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.4077 - val_weight_output_acc: 0.6101 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7837 - val_footwear_output_acc: 0.6518 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 8.46071\n",
            "Epoch 20/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.3878 - gender_output_loss: 0.4350 - image_quality_output_loss: 0.9136 - age_output_loss: 1.3327 - weight_output_loss: 0.9315 - bag_output_loss: 0.8303 - pose_output_loss: 0.5766 - footwear_output_loss: 0.8415 - emotion_output_loss: 0.8540 - gender_output_acc: 0.7929 - image_quality_output_acc: 0.5574 - age_output_acc: 0.4141 - weight_output_acc: 0.6490 - bag_output_acc: 0.6209 - pose_output_acc: 0.7705 - footwear_output_acc: 0.6140 - emotion_output_acc: 0.7118\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 8.3841 - gender_output_loss: 0.4349 - image_quality_output_loss: 0.9135 - age_output_loss: 1.3342 - weight_output_loss: 0.9308 - bag_output_loss: 0.8291 - pose_output_loss: 0.5757 - footwear_output_loss: 0.8420 - emotion_output_loss: 0.8514 - gender_output_acc: 0.7928 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4134 - weight_output_acc: 0.6488 - bag_output_acc: 0.6219 - pose_output_acc: 0.7709 - footwear_output_acc: 0.6138 - emotion_output_acc: 0.7134 - val_loss: 8.4903 - val_gender_output_loss: 0.4102 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.3575 - val_weight_output_loss: 0.9932 - val_bag_output_loss: 0.8559 - val_pose_output_loss: 0.5500 - val_footwear_output_loss: 0.7978 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7748 - val_footwear_output_acc: 0.6582 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 8.46071\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 70s 702ms/step - loss: 8.3988 - gender_output_loss: 0.4251 - image_quality_output_loss: 0.8993 - age_output_loss: 1.3370 - weight_output_loss: 0.9571 - bag_output_loss: 0.8419 - pose_output_loss: 0.5671 - footwear_output_loss: 0.8452 - emotion_output_loss: 0.8555 - gender_output_acc: 0.8050 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4072 - weight_output_acc: 0.6284 - bag_output_acc: 0.6262 - pose_output_acc: 0.7719 - footwear_output_acc: 0.6109 - emotion_output_acc: 0.7141 - val_loss: 8.5066 - val_gender_output_loss: 0.4082 - val_image_quality_output_loss: 1.0076 - val_age_output_loss: 1.3511 - val_weight_output_loss: 1.0045 - val_bag_output_loss: 0.8540 - val_pose_output_loss: 0.5255 - val_footwear_output_loss: 0.7998 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.8165 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5923 - val_bag_output_acc: 0.6205 - val_pose_output_acc: 0.7862 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 8.46071\n",
            "Epoch 22/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.3202 - gender_output_loss: 0.4304 - image_quality_output_loss: 0.8959 - age_output_loss: 1.3228 - weight_output_loss: 0.9335 - bag_output_loss: 0.8234 - pose_output_loss: 0.5546 - footwear_output_loss: 0.8287 - emotion_output_loss: 0.8617 - gender_output_acc: 0.8005 - image_quality_output_acc: 0.5685 - age_output_acc: 0.4205 - weight_output_acc: 0.6480 - bag_output_acc: 0.6411 - pose_output_acc: 0.7784 - footwear_output_acc: 0.6266 - emotion_output_acc: 0.7099\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 8.3205 - gender_output_loss: 0.4295 - image_quality_output_loss: 0.8956 - age_output_loss: 1.3240 - weight_output_loss: 0.9344 - bag_output_loss: 0.8234 - pose_output_loss: 0.5547 - footwear_output_loss: 0.8276 - emotion_output_loss: 0.8621 - gender_output_acc: 0.8006 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4206 - weight_output_acc: 0.6466 - bag_output_acc: 0.6416 - pose_output_acc: 0.7775 - footwear_output_acc: 0.6275 - emotion_output_acc: 0.7097 - val_loss: 8.4929 - val_gender_output_loss: 0.4144 - val_image_quality_output_loss: 0.9986 - val_age_output_loss: 1.3488 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.8557 - val_pose_output_loss: 0.5396 - val_footwear_output_loss: 0.7983 - val_emotion_output_loss: 0.8853 - val_gender_output_acc: 0.8180 - val_image_quality_output_acc: 0.5303 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7862 - val_footwear_output_acc: 0.6637 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 8.46071\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 71s 708ms/step - loss: 8.3711 - gender_output_loss: 0.4369 - image_quality_output_loss: 0.9002 - age_output_loss: 1.3150 - weight_output_loss: 0.9402 - bag_output_loss: 0.8424 - pose_output_loss: 0.5671 - footwear_output_loss: 0.8348 - emotion_output_loss: 0.8663 - gender_output_acc: 0.7959 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4188 - weight_output_acc: 0.6356 - bag_output_acc: 0.6303 - pose_output_acc: 0.7703 - footwear_output_acc: 0.6309 - emotion_output_acc: 0.7078 - val_loss: 8.5228 - val_gender_output_loss: 0.4142 - val_image_quality_output_loss: 1.0083 - val_age_output_loss: 1.3504 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.8547 - val_pose_output_loss: 0.5454 - val_footwear_output_loss: 0.8125 - val_emotion_output_loss: 0.8878 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7837 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 8.46071\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 71s 706ms/step - loss: 8.3264 - gender_output_loss: 0.4256 - image_quality_output_loss: 0.8823 - age_output_loss: 1.3435 - weight_output_loss: 0.9254 - bag_output_loss: 0.8263 - pose_output_loss: 0.5615 - footwear_output_loss: 0.8355 - emotion_output_loss: 0.8587 - gender_output_acc: 0.8075 - image_quality_output_acc: 0.5850 - age_output_acc: 0.4113 - weight_output_acc: 0.6481 - bag_output_acc: 0.6241 - pose_output_acc: 0.7744 - footwear_output_acc: 0.6197 - emotion_output_acc: 0.7097 - val_loss: 8.6192 - val_gender_output_loss: 0.4376 - val_image_quality_output_loss: 1.0238 - val_age_output_loss: 1.3608 - val_weight_output_loss: 0.9934 - val_bag_output_loss: 0.8663 - val_pose_output_loss: 0.5334 - val_footwear_output_loss: 0.8299 - val_emotion_output_loss: 0.9064 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5332 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.5952 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7976 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.6771\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 8.46071\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 71s 706ms/step - loss: 8.3090 - gender_output_loss: 0.4186 - image_quality_output_loss: 0.8988 - age_output_loss: 1.3311 - weight_output_loss: 0.9453 - bag_output_loss: 0.8215 - pose_output_loss: 0.5484 - footwear_output_loss: 0.8396 - emotion_output_loss: 0.8383 - gender_output_acc: 0.8081 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4156 - weight_output_acc: 0.6384 - bag_output_acc: 0.6344 - pose_output_acc: 0.7828 - footwear_output_acc: 0.6166 - emotion_output_acc: 0.7203 - val_loss: 8.5491 - val_gender_output_loss: 0.4234 - val_image_quality_output_loss: 1.0294 - val_age_output_loss: 1.3526 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 0.8600 - val_pose_output_loss: 0.5438 - val_footwear_output_loss: 0.7997 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.5139 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7847 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 8.46071\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 71s 711ms/step - loss: 8.3730 - gender_output_loss: 0.4268 - image_quality_output_loss: 0.8938 - age_output_loss: 1.3358 - weight_output_loss: 0.9614 - bag_output_loss: 0.8316 - pose_output_loss: 0.5549 - footwear_output_loss: 0.8123 - emotion_output_loss: 0.8894 - gender_output_acc: 0.7878 - image_quality_output_acc: 0.5772 - age_output_acc: 0.4134 - weight_output_acc: 0.6241 - bag_output_acc: 0.6375 - pose_output_acc: 0.7719 - footwear_output_acc: 0.6447 - emotion_output_acc: 0.6956 - val_loss: 8.4835 - val_gender_output_loss: 0.4049 - val_image_quality_output_loss: 1.0113 - val_age_output_loss: 1.3517 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.8543 - val_pose_output_loss: 0.5139 - val_footwear_output_loss: 0.7968 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5278 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.7976 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 8.46071\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 70s 703ms/step - loss: 8.3014 - gender_output_loss: 0.4266 - image_quality_output_loss: 0.9009 - age_output_loss: 1.3227 - weight_output_loss: 0.9117 - bag_output_loss: 0.8258 - pose_output_loss: 0.5695 - footwear_output_loss: 0.8359 - emotion_output_loss: 0.8424 - gender_output_acc: 0.8047 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4219 - weight_output_acc: 0.6531 - bag_output_acc: 0.6269 - pose_output_acc: 0.7734 - footwear_output_acc: 0.6225 - emotion_output_acc: 0.7175 - val_loss: 8.5177 - val_gender_output_loss: 0.4342 - val_image_quality_output_loss: 0.9927 - val_age_output_loss: 1.3499 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8617 - val_pose_output_loss: 0.5229 - val_footwear_output_loss: 0.8250 - val_emotion_output_loss: 0.8889 - val_gender_output_acc: 0.8125 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7927 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 8.46071\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json\n",
            "End of run with EPOCHS= 100 STEPS_PER_EPOCH= 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7R9p9acOd20",
        "colab_type": "code",
        "outputId": "71bc8396-ac63-4e9a-c4e4-b967c54ef420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.017.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=20)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 100\n",
            "Epoch 1/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3556 - gender_output_loss: 0.4306 - image_quality_output_loss: 0.8969 - age_output_loss: 1.3345 - weight_output_loss: 0.9422 - bag_output_loss: 0.8219 - pose_output_loss: 0.5615 - footwear_output_loss: 0.8358 - emotion_output_loss: 0.8570 - gender_output_acc: 0.8021 - image_quality_output_acc: 0.5723 - age_output_acc: 0.4167 - weight_output_acc: 0.6396 - bag_output_acc: 0.6326 - pose_output_acc: 0.7722 - footwear_output_acc: 0.6226 - emotion_output_acc: 0.7121\n",
            "\n",
            "360/360 [==============================] - 224s 622ms/step - loss: 8.3555 - gender_output_loss: 0.4309 - image_quality_output_loss: 0.8970 - age_output_loss: 1.3347 - weight_output_loss: 0.9420 - bag_output_loss: 0.8220 - pose_output_loss: 0.5609 - footwear_output_loss: 0.8359 - emotion_output_loss: 0.8571 - gender_output_acc: 0.8019 - image_quality_output_acc: 0.5721 - age_output_acc: 0.4163 - weight_output_acc: 0.6394 - bag_output_acc: 0.6326 - pose_output_acc: 0.7727 - footwear_output_acc: 0.6225 - emotion_output_acc: 0.7122 - val_loss: 8.5382 - val_gender_output_loss: 0.4162 - val_image_quality_output_loss: 1.0177 - val_age_output_loss: 1.3505 - val_weight_output_loss: 0.9887 - val_bag_output_loss: 0.8683 - val_pose_output_loss: 0.5379 - val_footwear_output_loss: 0.7978 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7778 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.53818, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.001.h5\n",
            "Epoch 2/100\n",
            "360/360 [==============================] - 211s 586ms/step - loss: 8.3596 - gender_output_loss: 0.4287 - image_quality_output_loss: 0.8966 - age_output_loss: 1.3315 - weight_output_loss: 0.9430 - bag_output_loss: 0.8295 - pose_output_loss: 0.5629 - footwear_output_loss: 0.8384 - emotion_output_loss: 0.8559 - gender_output_acc: 0.8003 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4155 - weight_output_acc: 0.6418 - bag_output_acc: 0.6313 - pose_output_acc: 0.7683 - footwear_output_acc: 0.6206 - emotion_output_acc: 0.7115 - val_loss: 8.4823 - val_gender_output_loss: 0.4168 - val_image_quality_output_loss: 0.9948 - val_age_output_loss: 1.3509 - val_weight_output_loss: 0.9800 - val_bag_output_loss: 0.8575 - val_pose_output_loss: 0.5172 - val_footwear_output_loss: 0.7999 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.8189 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7986 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.53818 to 8.48230, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.002.h5\n",
            "Epoch 3/100\n",
            "360/360 [==============================] - 213s 593ms/step - loss: 8.3610 - gender_output_loss: 0.4312 - image_quality_output_loss: 0.8974 - age_output_loss: 1.3350 - weight_output_loss: 0.9421 - bag_output_loss: 0.8282 - pose_output_loss: 0.5632 - footwear_output_loss: 0.8390 - emotion_output_loss: 0.8554 - gender_output_acc: 0.7980 - image_quality_output_acc: 0.5718 - age_output_acc: 0.4190 - weight_output_acc: 0.6389 - bag_output_acc: 0.6264 - pose_output_acc: 0.7685 - footwear_output_acc: 0.6154 - emotion_output_acc: 0.7128 - val_loss: 8.4165 - val_gender_output_loss: 0.4339 - val_image_quality_output_loss: 0.9250 - val_age_output_loss: 1.3486 - val_weight_output_loss: 0.9725 - val_bag_output_loss: 0.8672 - val_pose_output_loss: 0.5164 - val_footwear_output_loss: 0.7992 - val_emotion_output_loss: 0.8866 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7956 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.48230 to 8.41651, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.003.h5\n",
            "Epoch 4/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3597 - gender_output_loss: 0.4310 - image_quality_output_loss: 0.8972 - age_output_loss: 1.3347 - weight_output_loss: 0.9424 - bag_output_loss: 0.8321 - pose_output_loss: 0.5655 - footwear_output_loss: 0.8350 - emotion_output_loss: 0.8575 - gender_output_acc: 0.7966 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4184 - weight_output_acc: 0.6401 - bag_output_acc: 0.6288 - pose_output_acc: 0.7642 - footwear_output_acc: 0.6234 - emotion_output_acc: 0.7124\n",
            "Epoch 00003: val_loss improved from 8.48230 to 8.41651, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.003.h5\n",
            "360/360 [==============================] - 218s 605ms/step - loss: 8.3595 - gender_output_loss: 0.4308 - image_quality_output_loss: 0.8973 - age_output_loss: 1.3351 - weight_output_loss: 0.9425 - bag_output_loss: 0.8318 - pose_output_loss: 0.5653 - footwear_output_loss: 0.8355 - emotion_output_loss: 0.8572 - gender_output_acc: 0.7969 - image_quality_output_acc: 0.5714 - age_output_acc: 0.4183 - weight_output_acc: 0.6402 - bag_output_acc: 0.6290 - pose_output_acc: 0.7644 - footwear_output_acc: 0.6230 - emotion_output_acc: 0.7125 - val_loss: 8.5512 - val_gender_output_loss: 0.4095 - val_image_quality_output_loss: 1.0329 - val_age_output_loss: 1.3552 - val_weight_output_loss: 0.9911 - val_bag_output_loss: 0.8595 - val_pose_output_loss: 0.5325 - val_footwear_output_loss: 0.8136 - val_emotion_output_loss: 0.8961 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5243 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.7956 - val_footwear_output_acc: 0.6414 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 8.41651\n",
            "Epoch 5/100\n",
            "187/360 [==============>...............] - ETA: 1:35 - loss: 8.3397 - gender_output_loss: 0.4363 - image_quality_output_loss: 0.8945 - age_output_loss: 1.3422 - weight_output_loss: 0.9368 - bag_output_loss: 0.8337 - pose_output_loss: 0.5622 - footwear_output_loss: 0.8249 - emotion_output_loss: 0.8500 - gender_output_acc: 0.7953 - image_quality_output_acc: 0.5742 - age_output_acc: 0.4103 - weight_output_acc: 0.6434 - bag_output_acc: 0.6260 - pose_output_acc: 0.7716 - footwear_output_acc: 0.6292 - emotion_output_acc: 0.7139"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-194:\n",
            "Process ForkPoolWorker-196:\n",
            "Process ForkPoolWorker-193:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkPoolWorker-192:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Process ForkPoolWorker-195:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-191:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Process ForkPoolWorker-189:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-190:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 54, in __getitem__\n",
            "    train_mean = np.mean(image, axis=(0,1,2))\n",
            "  File \"<__array_function__ internals>\", line 6, in mean\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 3257, in mean\n",
            "    out=out, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 151, in _mean\n",
            "    ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8d4c18caa075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m#class_weight=loss_weights_train,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End of run with EPOCHS=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"STEPS_PER_EPOCH=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lVO1aWOeSrW",
        "colab_type": "code",
        "outputId": "f12fc576-2984-438f-bf77-a6ba3e909518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=300\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.017.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577602344_model.003.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=50)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 300\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json  to: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_1577595402.json1577603395_backup\n",
            "Epoch 1/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3412 - gender_output_loss: 0.4272 - image_quality_output_loss: 0.8991 - age_output_loss: 1.3307 - weight_output_loss: 0.9412 - bag_output_loss: 0.8254 - pose_output_loss: 0.5554 - footwear_output_loss: 0.8380 - emotion_output_loss: 0.8573 - gender_output_acc: 0.8015 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4197 - weight_output_acc: 0.6391 - bag_output_acc: 0.6355 - pose_output_acc: 0.7747 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7117\n",
            "\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 8.3417 - gender_output_loss: 0.4278 - image_quality_output_loss: 0.8989 - age_output_loss: 1.3311 - weight_output_loss: 0.9410 - bag_output_loss: 0.8261 - pose_output_loss: 0.5554 - footwear_output_loss: 0.8379 - emotion_output_loss: 0.8568 - gender_output_acc: 0.8012 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4194 - weight_output_acc: 0.6393 - bag_output_acc: 0.6352 - pose_output_acc: 0.7747 - footwear_output_acc: 0.6232 - emotion_output_acc: 0.7120 - val_loss: 8.4967 - val_gender_output_loss: 0.4286 - val_image_quality_output_loss: 0.9978 - val_age_output_loss: 1.3528 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8620 - val_pose_output_loss: 0.5191 - val_footwear_output_loss: 0.7924 - val_emotion_output_loss: 0.8978 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5293 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6290 - val_pose_output_acc: 0.7941 - val_footwear_output_acc: 0.6642 - val_emotion_output_acc: 0.6815\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.49672, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.001.h5\n",
            "Epoch 2/300\n",
            "360/360 [==============================] - 212s 589ms/step - loss: 8.3461 - gender_output_loss: 0.4270 - image_quality_output_loss: 0.8952 - age_output_loss: 1.3318 - weight_output_loss: 0.9432 - bag_output_loss: 0.8291 - pose_output_loss: 0.5626 - footwear_output_loss: 0.8363 - emotion_output_loss: 0.8559 - gender_output_acc: 0.8003 - image_quality_output_acc: 0.5718 - age_output_acc: 0.4185 - weight_output_acc: 0.6373 - bag_output_acc: 0.6244 - pose_output_acc: 0.7679 - footwear_output_acc: 0.6217 - emotion_output_acc: 0.7125 - val_loss: 8.6362 - val_gender_output_loss: 0.4419 - val_image_quality_output_loss: 1.0913 - val_age_output_loss: 1.3561 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.8648 - val_pose_output_loss: 0.5268 - val_footwear_output_loss: 0.8062 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.8046 - val_image_quality_output_acc: 0.5045 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.5987 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7867 - val_footwear_output_acc: 0.6637 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 8.49672\n",
            "Epoch 3/300\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 8.3578 - gender_output_loss: 0.4326 - image_quality_output_loss: 0.8931 - age_output_loss: 1.3331 - weight_output_loss: 0.9424 - bag_output_loss: 0.8361 - pose_output_loss: 0.5657 - footwear_output_loss: 0.8348 - emotion_output_loss: 0.8587 - gender_output_acc: 0.7973 - image_quality_output_acc: 0.5730 - age_output_acc: 0.4175 - weight_output_acc: 0.6385 - bag_output_acc: 0.6256 - pose_output_acc: 0.7693 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7118 - val_loss: 8.4551 - val_gender_output_loss: 0.4176 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.3498 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8504 - val_pose_output_loss: 0.5355 - val_footwear_output_loss: 0.7909 - val_emotion_output_loss: 0.8838 - val_gender_output_acc: 0.8165 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6210 - val_pose_output_acc: 0.7812 - val_footwear_output_acc: 0.6667 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.49672 to 8.45508, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.003.h5\n",
            "Epoch 4/300\n",
            "360/360 [==============================] - 217s 602ms/step - loss: 8.3553 - gender_output_loss: 0.4295 - image_quality_output_loss: 0.8936 - age_output_loss: 1.3352 - weight_output_loss: 0.9424 - bag_output_loss: 0.8305 - pose_output_loss: 0.5728 - footwear_output_loss: 0.8353 - emotion_output_loss: 0.8600 - gender_output_acc: 0.7967 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4145 - weight_output_acc: 0.6387 - bag_output_acc: 0.6273 - pose_output_acc: 0.7661 - footwear_output_acc: 0.6243 - emotion_output_acc: 0.7121 - val_loss: 8.4575 - val_gender_output_loss: 0.4182 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.3447 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.8657 - val_pose_output_loss: 0.5279 - val_footwear_output_loss: 0.7965 - val_emotion_output_loss: 0.8838 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.6151 - val_pose_output_acc: 0.7887 - val_footwear_output_acc: 0.6587 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 8.45508\n",
            "Epoch 5/300\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 8.3315 - gender_output_loss: 0.4297 - image_quality_output_loss: 0.8956 - age_output_loss: 1.3348 - weight_output_loss: 0.9426 - bag_output_loss: 0.8266 - pose_output_loss: 0.5639 - footwear_output_loss: 0.8333 - emotion_output_loss: 0.8556 - gender_output_acc: 0.7983 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4142 - weight_output_acc: 0.6393 - bag_output_acc: 0.6301 - pose_output_acc: 0.7678 - footwear_output_acc: 0.6214 - emotion_output_acc: 0.7124 - val_loss: 8.5802 - val_gender_output_loss: 0.4116 - val_image_quality_output_loss: 1.0223 - val_age_output_loss: 1.3518 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.8718 - val_pose_output_loss: 0.5899 - val_footwear_output_loss: 0.8070 - val_emotion_output_loss: 0.8897 - val_gender_output_acc: 0.8150 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7733 - val_footwear_output_acc: 0.6369 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 8.45508\n",
            "Epoch 6/300\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 8.3198 - gender_output_loss: 0.4274 - image_quality_output_loss: 0.8945 - age_output_loss: 1.3320 - weight_output_loss: 0.9400 - bag_output_loss: 0.8279 - pose_output_loss: 0.5632 - footwear_output_loss: 0.8326 - emotion_output_loss: 0.8581 - gender_output_acc: 0.8033 - image_quality_output_acc: 0.5744 - age_output_acc: 0.4158 - weight_output_acc: 0.6397 - bag_output_acc: 0.6306 - pose_output_acc: 0.7681 - footwear_output_acc: 0.6275 - emotion_output_acc: 0.7112 - val_loss: 8.4298 - val_gender_output_loss: 0.4191 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.3481 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8579 - val_pose_output_loss: 0.5127 - val_footwear_output_loss: 0.8056 - val_emotion_output_loss: 0.8846 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6498 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00006: val_loss improved from 8.45508 to 8.42979, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.006.h5\n",
            "\n",
            "Epoch 7/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.2938 - gender_output_loss: 0.4174 - image_quality_output_loss: 0.8927 - age_output_loss: 1.3306 - weight_output_loss: 0.9422 - bag_output_loss: 0.8255 - pose_output_loss: 0.5555 - footwear_output_loss: 0.8343 - emotion_output_loss: 0.8552 - gender_output_acc: 0.8105 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4203 - weight_output_acc: 0.6410 - bag_output_acc: 0.6320 - pose_output_acc: 0.7716 - footwear_output_acc: 0.6205 - emotion_output_acc: 0.7121 - val_loss: 8.4353 - val_gender_output_loss: 0.4010 - val_image_quality_output_loss: 0.9946 - val_age_output_loss: 1.3463 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.8603 - val_pose_output_loss: 0.5227 - val_footwear_output_loss: 0.7964 - val_emotion_output_loss: 0.8850 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.5327 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7817 - val_footwear_output_acc: 0.6617 - val_emotion_output_acc: 0.6910\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 8.42979\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 8/300\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 8.2935 - gender_output_loss: 0.4217 - image_quality_output_loss: 0.8935 - age_output_loss: 1.3320 - weight_output_loss: 0.9413 - bag_output_loss: 0.8264 - pose_output_loss: 0.5524 - footwear_output_loss: 0.8321 - emotion_output_loss: 0.8555 - gender_output_acc: 0.8087 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4183 - weight_output_acc: 0.6394 - bag_output_acc: 0.6320 - pose_output_acc: 0.7727 - footwear_output_acc: 0.6246 - emotion_output_acc: 0.7115 - val_loss: 8.4845 - val_gender_output_loss: 0.4254 - val_image_quality_output_loss: 1.0174 - val_age_output_loss: 1.3515 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8654 - val_pose_output_loss: 0.5198 - val_footwear_output_loss: 0.7902 - val_emotion_output_loss: 0.8893 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7832 - val_footwear_output_acc: 0.6677 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 8.42979\n",
            "Epoch 9/300\n",
            "360/360 [==============================] - 220s 612ms/step - loss: 8.2959 - gender_output_loss: 0.4211 - image_quality_output_loss: 0.8941 - age_output_loss: 1.3305 - weight_output_loss: 0.9394 - bag_output_loss: 0.8276 - pose_output_loss: 0.5546 - footwear_output_loss: 0.8367 - emotion_output_loss: 0.8538 - gender_output_acc: 0.8021 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4164 - weight_output_acc: 0.6392 - bag_output_acc: 0.6328 - pose_output_acc: 0.7763 - footwear_output_acc: 0.6204 - emotion_output_acc: 0.7128 - val_loss: 8.5251 - val_gender_output_loss: 0.4187 - val_image_quality_output_loss: 1.0308 - val_age_output_loss: 1.3558 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.8610 - val_pose_output_loss: 0.5333 - val_footwear_output_loss: 0.7995 - val_emotion_output_loss: 0.8911 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5203 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.5913 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7877 - val_footwear_output_acc: 0.6632 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 8.42979\n",
            "Epoch 10/300\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 8.3107 - gender_output_loss: 0.4265 - image_quality_output_loss: 0.8941 - age_output_loss: 1.3344 - weight_output_loss: 0.9435 - bag_output_loss: 0.8252 - pose_output_loss: 0.5561 - footwear_output_loss: 0.8351 - emotion_output_loss: 0.8596 - gender_output_acc: 0.8024 - image_quality_output_acc: 0.5746 - age_output_acc: 0.4161 - weight_output_acc: 0.6388 - bag_output_acc: 0.6302 - pose_output_acc: 0.7728 - footwear_output_acc: 0.6224 - emotion_output_acc: 0.7116 - val_loss: 8.3962 - val_gender_output_loss: 0.3968 - val_image_quality_output_loss: 0.9873 - val_age_output_loss: 1.3480 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 0.8493 - val_pose_output_loss: 0.5118 - val_footwear_output_loss: 0.7945 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00010: val_loss improved from 8.42979 to 8.39621, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.010.h5\n",
            "Epoch 11/300\n",
            "360/360 [==============================] - 217s 602ms/step - loss: 8.2943 - gender_output_loss: 0.4191 - image_quality_output_loss: 0.8965 - age_output_loss: 1.3283 - weight_output_loss: 0.9433 - bag_output_loss: 0.8235 - pose_output_loss: 0.5603 - footwear_output_loss: 0.8321 - emotion_output_loss: 0.8585 - gender_output_acc: 0.8091 - image_quality_output_acc: 0.5736 - age_output_acc: 0.4145 - weight_output_acc: 0.6397 - bag_output_acc: 0.6357 - pose_output_acc: 0.7695 - footwear_output_acc: 0.6193 - emotion_output_acc: 0.7121 - val_loss: 8.4413 - val_gender_output_loss: 0.4065 - val_image_quality_output_loss: 1.0016 - val_age_output_loss: 1.3420 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8547 - val_pose_output_loss: 0.5507 - val_footwear_output_loss: 0.7902 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7803 - val_footwear_output_acc: 0.6637 - val_emotion_output_acc: 0.7014\n",
            "360/360 [==============================] - 221s 613ms/step - loss: 8.3107 - gender_output_loss: 0.4265 - image_quality_output_loss: 0.8941 - age_output_loss: 1.3344 - weight_output_loss: 0.9435 - bag_output_loss: 0.8252 - pose_output_loss: 0.5561 - footwear_output_loss: 0.8351 - emotion_output_loss: 0.8596 - gender_output_acc: 0.8024 - image_quality_output_acc: 0.5746 - age_output_acc: 0.4161 - weight_output_acc: 0.6388 - bag_output_acc: 0.6302 - pose_output_acc: 0.7728 - footwear_output_acc: 0.6224 - emotion_output_acc: 0.7116 - val_loss: 8.3962 - val_gender_output_loss: 0.3968 - val_image_quality_output_loss: 0.9873 - val_age_output_loss: 1.3480 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 0.8493 - val_pose_output_loss: 0.5118 - val_footwear_output_loss: 0.7945 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 8.39621\n",
            "Epoch 12/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.2874 - gender_output_loss: 0.4257 - image_quality_output_loss: 0.8931 - age_output_loss: 1.3296 - weight_output_loss: 0.9411 - bag_output_loss: 0.8281 - pose_output_loss: 0.5538 - footwear_output_loss: 0.8309 - emotion_output_loss: 0.8578 - gender_output_acc: 0.7995 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4161 - weight_output_acc: 0.6393 - bag_output_acc: 0.6296 - pose_output_acc: 0.7757 - footwear_output_acc: 0.6261 - emotion_output_acc: 0.7126 - val_loss: 8.3482 - val_gender_output_loss: 0.4040 - val_image_quality_output_loss: 0.9568 - val_age_output_loss: 1.3463 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8502 - val_pose_output_loss: 0.5124 - val_footwear_output_loss: 0.7891 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.8224 - val_image_quality_output_acc: 0.5491 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7946 - val_footwear_output_acc: 0.6627 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00012: val_loss improved from 8.39621 to 8.34818, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.012.h5\n",
            "Epoch 13/300\n",
            "360/360 [==============================] - 219s 607ms/step - loss: 8.2835 - gender_output_loss: 0.4222 - image_quality_output_loss: 0.8950 - age_output_loss: 1.3304 - weight_output_loss: 0.9390 - bag_output_loss: 0.8246 - pose_output_loss: 0.5591 - footwear_output_loss: 0.8340 - emotion_output_loss: 0.8581 - gender_output_acc: 0.8057 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4194 - weight_output_acc: 0.6407 - bag_output_acc: 0.6325 - pose_output_acc: 0.7727 - footwear_output_acc: 0.6251 - emotion_output_acc: 0.7118 - val_loss: 8.4885 - val_gender_output_loss: 0.4029 - val_image_quality_output_loss: 1.0614 - val_age_output_loss: 1.3508 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.8478 - val_pose_output_loss: 0.5408 - val_footwear_output_loss: 0.7911 - val_emotion_output_loss: 0.8887 - val_gender_output_acc: 0.8180 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7788 - val_footwear_output_acc: 0.6597 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 8.34818\n",
            "Epoch 14/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2881 - gender_output_loss: 0.4254 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3369 - weight_output_loss: 0.9433 - bag_output_loss: 0.8261 - pose_output_loss: 0.5611 - footwear_output_loss: 0.8326 - emotion_output_loss: 0.8549 - gender_output_acc: 0.8015 - image_quality_output_acc: 0.5768 - age_output_acc: 0.4183 - weight_output_acc: 0.6388 - bag_output_acc: 0.6312 - pose_output_acc: 0.7679 - footwear_output_acc: 0.6245 - emotion_output_acc: 0.7122\n",
            "Epoch 00013: val_loss did not improve from 8.34818\n",
            "360/360 [==============================] - 222s 615ms/step - loss: 8.2890 - gender_output_loss: 0.4253 - image_quality_output_loss: 0.8921 - age_output_loss: 1.3368 - weight_output_loss: 0.9430 - bag_output_loss: 0.8266 - pose_output_loss: 0.5618 - footwear_output_loss: 0.8327 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8016 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4182 - weight_output_acc: 0.6391 - bag_output_acc: 0.6308 - pose_output_acc: 0.7679 - footwear_output_acc: 0.6246 - emotion_output_acc: 0.7122 - val_loss: 8.4021 - val_gender_output_loss: 0.4008 - val_image_quality_output_loss: 1.0026 - val_age_output_loss: 1.3481 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8580 - val_pose_output_loss: 0.5242 - val_footwear_output_loss: 0.7881 - val_emotion_output_loss: 0.8804 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7912 - val_footwear_output_acc: 0.6607 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 8.34818\n",
            "Epoch 15/300\n",
            "360/360 [==============================] - 222s 616ms/step - loss: 8.2688 - gender_output_loss: 0.4184 - image_quality_output_loss: 0.8960 - age_output_loss: 1.3311 - weight_output_loss: 0.9410 - bag_output_loss: 0.8248 - pose_output_loss: 0.5578 - footwear_output_loss: 0.8323 - emotion_output_loss: 0.8553 - gender_output_acc: 0.8098 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4157 - weight_output_acc: 0.6387 - bag_output_acc: 0.6358 - pose_output_acc: 0.7759 - footwear_output_acc: 0.6227 - emotion_output_acc: 0.7130 - val_loss: 8.5473 - val_gender_output_loss: 0.3999 - val_image_quality_output_loss: 1.1093 - val_age_output_loss: 1.3495 - val_weight_output_loss: 1.0014 - val_bag_output_loss: 0.8596 - val_pose_output_loss: 0.5252 - val_footwear_output_loss: 0.7970 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.4866 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.5928 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7951 - val_footwear_output_acc: 0.6597 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 8.34818\n",
            "Epoch 16/300\n",
            "360/360 [==============================] - 222s 616ms/step - loss: 8.2569 - gender_output_loss: 0.4222 - image_quality_output_loss: 0.8943 - age_output_loss: 1.3309 - weight_output_loss: 0.9400 - bag_output_loss: 0.8228 - pose_output_loss: 0.5548 - footwear_output_loss: 0.8257 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8050 - image_quality_output_acc: 0.5739 - age_output_acc: 0.4194 - weight_output_acc: 0.6383 - bag_output_acc: 0.6368 - pose_output_acc: 0.7746 - footwear_output_acc: 0.6247 - emotion_output_acc: 0.7120 - val_loss: 8.3572 - val_gender_output_loss: 0.3959 - val_image_quality_output_loss: 0.9532 - val_age_output_loss: 1.3404 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8588 - val_pose_output_loss: 0.5206 - val_footwear_output_loss: 0.8071 - val_emotion_output_loss: 0.8839 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.4132 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7937 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 8.34818\n",
            "Epoch 17/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2481 - gender_output_loss: 0.4133 - image_quality_output_loss: 0.8925 - age_output_loss: 1.3337 - weight_output_loss: 0.9414 - bag_output_loss: 0.8221 - pose_output_loss: 0.5530 - footwear_output_loss: 0.8272 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8099 - image_quality_output_acc: 0.5742 - age_output_acc: 0.4173 - weight_output_acc: 0.6380 - bag_output_acc: 0.6347 - pose_output_acc: 0.7724 - footwear_output_acc: 0.6299 - emotion_output_acc: 0.7128\n",
            "360/360 [==============================] - 226s 629ms/step - loss: 8.2471 - gender_output_loss: 0.4131 - image_quality_output_loss: 0.8926 - age_output_loss: 1.3337 - weight_output_loss: 0.9411 - bag_output_loss: 0.8220 - pose_output_loss: 0.5533 - footwear_output_loss: 0.8269 - emotion_output_loss: 0.8546 - gender_output_acc: 0.8102 - image_quality_output_acc: 0.5740 - age_output_acc: 0.4174 - weight_output_acc: 0.6380 - bag_output_acc: 0.6349 - pose_output_acc: 0.7724 - footwear_output_acc: 0.6299 - emotion_output_acc: 0.7130 - val_loss: 8.4469 - val_gender_output_loss: 0.4013 - val_image_quality_output_loss: 1.0255 - val_age_output_loss: 1.3523 - val_weight_output_loss: 0.9974 - val_bag_output_loss: 0.8566 - val_pose_output_loss: 0.5208 - val_footwear_output_loss: 0.7963 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.8244 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.5977 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7927 - val_footwear_output_acc: 0.6587 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 8.34818\n",
            "Epoch 18/300\n",
            "360/360 [==============================] - 225s 626ms/step - loss: 8.2558 - gender_output_loss: 0.4207 - image_quality_output_loss: 0.8927 - age_output_loss: 1.3349 - weight_output_loss: 0.9414 - bag_output_loss: 0.8211 - pose_output_loss: 0.5537 - footwear_output_loss: 0.8291 - emotion_output_loss: 0.8542 - gender_output_acc: 0.8066 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4138 - weight_output_acc: 0.6391 - bag_output_acc: 0.6370 - pose_output_acc: 0.7734 - footwear_output_acc: 0.6238 - emotion_output_acc: 0.7115 - val_loss: 8.4378 - val_gender_output_loss: 0.4066 - val_image_quality_output_loss: 1.0055 - val_age_output_loss: 1.3434 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8514 - val_pose_output_loss: 0.5710 - val_footwear_output_loss: 0.7894 - val_emotion_output_loss: 0.8833 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.7768 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 8.34818\n",
            "Epoch 19/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.2643 - gender_output_loss: 0.4204 - image_quality_output_loss: 0.8944 - age_output_loss: 1.3317 - weight_output_loss: 0.9409 - bag_output_loss: 0.8274 - pose_output_loss: 0.5540 - footwear_output_loss: 0.8339 - emotion_output_loss: 0.8570 - gender_output_acc: 0.8041 - image_quality_output_acc: 0.5724 - age_output_acc: 0.4164 - weight_output_acc: 0.6387 - bag_output_acc: 0.6328 - pose_output_acc: 0.7705 - footwear_output_acc: 0.6248 - emotion_output_acc: 0.7115\n",
            "Epoch 00018: val_loss did not improve from 8.34818\n",
            "360/360 [==============================] - 226s 628ms/step - loss: 8.2647 - gender_output_loss: 0.4203 - image_quality_output_loss: 0.8944 - age_output_loss: 1.3315 - weight_output_loss: 0.9413 - bag_output_loss: 0.8274 - pose_output_loss: 0.5541 - footwear_output_loss: 0.8341 - emotion_output_loss: 0.8570 - gender_output_acc: 0.8040 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4166 - weight_output_acc: 0.6385 - bag_output_acc: 0.6327 - pose_output_acc: 0.7705 - footwear_output_acc: 0.6246 - emotion_output_acc: 0.7115 - val_loss: 8.3294 - val_gender_output_loss: 0.4062 - val_image_quality_output_loss: 0.9482 - val_age_output_loss: 1.3397 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8499 - val_pose_output_loss: 0.5341 - val_footwear_output_loss: 0.7887 - val_emotion_output_loss: 0.8787 - val_gender_output_acc: 0.8125 - val_image_quality_output_acc: 0.5501 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6255 - val_pose_output_acc: 0.7912 - val_footwear_output_acc: 0.6587 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "Epoch 00019: val_loss improved from 8.34818 to 8.32937, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.019.h5\n",
            "Epoch 20/300\n",
            "360/360 [==============================] - 225s 625ms/step - loss: 8.2471 - gender_output_loss: 0.4193 - image_quality_output_loss: 0.8938 - age_output_loss: 1.3324 - weight_output_loss: 0.9394 - bag_output_loss: 0.8258 - pose_output_loss: 0.5569 - footwear_output_loss: 0.8276 - emotion_output_loss: 0.8525 - gender_output_acc: 0.8080 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4200 - weight_output_acc: 0.6379 - bag_output_acc: 0.6367 - pose_output_acc: 0.7717 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7118 - val_loss: 8.5837 - val_gender_output_loss: 0.4269 - val_image_quality_output_loss: 1.1035 - val_age_output_loss: 1.3558 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.8643 - val_pose_output_loss: 0.5584 - val_footwear_output_loss: 0.8106 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.8155 - val_image_quality_output_acc: 0.4856 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.7768 - val_footwear_output_acc: 0.6518 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 8.32937\n",
            "Epoch 21/300\n",
            "360/360 [==============================] - 227s 629ms/step - loss: 8.2517 - gender_output_loss: 0.4266 - image_quality_output_loss: 0.8949 - age_output_loss: 1.3329 - weight_output_loss: 0.9430 - bag_output_loss: 0.8251 - pose_output_loss: 0.5573 - footwear_output_loss: 0.8250 - emotion_output_loss: 0.8537 - gender_output_acc: 0.8023 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4158 - weight_output_acc: 0.6386 - bag_output_acc: 0.6347 - pose_output_acc: 0.7760 - footwear_output_acc: 0.6281 - emotion_output_acc: 0.7127 - val_loss: 8.4075 - val_gender_output_loss: 0.3976 - val_image_quality_output_loss: 1.0428 - val_age_output_loss: 1.3410 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.8507 - val_pose_output_loss: 0.5056 - val_footwear_output_loss: 0.8079 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.8224 - val_image_quality_output_acc: 0.5050 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7976 - val_footwear_output_acc: 0.6468 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 8.32937\n",
            "Epoch 22/300\n",
            "360/360 [==============================] - 226s 629ms/step - loss: 8.2458 - gender_output_loss: 0.4204 - image_quality_output_loss: 0.8948 - age_output_loss: 1.3322 - weight_output_loss: 0.9407 - bag_output_loss: 0.8235 - pose_output_loss: 0.5598 - footwear_output_loss: 0.8292 - emotion_output_loss: 0.8571 - gender_output_acc: 0.8067 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4171 - weight_output_acc: 0.6403 - bag_output_acc: 0.6344 - pose_output_acc: 0.7748 - footwear_output_acc: 0.6268 - emotion_output_acc: 0.7111 - val_loss: 8.3540 - val_gender_output_loss: 0.3878 - val_image_quality_output_loss: 1.0053 - val_age_output_loss: 1.3421 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8478 - val_pose_output_loss: 0.5159 - val_footwear_output_loss: 0.8003 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.8249 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7981 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6964\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 8.32937\n",
            "Epoch 23/300\n",
            "360/360 [==============================] - 226s 628ms/step - loss: 8.2132 - gender_output_loss: 0.4148 - image_quality_output_loss: 0.8905 - age_output_loss: 1.3327 - weight_output_loss: 0.9400 - bag_output_loss: 0.8235 - pose_output_loss: 0.5456 - footwear_output_loss: 0.8260 - emotion_output_loss: 0.8556 - gender_output_acc: 0.8114 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4157 - weight_output_acc: 0.6419 - bag_output_acc: 0.6364 - pose_output_acc: 0.7790 - footwear_output_acc: 0.6253 - emotion_output_acc: 0.7124 - val_loss: 8.3758 - val_gender_output_loss: 0.3832 - val_image_quality_output_loss: 1.0286 - val_age_output_loss: 1.3404 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.8427 - val_pose_output_loss: 0.5381 - val_footwear_output_loss: 0.7888 - val_emotion_output_loss: 0.8839 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5134 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6394 - val_pose_output_acc: 0.7827 - val_footwear_output_acc: 0.6607 - val_emotion_output_acc: 0.6999\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 8.32937\n",
            "Epoch 24/300\n",
            "360/360 [==============================] - 224s 621ms/step - loss: 8.2126 - gender_output_loss: 0.4138 - image_quality_output_loss: 0.8898 - age_output_loss: 1.3314 - weight_output_loss: 0.9396 - bag_output_loss: 0.8221 - pose_output_loss: 0.5469 - footwear_output_loss: 0.8301 - emotion_output_loss: 0.8561 - gender_output_acc: 0.8073 - image_quality_output_acc: 0.5806 - age_output_acc: 0.4167 - weight_output_acc: 0.6397 - bag_output_acc: 0.6331 - pose_output_acc: 0.7785 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7118 - val_loss: 8.3079 - val_gender_output_loss: 0.4002 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.3443 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8471 - val_pose_output_loss: 0.5065 - val_footwear_output_loss: 0.7909 - val_emotion_output_loss: 0.8833 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6399 - val_pose_output_acc: 0.7971 - val_footwear_output_acc: 0.6642 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00024: val_loss improved from 8.32937 to 8.30788, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.024.h5\n",
            "Epoch 25/300\n",
            "360/360 [==============================] - 229s 637ms/step - loss: 8.1796 - gender_output_loss: 0.4087 - image_quality_output_loss: 0.8902 - age_output_loss: 1.3235 - weight_output_loss: 0.9371 - bag_output_loss: 0.8196 - pose_output_loss: 0.5389 - footwear_output_loss: 0.8248 - emotion_output_loss: 0.8547 - gender_output_acc: 0.8087 - image_quality_output_acc: 0.5760 - age_output_acc: 0.4198 - weight_output_acc: 0.6418 - bag_output_acc: 0.6351 - pose_output_acc: 0.7826 - footwear_output_acc: 0.6294 - emotion_output_acc: 0.7120 - val_loss: 8.4134 - val_gender_output_loss: 0.3965 - val_image_quality_output_loss: 1.0527 - val_age_output_loss: 1.3449 - val_weight_output_loss: 0.9902 - val_bag_output_loss: 0.8499 - val_pose_output_loss: 0.5137 - val_footwear_output_loss: 0.7957 - val_emotion_output_loss: 0.8881 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6374 - val_pose_output_acc: 0.7971 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00024: val_loss improved from 8.32937 to 8.30788, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.024.h5\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 8.30788\n",
            "Epoch 26/300\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 8.2127 - gender_output_loss: 0.4195 - image_quality_output_loss: 0.8925 - age_output_loss: 1.3298 - weight_output_loss: 0.9413 - bag_output_loss: 0.8221 - pose_output_loss: 0.5468 - footwear_output_loss: 0.8265 - emotion_output_loss: 0.8537 - gender_output_acc: 0.8046 - image_quality_output_acc: 0.5775 - age_output_acc: 0.4159 - weight_output_acc: 0.6399 - bag_output_acc: 0.6327 - pose_output_acc: 0.7773 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7124 - val_loss: 8.4167 - val_gender_output_loss: 0.3968 - val_image_quality_output_loss: 1.0489 - val_age_output_loss: 1.3484 - val_weight_output_loss: 1.0032 - val_bag_output_loss: 0.8481 - val_pose_output_loss: 0.5049 - val_footwear_output_loss: 0.7964 - val_emotion_output_loss: 0.8910 - val_gender_output_acc: 0.8239 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 8.30788\n",
            "Epoch 27/300\n",
            "360/360 [==============================] - 224s 623ms/step - loss: 8.2154 - gender_output_loss: 0.4122 - image_quality_output_loss: 0.8904 - age_output_loss: 1.3285 - weight_output_loss: 0.9429 - bag_output_loss: 0.8220 - pose_output_loss: 0.5539 - footwear_output_loss: 0.8341 - emotion_output_loss: 0.8546 - gender_output_acc: 0.8059 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4168 - weight_output_acc: 0.6383 - bag_output_acc: 0.6302 - pose_output_acc: 0.7773 - footwear_output_acc: 0.6165 - emotion_output_acc: 0.7118 - val_loss: 8.4216 - val_gender_output_loss: 0.4222 - val_image_quality_output_loss: 1.0092 - val_age_output_loss: 1.3454 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.8607 - val_pose_output_loss: 0.5359 - val_footwear_output_loss: 0.8085 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.8194 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.6290 - val_pose_output_acc: 0.7857 - val_footwear_output_acc: 0.6543 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 8.30788\n",
            "Epoch 28/300\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 8.2408 - gender_output_loss: 0.4174 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3365 - weight_output_loss: 0.9430 - bag_output_loss: 0.8292 - pose_output_loss: 0.5612 - footwear_output_loss: 0.8304 - emotion_output_loss: 0.8593 - gender_output_acc: 0.8046 - image_quality_output_acc: 0.5768 - age_output_acc: 0.4152 - weight_output_acc: 0.6424 - bag_output_acc: 0.6299 - pose_output_acc: 0.7723 - footwear_output_acc: 0.6265 - emotion_output_acc: 0.7112 - val_loss: 8.5047 - val_gender_output_loss: 0.4067 - val_image_quality_output_loss: 1.1113 - val_age_output_loss: 1.3408 - val_weight_output_loss: 0.9977 - val_bag_output_loss: 0.8502 - val_pose_output_loss: 0.5396 - val_footwear_output_loss: 0.8070 - val_emotion_output_loss: 0.8825 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.4846 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.7872 - val_footwear_output_acc: 0.6513 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 8.30788\n",
            "Epoch 29/300\n",
            "360/360 [==============================] - 220s 611ms/step - loss: 8.2230 - gender_output_loss: 0.4226 - image_quality_output_loss: 0.8941 - age_output_loss: 1.3308 - weight_output_loss: 0.9393 - bag_output_loss: 0.8247 - pose_output_loss: 0.5595 - footwear_output_loss: 0.8280 - emotion_output_loss: 0.8583 - gender_output_acc: 0.8041 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4126 - weight_output_acc: 0.6405 - bag_output_acc: 0.6312 - pose_output_acc: 0.7727 - footwear_output_acc: 0.6247 - emotion_output_acc: 0.7117 - val_loss: 8.4596 - val_gender_output_loss: 0.4079 - val_image_quality_output_loss: 1.0917 - val_age_output_loss: 1.3433 - val_weight_output_loss: 0.9998 - val_bag_output_loss: 0.8636 - val_pose_output_loss: 0.5134 - val_footwear_output_loss: 0.7905 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.4916 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.5982 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.8070 - val_footwear_output_acc: 0.6657 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 8.30788\n",
            "Epoch 30/300\n",
            "360/360 [==============================] - 217s 603ms/step - loss: 8.2013 - gender_output_loss: 0.4185 - image_quality_output_loss: 0.8961 - age_output_loss: 1.3289 - weight_output_loss: 0.9416 - bag_output_loss: 0.8220 - pose_output_loss: 0.5533 - footwear_output_loss: 0.8225 - emotion_output_loss: 0.8577 - gender_output_acc: 0.8052 - image_quality_output_acc: 0.5735 - age_output_acc: 0.4157 - weight_output_acc: 0.6405 - bag_output_acc: 0.6343 - pose_output_acc: 0.7735 - footwear_output_acc: 0.6285 - emotion_output_acc: 0.7116 - val_loss: 8.5043 - val_gender_output_loss: 0.4053 - val_image_quality_output_loss: 1.0781 - val_age_output_loss: 1.3530 - val_weight_output_loss: 0.9969 - val_bag_output_loss: 0.8730 - val_pose_output_loss: 0.5375 - val_footwear_output_loss: 0.8093 - val_emotion_output_loss: 0.8925 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5144 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6305 - val_pose_output_acc: 0.7922 - val_footwear_output_acc: 0.6503 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 8.30788\n",
            "Epoch 31/300\n",
            "\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 8.1849 - gender_output_loss: 0.4138 - image_quality_output_loss: 0.8898 - age_output_loss: 1.3299 - weight_output_loss: 0.9400 - bag_output_loss: 0.8255 - pose_output_loss: 0.5456 - footwear_output_loss: 0.8265 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8126 - image_quality_output_acc: 0.5764 - age_output_acc: 0.4153 - weight_output_acc: 0.6391 - bag_output_acc: 0.6301 - pose_output_acc: 0.7756 - footwear_output_acc: 0.6270 - emotion_output_acc: 0.7125 - val_loss: 8.3192 - val_gender_output_loss: 0.3846 - val_image_quality_output_loss: 0.9945 - val_age_output_loss: 1.3412 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8516 - val_pose_output_loss: 0.5219 - val_footwear_output_loss: 0.7914 - val_emotion_output_loss: 0.8792 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4077 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7951 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 8.30788\n",
            "Epoch 32/300\n",
            "360/360 [==============================] - 217s 602ms/step - loss: 8.1610 - gender_output_loss: 0.4063 - image_quality_output_loss: 0.8889 - age_output_loss: 1.3249 - weight_output_loss: 0.9382 - bag_output_loss: 0.8176 - pose_output_loss: 0.5461 - footwear_output_loss: 0.8260 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8146 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4174 - weight_output_acc: 0.6399 - bag_output_acc: 0.6387 - pose_output_acc: 0.7782 - footwear_output_acc: 0.6252 - emotion_output_acc: 0.7120 - val_loss: 8.4364 - val_gender_output_loss: 0.3953 - val_image_quality_output_loss: 1.0579 - val_age_output_loss: 1.3450 - val_weight_output_loss: 1.0015 - val_bag_output_loss: 0.8578 - val_pose_output_loss: 0.5449 - val_footwear_output_loss: 0.7982 - val_emotion_output_loss: 0.8804 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5084 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7902 - val_footwear_output_acc: 0.6597 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 8.30788\n",
            "Epoch 33/300\n",
            "360/360 [==============================] - 218s 604ms/step - loss: 8.1630 - gender_output_loss: 0.4046 - image_quality_output_loss: 0.8918 - age_output_loss: 1.3258 - weight_output_loss: 0.9381 - bag_output_loss: 0.8185 - pose_output_loss: 0.5485 - footwear_output_loss: 0.8251 - emotion_output_loss: 0.8557 - gender_output_acc: 0.8161 - image_quality_output_acc: 0.5767 - age_output_acc: 0.4200 - weight_output_acc: 0.6427 - bag_output_acc: 0.6326 - pose_output_acc: 0.7707 - footwear_output_acc: 0.6250 - emotion_output_acc: 0.7128 - val_loss: 8.3544 - val_gender_output_loss: 0.3925 - val_image_quality_output_loss: 1.0263 - val_age_output_loss: 1.3378 - val_weight_output_loss: 0.9951 - val_bag_output_loss: 0.8529 - val_pose_output_loss: 0.5117 - val_footwear_output_loss: 0.7972 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.8264 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.4097 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.8080 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 8.30788\n",
            "Epoch 34/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.1515 - gender_output_loss: 0.4122 - image_quality_output_loss: 0.8897 - age_output_loss: 1.3241 - weight_output_loss: 0.9385 - bag_output_loss: 0.8172 - pose_output_loss: 0.5398 - footwear_output_loss: 0.8214 - emotion_output_loss: 0.8553 - gender_output_acc: 0.8092 - image_quality_output_acc: 0.5724 - age_output_acc: 0.4161 - weight_output_acc: 0.6395 - bag_output_acc: 0.6373 - pose_output_acc: 0.7813 - footwear_output_acc: 0.6308 - emotion_output_acc: 0.7115 - val_loss: 8.3563 - val_gender_output_loss: 0.4016 - val_image_quality_output_loss: 1.0229 - val_age_output_loss: 1.3414 - val_weight_output_loss: 0.9918 - val_bag_output_loss: 0.8531 - val_pose_output_loss: 0.5102 - val_footwear_output_loss: 0.8016 - val_emotion_output_loss: 0.8818 - val_gender_output_acc: 0.8224 - val_image_quality_output_acc: 0.5213 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8011 - val_footwear_output_acc: 0.6538 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 8.30788\n",
            "Epoch 35/300\n",
            "360/360 [==============================] - 218s 605ms/step - loss: 8.1660 - gender_output_loss: 0.4152 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3273 - weight_output_loss: 0.9407 - bag_output_loss: 0.8217 - pose_output_loss: 0.5464 - footwear_output_loss: 0.8190 - emotion_output_loss: 0.8538 - gender_output_acc: 0.8065 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4186 - weight_output_acc: 0.6397 - bag_output_acc: 0.6386 - pose_output_acc: 0.7750 - footwear_output_acc: 0.6289 - emotion_output_acc: 0.7115 - val_loss: 8.4333 - val_gender_output_loss: 0.3884 - val_image_quality_output_loss: 1.0763 - val_age_output_loss: 1.3408 - val_weight_output_loss: 1.0021 - val_bag_output_loss: 0.8562 - val_pose_output_loss: 0.5225 - val_footwear_output_loss: 0.8061 - val_emotion_output_loss: 0.8932 - val_gender_output_acc: 0.8299 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6310 - val_pose_output_acc: 0.7976 - val_footwear_output_acc: 0.6463 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 8.30788\n",
            "Epoch 36/300\n",
            "360/360 [==============================] - 217s 604ms/step - loss: 8.1864 - gender_output_loss: 0.4132 - image_quality_output_loss: 0.8929 - age_output_loss: 1.3363 - weight_output_loss: 0.9406 - bag_output_loss: 0.8206 - pose_output_loss: 0.5540 - footwear_output_loss: 0.8249 - emotion_output_loss: 0.8589 - gender_output_acc: 0.8100 - image_quality_output_acc: 0.5735 - age_output_acc: 0.4160 - weight_output_acc: 0.6380 - bag_output_acc: 0.6372 - pose_output_acc: 0.7770 - footwear_output_acc: 0.6312 - emotion_output_acc: 0.7125 - val_loss: 8.3083 - val_gender_output_loss: 0.3868 - val_image_quality_output_loss: 1.0088 - val_age_output_loss: 1.3385 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.8459 - val_pose_output_loss: 0.5125 - val_footwear_output_loss: 0.7964 - val_emotion_output_loss: 0.8855 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6354 - val_pose_output_acc: 0.7961 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 8.30788\n",
            "Epoch 37/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.1573 - gender_output_loss: 0.4093 - image_quality_output_loss: 0.8884 - age_output_loss: 1.3309 - weight_output_loss: 0.9418 - bag_output_loss: 0.8217 - pose_output_loss: 0.5468 - footwear_output_loss: 0.8250 - emotion_output_loss: 0.8544 - gender_output_acc: 0.8113 - image_quality_output_acc: 0.5800 - age_output_acc: 0.4128 - weight_output_acc: 0.6405 - bag_output_acc: 0.6292 - pose_output_acc: 0.7775 - footwear_output_acc: 0.6286 - emotion_output_acc: 0.7119 - val_loss: 8.3835 - val_gender_output_loss: 0.3872 - val_image_quality_output_loss: 1.0359 - val_age_output_loss: 1.3360 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.8499 - val_pose_output_loss: 0.5601 - val_footwear_output_loss: 0.8034 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.8284 - val_image_quality_output_acc: 0.5208 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6503 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 8.30788\n",
            "Epoch 38/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.1573 - gender_output_loss: 0.4093 - image_quality_output_loss: 0.8884 - age_output_loss: 1.3309 - weight_output_loss: 0.9418 - bag_output_loss: 0.8217 - pose_output_loss: 0.5468 - footwear_output_loss: 0.8250 - emotion_output_loss: 0.8544 - gender_output_acc: 0.8113 - image_quality_output_acc: 0.5800 - age_output_acc: 0.4128 - weight_output_acc: 0.6405 - bag_output_acc: 0.6292 - pose_output_acc: 0.7775 - footwear_output_acc: 0.6286 - emotion_output_acc: 0.7119 - val_loss: 8.3835 - val_gender_output_loss: 0.3872 - val_image_quality_output_loss: 1.0359 - val_age_output_loss: 1.3360 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.8499 - val_pose_output_loss: 0.5601 - val_footwear_output_loss: 0.8034 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.8284 - val_image_quality_output_acc: 0.5208 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7713 - val_footwear_output_acc: 0.6503 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 8.30788\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.1473 - gender_output_loss: 0.4088 - image_quality_output_loss: 0.8901 - age_output_loss: 1.3314 - weight_output_loss: 0.9369 - bag_output_loss: 0.8250 - pose_output_loss: 0.5457 - footwear_output_loss: 0.8214 - emotion_output_loss: 0.8540 - gender_output_acc: 0.8099 - image_quality_output_acc: 0.5757 - age_output_acc: 0.4163 - weight_output_acc: 0.6398 - bag_output_acc: 0.6328 - pose_output_acc: 0.7806 - footwear_output_acc: 0.6309 - emotion_output_acc: 0.7124 - val_loss: 8.3473 - val_gender_output_loss: 0.3917 - val_image_quality_output_loss: 1.0636 - val_age_output_loss: 1.3375 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8449 - val_pose_output_loss: 0.5224 - val_footwear_output_loss: 0.7884 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.6374 - val_pose_output_acc: 0.7887 - val_footwear_output_acc: 0.6607 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 8.30788\n",
            "Epoch 39/300\n",
            "360/360 [==============================] - 218s 604ms/step - loss: 8.1585 - gender_output_loss: 0.4036 - image_quality_output_loss: 0.8901 - age_output_loss: 1.3292 - weight_output_loss: 0.9389 - bag_output_loss: 0.8238 - pose_output_loss: 0.5526 - footwear_output_loss: 0.8302 - emotion_output_loss: 0.8596 - gender_output_acc: 0.8171 - image_quality_output_acc: 0.5751 - age_output_acc: 0.4153 - weight_output_acc: 0.6411 - bag_output_acc: 0.6330 - pose_output_acc: 0.7750 - footwear_output_acc: 0.6250 - emotion_output_acc: 0.7120 - val_loss: 8.3679 - val_gender_output_loss: 0.3837 - val_image_quality_output_loss: 1.0640 - val_age_output_loss: 1.3443 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.8507 - val_pose_output_loss: 0.5091 - val_footwear_output_loss: 0.8011 - val_emotion_output_loss: 0.8908 - val_gender_output_acc: 0.8299 - val_image_quality_output_acc: 0.5124 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6012 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 8.30788\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 8.30788\n",
            "Epoch 40/300\n",
            "360/360 [==============================] - 221s 614ms/step - loss: 8.1386 - gender_output_loss: 0.4043 - image_quality_output_loss: 0.8897 - age_output_loss: 1.3285 - weight_output_loss: 0.9374 - bag_output_loss: 0.8222 - pose_output_loss: 0.5493 - footwear_output_loss: 0.8234 - emotion_output_loss: 0.8548 - gender_output_acc: 0.8176 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4169 - weight_output_acc: 0.6415 - bag_output_acc: 0.6333 - pose_output_acc: 0.7780 - footwear_output_acc: 0.6277 - emotion_output_acc: 0.7120 - val_loss: 8.2291 - val_gender_output_loss: 0.3837 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.3406 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8441 - val_pose_output_loss: 0.4955 - val_footwear_output_loss: 0.7907 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8036 - val_footwear_output_acc: 0.6632 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "Epoch 00040: val_loss improved from 8.30788 to 8.22907, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.040.h5\n",
            "Epoch 41/300\n",
            "360/360 [==============================] - 223s 618ms/step - loss: 8.1165 - gender_output_loss: 0.4018 - image_quality_output_loss: 0.8869 - age_output_loss: 1.3298 - weight_output_loss: 0.9353 - bag_output_loss: 0.8173 - pose_output_loss: 0.5379 - footwear_output_loss: 0.8252 - emotion_output_loss: 0.8539 - gender_output_acc: 0.8135 - image_quality_output_acc: 0.5730 - age_output_acc: 0.4204 - weight_output_acc: 0.6411 - bag_output_acc: 0.6419 - pose_output_acc: 0.7810 - footwear_output_acc: 0.6257 - emotion_output_acc: 0.7116 - val_loss: 8.3049 - val_gender_output_loss: 0.3917 - val_image_quality_output_loss: 1.0189 - val_age_output_loss: 1.3406 - val_weight_output_loss: 0.9939 - val_bag_output_loss: 0.8488 - val_pose_output_loss: 0.5087 - val_footwear_output_loss: 0.7908 - val_emotion_output_loss: 0.8837 - val_gender_output_acc: 0.8254 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6587 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 8.22907\n",
            "Epoch 42/300\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 8.1303 - gender_output_loss: 0.4058 - image_quality_output_loss: 0.8892 - age_output_loss: 1.3289 - weight_output_loss: 0.9384 - bag_output_loss: 0.8186 - pose_output_loss: 0.5465 - footwear_output_loss: 0.8199 - emotion_output_loss: 0.8564 - gender_output_acc: 0.8159 - image_quality_output_acc: 0.5767 - age_output_acc: 0.4185 - weight_output_acc: 0.6417 - bag_output_acc: 0.6386 - pose_output_acc: 0.7791 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7122 - val_loss: 8.4074 - val_gender_output_loss: 0.4198 - val_image_quality_output_loss: 1.0220 - val_age_output_loss: 1.3438 - val_weight_output_loss: 0.9932 - val_bag_output_loss: 0.8760 - val_pose_output_loss: 0.5423 - val_footwear_output_loss: 0.7971 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7887 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6974\n",
            "Epoch 42/300\n",
            "Epoch 00042: val_loss did not improve from 8.22907\n",
            "Epoch 43/300\n",
            "360/360 [==============================] - 222s 618ms/step - loss: 8.1423 - gender_output_loss: 0.4058 - image_quality_output_loss: 0.8895 - age_output_loss: 1.3309 - weight_output_loss: 0.9392 - bag_output_loss: 0.8205 - pose_output_loss: 0.5591 - footwear_output_loss: 0.8233 - emotion_output_loss: 0.8506 - gender_output_acc: 0.8114 - image_quality_output_acc: 0.5784 - age_output_acc: 0.4177 - weight_output_acc: 0.6395 - bag_output_acc: 0.6365 - pose_output_acc: 0.7734 - footwear_output_acc: 0.6288 - emotion_output_acc: 0.7116 - val_loss: 8.3243 - val_gender_output_loss: 0.4057 - val_image_quality_output_loss: 1.0341 - val_age_output_loss: 1.3396 - val_weight_output_loss: 0.9894 - val_bag_output_loss: 0.8457 - val_pose_output_loss: 0.5047 - val_footwear_output_loss: 0.7924 - val_emotion_output_loss: 0.8915 - val_gender_output_acc: 0.8199 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6379 - val_pose_output_acc: 0.8041 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 8.22907\n",
            "Epoch 44/300\n",
            "360/360 [==============================] - 223s 620ms/step - loss: 8.1489 - gender_output_loss: 0.4112 - image_quality_output_loss: 0.8958 - age_output_loss: 1.3279 - weight_output_loss: 0.9421 - bag_output_loss: 0.8198 - pose_output_loss: 0.5534 - footwear_output_loss: 0.8236 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8106 - image_quality_output_acc: 0.5757 - age_output_acc: 0.4161 - weight_output_acc: 0.6392 - bag_output_acc: 0.6378 - pose_output_acc: 0.7737 - footwear_output_acc: 0.6322 - emotion_output_acc: 0.7116 - val_loss: 8.3543 - val_gender_output_loss: 0.3864 - val_image_quality_output_loss: 1.0520 - val_age_output_loss: 1.3408 - val_weight_output_loss: 0.9895 - val_bag_output_loss: 0.8712 - val_pose_output_loss: 0.5052 - val_footwear_output_loss: 0.8045 - val_emotion_output_loss: 0.8891 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5050 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6458 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 8.22907\n",
            "Epoch 45/300\n",
            "360/360 [==============================] - 223s 621ms/step - loss: 8.1419 - gender_output_loss: 0.4110 - image_quality_output_loss: 0.8926 - age_output_loss: 1.3310 - weight_output_loss: 0.9397 - bag_output_loss: 0.8250 - pose_output_loss: 0.5479 - footwear_output_loss: 0.8237 - emotion_output_loss: 0.8582 - gender_output_acc: 0.8092 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4185 - weight_output_acc: 0.6389 - bag_output_acc: 0.6314 - pose_output_acc: 0.7755 - footwear_output_acc: 0.6276 - emotion_output_acc: 0.7122 - val_loss: 8.3232 - val_gender_output_loss: 0.3823 - val_image_quality_output_loss: 1.0524 - val_age_output_loss: 1.3398 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8559 - val_pose_output_loss: 0.5272 - val_footwear_output_loss: 0.7920 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7922 - val_footwear_output_acc: 0.6607 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 8.22907\n",
            "Epoch 46/300\n",
            "360/360 [==============================] - 224s 621ms/step - loss: 8.1336 - gender_output_loss: 0.4091 - image_quality_output_loss: 0.8918 - age_output_loss: 1.3302 - weight_output_loss: 0.9419 - bag_output_loss: 0.8269 - pose_output_loss: 0.5459 - footwear_output_loss: 0.8249 - emotion_output_loss: 0.8552 - gender_output_acc: 0.8110 - image_quality_output_acc: 0.5798 - age_output_acc: 0.4176 - weight_output_acc: 0.6391 - bag_output_acc: 0.6338 - pose_output_acc: 0.7777 - footwear_output_acc: 0.6290 - emotion_output_acc: 0.7122 - val_loss: 8.4675 - val_gender_output_loss: 0.4259 - val_image_quality_output_loss: 1.0853 - val_age_output_loss: 1.3446 - val_weight_output_loss: 1.0007 - val_bag_output_loss: 0.9029 - val_pose_output_loss: 0.5228 - val_footwear_output_loss: 0.7933 - val_emotion_output_loss: 0.8861 - val_gender_output_acc: 0.8160 - val_image_quality_output_acc: 0.4886 - val_age_output_acc: 0.3904 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7986 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 8.22907\n",
            "Epoch 47/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.1038 - gender_output_loss: 0.4030 - image_quality_output_loss: 0.8900 - age_output_loss: 1.3284 - weight_output_loss: 0.9382 - bag_output_loss: 0.8150 - pose_output_loss: 0.5428 - footwear_output_loss: 0.8261 - emotion_output_loss: 0.8557 - gender_output_acc: 0.8169 - image_quality_output_acc: 0.5783 - age_output_acc: 0.4146 - weight_output_acc: 0.6407 - bag_output_acc: 0.6418 - pose_output_acc: 0.7835 - footwear_output_acc: 0.6258 - emotion_output_acc: 0.7127\n",
            "360/360 [==============================] - 225s 624ms/step - loss: 8.1044 - gender_output_loss: 0.4026 - image_quality_output_loss: 0.8900 - age_output_loss: 1.3287 - weight_output_loss: 0.9390 - bag_output_loss: 0.8150 - pose_output_loss: 0.5431 - footwear_output_loss: 0.8258 - emotion_output_loss: 0.8556 - gender_output_acc: 0.8170 - image_quality_output_acc: 0.5784 - age_output_acc: 0.4147 - weight_output_acc: 0.6405 - bag_output_acc: 0.6419 - pose_output_acc: 0.7835 - footwear_output_acc: 0.6261 - emotion_output_acc: 0.7128 - val_loss: 8.2030 - val_gender_output_loss: 0.3779 - val_image_quality_output_loss: 0.9743 - val_age_output_loss: 1.3394 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8408 - val_pose_output_loss: 0.4999 - val_footwear_output_loss: 0.7988 - val_emotion_output_loss: 0.8776 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6349 - val_pose_output_acc: 0.8080 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00047: val_loss improved from 8.22907 to 8.20301, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.047.h5\n",
            "Epoch 48/300\n",
            "360/360 [==============================] - 222s 615ms/step - loss: 8.0974 - gender_output_loss: 0.3986 - image_quality_output_loss: 0.8893 - age_output_loss: 1.3268 - weight_output_loss: 0.9401 - bag_output_loss: 0.8192 - pose_output_loss: 0.5443 - footwear_output_loss: 0.8210 - emotion_output_loss: 0.8553 - gender_output_acc: 0.8163 - image_quality_output_acc: 0.5786 - age_output_acc: 0.4188 - weight_output_acc: 0.6420 - bag_output_acc: 0.6354 - pose_output_acc: 0.7766 - footwear_output_acc: 0.6306 - emotion_output_acc: 0.7122 - val_loss: 8.2448 - val_gender_output_loss: 0.3803 - val_image_quality_output_loss: 0.9972 - val_age_output_loss: 1.3369 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.8487 - val_pose_output_loss: 0.5148 - val_footwear_output_loss: 0.7910 - val_emotion_output_loss: 0.8794 - val_gender_output_acc: 0.8289 - val_image_quality_output_acc: 0.5268 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.8041 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 8.20301\n",
            "Epoch 49/300\n",
            "360/360 [==============================] - 219s 608ms/step - loss: 8.0865 - gender_output_loss: 0.3987 - image_quality_output_loss: 0.8826 - age_output_loss: 1.3234 - weight_output_loss: 0.9393 - bag_output_loss: 0.8173 - pose_output_loss: 0.5469 - footwear_output_loss: 0.8195 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8174 - image_quality_output_acc: 0.5814 - age_output_acc: 0.4222 - weight_output_acc: 0.6405 - bag_output_acc: 0.6379 - pose_output_acc: 0.7739 - footwear_output_acc: 0.6316 - emotion_output_acc: 0.7125 - val_loss: 8.2975 - val_gender_output_loss: 0.3837 - val_image_quality_output_loss: 1.0017 - val_age_output_loss: 1.3541 - val_weight_output_loss: 1.0187 - val_bag_output_loss: 0.8494 - val_pose_output_loss: 0.5055 - val_footwear_output_loss: 0.7970 - val_emotion_output_loss: 0.8858 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5342 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.8100 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 8.20301\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 8.20301\n",
            "Epoch 50/300\n",
            "360/360 [==============================] - 218s 606ms/step - loss: 8.1062 - gender_output_loss: 0.4042 - image_quality_output_loss: 0.8932 - age_output_loss: 1.3263 - weight_output_loss: 0.9392 - bag_output_loss: 0.8205 - pose_output_loss: 0.5404 - footwear_output_loss: 0.8260 - emotion_output_loss: 0.8559 - gender_output_acc: 0.8144 - image_quality_output_acc: 0.5772 - age_output_acc: 0.4144 - weight_output_acc: 0.6399 - bag_output_acc: 0.6378 - pose_output_acc: 0.7804 - footwear_output_acc: 0.6222 - emotion_output_acc: 0.7122 - val_loss: 8.2823 - val_gender_output_loss: 0.3838 - val_image_quality_output_loss: 1.0127 - val_age_output_loss: 1.3354 - val_weight_output_loss: 1.0002 - val_bag_output_loss: 0.8549 - val_pose_output_loss: 0.5127 - val_footwear_output_loss: 0.8002 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.8304 - val_image_quality_output_acc: 0.5357 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6349 - val_pose_output_acc: 0.7996 - val_footwear_output_acc: 0.6538 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 8.20301\n",
            "Epoch 51/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0843 - gender_output_loss: 0.3983 - image_quality_output_loss: 0.8890 - age_output_loss: 1.3209 - weight_output_loss: 0.9377 - bag_output_loss: 0.8217 - pose_output_loss: 0.5427 - footwear_output_loss: 0.8237 - emotion_output_loss: 0.8530 - gender_output_acc: 0.8201 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4213 - weight_output_acc: 0.6407 - bag_output_acc: 0.6385 - pose_output_acc: 0.7801 - footwear_output_acc: 0.6321 - emotion_output_acc: 0.7128\n",
            "Epoch 00050: val_loss did not improve from 8.20301\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 8.0840 - gender_output_loss: 0.3980 - image_quality_output_loss: 0.8892 - age_output_loss: 1.3209 - weight_output_loss: 0.9378 - bag_output_loss: 0.8214 - pose_output_loss: 0.5424 - footwear_output_loss: 0.8236 - emotion_output_loss: 0.8535 - gender_output_acc: 0.8201 - image_quality_output_acc: 0.5768 - age_output_acc: 0.4214 - weight_output_acc: 0.6406 - bag_output_acc: 0.6387 - pose_output_acc: 0.7799 - footwear_output_acc: 0.6319 - emotion_output_acc: 0.7127 - val_loss: 8.3988 - val_gender_output_loss: 0.4037 - val_image_quality_output_loss: 1.0341 - val_age_output_loss: 1.3431 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.8495 - val_pose_output_loss: 0.6065 - val_footwear_output_loss: 0.8022 - val_emotion_output_loss: 0.8788 - val_gender_output_acc: 0.8199 - val_image_quality_output_acc: 0.5203 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7604 - val_footwear_output_acc: 0.6503 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 8.20301\n",
            "Epoch 52/300\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 8.1109 - gender_output_loss: 0.4088 - image_quality_output_loss: 0.8902 - age_output_loss: 1.3331 - weight_output_loss: 0.9379 - bag_output_loss: 0.8185 - pose_output_loss: 0.5486 - footwear_output_loss: 0.8250 - emotion_output_loss: 0.8562 - gender_output_acc: 0.8179 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4200 - weight_output_acc: 0.6411 - bag_output_acc: 0.6408 - pose_output_acc: 0.7804 - footwear_output_acc: 0.6285 - emotion_output_acc: 0.7132 - val_loss: 8.3756 - val_gender_output_loss: 0.3964 - val_image_quality_output_loss: 1.0599 - val_age_output_loss: 1.3496 - val_weight_output_loss: 1.0336 - val_bag_output_loss: 0.8543 - val_pose_output_loss: 0.5094 - val_footwear_output_loss: 0.7969 - val_emotion_output_loss: 0.8857 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.5779 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6538 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 8.20301\n",
            "Epoch 53/300\n",
            "360/360 [==============================] - 216s 599ms/step - loss: 8.1096 - gender_output_loss: 0.4083 - image_quality_output_loss: 0.8920 - age_output_loss: 1.3300 - weight_output_loss: 0.9431 - bag_output_loss: 0.8200 - pose_output_loss: 0.5456 - footwear_output_loss: 0.8246 - emotion_output_loss: 0.8591 - gender_output_acc: 0.8139 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4141 - weight_output_acc: 0.6377 - bag_output_acc: 0.6408 - pose_output_acc: 0.7811 - footwear_output_acc: 0.6289 - emotion_output_acc: 0.7113 - val_loss: 8.2031 - val_gender_output_loss: 0.3982 - val_image_quality_output_loss: 0.9458 - val_age_output_loss: 1.3357 - val_weight_output_loss: 1.0069 - val_bag_output_loss: 0.8420 - val_pose_output_loss: 0.5100 - val_footwear_output_loss: 0.7989 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.5933 - val_bag_output_acc: 0.6349 - val_pose_output_acc: 0.8021 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6964\n",
            "\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 8.20301\n",
            "Epoch 54/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 8.0866 - gender_output_loss: 0.3971 - image_quality_output_loss: 0.8890 - age_output_loss: 1.3323 - weight_output_loss: 0.9405 - bag_output_loss: 0.8194 - pose_output_loss: 0.5493 - footwear_output_loss: 0.8220 - emotion_output_loss: 0.8549 - gender_output_acc: 0.8213 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4167 - weight_output_acc: 0.6395 - bag_output_acc: 0.6407 - pose_output_acc: 0.7766 - footwear_output_acc: 0.6293 - emotion_output_acc: 0.7115 - val_loss: 8.3306 - val_gender_output_loss: 0.3875 - val_image_quality_output_loss: 1.0686 - val_age_output_loss: 1.3357 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.8568 - val_pose_output_loss: 0.5386 - val_footwear_output_loss: 0.7938 - val_emotion_output_loss: 0.8795 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.4936 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7932 - val_footwear_output_acc: 0.6612 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 8.20301\n",
            "Epoch 55/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0522 - gender_output_loss: 0.3967 - image_quality_output_loss: 0.8872 - age_output_loss: 1.3210 - weight_output_loss: 0.9387 - bag_output_loss: 0.8128 - pose_output_loss: 0.5440 - footwear_output_loss: 0.8162 - emotion_output_loss: 0.8567 - gender_output_acc: 0.8216 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4233 - weight_output_acc: 0.6409 - bag_output_acc: 0.6414 - pose_output_acc: 0.7787 - footwear_output_acc: 0.6306 - emotion_output_acc: 0.7112\n",
            "360/360 [==============================] - 214s 596ms/step - loss: 8.0516 - gender_output_loss: 0.3971 - image_quality_output_loss: 0.8873 - age_output_loss: 1.3210 - weight_output_loss: 0.9384 - bag_output_loss: 0.8126 - pose_output_loss: 0.5439 - footwear_output_loss: 0.8161 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8214 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4233 - weight_output_acc: 0.6410 - bag_output_acc: 0.6418 - pose_output_acc: 0.7786 - footwear_output_acc: 0.6307 - emotion_output_acc: 0.7114 - val_loss: 8.2079 - val_gender_output_loss: 0.3823 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.3357 - val_weight_output_loss: 0.9882 - val_bag_output_loss: 0.8582 - val_pose_output_loss: 0.5168 - val_footwear_output_loss: 0.7940 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6071 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6622 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 8.20301\n",
            "Epoch 56/300\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 8.0577 - gender_output_loss: 0.3918 - image_quality_output_loss: 0.8881 - age_output_loss: 1.3265 - weight_output_loss: 0.9406 - bag_output_loss: 0.8149 - pose_output_loss: 0.5462 - footwear_output_loss: 0.8193 - emotion_output_loss: 0.8530 - gender_output_acc: 0.8197 - image_quality_output_acc: 0.5812 - age_output_acc: 0.4194 - weight_output_acc: 0.6392 - bag_output_acc: 0.6408 - pose_output_acc: 0.7780 - footwear_output_acc: 0.6306 - emotion_output_acc: 0.7124 - val_loss: 8.3494 - val_gender_output_loss: 0.3946 - val_image_quality_output_loss: 1.0787 - val_age_output_loss: 1.3391 - val_weight_output_loss: 0.9908 - val_bag_output_loss: 0.8536 - val_pose_output_loss: 0.5398 - val_footwear_output_loss: 0.7913 - val_emotion_output_loss: 0.8846 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6434 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 8.20301\n",
            "Epoch 57/300\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 8.0254 - gender_output_loss: 0.3914 - image_quality_output_loss: 0.8857 - age_output_loss: 1.3202 - weight_output_loss: 0.9354 - bag_output_loss: 0.8139 - pose_output_loss: 0.5325 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8524 - gender_output_acc: 0.8254 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4213 - weight_output_acc: 0.6400 - bag_output_acc: 0.6406 - pose_output_acc: 0.7834 - footwear_output_acc: 0.6326 - emotion_output_acc: 0.7121 - val_loss: 8.1885 - val_gender_output_loss: 0.3783 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.3362 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.8477 - val_pose_output_loss: 0.4996 - val_footwear_output_loss: 0.7874 - val_emotion_output_loss: 0.8825 - val_gender_output_acc: 0.8378 - val_image_quality_output_acc: 0.5471 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6374 - val_pose_output_acc: 0.8165 - val_footwear_output_acc: 0.6637 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00057: val_loss improved from 8.20301 to 8.18848, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.057.h5\n",
            "Epoch 58/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0357 - gender_output_loss: 0.3957 - image_quality_output_loss: 0.8894 - age_output_loss: 1.3257 - weight_output_loss: 0.9375 - bag_output_loss: 0.8133 - pose_output_loss: 0.5339 - footwear_output_loss: 0.8129 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8216 - image_quality_output_acc: 0.5828 - age_output_acc: 0.4203 - weight_output_acc: 0.6389 - bag_output_acc: 0.6420 - pose_output_acc: 0.7824 - footwear_output_acc: 0.6377 - emotion_output_acc: 0.7117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00057: val_loss improved from 8.20301 to 8.18848, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.057.h5\n",
            "360/360 [==============================] - 214s 594ms/step - loss: 8.0356 - gender_output_loss: 0.3954 - image_quality_output_loss: 0.8893 - age_output_loss: 1.3254 - weight_output_loss: 0.9372 - bag_output_loss: 0.8138 - pose_output_loss: 0.5335 - footwear_output_loss: 0.8128 - emotion_output_loss: 0.8531 - gender_output_acc: 0.8219 - image_quality_output_acc: 0.5827 - age_output_acc: 0.4204 - weight_output_acc: 0.6390 - bag_output_acc: 0.6418 - pose_output_acc: 0.7826 - footwear_output_acc: 0.6378 - emotion_output_acc: 0.7113 - val_loss: 8.3085 - val_gender_output_loss: 0.3901 - val_image_quality_output_loss: 1.0196 - val_age_output_loss: 1.3468 - val_weight_output_loss: 1.0026 - val_bag_output_loss: 0.8555 - val_pose_output_loss: 0.5129 - val_footwear_output_loss: 0.8122 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5248 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.8105 - val_footwear_output_acc: 0.6483 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 8.18848\n",
            "Epoch 59/300\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 8.0722 - gender_output_loss: 0.4037 - image_quality_output_loss: 0.8889 - age_output_loss: 1.3290 - weight_output_loss: 0.9415 - bag_output_loss: 0.8183 - pose_output_loss: 0.5404 - footwear_output_loss: 0.8228 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8137 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4151 - weight_output_acc: 0.6393 - bag_output_acc: 0.6377 - pose_output_acc: 0.7803 - footwear_output_acc: 0.6293 - emotion_output_acc: 0.7115 - val_loss: 8.3023 - val_gender_output_loss: 0.4021 - val_image_quality_output_loss: 1.0395 - val_age_output_loss: 1.3416 - val_weight_output_loss: 0.9938 - val_bag_output_loss: 0.8493 - val_pose_output_loss: 0.5250 - val_footwear_output_loss: 0.7994 - val_emotion_output_loss: 0.8818 - val_gender_output_acc: 0.8239 - val_image_quality_output_acc: 0.5169 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7956 - val_footwear_output_acc: 0.6508 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 8.18848\n",
            "Epoch 60/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0719 - gender_output_loss: 0.4096 - image_quality_output_loss: 0.8889 - age_output_loss: 1.3315 - weight_output_loss: 0.9371 - bag_output_loss: 0.8192 - pose_output_loss: 0.5389 - footwear_output_loss: 0.8214 - emotion_output_loss: 0.8582 - gender_output_acc: 0.8125 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4192 - weight_output_acc: 0.6394 - bag_output_acc: 0.6401 - pose_output_acc: 0.7792 - footwear_output_acc: 0.6302 - emotion_output_acc: 0.7111Epoch 60/300\n",
            "360/360 [==============================] - 215s 596ms/step - loss: 8.0724 - gender_output_loss: 0.4098 - image_quality_output_loss: 0.8887 - age_output_loss: 1.3317 - weight_output_loss: 0.9368 - bag_output_loss: 0.8193 - pose_output_loss: 0.5393 - footwear_output_loss: 0.8217 - emotion_output_loss: 0.8579 - gender_output_acc: 0.8125 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4190 - weight_output_acc: 0.6394 - bag_output_acc: 0.6398 - pose_output_acc: 0.7788 - footwear_output_acc: 0.6300 - emotion_output_acc: 0.7112 - val_loss: 8.4528 - val_gender_output_loss: 0.3986 - val_image_quality_output_loss: 1.0988 - val_age_output_loss: 1.3437 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.8547 - val_pose_output_loss: 0.6053 - val_footwear_output_loss: 0.7977 - val_emotion_output_loss: 0.8938 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.4965 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6364 - val_pose_output_acc: 0.7629 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 8.18848\n",
            "Epoch 61/300\n",
            "360/360 [==============================] - 214s 594ms/step - loss: 8.0607 - gender_output_loss: 0.4035 - image_quality_output_loss: 0.8901 - age_output_loss: 1.3296 - weight_output_loss: 0.9402 - bag_output_loss: 0.8162 - pose_output_loss: 0.5433 - footwear_output_loss: 0.8205 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8154 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4163 - weight_output_acc: 0.6369 - bag_output_acc: 0.6390 - pose_output_acc: 0.7788 - footwear_output_acc: 0.6298 - emotion_output_acc: 0.7121 - val_loss: 8.2631 - val_gender_output_loss: 0.3816 - val_image_quality_output_loss: 1.0637 - val_age_output_loss: 1.3368 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.8463 - val_pose_output_loss: 0.5124 - val_footwear_output_loss: 0.8004 - val_emotion_output_loss: 0.8792 - val_gender_output_acc: 0.8309 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6121 - val_bag_output_acc: 0.6384 - val_pose_output_acc: 0.8051 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 8.18848\n",
            "Epoch 62/300\n",
            "360/360 [==============================] - 213s 593ms/step - loss: 8.0438 - gender_output_loss: 0.3997 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3272 - weight_output_loss: 0.9380 - bag_output_loss: 0.8128 - pose_output_loss: 0.5445 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8555 - gender_output_acc: 0.8169 - image_quality_output_acc: 0.5749 - age_output_acc: 0.4152 - weight_output_acc: 0.6406 - bag_output_acc: 0.6419 - pose_output_acc: 0.7780 - footwear_output_acc: 0.6325 - emotion_output_acc: 0.7105 - val_loss: 8.2594 - val_gender_output_loss: 0.3857 - val_image_quality_output_loss: 1.0471 - val_age_output_loss: 1.3434 - val_weight_output_loss: 0.9960 - val_bag_output_loss: 0.8405 - val_pose_output_loss: 0.5197 - val_footwear_output_loss: 0.7932 - val_emotion_output_loss: 0.8789 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6007 - val_bag_output_acc: 0.6424 - val_pose_output_acc: 0.7877 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 8.18848\n",
            "Epoch 63/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0331 - gender_output_loss: 0.3986 - image_quality_output_loss: 0.8867 - age_output_loss: 1.3236 - weight_output_loss: 0.9381 - bag_output_loss: 0.8137 - pose_output_loss: 0.5411 - footwear_output_loss: 0.8214 - emotion_output_loss: 0.8562 - gender_output_acc: 0.8199 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4181 - weight_output_acc: 0.6410 - bag_output_acc: 0.6394 - pose_output_acc: 0.7783 - footwear_output_acc: 0.6275 - emotion_output_acc: 0.7115Epoch 63/300\n",
            "360/360 [==============================] - 214s 593ms/step - loss: 8.0324 - gender_output_loss: 0.3987 - image_quality_output_loss: 0.8865 - age_output_loss: 1.3238 - weight_output_loss: 0.9383 - bag_output_loss: 0.8137 - pose_output_loss: 0.5409 - footwear_output_loss: 0.8211 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8199 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4179 - weight_output_acc: 0.6410 - bag_output_acc: 0.6394 - pose_output_acc: 0.7783 - footwear_output_acc: 0.6277 - emotion_output_acc: 0.7117 - val_loss: 8.2256 - val_gender_output_loss: 0.3834 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.3553 - val_weight_output_loss: 1.0133 - val_bag_output_loss: 0.8487 - val_pose_output_loss: 0.5185 - val_footwear_output_loss: 0.7886 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.5883 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6647 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 8.18848\n",
            "Epoch 64/300\n",
            "360/360 [==============================] - 213s 591ms/step - loss: 8.0061 - gender_output_loss: 0.3912 - image_quality_output_loss: 0.8847 - age_output_loss: 1.3237 - weight_output_loss: 0.9384 - bag_output_loss: 0.8154 - pose_output_loss: 0.5312 - footwear_output_loss: 0.8160 - emotion_output_loss: 0.8536 - gender_output_acc: 0.8226 - image_quality_output_acc: 0.5809 - age_output_acc: 0.4214 - weight_output_acc: 0.6400 - bag_output_acc: 0.6390 - pose_output_acc: 0.7833 - footwear_output_acc: 0.6347 - emotion_output_acc: 0.7122 - val_loss: 8.1709 - val_gender_output_loss: 0.3763 - val_image_quality_output_loss: 1.0088 - val_age_output_loss: 1.3367 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.8386 - val_pose_output_loss: 0.4923 - val_footwear_output_loss: 0.7934 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.4097 - val_weight_output_acc: 0.6052 - val_bag_output_acc: 0.6429 - val_pose_output_acc: 0.8155 - val_footwear_output_acc: 0.6622 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00064: val_loss improved from 8.18848 to 8.17085, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.064.h5\n",
            "Epoch 65/300\n",
            "360/360 [==============================] - 216s 599ms/step - loss: 8.0116 - gender_output_loss: 0.3916 - image_quality_output_loss: 0.8807 - age_output_loss: 1.3240 - weight_output_loss: 0.9365 - bag_output_loss: 0.8175 - pose_output_loss: 0.5373 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8550 - gender_output_acc: 0.8261 - image_quality_output_acc: 0.5806 - age_output_acc: 0.4128 - weight_output_acc: 0.6408 - bag_output_acc: 0.6403 - pose_output_acc: 0.7824 - footwear_output_acc: 0.6320 - emotion_output_acc: 0.7119 - val_loss: 8.1119 - val_gender_output_loss: 0.3782 - val_image_quality_output_loss: 0.9401 - val_age_output_loss: 1.3376 - val_weight_output_loss: 0.9913 - val_bag_output_loss: 0.8594 - val_pose_output_loss: 0.4963 - val_footwear_output_loss: 0.7815 - val_emotion_output_loss: 0.8767 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.8095 - val_footwear_output_acc: 0.6721 - val_emotion_output_acc: 0.7029\n",
            "\n",
            "\n",
            "Epoch 00065: val_loss improved from 8.17085 to 8.11192, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.065.h5\n",
            "Epoch 66/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 8.0249 - gender_output_loss: 0.3950 - image_quality_output_loss: 0.8848 - age_output_loss: 1.3271 - weight_output_loss: 0.9381 - bag_output_loss: 0.8183 - pose_output_loss: 0.5375 - footwear_output_loss: 0.8182 - emotion_output_loss: 0.8560 - gender_output_acc: 0.8249 - image_quality_output_acc: 0.5773 - age_output_acc: 0.4140 - weight_output_acc: 0.6410 - bag_output_acc: 0.6401 - pose_output_acc: 0.7813 - footwear_output_acc: 0.6343 - emotion_output_acc: 0.7120 - val_loss: 8.3143 - val_gender_output_loss: 0.3855 - val_image_quality_output_loss: 1.0565 - val_age_output_loss: 1.3528 - val_weight_output_loss: 1.0168 - val_bag_output_loss: 0.8688 - val_pose_output_loss: 0.5087 - val_footwear_output_loss: 0.7924 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.8299 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.5908 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.8070 - val_footwear_output_acc: 0.6572 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00065: val_loss improved from 8.17085 to 8.11192, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.065.h5\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 8.11192\n",
            "Epoch 67/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 8.0446 - gender_output_loss: 0.3997 - image_quality_output_loss: 0.8857 - age_output_loss: 1.3294 - weight_output_loss: 0.9402 - bag_output_loss: 0.8161 - pose_output_loss: 0.5485 - footwear_output_loss: 0.8233 - emotion_output_loss: 0.8549 - gender_output_acc: 0.8132 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4166 - weight_output_acc: 0.6409 - bag_output_acc: 0.6392 - pose_output_acc: 0.7766 - footwear_output_acc: 0.6290 - emotion_output_acc: 0.7122 - val_loss: 8.1849 - val_gender_output_loss: 0.3754 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.3378 - val_weight_output_loss: 0.9963 - val_bag_output_loss: 0.8442 - val_pose_output_loss: 0.5127 - val_footwear_output_loss: 0.8129 - val_emotion_output_loss: 0.8793 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6359 - val_pose_output_acc: 0.8026 - val_footwear_output_acc: 0.6493 - val_emotion_output_acc: 0.7019\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 8.11192\n",
            "Epoch 68/300\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 8.0428 - gender_output_loss: 0.4081 - image_quality_output_loss: 0.8872 - age_output_loss: 1.3268 - weight_output_loss: 0.9390 - bag_output_loss: 0.8169 - pose_output_loss: 0.5443 - footwear_output_loss: 0.8221 - emotion_output_loss: 0.8561 - gender_output_acc: 0.8139 - image_quality_output_acc: 0.5774 - age_output_acc: 0.4161 - weight_output_acc: 0.6407 - bag_output_acc: 0.6404 - pose_output_acc: 0.7816 - footwear_output_acc: 0.6378 - emotion_output_acc: 0.7135 - val_loss: 8.5270 - val_gender_output_loss: 0.4398 - val_image_quality_output_loss: 1.0673 - val_age_output_loss: 1.4453 - val_weight_output_loss: 1.0474 - val_bag_output_loss: 0.8646 - val_pose_output_loss: 0.5289 - val_footwear_output_loss: 0.7937 - val_emotion_output_loss: 0.9006 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.3502 - val_weight_output_acc: 0.5655 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7981 - val_footwear_output_acc: 0.6632 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 8.11192\n",
            "Epoch 69/300\n",
            "360/360 [==============================] - 215s 596ms/step - loss: 8.0394 - gender_output_loss: 0.4040 - image_quality_output_loss: 0.8890 - age_output_loss: 1.3280 - weight_output_loss: 0.9382 - bag_output_loss: 0.8191 - pose_output_loss: 0.5460 - footwear_output_loss: 0.8239 - emotion_output_loss: 0.8545 - gender_output_acc: 0.8148 - image_quality_output_acc: 0.5768 - age_output_acc: 0.4135 - weight_output_acc: 0.6391 - bag_output_acc: 0.6385 - pose_output_acc: 0.7753 - footwear_output_acc: 0.6307 - emotion_output_acc: 0.7122 - val_loss: 8.1257 - val_gender_output_loss: 0.3916 - val_image_quality_output_loss: 0.9310 - val_age_output_loss: 1.3362 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8337 - val_pose_output_loss: 0.4935 - val_footwear_output_loss: 0.8437 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.8289 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6389 - val_pose_output_acc: 0.8120 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 8.11192\n",
            "Epoch 70/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.9939 - gender_output_loss: 0.3919 - image_quality_output_loss: 0.8863 - age_output_loss: 1.3260 - weight_output_loss: 0.9384 - bag_output_loss: 0.8111 - pose_output_loss: 0.5338 - footwear_output_loss: 0.8196 - emotion_output_loss: 0.8546 - gender_output_acc: 0.8233 - image_quality_output_acc: 0.5777 - age_output_acc: 0.4183 - weight_output_acc: 0.6391 - bag_output_acc: 0.6389 - pose_output_acc: 0.7882 - footwear_output_acc: 0.6321 - emotion_output_acc: 0.7128 - val_loss: 8.2563 - val_gender_output_loss: 0.3796 - val_image_quality_output_loss: 1.0743 - val_age_output_loss: 1.3447 - val_weight_output_loss: 0.9987 - val_bag_output_loss: 0.8462 - val_pose_output_loss: 0.5020 - val_footwear_output_loss: 0.7903 - val_emotion_output_loss: 0.8903 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.8090 - val_footwear_output_acc: 0.6627 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 8.11192\n",
            "Epoch 71/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.9922 - gender_output_loss: 0.3940 - image_quality_output_loss: 0.8842 - age_output_loss: 1.3242 - weight_output_loss: 0.9390 - bag_output_loss: 0.8135 - pose_output_loss: 0.5381 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8528 - gender_output_acc: 0.8203 - image_quality_output_acc: 0.5777 - age_output_acc: 0.4224 - weight_output_acc: 0.6408 - bag_output_acc: 0.6425 - pose_output_acc: 0.7811 - footwear_output_acc: 0.6289 - emotion_output_acc: 0.7114 - val_loss: 8.2442 - val_gender_output_loss: 0.3748 - val_image_quality_output_loss: 1.0491 - val_age_output_loss: 1.3421 - val_weight_output_loss: 1.0147 - val_bag_output_loss: 0.8429 - val_pose_output_loss: 0.4992 - val_footwear_output_loss: 0.7918 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.5863 - val_bag_output_acc: 0.6374 - val_pose_output_acc: 0.8194 - val_footwear_output_acc: 0.6577 - val_emotion_output_acc: 0.6761\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 8.11192\n",
            "Epoch 72/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9717 - gender_output_loss: 0.3904 - image_quality_output_loss: 0.8859 - age_output_loss: 1.3194 - weight_output_loss: 0.9361 - bag_output_loss: 0.8130 - pose_output_loss: 0.5317 - footwear_output_loss: 0.8158 - emotion_output_loss: 0.8521 - gender_output_acc: 0.8248 - image_quality_output_acc: 0.5812 - age_output_acc: 0.4201 - weight_output_acc: 0.6398 - bag_output_acc: 0.6451 - pose_output_acc: 0.7863 - footwear_output_acc: 0.6326 - emotion_output_acc: 0.7123 - val_loss: 8.1501 - val_gender_output_loss: 0.3908 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.3364 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.8582 - val_pose_output_loss: 0.5001 - val_footwear_output_loss: 0.7935 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.8318 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.8105 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 8.11192\n",
            "Epoch 73/300\n",
            "360/360 [==============================] - 216s 599ms/step - loss: 7.9716 - gender_output_loss: 0.3840 - image_quality_output_loss: 0.8837 - age_output_loss: 1.3237 - weight_output_loss: 0.9355 - bag_output_loss: 0.8148 - pose_output_loss: 0.5364 - footwear_output_loss: 0.8101 - emotion_output_loss: 0.8565 - gender_output_acc: 0.8243 - image_quality_output_acc: 0.5805 - age_output_acc: 0.4239 - weight_output_acc: 0.6419 - bag_output_acc: 0.6399 - pose_output_acc: 0.7835 - footwear_output_acc: 0.6381 - emotion_output_acc: 0.7119 - val_loss: 8.0848 - val_gender_output_loss: 0.3787 - val_image_quality_output_loss: 0.9548 - val_age_output_loss: 1.3309 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.8454 - val_pose_output_loss: 0.4962 - val_footwear_output_loss: 0.7922 - val_emotion_output_loss: 0.8787 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.4023 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6349 - val_pose_output_acc: 0.8095 - val_footwear_output_acc: 0.6597 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00073: val_loss improved from 8.11192 to 8.08484, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.073.h5\n",
            "Epoch 74/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9687 - gender_output_loss: 0.3880 - image_quality_output_loss: 0.8834 - age_output_loss: 1.3189 - weight_output_loss: 0.9388 - bag_output_loss: 0.8118 - pose_output_loss: 0.5367 - footwear_output_loss: 0.8123 - emotion_output_loss: 0.8537 - gender_output_acc: 0.8277 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4212 - weight_output_acc: 0.6395 - bag_output_acc: 0.6448 - pose_output_acc: 0.7793 - footwear_output_acc: 0.6368 - emotion_output_acc: 0.7112 - val_loss: 8.4312 - val_gender_output_loss: 0.4555 - val_image_quality_output_loss: 1.0349 - val_age_output_loss: 1.3788 - val_weight_output_loss: 1.0157 - val_bag_output_loss: 0.8635 - val_pose_output_loss: 0.5241 - val_footwear_output_loss: 0.8388 - val_emotion_output_loss: 0.8958 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.5873 - val_bag_output_acc: 0.6374 - val_pose_output_acc: 0.8125 - val_footwear_output_acc: 0.6399 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 8.08484\n",
            "Epoch 75/300\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 7.9935 - gender_output_loss: 0.3960 - image_quality_output_loss: 0.8846 - age_output_loss: 1.3292 - weight_output_loss: 0.9382 - bag_output_loss: 0.8175 - pose_output_loss: 0.5396 - footwear_output_loss: 0.8085 - emotion_output_loss: 0.8577 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4181 - weight_output_acc: 0.6393 - bag_output_acc: 0.6388 - pose_output_acc: 0.7810 - footwear_output_acc: 0.6380 - emotion_output_acc: 0.7124 - val_loss: 8.4345 - val_gender_output_loss: 0.4576 - val_image_quality_output_loss: 1.0653 - val_age_output_loss: 1.3580 - val_weight_output_loss: 1.0052 - val_bag_output_loss: 0.8699 - val_pose_output_loss: 0.5294 - val_footwear_output_loss: 0.8291 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.5238 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.8061 - val_footwear_output_acc: 0.6483 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 8.08484\n",
            "Epoch 76/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 8.0097 - gender_output_loss: 0.4027 - image_quality_output_loss: 0.8869 - age_output_loss: 1.3276 - weight_output_loss: 0.9386 - bag_output_loss: 0.8182 - pose_output_loss: 0.5454 - footwear_output_loss: 0.8180 - emotion_output_loss: 0.8546 - gender_output_acc: 0.8117 - image_quality_output_acc: 0.5749 - age_output_acc: 0.4189 - weight_output_acc: 0.6398 - bag_output_acc: 0.6391 - pose_output_acc: 0.7810 - footwear_output_acc: 0.6319 - emotion_output_acc: 0.7123 - val_loss: 8.1702 - val_gender_output_loss: 0.3699 - val_image_quality_output_loss: 1.0227 - val_age_output_loss: 1.3455 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8307 - val_pose_output_loss: 0.5074 - val_footwear_output_loss: 0.7805 - val_emotion_output_loss: 0.9044 - val_gender_output_acc: 0.8403 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6463 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6647 - val_emotion_output_acc: 0.6726\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 8.08484\n",
            "Epoch 77/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0071 - gender_output_loss: 0.4042 - image_quality_output_loss: 0.8891 - age_output_loss: 1.3327 - weight_output_loss: 0.9398 - bag_output_loss: 0.8149 - pose_output_loss: 0.5466 - footwear_output_loss: 0.8141 - emotion_output_loss: 0.8534 - gender_output_acc: 0.8170 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4138 - weight_output_acc: 0.6396 - bag_output_acc: 0.6392 - pose_output_acc: 0.7779 - footwear_output_acc: 0.6334 - emotion_output_acc: 0.7134\n",
            "Epoch 00076: val_loss did not improve from 8.08484\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 8.0063 - gender_output_loss: 0.4038 - image_quality_output_loss: 0.8892 - age_output_loss: 1.3326 - weight_output_loss: 0.9399 - bag_output_loss: 0.8151 - pose_output_loss: 0.5462 - footwear_output_loss: 0.8140 - emotion_output_loss: 0.8531 - gender_output_acc: 0.8173 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4141 - weight_output_acc: 0.6395 - bag_output_acc: 0.6391 - pose_output_acc: 0.7780 - footwear_output_acc: 0.6334 - emotion_output_acc: 0.7136 - val_loss: 8.0905 - val_gender_output_loss: 0.3778 - val_image_quality_output_loss: 0.9339 - val_age_output_loss: 1.3416 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.8429 - val_pose_output_loss: 0.5121 - val_footwear_output_loss: 0.8026 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.4043 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8110 - val_footwear_output_acc: 0.6582 - val_emotion_output_acc: 0.7034\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 8.08484\n",
            "Epoch 78/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9736 - gender_output_loss: 0.3946 - image_quality_output_loss: 0.8849 - age_output_loss: 1.3248 - weight_output_loss: 0.9339 - bag_output_loss: 0.8138 - pose_output_loss: 0.5433 - footwear_output_loss: 0.8161 - emotion_output_loss: 0.8545 - gender_output_acc: 0.8220 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4202 - weight_output_acc: 0.6391 - bag_output_acc: 0.6409 - pose_output_acc: 0.7762 - footwear_output_acc: 0.6324 - emotion_output_acc: 0.7125\n",
            "Epoch 00077: val_loss did not improve from 8.08484\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.9748 - gender_output_loss: 0.3949 - image_quality_output_loss: 0.8849 - age_output_loss: 1.3245 - weight_output_loss: 0.9340 - bag_output_loss: 0.8136 - pose_output_loss: 0.5437 - footwear_output_loss: 0.8171 - emotion_output_loss: 0.8544 - gender_output_acc: 0.8217 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4200 - weight_output_acc: 0.6391 - bag_output_acc: 0.6410 - pose_output_acc: 0.7760 - footwear_output_acc: 0.6319 - emotion_output_acc: 0.7125 - val_loss: 8.1850 - val_gender_output_loss: 0.3848 - val_image_quality_output_loss: 1.0009 - val_age_output_loss: 1.3422 - val_weight_output_loss: 0.9914 - val_bag_output_loss: 0.8435 - val_pose_output_loss: 0.5247 - val_footwear_output_loss: 0.8102 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5322 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.6091 - val_bag_output_acc: 0.6344 - val_pose_output_acc: 0.8021 - val_footwear_output_acc: 0.6498 - val_emotion_output_acc: 0.6994\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 8.08484\n",
            "Epoch 79/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9650 - gender_output_loss: 0.3909 - image_quality_output_loss: 0.8851 - age_output_loss: 1.3233 - weight_output_loss: 0.9368 - bag_output_loss: 0.8159 - pose_output_loss: 0.5324 - footwear_output_loss: 0.8226 - emotion_output_loss: 0.8532 - gender_output_acc: 0.8220 - image_quality_output_acc: 0.5790 - age_output_acc: 0.4181 - weight_output_acc: 0.6397 - bag_output_acc: 0.6421 - pose_output_acc: 0.7856 - footwear_output_acc: 0.6291 - emotion_output_acc: 0.7116\n",
            "Epoch 00078: val_loss did not improve from 8.08484\n",
            "Epoch 79/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9660 - gender_output_loss: 0.3911 - image_quality_output_loss: 0.8854 - age_output_loss: 1.3231 - weight_output_loss: 0.9375 - bag_output_loss: 0.8162 - pose_output_loss: 0.5325 - footwear_output_loss: 0.8225 - emotion_output_loss: 0.8530 - gender_output_acc: 0.8217 - image_quality_output_acc: 0.5788 - age_output_acc: 0.4184 - weight_output_acc: 0.6396 - bag_output_acc: 0.6417 - pose_output_acc: 0.7856 - footwear_output_acc: 0.6291 - emotion_output_acc: 0.7116 - val_loss: 8.1802 - val_gender_output_loss: 0.3814 - val_image_quality_output_loss: 1.0340 - val_age_output_loss: 1.3418 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8417 - val_pose_output_loss: 0.5101 - val_footwear_output_loss: 0.7875 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5079 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6131 - val_bag_output_acc: 0.6364 - val_pose_output_acc: 0.8110 - val_footwear_output_acc: 0.6612 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 8.08484\n",
            "Epoch 80/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9454 - gender_output_loss: 0.3855 - image_quality_output_loss: 0.8846 - age_output_loss: 1.3235 - weight_output_loss: 0.9350 - bag_output_loss: 0.8119 - pose_output_loss: 0.5368 - footwear_output_loss: 0.8139 - emotion_output_loss: 0.8511 - gender_output_acc: 0.8285 - image_quality_output_acc: 0.5801 - age_output_acc: 0.4207 - weight_output_acc: 0.6408 - bag_output_acc: 0.6452 - pose_output_acc: 0.7851 - footwear_output_acc: 0.6383 - emotion_output_acc: 0.7121Epoch 80/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.9458 - gender_output_loss: 0.3853 - image_quality_output_loss: 0.8848 - age_output_loss: 1.3236 - weight_output_loss: 0.9351 - bag_output_loss: 0.8121 - pose_output_loss: 0.5368 - footwear_output_loss: 0.8138 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8284 - image_quality_output_acc: 0.5800 - age_output_acc: 0.4206 - weight_output_acc: 0.6409 - bag_output_acc: 0.6451 - pose_output_acc: 0.7850 - footwear_output_acc: 0.6384 - emotion_output_acc: 0.7122 - val_loss: 8.1901 - val_gender_output_loss: 0.3776 - val_image_quality_output_loss: 1.0440 - val_age_output_loss: 1.3379 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.8419 - val_pose_output_loss: 0.5082 - val_footwear_output_loss: 0.7974 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5149 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6409 - val_pose_output_acc: 0.8075 - val_footwear_output_acc: 0.6533 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 8.08484\n",
            "Epoch 81/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9089 - gender_output_loss: 0.3798 - image_quality_output_loss: 0.8829 - age_output_loss: 1.3234 - weight_output_loss: 0.9347 - bag_output_loss: 0.8087 - pose_output_loss: 0.5174 - footwear_output_loss: 0.8089 - emotion_output_loss: 0.8505 - gender_output_acc: 0.8268 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4221 - weight_output_acc: 0.6404 - bag_output_acc: 0.6457 - pose_output_acc: 0.7922 - footwear_output_acc: 0.6377 - emotion_output_acc: 0.7122 - val_loss: 8.0369 - val_gender_output_loss: 0.3702 - val_image_quality_output_loss: 0.9459 - val_age_output_loss: 1.3312 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8496 - val_pose_output_loss: 0.4882 - val_footwear_output_loss: 0.7824 - val_emotion_output_loss: 0.8827 - val_gender_output_acc: 0.8408 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.4082 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.8189 - val_footwear_output_acc: 0.6642 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00081: val_loss improved from 8.08484 to 8.03687, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.081.h5\n",
            "Epoch 82/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9395 - gender_output_loss: 0.3853 - image_quality_output_loss: 0.8848 - age_output_loss: 1.3260 - weight_output_loss: 0.9377 - bag_output_loss: 0.8134 - pose_output_loss: 0.5294 - footwear_output_loss: 0.8107 - emotion_output_loss: 0.8511 - gender_output_acc: 0.8263 - image_quality_output_acc: 0.5794 - age_output_acc: 0.4145 - weight_output_acc: 0.6411 - bag_output_acc: 0.6463 - pose_output_acc: 0.7865 - footwear_output_acc: 0.6365 - emotion_output_acc: 0.7122 - val_loss: 8.2006 - val_gender_output_loss: 0.3732 - val_image_quality_output_loss: 1.0390 - val_age_output_loss: 1.3379 - val_weight_output_loss: 0.9947 - val_bag_output_loss: 0.8528 - val_pose_output_loss: 0.5363 - val_footwear_output_loss: 0.7875 - val_emotion_output_loss: 0.8794 - val_gender_output_acc: 0.8418 - val_image_quality_output_acc: 0.5114 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7961 - val_footwear_output_acc: 0.6612 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 8.03687\n",
            "Epoch 83/300\n",
            "360/360 [==============================] - 216s 601ms/step - loss: 7.9443 - gender_output_loss: 0.3909 - image_quality_output_loss: 0.8859 - age_output_loss: 1.3270 - weight_output_loss: 0.9356 - bag_output_loss: 0.8116 - pose_output_loss: 0.5333 - footwear_output_loss: 0.8104 - emotion_output_loss: 0.8516 - gender_output_acc: 0.8236 - image_quality_output_acc: 0.5779 - age_output_acc: 0.4221 - weight_output_acc: 0.6427 - bag_output_acc: 0.6398 - pose_output_acc: 0.7845 - footwear_output_acc: 0.6375 - emotion_output_acc: 0.7129 - val_loss: 8.3251 - val_gender_output_loss: 0.3739 - val_image_quality_output_loss: 1.1287 - val_age_output_loss: 1.3787 - val_weight_output_loss: 1.0184 - val_bag_output_loss: 0.8455 - val_pose_output_loss: 0.4960 - val_footwear_output_loss: 0.7941 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.4807 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.5923 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.8175 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 8.03687\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 8.03687\n",
            "Epoch 84/300\n",
            "360/360 [==============================] - 216s 599ms/step - loss: 7.9796 - gender_output_loss: 0.3948 - image_quality_output_loss: 0.8866 - age_output_loss: 1.3290 - weight_output_loss: 0.9399 - bag_output_loss: 0.8171 - pose_output_loss: 0.5469 - footwear_output_loss: 0.8178 - emotion_output_loss: 0.8538 - gender_output_acc: 0.8225 - image_quality_output_acc: 0.5780 - age_output_acc: 0.4186 - weight_output_acc: 0.6387 - bag_output_acc: 0.6378 - pose_output_acc: 0.7768 - footwear_output_acc: 0.6283 - emotion_output_acc: 0.7126 - val_loss: 8.2714 - val_gender_output_loss: 0.3932 - val_image_quality_output_loss: 0.9655 - val_age_output_loss: 1.3460 - val_weight_output_loss: 0.9968 - val_bag_output_loss: 0.8431 - val_pose_output_loss: 0.6118 - val_footwear_output_loss: 0.8389 - val_emotion_output_loss: 0.8851 - val_gender_output_acc: 0.8294 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.4038 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.7614 - val_footwear_output_acc: 0.6438 - val_emotion_output_acc: 0.6935\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 8.03687\n",
            "Epoch 85/300\n",
            "360/360 [==============================] - 215s 599ms/step - loss: 7.9642 - gender_output_loss: 0.3891 - image_quality_output_loss: 0.8892 - age_output_loss: 1.3283 - weight_output_loss: 0.9414 - bag_output_loss: 0.8156 - pose_output_loss: 0.5396 - footwear_output_loss: 0.8181 - emotion_output_loss: 0.8546 - gender_output_acc: 0.8266 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4181 - weight_output_acc: 0.6385 - bag_output_acc: 0.6385 - pose_output_acc: 0.7839 - footwear_output_acc: 0.6326 - emotion_output_acc: 0.7124 - val_loss: 8.1992 - val_gender_output_loss: 0.4168 - val_image_quality_output_loss: 1.0042 - val_age_output_loss: 1.3447 - val_weight_output_loss: 0.9978 - val_bag_output_loss: 0.8735 - val_pose_output_loss: 0.5072 - val_footwear_output_loss: 0.7850 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.8254 - val_image_quality_output_acc: 0.5288 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.8110 - val_footwear_output_acc: 0.6592 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 8.03687\n",
            "Epoch 86/300\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 7.9337 - gender_output_loss: 0.3932 - image_quality_output_loss: 0.8864 - age_output_loss: 1.3234 - weight_output_loss: 0.9400 - bag_output_loss: 0.8117 - pose_output_loss: 0.5291 - footwear_output_loss: 0.8130 - emotion_output_loss: 0.8530 - gender_output_acc: 0.8207 - image_quality_output_acc: 0.5774 - age_output_acc: 0.4168 - weight_output_acc: 0.6400 - bag_output_acc: 0.6436 - pose_output_acc: 0.7866 - footwear_output_acc: 0.6379 - emotion_output_acc: 0.7121 - val_loss: 8.2730 - val_gender_output_loss: 0.4210 - val_image_quality_output_loss: 1.0307 - val_age_output_loss: 1.3425 - val_weight_output_loss: 1.0169 - val_bag_output_loss: 0.8521 - val_pose_output_loss: 0.5392 - val_footwear_output_loss: 0.8038 - val_emotion_output_loss: 0.8845 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.5947 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.8006 - val_footwear_output_acc: 0.6473 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 8.03687\n",
            "Epoch 87/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9191 - gender_output_loss: 0.3839 - image_quality_output_loss: 0.8846 - age_output_loss: 1.3226 - weight_output_loss: 0.9369 - bag_output_loss: 0.8136 - pose_output_loss: 0.5314 - footwear_output_loss: 0.8116 - emotion_output_loss: 0.8536 - gender_output_acc: 0.8280 - image_quality_output_acc: 0.5801 - age_output_acc: 0.4216 - weight_output_acc: 0.6416 - bag_output_acc: 0.6425 - pose_output_acc: 0.7812 - footwear_output_acc: 0.6372 - emotion_output_acc: 0.7124 - val_loss: 8.1005 - val_gender_output_loss: 0.3796 - val_image_quality_output_loss: 0.9879 - val_age_output_loss: 1.3305 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.8428 - val_pose_output_loss: 0.5016 - val_footwear_output_loss: 0.7995 - val_emotion_output_loss: 0.8831 - val_gender_output_acc: 0.8343 - val_image_quality_output_acc: 0.5367 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6359 - val_pose_output_acc: 0.8120 - val_footwear_output_acc: 0.6498 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 87/300\n",
            "Epoch 00087: val_loss did not improve from 8.03687\n",
            "Epoch 88/300\n",
            "360/360 [==============================] - 216s 600ms/step - loss: 7.9106 - gender_output_loss: 0.3848 - image_quality_output_loss: 0.8837 - age_output_loss: 1.3207 - weight_output_loss: 0.9349 - bag_output_loss: 0.8167 - pose_output_loss: 0.5293 - footwear_output_loss: 0.8101 - emotion_output_loss: 0.8511 - gender_output_acc: 0.8249 - image_quality_output_acc: 0.5806 - age_output_acc: 0.4236 - weight_output_acc: 0.6403 - bag_output_acc: 0.6381 - pose_output_acc: 0.7891 - footwear_output_acc: 0.6380 - emotion_output_acc: 0.7128 - val_loss: 8.1896 - val_gender_output_loss: 0.3849 - val_image_quality_output_loss: 1.0401 - val_age_output_loss: 1.3379 - val_weight_output_loss: 1.0021 - val_bag_output_loss: 0.8526 - val_pose_output_loss: 0.5183 - val_footwear_output_loss: 0.7938 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.8363 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6032 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8041 - val_footwear_output_acc: 0.6562 - val_emotion_output_acc: 0.6974\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 8.03687\n",
            "Epoch 89/300\n",
            "360/360 [==============================] - 215s 599ms/step - loss: 7.9011 - gender_output_loss: 0.3834 - image_quality_output_loss: 0.8824 - age_output_loss: 1.3216 - weight_output_loss: 0.9349 - bag_output_loss: 0.8126 - pose_output_loss: 0.5244 - footwear_output_loss: 0.8115 - emotion_output_loss: 0.8515 - gender_output_acc: 0.8254 - image_quality_output_acc: 0.5790 - age_output_acc: 0.4192 - weight_output_acc: 0.6414 - bag_output_acc: 0.6426 - pose_output_acc: 0.7920 - footwear_output_acc: 0.6376 - emotion_output_acc: 0.7121 - val_loss: 8.2024 - val_gender_output_loss: 0.3838 - val_image_quality_output_loss: 1.0519 - val_age_output_loss: 1.3383 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.8470 - val_pose_output_loss: 0.5036 - val_footwear_output_loss: 0.8118 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.8368 - val_image_quality_output_acc: 0.5253 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6066 - val_bag_output_acc: 0.6354 - val_pose_output_acc: 0.8165 - val_footwear_output_acc: 0.6429 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 8.03687\n",
            "Epoch 90/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.9177 - gender_output_loss: 0.3833 - image_quality_output_loss: 0.8912 - age_output_loss: 1.3243 - weight_output_loss: 0.9377 - bag_output_loss: 0.8154 - pose_output_loss: 0.5233 - footwear_output_loss: 0.8102 - emotion_output_loss: 0.8549 - gender_output_acc: 0.8325 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4174 - weight_output_acc: 0.6392 - bag_output_acc: 0.6385 - pose_output_acc: 0.7891 - footwear_output_acc: 0.6382 - emotion_output_acc: 0.7126 - val_loss: 8.2483 - val_gender_output_loss: 0.3882 - val_image_quality_output_loss: 1.1112 - val_age_output_loss: 1.3359 - val_weight_output_loss: 1.0066 - val_bag_output_loss: 0.8356 - val_pose_output_loss: 0.5018 - val_footwear_output_loss: 0.8004 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.8274 - val_image_quality_output_acc: 0.4926 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8199 - val_footwear_output_acc: 0.6503 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 8.03687\n",
            "\n",
            "Epoch 91/300\n",
            "360/360 [==============================] - 215s 598ms/step - loss: 7.8993 - gender_output_loss: 0.3901 - image_quality_output_loss: 0.8834 - age_output_loss: 1.3219 - weight_output_loss: 0.9364 - bag_output_loss: 0.8128 - pose_output_loss: 0.5220 - footwear_output_loss: 0.8036 - emotion_output_loss: 0.8547 - gender_output_acc: 0.8244 - image_quality_output_acc: 0.5782 - age_output_acc: 0.4175 - weight_output_acc: 0.6398 - bag_output_acc: 0.6434 - pose_output_acc: 0.7849 - footwear_output_acc: 0.6383 - emotion_output_acc: 0.7116 - val_loss: 8.0619 - val_gender_output_loss: 0.3739 - val_image_quality_output_loss: 0.9718 - val_age_output_loss: 1.3326 - val_weight_output_loss: 0.9984 - val_bag_output_loss: 0.8355 - val_pose_output_loss: 0.5060 - val_footwear_output_loss: 0.7936 - val_emotion_output_loss: 0.8776 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6002 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6523 - val_emotion_output_acc: 0.7014\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 8.03687\n",
            "Epoch 92/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9366 - gender_output_loss: 0.3984 - image_quality_output_loss: 0.8860 - age_output_loss: 1.3235 - weight_output_loss: 0.9425 - bag_output_loss: 0.8108 - pose_output_loss: 0.5354 - footwear_output_loss: 0.8152 - emotion_output_loss: 0.8548 - gender_output_acc: 0.8148 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4188 - weight_output_acc: 0.6398 - bag_output_acc: 0.6423 - pose_output_acc: 0.7840 - footwear_output_acc: 0.6336 - emotion_output_acc: 0.7118 - val_loss: 8.3209 - val_gender_output_loss: 0.4189 - val_image_quality_output_loss: 1.0178 - val_age_output_loss: 1.3816 - val_weight_output_loss: 1.0110 - val_bag_output_loss: 0.8496 - val_pose_output_loss: 0.5931 - val_footwear_output_loss: 0.7978 - val_emotion_output_loss: 0.8836 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5134 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.7679 - val_footwear_output_acc: 0.6558 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 8.03687\n",
            "Epoch 93/300\n",
            "360/360 [==============================] - 215s 597ms/step - loss: 7.9374 - gender_output_loss: 0.3892 - image_quality_output_loss: 0.8846 - age_output_loss: 1.3279 - weight_output_loss: 0.9385 - bag_output_loss: 0.8163 - pose_output_loss: 0.5418 - footwear_output_loss: 0.8176 - emotion_output_loss: 0.8567 - gender_output_acc: 0.8203 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4143 - weight_output_acc: 0.6405 - bag_output_acc: 0.6389 - pose_output_acc: 0.7812 - footwear_output_acc: 0.6299 - emotion_output_acc: 0.7118 - val_loss: 8.1559 - val_gender_output_loss: 0.3802 - val_image_quality_output_loss: 1.0205 - val_age_output_loss: 1.3337 - val_weight_output_loss: 1.0057 - val_bag_output_loss: 0.8426 - val_pose_output_loss: 0.5204 - val_footwear_output_loss: 0.7959 - val_emotion_output_loss: 0.8946 - val_gender_output_acc: 0.8348 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4142 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.8026 - val_footwear_output_acc: 0.6513 - val_emotion_output_acc: 0.6791\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 8.03687\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 8.03687\n",
            "Epoch 94/300\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.9074 - gender_output_loss: 0.3877 - image_quality_output_loss: 0.8869 - age_output_loss: 1.3201 - weight_output_loss: 0.9358 - bag_output_loss: 0.8131 - pose_output_loss: 0.5343 - footwear_output_loss: 0.8115 - emotion_output_loss: 0.8576 - gender_output_acc: 0.8267 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4168 - weight_output_acc: 0.6400 - bag_output_acc: 0.6444 - pose_output_acc: 0.7839 - footwear_output_acc: 0.6379 - emotion_output_acc: 0.7115\n",
            "Epoch 00093: val_loss did not improve from 8.03687\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 7.9067 - gender_output_loss: 0.3875 - image_quality_output_loss: 0.8868 - age_output_loss: 1.3202 - weight_output_loss: 0.9358 - bag_output_loss: 0.8128 - pose_output_loss: 0.5338 - footwear_output_loss: 0.8122 - emotion_output_loss: 0.8569 - gender_output_acc: 0.8267 - image_quality_output_acc: 0.5777 - age_output_acc: 0.4168 - weight_output_acc: 0.6400 - bag_output_acc: 0.6446 - pose_output_acc: 0.7841 - footwear_output_acc: 0.6372 - emotion_output_acc: 0.7119 - val_loss: 8.1622 - val_gender_output_loss: 0.3822 - val_image_quality_output_loss: 1.0195 - val_age_output_loss: 1.3447 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.8477 - val_pose_output_loss: 0.5376 - val_footwear_output_loss: 0.7941 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5213 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7971 - val_footwear_output_acc: 0.6602 - val_emotion_output_acc: 0.6984\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 8.03687\n",
            "Epoch 95/300\n",
            "222/360 [=================>............] - ETA: 1:15 - loss: 7.8765 - gender_output_loss: 0.3805 - image_quality_output_loss: 0.8781 - age_output_loss: 1.3167 - weight_output_loss: 0.9421 - bag_output_loss: 0.8104 - pose_output_loss: 0.5276 - footwear_output_loss: 0.8115 - emotion_output_loss: 0.8518 - gender_output_acc: 0.8270 - image_quality_output_acc: 0.5816 - age_output_acc: 0.4203 - weight_output_acc: 0.6387 - bag_output_acc: 0.6413 - pose_output_acc: 0.7866 - footwear_output_acc: 0.6349 - emotion_output_acc: 0.7162"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s30bKwOaavQp",
        "colab_type": "code",
        "outputId": "9babb1db-0c9c-4826-8ea2-113c0d12d83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import glob\n",
        "def get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Base/\", pattern=\"*.json\",count=2):\n",
        "    #path = \"/content/gdrive/My Drive/WRN_Extend/\"\n",
        "    file_path = os.path.join(path)+pattern\n",
        "    files = [f for f in glob.iglob(file_path, recursive=False)]\n",
        "    latest_file = sorted(files, key=os.path.getctime,reverse=True)\n",
        "    return latest_file[:count]\n",
        "#files = os.listdir(path)\n",
        "#filenamere.split(r'/',files[len(files)-1])\n",
        "get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Base/\", pattern=\"*.h5\",count=1)[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.081.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waaR4kY9aicL",
        "colab_type": "code",
        "outputId": "6213716b-7571-4478-8db6-34ca9d802c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=50\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.017.h5')\n",
        "\n",
        "last_saved_weights = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Base/\", pattern=\"*.h5\",count=1)[0]\n",
        "print(last_saved_weights)\n",
        "wrn_28_10.load_weights(last_saved_weights)\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.081.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484_1577624806_model.{epoch:03d}.h5\n",
            "/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484_1577624806_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 3.0275501999999997e-05 max_lr= 0.0030275501999999996 epoch_count= 50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "360/360 [==============================] - 238s 661ms/step - loss: 7.9174 - gender_output_loss: 0.3808 - image_quality_output_loss: 0.8846 - age_output_loss: 1.3208 - weight_output_loss: 0.9351 - bag_output_loss: 0.8099 - pose_output_loss: 0.5250 - footwear_output_loss: 0.8072 - emotion_output_loss: 0.8522 - gender_output_acc: 0.8266 - image_quality_output_acc: 0.5794 - age_output_acc: 0.4224 - weight_output_acc: 0.6397 - bag_output_acc: 0.6431 - pose_output_acc: 0.7831 - footwear_output_acc: 0.6434 - emotion_output_acc: 0.7121 - val_loss: 8.0735 - val_gender_output_loss: 0.3787 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.3324 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.8442 - val_pose_output_loss: 0.4932 - val_footwear_output_loss: 0.7877 - val_emotion_output_loss: 0.8877 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5511 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.8150 - val_footwear_output_acc: 0.6612 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.07355, saving model to /content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484_1577624806_model.001.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 214s 595ms/step - loss: 7.9441 - gender_output_loss: 0.3902 - image_quality_output_loss: 0.8838 - age_output_loss: 1.3205 - weight_output_loss: 0.9375 - bag_output_loss: 0.8100 - pose_output_loss: 0.5370 - footwear_output_loss: 0.8135 - emotion_output_loss: 0.8514 - gender_output_acc: 0.8255 - image_quality_output_acc: 0.5825 - age_output_acc: 0.4173 - weight_output_acc: 0.6389 - bag_output_acc: 0.6452 - pose_output_acc: 0.7859 - footwear_output_acc: 0.6356 - emotion_output_acc: 0.7117 - val_loss: 8.2790 - val_gender_output_loss: 0.3882 - val_image_quality_output_loss: 1.0651 - val_age_output_loss: 1.3457 - val_weight_output_loss: 1.0063 - val_bag_output_loss: 0.8460 - val_pose_output_loss: 0.5474 - val_footwear_output_loss: 0.7935 - val_emotion_output_loss: 0.8878 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.4881 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7937 - val_footwear_output_acc: 0.6567 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 8.07355\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 218s 607ms/step - loss: 7.9447 - gender_output_loss: 0.3907 - image_quality_output_loss: 0.8875 - age_output_loss: 1.3255 - weight_output_loss: 0.9380 - bag_output_loss: 0.8109 - pose_output_loss: 0.5365 - footwear_output_loss: 0.8077 - emotion_output_loss: 0.8505 - gender_output_acc: 0.8203 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4182 - weight_output_acc: 0.6391 - bag_output_acc: 0.6438 - pose_output_acc: 0.7806 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.7117 - val_loss: 8.1874 - val_gender_output_loss: 0.3841 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.3479 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8615 - val_pose_output_loss: 0.5251 - val_footwear_output_loss: 0.7969 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.8219 - val_footwear_output_acc: 0.6528 - val_emotion_output_acc: 0.6811\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 8.07355\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 226s 627ms/step - loss: 7.9627 - gender_output_loss: 0.4033 - image_quality_output_loss: 0.8815 - age_output_loss: 1.3237 - weight_output_loss: 0.9401 - bag_output_loss: 0.8156 - pose_output_loss: 0.5346 - footwear_output_loss: 0.8171 - emotion_output_loss: 0.8539 - gender_output_acc: 0.8198 - image_quality_output_acc: 0.5826 - age_output_acc: 0.4225 - weight_output_acc: 0.6383 - bag_output_acc: 0.6394 - pose_output_acc: 0.7840 - footwear_output_acc: 0.6368 - emotion_output_acc: 0.7120 - val_loss: 8.2805 - val_gender_output_loss: 0.4037 - val_image_quality_output_loss: 1.0530 - val_age_output_loss: 1.3474 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8522 - val_pose_output_loss: 0.5007 - val_footwear_output_loss: 0.8604 - val_emotion_output_loss: 0.8915 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5179 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8189 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 8.07355\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 227s 630ms/step - loss: 7.9516 - gender_output_loss: 0.3929 - image_quality_output_loss: 0.8877 - age_output_loss: 1.3200 - weight_output_loss: 0.9412 - bag_output_loss: 0.8159 - pose_output_loss: 0.5386 - footwear_output_loss: 0.8125 - emotion_output_loss: 0.8554 - gender_output_acc: 0.8240 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4181 - weight_output_acc: 0.6374 - bag_output_acc: 0.6391 - pose_output_acc: 0.7822 - footwear_output_acc: 0.6381 - emotion_output_acc: 0.7119 - val_loss: 8.2251 - val_gender_output_loss: 0.4027 - val_image_quality_output_loss: 1.0435 - val_age_output_loss: 1.3364 - val_weight_output_loss: 0.9913 - val_bag_output_loss: 0.8466 - val_pose_output_loss: 0.5163 - val_footwear_output_loss: 0.8153 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.8269 - val_image_quality_output_acc: 0.5188 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.6076 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.8056 - val_footwear_output_acc: 0.6359 - val_emotion_output_acc: 0.6930\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 8.07355\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 229s 636ms/step - loss: 7.9451 - gender_output_loss: 0.3923 - image_quality_output_loss: 0.8852 - age_output_loss: 1.3260 - weight_output_loss: 0.9381 - bag_output_loss: 0.8172 - pose_output_loss: 0.5308 - footwear_output_loss: 0.8182 - emotion_output_loss: 0.8541 - gender_output_acc: 0.8267 - image_quality_output_acc: 0.5790 - age_output_acc: 0.4149 - weight_output_acc: 0.6388 - bag_output_acc: 0.6349 - pose_output_acc: 0.7867 - footwear_output_acc: 0.6260 - emotion_output_acc: 0.7125 - val_loss: 8.1676 - val_gender_output_loss: 0.3791 - val_image_quality_output_loss: 1.0081 - val_age_output_loss: 1.3332 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8592 - val_pose_output_loss: 0.5061 - val_footwear_output_loss: 0.8069 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5258 - val_age_output_acc: 0.4127 - val_weight_output_acc: 0.6096 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.8175 - val_footwear_output_acc: 0.6453 - val_emotion_output_acc: 0.6781\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 8.07355\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 232s 643ms/step - loss: 7.9164 - gender_output_loss: 0.3904 - image_quality_output_loss: 0.8849 - age_output_loss: 1.3225 - weight_output_loss: 0.9354 - bag_output_loss: 0.8143 - pose_output_loss: 0.5330 - footwear_output_loss: 0.8067 - emotion_output_loss: 0.8492 - gender_output_acc: 0.8252 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4214 - weight_output_acc: 0.6396 - bag_output_acc: 0.6420 - pose_output_acc: 0.7885 - footwear_output_acc: 0.6390 - emotion_output_acc: 0.7125 - val_loss: 8.1889 - val_gender_output_loss: 0.3814 - val_image_quality_output_loss: 1.0057 - val_age_output_loss: 1.3414 - val_weight_output_loss: 1.0017 - val_bag_output_loss: 0.8475 - val_pose_output_loss: 0.5543 - val_footwear_output_loss: 0.7956 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.8373 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6305 - val_pose_output_acc: 0.7902 - val_footwear_output_acc: 0.6617 - val_emotion_output_acc: 0.7009\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 8.07355\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.8912 - gender_output_loss: 0.3778 - image_quality_output_loss: 0.8833 - age_output_loss: 1.3160 - weight_output_loss: 0.9362 - bag_output_loss: 0.8102 - pose_output_loss: 0.5257 - footwear_output_loss: 0.8107 - emotion_output_loss: 0.8527 - gender_output_acc: 0.8309 - image_quality_output_acc: 0.5815 - age_output_acc: 0.4227 - weight_output_acc: 0.6406 - bag_output_acc: 0.6425 - pose_output_acc: 0.7869 - footwear_output_acc: 0.6335 - emotion_output_acc: 0.7120 - val_loss: 8.1857 - val_gender_output_loss: 0.3908 - val_image_quality_output_loss: 1.0289 - val_age_output_loss: 1.3362 - val_weight_output_loss: 0.9934 - val_bag_output_loss: 0.8590 - val_pose_output_loss: 0.5168 - val_footwear_output_loss: 0.7946 - val_emotion_output_loss: 0.8877 - val_gender_output_acc: 0.8313 - val_image_quality_output_acc: 0.5223 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6062 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.8095 - val_footwear_output_acc: 0.6627 - val_emotion_output_acc: 0.6925\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 8.07355\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 231s 642ms/step - loss: 7.8770 - gender_output_loss: 0.3822 - image_quality_output_loss: 0.8821 - age_output_loss: 1.3145 - weight_output_loss: 0.9312 - bag_output_loss: 0.8098 - pose_output_loss: 0.5202 - footwear_output_loss: 0.8068 - emotion_output_loss: 0.8521 - gender_output_acc: 0.8244 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4249 - weight_output_acc: 0.6405 - bag_output_acc: 0.6457 - pose_output_acc: 0.7913 - footwear_output_acc: 0.6423 - emotion_output_acc: 0.7112 - val_loss: 8.1807 - val_gender_output_loss: 0.3769 - val_image_quality_output_loss: 1.0712 - val_age_output_loss: 1.3355 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.8424 - val_pose_output_loss: 0.4983 - val_footwear_output_loss: 0.7940 - val_emotion_output_loss: 0.8921 - val_gender_output_acc: 0.8383 - val_image_quality_output_acc: 0.5099 - val_age_output_acc: 0.4043 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6379 - val_pose_output_acc: 0.8189 - val_footwear_output_acc: 0.6558 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 8.07355\n",
            "Epoch 10/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8921 - gender_output_loss: 0.3744 - image_quality_output_loss: 0.8857 - age_output_loss: 1.3156 - weight_output_loss: 0.9374 - bag_output_loss: 0.8090 - pose_output_loss: 0.5312 - footwear_output_loss: 0.8119 - emotion_output_loss: 0.8504 - gender_output_acc: 0.8344 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4237 - weight_output_acc: 0.6403 - bag_output_acc: 0.6452 - pose_output_acc: 0.7829 - footwear_output_acc: 0.6366 - emotion_output_acc: 0.7123Epoch 10/50\n",
            "360/360 [==============================] - 233s 648ms/step - loss: 7.8934 - gender_output_loss: 0.3744 - image_quality_output_loss: 0.8855 - age_output_loss: 1.3158 - weight_output_loss: 0.9375 - bag_output_loss: 0.8094 - pose_output_loss: 0.5312 - footwear_output_loss: 0.8121 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8343 - image_quality_output_acc: 0.5798 - age_output_acc: 0.4237 - weight_output_acc: 0.6403 - bag_output_acc: 0.6451 - pose_output_acc: 0.7831 - footwear_output_acc: 0.6365 - emotion_output_acc: 0.7119 - val_loss: 8.2829 - val_gender_output_loss: 0.3808 - val_image_quality_output_loss: 1.0708 - val_age_output_loss: 1.3342 - val_weight_output_loss: 1.0015 - val_bag_output_loss: 0.8571 - val_pose_output_loss: 0.5569 - val_footwear_output_loss: 0.8144 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5119 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7917 - val_footwear_output_acc: 0.6389 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 8.07355\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 234s 651ms/step - loss: 7.9282 - gender_output_loss: 0.3888 - image_quality_output_loss: 0.8862 - age_output_loss: 1.3216 - weight_output_loss: 0.9364 - bag_output_loss: 0.8130 - pose_output_loss: 0.5416 - footwear_output_loss: 0.8099 - emotion_output_loss: 0.8572 - gender_output_acc: 0.8250 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4188 - weight_output_acc: 0.6391 - bag_output_acc: 0.6382 - pose_output_acc: 0.7830 - footwear_output_acc: 0.6375 - emotion_output_acc: 0.7114 - val_loss: 8.1724 - val_gender_output_loss: 0.3900 - val_image_quality_output_loss: 1.0120 - val_age_output_loss: 1.3448 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8407 - val_pose_output_loss: 0.5152 - val_footwear_output_loss: 0.8089 - val_emotion_output_loss: 0.8908 - val_gender_output_acc: 0.8413 - val_image_quality_output_acc: 0.5481 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6349 - val_pose_output_acc: 0.8080 - val_footwear_output_acc: 0.6538 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 8.07355\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_rjy2_gml2_1577624484.json1577627329_backup\n",
            "End of run with EPOCHS= 50 STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3ESNUrJ6Rli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE=0.020183668*0.15\n",
        "BATCH_SIZE=32\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=50\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "# create train and validation data generators\n",
        "\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=4,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,3.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)\n",
        "#model_name_itr = 'wrn2_rkg_fresh_wrn_'+str(get_curr_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_weights_compile = {'gender_output': 4, \n",
        "                        'image_quality_output': 4, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577514347rd2_model.009.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577557999_model.049.h5')\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_1577544670_1577562060_model.009.h5')\n",
        "\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577595654_model.017.h5')\n",
        "\n",
        "last_saved_weights = '/content/gdrive/My Drive/WRN_Base/assignment5_wrn2_rkg_fresh_wrn_rjy2_1577595402_1577603392_model.081.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Base/\", pattern=\"*.h5\",count=1)[0]\n",
        "print(last_saved_weights)\n",
        "wrn_28_10.load_weights(last_saved_weights)\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     #loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                     patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "print(\"End of run with EPOCHS=\",EPOCHS,\"STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "############################## Rejected as Loss was more ############################\n",
        "########## Reference JSON\n",
        "############/content/gdrive/My Drive/WRN_Base/json_wrn2_rkg_fresh_wrn_1577544670.json1577564491_backup\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx79InfALyOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(weights_file+\"py\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caOrywH7ZKU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 3, \n",
        "                        'age_output': 5, \n",
        "                        'weight_output': 4, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 3, \n",
        "                        'emotion_output': 15}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     weighted_metrics=[\"accuracy\"]\n",
        "     #metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S579mj2m6OBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(weights_file+\"py_1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsf18ITn5POM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Dropout \n",
        "### lossweight change\n",
        "### Augchange\n",
        "\n",
        "### lossweight change\n",
        "\n",
        "### Higher LR\n",
        "### Weight decay increase\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_widrn_acc_run2_sth_rkg_1577280556_model.035.h5')\n",
        "\n",
        "# loss_weights_compile = {'gender_output': 4, \n",
        "#                         'image_quality_output': 3, \n",
        "#                         'age_output': 5, \n",
        "#                         'weight_output': 4, \n",
        "#                         'bag_output': 3, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 3, \n",
        "#                         'emotion_output': 15}\n",
        "\n",
        "loss_weights_compile = {'gender_output': 3, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 1, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 1, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 1, \n",
        "                        'emotion_output': 1}\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7Wj-ZigCEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save_weights('/content/gdrive/My Drive/h5_wrn2_widrn_acc_run3_sth_rkg_1577280556_round1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrvIUyiyvrA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run2_sth_rkg_1577280556.json\" \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run2_sth_rkg_1577280556_round4.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9lhN57rBAVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### No loss weights or class weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLiUc0V1ZL0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_weights_compile = {'gender_output': 1, \n",
        "#                         'image_quality_output': 1, \n",
        "#                         'age_output': 1, \n",
        "#                         'weight_output': 1, \n",
        "#                         'bag_output': 1, \n",
        "#                         'pose_output': 1, \n",
        "#                         'footwear_output': 1, \n",
        "#                         'emotion_output': 1}\n",
        "\n",
        "# wrn_28_10.compile(\n",
        "#      #optimizer=SGD(lr=1.3513402*0.1),\n",
        "#      optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "#      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#      #loss_weights=loss_weights_compile,\n",
        "#      #weighted_metrics=[\"accuracy\"]\n",
        "#      metrics=[\"accuracy\"]\n",
        "# )\n",
        "\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8nm4o25E8eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(model_name_itr+\"py2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LElDU29KFtiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-S1_l-3ZNfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q97oxTd0rHZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_compile = {'gender_output': 3, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 1, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 1, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 1, \n",
        "                        'emotion_output': 1}\n",
        "del wrn_28_10\n",
        "wrn_28_10=create_model()\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_widrn_acc_run3_sth_rkg_1577334362rd2_model.037.h5')\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caDPow5EO2KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run3_sth_rkg_1577371122.json\" \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run3_sth_rkg_1577371122_round5.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43zRtiCN9M68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_weights_compile = {'gender_output': 3, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 1, \n",
        "#                         'weight_output': 1, \n",
        "#                         'bag_output': 2, \n",
        "#                         'pose_output': 3, \n",
        "#                         'footwear_output': 2, \n",
        "#                         'emotion_output': 1}\n",
        "# #del wrn_28_10\n",
        "# wrn_28_10=create_model()\n",
        "# wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_widrn_acc_run3_sth_rkg_1577371122rd2_model.047.h5')\n",
        "# wrn_28_10.compile(\n",
        "#      #optimizer=SGD(lr=1.3513402*0.1),\n",
        "#      optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "#      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#      loss_weights=loss_weights_compile,\n",
        "#      #weighted_metrics=[\"accuracy\"]\n",
        "#      metrics=[\"accuracy\"]\n",
        "# )\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=20\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1RgcU2AYuuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 3, \n",
        "                        'age_output': 5, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 1}\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_widrn_acc_run4_sth_rkg_1577444509rd2_model.066.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=20\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyuh0yfp3D9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run4_sth_rkg_1577444509.json\" \"/content/gdrive/My Drive/json_wrn2_widrn_acc_run4_sth_rkg_1577444509_round2.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BDfV4RHV4dKX",
        "colab": {}
      },
      "source": [
        "STEPS_PER_EPOCH=50\n",
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 1, \n",
        "                        'age_output': 1, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2,\n",
        "                        'emotion_output': 5}\n",
        "\n",
        "clr = CyclicLR(\n",
        "    base_lr=LEARNING_RATE*0.01,\n",
        "    max_lr=LEARNING_RATE,\n",
        "    step_size=STEPS_PER_EPOCH*4,\n",
        "    mode='triangular')\n",
        "\n",
        "\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)\n",
        "callbacks = [checkpoint, \n",
        "             #lr_scheduler,\n",
        "             clr,\n",
        "             TrainingMonitor(figPath=plotPath,jsonPath=jsonPath,startAt=2)]\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_widrn_acc_run4_sth_rkg_1577444509rd2_model.066.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=one_lr_schedule(0)),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcK3vIhrO-hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXe3ltI_-gIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuOro_7LF0RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwUEJewRFxtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Psv83SGIpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b41UaIQ9F_hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BespGPlFHKHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict['val_age_output_acc'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-39qKvhHFHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict_new=get_indexed_results(y_pred_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2VU5EiI1dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def get_indexed_results(y_pred_results):\n",
        "    class_output_arr = ['gender',\n",
        "                        'imagequality',\n",
        "                        'age',\n",
        "                        'weight',\n",
        "                        'carryingbag',\n",
        "                        'bodypose',    \n",
        "                        'footwear',   \n",
        "                        'emotion']\n",
        "    y_pred_dict = {}\n",
        "    for len_val in range(len(y_pred_results)):\n",
        "        y_pred_local = y_pred_results[len_val][:-12]\n",
        "        y_pred_final = np.argmax(y_pred_local, axis=1)\n",
        "        y_pred_dict[class_output_arr[len_val]]=y_pred_final\n",
        "    return y_pred_dict\n",
        "def print_confusion_matrix(y_pred_dict, val_df_to_use, attribute_to_select):\n",
        "    cols_to_select=[col for col in one_hot_df.columns if col.startswith(attribute_to_select)]\n",
        "    y_true = val_df_to_use[cols_to_select].values\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    y_pred=y_pred_dict[attribute_to_select]\n",
        "    #print(y_pred_results[].shape, y_true.shape)\n",
        "    #print(y_true_classes)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    print(\"Confusion Matrix for\", attribute_to_select)\n",
        "    matrix = confusion_matrix(y_true_classes, y_pred)\n",
        "    print(matrix)\n",
        "    true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "    print(\"True Class Dist\",true_class_dist)\n",
        "    pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "    print(\"Predicted class dist\",pred_class_dist)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)\n",
        "\n",
        "y_pred_dict_new=get_indexed_results(y_pred_results)\n",
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvBCk7s2M62K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNRMbh8Jy4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=y_pred_dict['val_emotion_output_acc']\n",
        "true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "print(\"True Class Dist\",true_class_dist)\n",
        "pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "print(\"Predicted class dist\",pred_class_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkFGgymGq47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_gender_output_acc   \n",
        "val_image_quality_output_acc    \n",
        "val_age_output_acc    \n",
        "val_weight_output_acc    \n",
        "val_bag_output_acc   \n",
        "val_pose_output_acc    \n",
        "val_footwear_output_acc   \n",
        "val_emotion_output_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdDtDwn1hc_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save('/content/gdrive/My Drive/wrn_step1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgL4X4gKcbe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = [0] * 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PYLix3brV8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=50\n",
        "test_y = np.linspace(0,EPOCHS,EPOCHS)\n",
        "x=[0, (EPOCHS+1)//5, EPOCHS]\n",
        "y=[LEARNING_RATE*0.001, LEARNING_RATE, LEARNING_RATE*0.001]\n",
        "interp_lr = np.interp(test_y, x, y)\n",
        "def one_lr_schedule(epoch):\n",
        "    print(\"lr:\",interp_lr[epoch])\n",
        "    return interp_lr[epoch]\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InLbMaqiiuIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join('/content/gdrive/', 'My Drive')\n",
        "model_name = 'assignment5_%s_model.{epoch:03d}.h5' % 'wrn'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "callbacks = [checkpoint, lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkzpvNO4ccI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[0]=wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr0VoxQ_v277",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[1]=wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryMZzx87vsNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history[1]=wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LRFinder_new:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            print(\"\")\n",
        "            print(\"Training stopped due to high loss\",loss)\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        # If x_train contains data for multiple inputs, use length of the first input.\n",
        "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
        "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
        "\n",
        "        # Compute number of batches and LR multiplier\n",
        "        num_batches = epochs * N / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=batch_size, epochs=epochs,\n",
        "                       callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "        if steps_per_epoch is None:\n",
        "            try:\n",
        "                steps_per_epoch = len(generator)\n",
        "            except (ValueError, NotImplementedError) as e:\n",
        "                raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                        ' generator based on the '\n",
        "                        '`keras.utils.Sequence`'\n",
        "                        ' class. Please specify `steps_per_epoch` '\n",
        "                        'or use the `keras.utils.Sequence` class.')\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit_generator(generator=generator,\n",
        "                                 epochs=epochs,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 callbacks=[callback],\n",
        "                                 **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(x_scale)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIcGpeISVW0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}