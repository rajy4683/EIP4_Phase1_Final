{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WRN_A5_wide_rkg_wrn_gml_rkg_rjy_gml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EIP4_Phase1_Final/blob/master/WRN_A5_wide_rkg_wrn_gml_rkg_rjy_gml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "ab4936f6-f4fa-4fff-81ac-aef1e527011d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "66d7401e-07d0-44f1-9b2b-6dc65f8380cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6OZ3Li_RAJ",
        "colab_type": "code",
        "outputId": "99cce187-6c4b-4898-d05b-db5661861c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "83504501-2d2b-4007-cf1b-21426c236657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "cd1197f7-e48d-47f3-be0e-7531338f041f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PZZE92dNYM",
        "colab_type": "code",
        "outputId": "4c7fb041-e07d-4b5b-e1ea-7c14a7747868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "one_hot_df.head()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "#class PersonDataGenerator(keras.utils.Sequence):\n",
        "class PersonDataGenerator(Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,normalize=False,aug_flow=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.on_epoch_end()\n",
        "        self.aug_flow=aug_flow\n",
        "        #print(\"Shuffle = \",self.shuffle)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        #print(batch_slice)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        #print(items[\"image_path\"])\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        if(self.aug_flow is not None):\n",
        "            image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
        "        if(self.normalize == True):\n",
        "            train_mean = np.mean(image, axis=(0,1,2))\n",
        "            train_std = np.std(image, axis=(0,1,2))\n",
        "            #print(train_mean, train_std)\n",
        "            normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "            image = normalize(image)\n",
        "\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "3a11a1f3-7b5f-4a31-f290-7ed51e4eed24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmFlcRB7pOTD",
        "colab_type": "code",
        "outputId": "a3ec81b1-5083-4cb9-961d-ef3c1decbdd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_old_df =pd.read_csv('/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_widrn_acc_1577202722.csv') \n",
        "train_df = train_old_df\n",
        "val_old_df = pd.read_csv('/content/gdrive/My Drive/WRN_Extend/val_df_wrn2_widrn_acc_1577202722.csv')\n",
        "val_df=val_old_df\n",
        "print(train_df.shape, val_df.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11537, 28) (2036, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzg--2pv7G4M",
        "colab_type": "code",
        "outputId": "81fb0ad5-0207-48c7-9cf4-d9ae6d3e7d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/7499.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/9410.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/2396.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/9839.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/80.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11532</th>\n",
              "      <td>resized/2692.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11533</th>\n",
              "      <td>resized/2657.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11534</th>\n",
              "      <td>resized/1842.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11535</th>\n",
              "      <td>resized/7815.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11536</th>\n",
              "      <td>resized/649.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11537 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0      resized/7499.jpg              1  ...                        1              0\n",
              "1      resized/9410.jpg              1  ...                        1              0\n",
              "2      resized/2396.jpg              0  ...                        1              0\n",
              "3      resized/9839.jpg              1  ...                        0              1\n",
              "4        resized/80.jpg              0  ...                        0              0\n",
              "...                 ...            ...  ...                      ...            ...\n",
              "11532  resized/2692.jpg              1  ...                        1              0\n",
              "11533  resized/2657.jpg              1  ...                        1              0\n",
              "11534  resized/1842.jpg              0  ...                        1              0\n",
              "11535  resized/7815.jpg              0  ...                        0              1\n",
              "11536   resized/649.jpg              1  ...                        1              0\n",
              "\n",
              "[11537 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIYrhzCAbWv0",
        "colab_type": "code",
        "outputId": "df9fb8f3-d2e7-498e-ff1e-b32f65353a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from datetime import datetime\n",
        "def get_curr_time():\n",
        "    return int(datetime.utcnow().strftime(\"%s\"))\n",
        "\n",
        "model_name_itr = 'wrn2_rkg_fresh_wrn_wide_rjy2_gml2_'+str(get_curr_time())\n",
        "gdrive_home_path=\"/content/gdrive/My Drive/WRN_Extend/\"\n",
        "train_csv=gdrive_home_path+\"train_df_\"+model_name_itr+\".csv\"\n",
        "val_csv=gdrive_home_path+\"val_df_\"+model_name_itr+\".csv\"\n",
        "json_file=gdrive_home_path+\"json_\"+model_name_itr+\".json\"\n",
        "png_file=gdrive_home_path+\"png_\"+model_name_itr+\".png\"\n",
        "weights_file=gdrive_home_path+\"h5_\"+model_name_itr+\".h5\"\n",
        "\n",
        "print(\"Model-name:\",model_name_itr)\n",
        "print(train_csv,val_csv,json_file,png_file,weights_file)\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "# /content/gdrive/My Drive/train_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/val_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722.json\n",
        "#  /content/gdrive/My Drive/png_wrn2_widrn_acc_1577202722.png \n",
        "#  /content/gdrive/My Drive/h5_wrn2_widrn_acc_1577202722.h5"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model-name: wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977\n",
            "/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.csv /content/gdrive/My Drive/WRN_Extend/val_df_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.csv /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json /content/gdrive/My Drive/WRN_Extend/png_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.png /content/gdrive/My Drive/WRN_Extend/h5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1L33i_QqLRe",
        "colab_type": "code",
        "outputId": "9bd75037-5605-4012-8430-ee6db9812f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "print(\"Model-name:\",model_name_itr)\n",
        "for var_name in [train_csv,val_csv,json_file,png_file,weights_file]:\n",
        "  print(var_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model-name: wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977\n",
            "/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.csv\n",
            "/content/gdrive/My Drive/WRN_Extend/val_df_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.csv\n",
            "/content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "/content/gdrive/My Drive/WRN_Extend/png_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.png\n",
            "/content/gdrive/My Drive/WRN_Extend/h5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdL4WRuG2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_weights = [\"gender_output\", \"imagequality_ouput\",\"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\",\"emotion_output\", \"pose_output\"]\n",
        "col_splits = [_gender_cols_, _imagequality_cols_,_age_cols_, _weight_cols_, _carryingbag_cols_, _footwear_cols_, _emotion_cols_, _bodypose_cols_]\n",
        "def get_dist(train_df,equalize_classes=False):\n",
        "    loss_weights = {}\n",
        "    index=0\n",
        "    for selector_column in col_splits:\n",
        "        print(selector_column)\n",
        "        count = []\n",
        "        percentile = []\n",
        "        for age_split in selector_column:\n",
        "            count.append( train_df[selector_column][train_df[age_split] == 1].shape[0])\n",
        "        #print(count, np.round((count/11537.0)*100.0, 2))\n",
        "\n",
        "        max_val = np.max(count)\n",
        "        total_count = np.float32(train_df.shape[0])\n",
        "        #print(count, )\n",
        "        count_weights= [np.round(max_val/current_val,3) for current_val in count]\n",
        "        print(count_weights)\n",
        "        print(np.round((np.asarray(count)/total_count)*100.0, 2))\n",
        "\n",
        "        #print(\"Top Class:\",selector_column[np.argmax(count)],\"Max Count\",np.max(count))\n",
        "        #print(\"Bottom Class:\",selector_column[np.argmin(count)])\n",
        "        #weights_dist = dict(zip(selector_column, count_weights))\n",
        "        \n",
        "        #print(weights_dist)\n",
        "        weights_vals_dist={}\n",
        "        index_val=0\n",
        "        for y in range(len(count_weights)):\n",
        "            weights_vals_dist[y]=count_weights[y]\n",
        "            print(weights_vals_dist[y],y)\n",
        "            #index_val+=index_val\n",
        "            \n",
        "            #loss_weights[output_weights[index]]={x,y}\n",
        "        loss_weights[output_weights[index]]=weights_vals_dist\n",
        "        #if equalize_classes == True:\n",
        "        #    expanded_df = equalize_classwise_dist(train_df, selector_column, count)\n",
        "        #    train_df = train_df.append(expanded_df, ignore_index=True)\n",
        "        index+=1\n",
        "    #print(loss_weights)\n",
        "    return train_df,loss_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcR5l-K0HESp",
        "colab_type": "code",
        "outputId": "3d5037c9-7051-41fb-9731-90ea1fac5568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_,loss_weights_train=get_dist(train_df, equalize_classes=False)\n",
        "loss_weights_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gender_female', 'gender_male']\n",
            "[1.304, 1.0]\n",
            "[43.41 56.59]\n",
            "1.304 0\n",
            "1.0 1\n",
            "['imagequality_Average', 'imagequality_Bad', 'imagequality_Good']\n",
            "[1.0, 3.361, 1.979]\n",
            "[55.47 16.5  28.03]\n",
            "1.0 0\n",
            "3.361 1\n",
            "1.979 2\n",
            "['age_15-25', 'age_25-35', 'age_35-45', 'age_45-55', 'age_55+']\n",
            "[2.172, 1.0, 1.589, 3.556, 7.326]\n",
            "[18.36 39.88 25.1  11.22  5.44]\n",
            "2.172 0\n",
            "1.0 1\n",
            "1.589 2\n",
            "3.556 3\n",
            "7.326 4\n",
            "['weight_normal-healthy', 'weight_over-weight', 'weight_slightly-overweight', 'weight_underweight']\n",
            "[1.0, 9.632, 2.753, 9.931]\n",
            "[63.79  6.62 23.17  6.42]\n",
            "1.0 0\n",
            "9.632 1\n",
            "2.753 2\n",
            "9.931 3\n",
            "['carryingbag_Daily/Office/Work Bag', 'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None']\n",
            "[1.667, 5.837, 1.0]\n",
            "[33.86  9.67 56.46]\n",
            "1.667 0\n",
            "5.837 1\n",
            "1.0 2\n",
            "['footwear_CantSee', 'footwear_Fancy', 'footwear_Normal']\n",
            "[1.23, 2.38, 1.0]\n",
            "[36.4  18.82 44.78]\n",
            "1.23 0\n",
            "2.38 1\n",
            "1.0 2\n",
            "['emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral', 'emotion_Sad']\n",
            "[6.454, 6.05, 1.0, 12.056]\n",
            "[11.04 11.78 71.27  5.91]\n",
            "6.454 0\n",
            "6.05 1\n",
            "1.0 2\n",
            "12.056 3\n",
            "['bodypose_Back', 'bodypose_Front-Frontish', 'bodypose_Side']\n",
            "[3.836, 1.0, 2.786]\n",
            "[16.1  61.74 22.16]\n",
            "3.836 0\n",
            "1.0 1\n",
            "2.786 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output': {0: 2.172, 1: 1.0, 2: 1.589, 3: 3.556, 4: 7.326},\n",
              " 'bag_output': {0: 1.667, 1: 5.837, 2: 1.0},\n",
              " 'emotion_output': {0: 6.454, 1: 6.05, 2: 1.0, 3: 12.056},\n",
              " 'footwear_output': {0: 1.23, 1: 2.38, 2: 1.0},\n",
              " 'gender_output': {0: 1.304, 1: 1.0},\n",
              " 'imagequality_ouput': {0: 1.0, 1: 3.361, 2: 1.979},\n",
              " 'pose_output': {0: 3.836, 1: 1.0, 2: 2.786},\n",
              " 'weight_output': {0: 1.0, 1: 9.632, 2: 2.753, 3: 9.931}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZIV48it799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(iter(batches))\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hco1q4yMxji3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "BATCH_SIZE=16\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2y9D1I-udt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CROP_LENGTH=64\n",
        "train_crops = crop_generator(train_gen, CROP_LENGTH)\n",
        "valid_crops = crop_generator(valid_gen, CROP_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BbkJCyZXZx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_zwfsq5qPZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_batch(data_df, batch_size=32, shuffle=True,normalize=True, selected_field='age_output'):\n",
        "    new_batch = PersonDataGenerator(data_df, batch_size,shuffle, normalize)\n",
        "    images, targets = next(iter(new_batch))\n",
        "    num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "    labels = np.asarray([ np.argmax(targets['age_output'][pos]) for pos in range(len(targets['age_output'])) ])\n",
        "    return images,labels, targets, len(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZu45hUsPkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, y_train, targets, len_train = get_image_batch(train_df, batch_size=32,normalize=True, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usX4GK8btqZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, y_test, targets_test, len_test = get_image_batch(val_df, batch_size=32,normalize=True, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cb1XGwFvwmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_mean_std_for_batch(datagen_process):\n",
        "    image_val,target = next(iter(datagen_process))\n",
        "    print(image_val.shape)\n",
        "    print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )\n",
        "    #print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyX3wQN8ZQz",
        "colab_type": "code",
        "outputId": "00649757-aa14-4b42-b38f-668d6886921e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "\n",
        "images_test, targets_test = next(iter(valid_gen))\n",
        "\n",
        "print(num_units)\n",
        "#print(np.mean(images.round(2), axis=(0,1,2)),np.std(images.round(2), axis=(0,1,2)) )\n",
        "#print(np.mean(images_test.round(2), axis=(0,1,2)),np.std(images_test.round(2), axis=(0,1,2)) )\n",
        "\n",
        "print_mean_std_for_batch(train_gen)\n",
        "print_mean_std_for_batch(valid_gen)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gender': 2, 'image_quality': 3, 'age': 5, 'weight': 4, 'bag': 3, 'pose': 3, 'footwear': 3, 'emotion': 4}\n",
            "(16, 224, 224, 3)\n",
            "[-0.00228527 -0.00100717  0.00214946] [1.0009955 1.0035876 1.0004818]\n",
            "(16, 224, 224, 3)\n",
            "[ 0.0002495   0.00032183 -0.00125983] [1.0028176 1.0014371 1.0019739]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uwZZR7lpd54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, targets_test = next(iter(valid_gen))\n",
        "num_units_test = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets_test.items()}\n",
        "num_units_test\n",
        "#images.shape\n",
        "y_test  = np.asarray([ np.argmax(targets_test['age_output'][pos]) for pos in range(len(targets_test['age_output'])) ]) ## Taking the argmax to select the correct class\n",
        "cv2_imshow(cv2.resize(images_test[0], (images_test[0].shape[1], images_test[0].shape[0])))\n",
        "y_test.shape\n",
        "y_test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjP5xE2CcM_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_single_image(image):\n",
        "    cv2_imshow(cv2.resize(image, (image.shape[1], image.shape[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TKbaDy5pO_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_train, len_test = len(images), len(images_test)\n",
        "\n",
        "# train_mean = np.mean(images, axis=(0,1,2))\n",
        "# train_std = np.std(images, axis=(0,1,2))\n",
        "\n",
        "# normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n",
        "# #pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "# images_norm = normalize(images)\n",
        "# images_test_norm = normalize(images_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FdbGK4nb8gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cv2_imshow(cv2.resize(images_norm[0], (images_norm[0].shape[1], images_norm[0].shape[0])))\n",
        "#images_norm.shape\n",
        "#images_test[0].shape\n",
        "display_single_image(images[10])\n",
        "#display_single_image(images_test_norm[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6-3atp3iAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import time,math\n",
        "############# Weights initializer #################\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMcoEEmPApbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06AAPOdlNSKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSxGw9KOLfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weight_decay = 0.0005\n",
        "weight_decay = 0.001\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1# if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0O2LzQOPzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1,num_units_in=num_units):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    #x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "    output_layers = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(x) for name in num_units_in.keys()]\n",
        "    model = Model(ip, output_layers)\n",
        "\n",
        "    #if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "    #return x\n",
        "\n",
        "def add_final_layer(model,num_units_in):\n",
        "    output_vals = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(model) for name in num_units_in.keys()]\n",
        "    return output_vals\n",
        "\n",
        "def create_model(dropout=0.05):\n",
        "    from keras.utils import plot_model\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "\n",
        "    init = (224, 224, 3)\n",
        "\n",
        "    #wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=2, dropout=0.0)\n",
        "    wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=4, dropout=dropout)\n",
        "    return wrn_28_10\n",
        "    #output_layers = add_final_layer(wrn_28_10_backbone, num_units)\n",
        "    #model = Model(Input(shape=init), output_layers)\n",
        "    #wrn_28_10.summary()\n",
        "\n",
        "    #plot_model(wrn_28_10, \"WRN-16-2.png\", show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJcOZ_VAQzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5leq1Jrxz3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJhxE2F1HmKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrn_28_10.compile(\n",
        "#     optimizer=SGD(lr=0.5),\n",
        "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#     # loss_weights=loss_weights, \n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.compile(\n",
        "    optimizer=SGD(lr=0.049203925),\n",
        "    #,momentum=MOMENTUM, nesterov=True),\n",
        "    #optimizer=SGD(lr=0.01191919191919192),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),     \n",
        "    weighted_metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfzXsa5sVZbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Min: 12.520438 0.57330394\n",
        "#Min: 12.54081 0.586987\n",
        "#Min: 12.628042 0.18920188\n",
        "#del wrn_28_10\n",
        "#Min: 7.8906326 1.3513402\n",
        "def iterate_lr_finder(start_lr=0.1,end_lr=1):\n",
        "  \n",
        "    model=create_model()\n",
        "    loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "    model.compile(\n",
        "        optimizer=SGD(lr=0.049203925),\n",
        "        #,momentum=MOMENTUM, nesterov=True),\n",
        "        #optimizer=SGD(lr=0.01191919191919192),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        loss_weights=loss_weights_compile,  \n",
        "        #metrics=[\"accuracy\"]\n",
        "        weighted_metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "    lr_finder = LRFinder_new(model)\n",
        "    lr_finder.find_generator(train_gen,start_lr=start_lr, end_lr=end_lr,epochs=10,steps_per_epoch=20)#,class_weight=loss_weights_train)\n",
        "    #lr_finder.find_generator(train_gen,start_lr=0.0001, end_lr=1,epochs=10,steps_per_epoch=20)\n",
        "\n",
        "    print(\"#start_lr\",start_lr,\"end_lr\",end_lr)\n",
        "    print(\"#Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "    print(\"#Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])\n",
        "    \n",
        "    del model\n",
        "    return lr_finder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya1WsABSbV-8",
        "colab_type": "code",
        "outputId": "0735626a-220e-4e40-be4e-90086621d40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "lr_1 = iterate_lr_finder(start_lr=0.1,end_lr=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 16s 813ms/step - loss: 29.4717 - gender_output_loss: 0.7113 - image_quality_output_loss: 1.0364 - age_output_loss: 1.5485 - weight_output_loss: 1.0120 - bag_output_loss: 0.9489 - pose_output_loss: 0.9841 - footwear_output_loss: 1.0545 - emotion_output_loss: 1.0867 - gender_output_weighted_acc: 0.5500 - image_quality_output_weighted_acc: 0.4812 - age_output_weighted_acc: 0.3812 - weight_output_weighted_acc: 0.6656 - bag_output_weighted_acc: 0.5656 - pose_output_weighted_acc: 0.5906 - footwear_output_weighted_acc: 0.4156 - emotion_output_weighted_acc: 0.6750\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 6s 316ms/step - loss: 28.0041 - gender_output_loss: 0.6930 - image_quality_output_loss: 0.9674 - age_output_loss: 1.4223 - weight_output_loss: 1.0466 - bag_output_loss: 0.9494 - pose_output_loss: 0.9600 - footwear_output_loss: 1.0506 - emotion_output_loss: 0.8943 - gender_output_weighted_acc: 0.5500 - image_quality_output_weighted_acc: 0.5875 - age_output_weighted_acc: 0.3656 - weight_output_weighted_acc: 0.5969 - bag_output_weighted_acc: 0.5594 - pose_output_weighted_acc: 0.6000 - footwear_output_weighted_acc: 0.3656 - emotion_output_weighted_acc: 0.7281\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 8s 376ms/step - loss: 28.0157 - gender_output_loss: 0.6870 - image_quality_output_loss: 1.0262 - age_output_loss: 1.4772 - weight_output_loss: 0.9907 - bag_output_loss: 0.9531 - pose_output_loss: 0.9523 - footwear_output_loss: 1.0612 - emotion_output_loss: 0.8693 - gender_output_weighted_acc: 0.5781 - image_quality_output_weighted_acc: 0.5031 - age_output_weighted_acc: 0.3844 - weight_output_weighted_acc: 0.6469 - bag_output_weighted_acc: 0.5563 - pose_output_weighted_acc: 0.6000 - footwear_output_weighted_acc: 0.4156 - emotion_output_weighted_acc: 0.7250\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 8s 376ms/step - loss: 27.6900 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9315 - age_output_loss: 1.4055 - weight_output_loss: 1.0412 - bag_output_loss: 0.9352 - pose_output_loss: 0.9478 - footwear_output_loss: 1.0144 - emotion_output_loss: 0.9240 - gender_output_weighted_acc: 0.5563 - image_quality_output_weighted_acc: 0.6094 - age_output_weighted_acc: 0.4437 - weight_output_weighted_acc: 0.6062 - bag_output_weighted_acc: 0.5375 - pose_output_weighted_acc: 0.6031 - footwear_output_weighted_acc: 0.4719 - emotion_output_weighted_acc: 0.7125\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 8s 381ms/step - loss: 27.7649 - gender_output_loss: 0.6953 - image_quality_output_loss: 0.9909 - age_output_loss: 1.4225 - weight_output_loss: 0.9904 - bag_output_loss: 0.9015 - pose_output_loss: 0.9839 - footwear_output_loss: 1.0641 - emotion_output_loss: 0.9272 - gender_output_weighted_acc: 0.5062 - image_quality_output_weighted_acc: 0.5437 - age_output_weighted_acc: 0.3906 - weight_output_weighted_acc: 0.6250 - bag_output_weighted_acc: 0.5125 - pose_output_weighted_acc: 0.5813 - footwear_output_weighted_acc: 0.3906 - emotion_output_weighted_acc: 0.7094\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 8s 375ms/step - loss: 27.7123 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9550 - age_output_loss: 1.4517 - weight_output_loss: 0.9169 - bag_output_loss: 0.9576 - pose_output_loss: 0.9164 - footwear_output_loss: 1.0391 - emotion_output_loss: 1.0068 - gender_output_weighted_acc: 0.5531 - image_quality_output_weighted_acc: 0.5594 - age_output_weighted_acc: 0.4156 - weight_output_weighted_acc: 0.6719 - bag_output_weighted_acc: 0.5125 - pose_output_weighted_acc: 0.6375 - footwear_output_weighted_acc: 0.4781 - emotion_output_weighted_acc: 0.6656\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 8s 379ms/step - loss: 27.5132 - gender_output_loss: 0.6954 - image_quality_output_loss: 1.0235 - age_output_loss: 1.4555 - weight_output_loss: 1.0019 - bag_output_loss: 0.9187 - pose_output_loss: 0.9262 - footwear_output_loss: 1.0150 - emotion_output_loss: 0.9196 - gender_output_weighted_acc: 0.5625 - image_quality_output_weighted_acc: 0.5344 - age_output_weighted_acc: 0.4187 - weight_output_weighted_acc: 0.6312 - bag_output_weighted_acc: 0.5750 - pose_output_weighted_acc: 0.6406 - footwear_output_weighted_acc: 0.4625 - emotion_output_weighted_acc: 0.7031\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 8s 378ms/step - loss: 27.5624 - gender_output_loss: 0.6886 - image_quality_output_loss: 1.0160 - age_output_loss: 1.4957 - weight_output_loss: 0.9746 - bag_output_loss: 0.9685 - pose_output_loss: 0.9094 - footwear_output_loss: 1.0904 - emotion_output_loss: 0.8961 - gender_output_weighted_acc: 0.5437 - image_quality_output_weighted_acc: 0.5312 - age_output_weighted_acc: 0.2906 - weight_output_weighted_acc: 0.6438 - bag_output_weighted_acc: 0.5188 - pose_output_weighted_acc: 0.6438 - footwear_output_weighted_acc: 0.4094 - emotion_output_weighted_acc: 0.7219\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 7s 373ms/step - loss: 27.7668 - gender_output_loss: 0.7046 - image_quality_output_loss: 1.0377 - age_output_loss: 1.4928 - weight_output_loss: 1.0091 - bag_output_loss: 0.9761 - pose_output_loss: 0.9148 - footwear_output_loss: 1.0384 - emotion_output_loss: 0.9685 - gender_output_weighted_acc: 0.5188 - image_quality_output_weighted_acc: 0.5094 - age_output_weighted_acc: 0.3031 - weight_output_weighted_acc: 0.6500 - bag_output_weighted_acc: 0.4969 - pose_output_weighted_acc: 0.6031 - footwear_output_weighted_acc: 0.4094 - emotion_output_weighted_acc: 0.7000\n",
            "Epoch 10/10\n",
            " 7/20 [=========>....................] - ETA: 4s - loss: 73385.2190 - gender_output_loss: 104.2590 - image_quality_output_loss: 77.9357 - age_output_loss: 91.8419 - weight_output_loss: 149.2958 - bag_output_loss: 106.0529 - pose_output_loss: 64.2446 - footwear_output_loss: 26.9643 - emotion_output_loss: 50.1425 - gender_output_weighted_acc: 0.5089 - image_quality_output_weighted_acc: 0.5625 - age_output_weighted_acc: 0.3125 - weight_output_weighted_acc: 0.5357 - bag_output_weighted_acc: 0.5179 - pose_output_weighted_acc: 0.5804 - footwear_output_weighted_acc: 0.3839 - emotion_output_weighted_acc: 0.6161\n",
            "Training stopped due to high loss 513135.75\n",
            "#Max: 513135.75 0.8511381\n",
            "#Min: 24.815857 0.2511886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGkNg0KScLpv",
        "colab_type": "code",
        "outputId": "16597d1e-df7a-4c07-a883-6baf90297e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "lr_1.plot_loss(n_skip_end=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEJCAYAAACT/UyFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBgetG9scRDx",
        "colab_type": "code",
        "outputId": "2880008b-33e8-47a7-d04e-ac2aebc3585b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "lr_2 = iterate_lr_finder(start_lr=0.001,end_lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 10s 508ms/step - loss: 32.6087 - gender_output_loss: 0.6935 - image_quality_output_loss: 1.0969 - age_output_loss: 1.6000 - weight_output_loss: 1.3609 - bag_output_loss: 1.0863 - pose_output_loss: 1.0737 - footwear_output_loss: 1.0977 - emotion_output_loss: 1.3425 - gender_output_weighted_acc: 0.5156 - image_quality_output_weighted_acc: 0.3938 - age_output_weighted_acc: 0.3250 - weight_output_weighted_acc: 0.5563 - bag_output_weighted_acc: 0.4125 - pose_output_weighted_acc: 0.6250 - footwear_output_weighted_acc: 0.3375 - emotion_output_weighted_acc: 0.6594\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 7s 337ms/step - loss: 31.1711 - gender_output_loss: 0.6918 - image_quality_output_loss: 1.0725 - age_output_loss: 1.5493 - weight_output_loss: 1.2653 - bag_output_loss: 1.0442 - pose_output_loss: 1.0237 - footwear_output_loss: 1.0874 - emotion_output_loss: 1.1929 - gender_output_weighted_acc: 0.5781 - image_quality_output_weighted_acc: 0.5750 - age_output_weighted_acc: 0.4031 - weight_output_weighted_acc: 0.6500 - bag_output_weighted_acc: 0.5719 - pose_output_weighted_acc: 0.6312 - footwear_output_weighted_acc: 0.4594 - emotion_output_weighted_acc: 0.7188\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 8s 380ms/step - loss: 28.9018 - gender_output_loss: 0.6848 - image_quality_output_loss: 1.0259 - age_output_loss: 1.4901 - weight_output_loss: 1.0361 - bag_output_loss: 1.0046 - pose_output_loss: 0.9576 - footwear_output_loss: 1.0706 - emotion_output_loss: 0.9714 - gender_output_weighted_acc: 0.5781 - image_quality_output_weighted_acc: 0.5500 - age_output_weighted_acc: 0.3688 - weight_output_weighted_acc: 0.6906 - bag_output_weighted_acc: 0.5156 - pose_output_weighted_acc: 0.6000 - footwear_output_weighted_acc: 0.4406 - emotion_output_weighted_acc: 0.7156\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 8s 381ms/step - loss: 28.1287 - gender_output_loss: 0.6821 - image_quality_output_loss: 1.0081 - age_output_loss: 1.4533 - weight_output_loss: 0.9933 - bag_output_loss: 0.9268 - pose_output_loss: 0.8992 - footwear_output_loss: 1.0307 - emotion_output_loss: 0.9796 - gender_output_weighted_acc: 0.5750 - image_quality_output_weighted_acc: 0.5281 - age_output_weighted_acc: 0.4156 - weight_output_weighted_acc: 0.6406 - bag_output_weighted_acc: 0.5719 - pose_output_weighted_acc: 0.6406 - footwear_output_weighted_acc: 0.4844 - emotion_output_weighted_acc: 0.6781\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 7s 374ms/step - loss: 27.5020 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9847 - age_output_loss: 1.4094 - weight_output_loss: 1.0757 - bag_output_loss: 0.8929 - pose_output_loss: 0.9111 - footwear_output_loss: 1.0523 - emotion_output_loss: 0.8215 - gender_output_weighted_acc: 0.5719 - image_quality_output_weighted_acc: 0.5469 - age_output_weighted_acc: 0.4000 - weight_output_weighted_acc: 0.5844 - bag_output_weighted_acc: 0.5969 - pose_output_weighted_acc: 0.6312 - footwear_output_weighted_acc: 0.4688 - emotion_output_weighted_acc: 0.7562\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 8s 377ms/step - loss: 28.2481 - gender_output_loss: 0.6742 - image_quality_output_loss: 0.9950 - age_output_loss: 1.4865 - weight_output_loss: 1.0202 - bag_output_loss: 0.9428 - pose_output_loss: 0.9721 - footwear_output_loss: 1.0454 - emotion_output_loss: 0.8940 - gender_output_weighted_acc: 0.6000 - image_quality_output_weighted_acc: 0.5469 - age_output_weighted_acc: 0.4000 - weight_output_weighted_acc: 0.6188 - bag_output_weighted_acc: 0.5406 - pose_output_weighted_acc: 0.5844 - footwear_output_weighted_acc: 0.4563 - emotion_output_weighted_acc: 0.7219\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 8s 377ms/step - loss: 28.2498 - gender_output_loss: 0.6762 - image_quality_output_loss: 0.9939 - age_output_loss: 1.4425 - weight_output_loss: 1.0185 - bag_output_loss: 0.9686 - pose_output_loss: 0.9637 - footwear_output_loss: 1.0482 - emotion_output_loss: 0.9263 - gender_output_weighted_acc: 0.6188 - image_quality_output_weighted_acc: 0.5531 - age_output_weighted_acc: 0.3906 - weight_output_weighted_acc: 0.6219 - bag_output_weighted_acc: 0.5094 - pose_output_weighted_acc: 0.5875 - footwear_output_weighted_acc: 0.4313 - emotion_output_weighted_acc: 0.7156\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 8s 379ms/step - loss: 28.2266 - gender_output_loss: 0.6962 - image_quality_output_loss: 0.9656 - age_output_loss: 1.4573 - weight_output_loss: 1.0293 - bag_output_loss: 0.9718 - pose_output_loss: 0.9389 - footwear_output_loss: 1.0370 - emotion_output_loss: 0.9257 - gender_output_weighted_acc: 0.5437 - image_quality_output_weighted_acc: 0.5750 - age_output_weighted_acc: 0.3531 - weight_output_weighted_acc: 0.6406 - bag_output_weighted_acc: 0.5625 - pose_output_weighted_acc: 0.6156 - footwear_output_weighted_acc: 0.4688 - emotion_output_weighted_acc: 0.7125\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 7s 374ms/step - loss: 29.0810 - gender_output_loss: 0.6985 - image_quality_output_loss: 0.9895 - age_output_loss: 1.5110 - weight_output_loss: 1.0731 - bag_output_loss: 1.0103 - pose_output_loss: 0.9430 - footwear_output_loss: 1.0477 - emotion_output_loss: 1.0059 - gender_output_weighted_acc: 0.5500 - image_quality_output_weighted_acc: 0.5656 - age_output_weighted_acc: 0.3312 - weight_output_weighted_acc: 0.6188 - bag_output_weighted_acc: 0.4938 - pose_output_weighted_acc: 0.6250 - footwear_output_weighted_acc: 0.4531 - emotion_output_weighted_acc: 0.7031\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 7s 375ms/step - loss: 28.5500 - gender_output_loss: 0.7017 - image_quality_output_loss: 1.0231 - age_output_loss: 1.4922 - weight_output_loss: 0.9743 - bag_output_loss: 0.9645 - pose_output_loss: 0.9612 - footwear_output_loss: 1.0534 - emotion_output_loss: 0.9710 - gender_output_weighted_acc: 0.4875 - image_quality_output_weighted_acc: 0.5156 - age_output_weighted_acc: 0.3781 - weight_output_weighted_acc: 0.6531 - bag_output_weighted_acc: 0.5375 - pose_output_weighted_acc: 0.6156 - footwear_output_weighted_acc: 0.3938 - emotion_output_weighted_acc: 0.7000\n",
            "#Max: 32.954185 0.001023293\n",
            "#Min: 25.436108 0.06456545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Fbgpt3cuaD",
        "colab_type": "code",
        "outputId": "5b66c7a0-5fb0-48cd-e34e-e1aa25096cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "lr_3 = iterate_lr_finder(start_lr=0.0001,end_lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 11s 541ms/step - loss: 32.9647 - gender_output_loss: 0.6936 - image_quality_output_loss: 1.0982 - age_output_loss: 1.6099 - weight_output_loss: 1.3808 - bag_output_loss: 1.0904 - pose_output_loss: 1.0950 - footwear_output_loss: 1.0979 - emotion_output_loss: 1.3849 - gender_output_weighted_acc: 0.4688 - image_quality_output_weighted_acc: 0.3531 - age_output_weighted_acc: 0.1437 - weight_output_weighted_acc: 0.2812 - bag_output_weighted_acc: 0.5000 - pose_output_weighted_acc: 0.4281 - footwear_output_weighted_acc: 0.3625 - emotion_output_weighted_acc: 0.2438\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 7s 328ms/step - loss: 32.8812 - gender_output_loss: 0.6934 - image_quality_output_loss: 1.0964 - age_output_loss: 1.6076 - weight_output_loss: 1.3756 - bag_output_loss: 1.0898 - pose_output_loss: 1.0906 - footwear_output_loss: 1.0975 - emotion_output_loss: 1.3752 - gender_output_weighted_acc: 0.4906 - image_quality_output_weighted_acc: 0.3938 - age_output_weighted_acc: 0.2094 - weight_output_weighted_acc: 0.5281 - bag_output_weighted_acc: 0.4844 - pose_output_weighted_acc: 0.6031 - footwear_output_weighted_acc: 0.3875 - emotion_output_weighted_acc: 0.6000\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 8s 376ms/step - loss: 32.7814 - gender_output_loss: 0.6924 - image_quality_output_loss: 1.0948 - age_output_loss: 1.6049 - weight_output_loss: 1.3683 - bag_output_loss: 1.0868 - pose_output_loss: 1.0864 - footwear_output_loss: 1.0975 - emotion_output_loss: 1.3651 - gender_output_weighted_acc: 0.5125 - image_quality_output_weighted_acc: 0.5094 - age_output_weighted_acc: 0.2938 - weight_output_weighted_acc: 0.6375 - bag_output_weighted_acc: 0.5094 - pose_output_weighted_acc: 0.6125 - footwear_output_weighted_acc: 0.3844 - emotion_output_weighted_acc: 0.7031\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 8s 387ms/step - loss: 32.6530 - gender_output_loss: 0.6917 - image_quality_output_loss: 1.0928 - age_output_loss: 1.6010 - weight_output_loss: 1.3578 - bag_output_loss: 1.0839 - pose_output_loss: 1.0828 - footwear_output_loss: 1.0953 - emotion_output_loss: 1.3522 - gender_output_weighted_acc: 0.5625 - image_quality_output_weighted_acc: 0.5031 - age_output_weighted_acc: 0.3812 - weight_output_weighted_acc: 0.6719 - bag_output_weighted_acc: 0.5375 - pose_output_weighted_acc: 0.5938 - footwear_output_weighted_acc: 0.4437 - emotion_output_weighted_acc: 0.6844\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 8s 379ms/step - loss: 32.4578 - gender_output_loss: 0.6933 - image_quality_output_loss: 1.0896 - age_output_loss: 1.5957 - weight_output_loss: 1.3421 - bag_output_loss: 1.0749 - pose_output_loss: 1.0781 - footwear_output_loss: 1.0944 - emotion_output_loss: 1.3320 - gender_output_weighted_acc: 0.4719 - image_quality_output_weighted_acc: 0.5469 - age_output_weighted_acc: 0.4062 - weight_output_weighted_acc: 0.6750 - bag_output_weighted_acc: 0.5625 - pose_output_weighted_acc: 0.5906 - footwear_output_weighted_acc: 0.4437 - emotion_output_weighted_acc: 0.7063\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 7s 372ms/step - loss: 32.2061 - gender_output_loss: 0.6913 - image_quality_output_loss: 1.0838 - age_output_loss: 1.5900 - weight_output_loss: 1.3275 - bag_output_loss: 1.0683 - pose_output_loss: 1.0672 - footwear_output_loss: 1.0920 - emotion_output_loss: 1.3039 - gender_output_weighted_acc: 0.5813 - image_quality_output_weighted_acc: 0.5625 - age_output_weighted_acc: 0.4187 - weight_output_weighted_acc: 0.6875 - bag_output_weighted_acc: 0.5813 - pose_output_weighted_acc: 0.6219 - footwear_output_weighted_acc: 0.4719 - emotion_output_weighted_acc: 0.7250\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 8s 380ms/step - loss: 31.9552 - gender_output_loss: 0.6898 - image_quality_output_loss: 1.0825 - age_output_loss: 1.5835 - weight_output_loss: 1.3140 - bag_output_loss: 1.0579 - pose_output_loss: 1.0606 - footwear_output_loss: 1.0892 - emotion_output_loss: 1.2735 - gender_output_weighted_acc: 0.5719 - image_quality_output_weighted_acc: 0.5375 - age_output_weighted_acc: 0.3688 - weight_output_weighted_acc: 0.6312 - bag_output_weighted_acc: 0.6031 - pose_output_weighted_acc: 0.6000 - footwear_output_weighted_acc: 0.4500 - emotion_output_weighted_acc: 0.7125\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 8s 380ms/step - loss: 31.4890 - gender_output_loss: 0.6870 - image_quality_output_loss: 1.0759 - age_output_loss: 1.5723 - weight_output_loss: 1.2807 - bag_output_loss: 1.0458 - pose_output_loss: 1.0472 - footwear_output_loss: 1.0858 - emotion_output_loss: 1.2186 - gender_output_weighted_acc: 0.6094 - image_quality_output_weighted_acc: 0.5375 - age_output_weighted_acc: 0.3719 - weight_output_weighted_acc: 0.6531 - bag_output_weighted_acc: 0.5844 - pose_output_weighted_acc: 0.5813 - footwear_output_weighted_acc: 0.4531 - emotion_output_weighted_acc: 0.7312\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 8s 392ms/step - loss: 30.8801 - gender_output_loss: 0.6838 - image_quality_output_loss: 1.0524 - age_output_loss: 1.5500 - weight_output_loss: 1.2519 - bag_output_loss: 1.0199 - pose_output_loss: 1.0106 - footwear_output_loss: 1.0805 - emotion_output_loss: 1.1733 - gender_output_weighted_acc: 0.6062 - image_quality_output_weighted_acc: 0.5969 - age_output_weighted_acc: 0.4437 - weight_output_weighted_acc: 0.6062 - bag_output_weighted_acc: 0.5844 - pose_output_weighted_acc: 0.6406 - footwear_output_weighted_acc: 0.4688 - emotion_output_weighted_acc: 0.6937\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 8s 376ms/step - loss: 30.0585 - gender_output_loss: 0.6923 - image_quality_output_loss: 1.0550 - age_output_loss: 1.5190 - weight_output_loss: 1.1981 - bag_output_loss: 1.0147 - pose_output_loss: 0.9681 - footwear_output_loss: 1.0695 - emotion_output_loss: 1.0750 - gender_output_weighted_acc: 0.5281 - image_quality_output_weighted_acc: 0.5062 - age_output_weighted_acc: 0.4219 - weight_output_weighted_acc: 0.6094 - bag_output_weighted_acc: 0.5594 - pose_output_weighted_acc: 0.6531 - footwear_output_weighted_acc: 0.4906 - emotion_output_weighted_acc: 0.7156\n",
            "#start_lr 0.0001 end_lr 0.001\n",
            "#Max: 33.01841 0.0001083927\n",
            "#Min: 28.892464 0.00094406144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyzEOah2Z_mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_finder.get_best_lr(1,1,1))\n",
        "print(lr_finder.get_best_lr(10,1,1))\n",
        "print(lr_finder.get_best_lr(20,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND4LUezqnx96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses = pd.DataFrame({'lrs':lr_finder.lrs, 'losses':lr_finder.losses})\n",
        "lrs_to_losses.loc[lrs_to_losses.shape[0]-1]\n",
        "sma_10_lr = lr_finder.get_best_lr(10,1,1)\n",
        "lrs_to_losses.loc[lrs_to_losses[lrs_to_losses['lrs'].between(sma_10_lr*0.1, sma_10_lr) ]['losses'].idxmin()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PasUsGbpr80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6LOmnuLSdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.min(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMw7fR8XQK1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2TfaqULMg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss(n_skip_end=1)\n",
        "#lr_finder.plot_loss(n_skip_end=1,x_scale='linear')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5qv_Ndp8Q-",
        "colab_type": "code",
        "outputId": "ff2da6fd-5887-4d39-8a1a-a17fa8daed09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "for (k,v) in num_units.items():\n",
        "    print(k,v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gender 2\n",
            "image_quality 3\n",
            "age 5\n",
            "weight 4\n",
            "bag 3\n",
            "pose 3\n",
            "footwear 3\n",
            "emotion 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qS64vScOD5",
        "colab_type": "code",
        "outputId": "54bf7d30-b64c-4e45-fbc8-184ced7265e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#wrn_28_10=create_model()\n",
        "from keras.models import load_model\n",
        "# import the necessary packages\n",
        "from keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# class LossHistory(keras.callbacks.Callback):\n",
        "# \tdef on_train_begin(self, logs={}):\n",
        "# \t\tprint(\"Clearing saved content on training start\")\n",
        "# \t\tself.losses = []\n",
        "# \t\tself.best = np.Inf\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0, backup_hist=True):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\t\tself.backup_hist = backup_hist\n",
        "\t\tprint(\"JSON path:\",self.jsonPath)\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\t\t#self.losses = []\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath) and (self.backup_hist == True):\n",
        "\t\t\t\t#self.H = json.loads(open(self.jsonPath).read())\n",
        "\t\t\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t\t\tprint(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\t\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\n",
        "\t\t\t\t# # check to see if a starting epoch was supplied\n",
        "\t\t\t\t# if self.startAt > 0:\n",
        "\t\t\t\t# \t# loop over the entries in the history log and\n",
        "\t\t\t\t# \t# trim any entries that are past the starting\n",
        "\t\t\t\t# \t# epoch\n",
        "\t\t\t\t# \tfor k in self.H.keys():\n",
        "\t\t\t\t# \t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\tdef on_train_end(self, logs={}):\n",
        "\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t#print(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\t\tprint(\"Current JSON PATH:\",self.jsonPath)\n",
        "\t\tprint(\"Final JSON PATH:\",backup_file_name)\t\t\n",
        "\n",
        "import os\n",
        "plotPath = png_file\n",
        "jsonPath = json_file\n",
        "print(plotPath,jsonPath)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/png_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.png /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC-pZQP-WaNO",
        "colab_type": "code",
        "outputId": "8ba18e8c-b299-4937-ab4a-53971c9ebd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from datetime import datetime\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join('/content/gdrive/', 'My Drive')\n",
        "\n",
        "model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr+\"rd2\")\n",
        "print(model_name)\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "# #                              save_best_only=True)\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True,mode='min')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233rd2_model.{epoch:03d}.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1HpOmnIWPtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=50\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#LEARNING_RATE=0.18883973*0.1\n",
        "#LEARNING_RATE=0.5037587*0.1\n",
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=100\n",
        "test_y = np.linspace(0,EPOCHS,EPOCHS)\n",
        "x=[0, (EPOCHS+1)//5, EPOCHS]\n",
        "y=[LEARNING_RATE*0.01, LEARNING_RATE, LEARNING_RATE*0.0001]\n",
        "interp_lr = np.interp(test_y, x, y)\n",
        "def one_lr_schedule(epoch):\n",
        "    # if(epoch <= 15):\n",
        "    #     print(\"lr:\",interp_lr[epoch+84],epoch)\n",
        "    #     return interp_lr[epoch+84]\n",
        "    print(\"lr:\",interp_lr[epoch],epoch)\n",
        "    return interp_lr[epoch]\n",
        "#interp_values = np.interp(, [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)\n",
        "# callbacks = [checkpoint, lr_scheduler,TrainingMonitor(figPath=plotPath,\n",
        "#                                                       jsonPath=jsonPath,startAt=2)]\n",
        "callbacks = [checkpoint, clr,TrainingMonitor(figPath=plotPath,\n",
        "                                                      jsonPath=jsonPath,startAt=0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHx46QbROE-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_dir = os.path.join('/content/gdrive/', 'My Drive/WRN_Extend')\n",
        "\n",
        "\n",
        "def generate_new_callbacks(steps_per_epoch=50,\n",
        "                           epoch_count=50,\n",
        "                           min_lr=0.00001, \n",
        "                           max_lr=0.1,\n",
        "                           patience=25,\n",
        "                           check_point=True,\n",
        "                           clr_mode='triangular',\n",
        "                           clr_multiplier=4):\n",
        "    model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr+\"_\"+str(get_curr_time()))\n",
        "    print(model_name)\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    filepath = os.path.join(save_dir, model_name)\n",
        "    local_clr = CyclicLR(base_lr=min_lr,\n",
        "                            max_lr=max_lr,\n",
        "                            step_size=steps_per_epoch*clr_multiplier,\n",
        "                            mode=clr_mode)\n",
        "\n",
        "    training_mon = TrainingMonitor(figPath=plotPath,\n",
        "                                   jsonPath=jsonPath,\n",
        "                                   startAt=0)\n",
        "    ########## Introduced after 2x100 Epochs\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                    patience=patience, \n",
        "                                                    restore_best_weights=True)\n",
        "    print(\"Returning new callback array with steps_per_epoch=\",steps_per_epoch,\n",
        "          \"min_lr=\",min_lr,\n",
        "          \"max_lr=\",max_lr,\n",
        "          \"epoch_count=\",epoch_count,\n",
        "          \"patience=\",patience\n",
        "          )\n",
        "    callback_array = [local_clr, early_stop,training_mon]\n",
        "    if(check_point == True):\n",
        "        checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                                monitor='val_loss',\n",
        "                                verbose=1,\n",
        "                                save_best_only=True,\n",
        "                                mode='min')\n",
        "        callback_array.append(checkpoint)\n",
        "    \n",
        "    return callback_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09u1PORzY1LS",
        "colab_type": "code",
        "outputId": "94776d21-8ae5-413f-f460-d58c8d434663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.callbacks import CyclicLR\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-soxqqh64\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-soxqqh64\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101065 sha256=3652db5e80a58fe2b0e65cd94b3f2aa188a5c6d4df7e5abdfedc77606abc7ce4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-utt2zxb6/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUomQO7jBhQ",
        "colab_type": "code",
        "outputId": "49c2e408-a7f6-417b-ca83-141f2982ebb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output': {0: 2.172, 1: 1.0, 2: 1.589, 3: 3.556, 4: 7.326},\n",
              " 'bag_output': {0: 1.667, 1: 5.837, 2: 1.0},\n",
              " 'emotion_output': {0: 6.454, 1: 6.05, 2: 1.0, 3: 12.056},\n",
              " 'footwear_output': {0: 1.23, 1: 2.38, 2: 1.0},\n",
              " 'gender_output': {0: 1.304, 1: 1.0},\n",
              " 'imagequality_ouput': {0: 1.0, 1: 3.361, 2: 1.979},\n",
              " 'pose_output': {0: 3.836, 1: 1.0, 2: 2.786},\n",
              " 'weight_output': {0: 1.0, 1: 9.632, 2: 2.753, 3: 9.931}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG49QJrPglub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model_iterations(model,\n",
        "                         re_compile=True,\n",
        "                         epoch_count=50, \n",
        "                         steps_per_epoch=50, \n",
        "                         min_lr=LEARNING_RATE*0.01, \n",
        "                         max_lr=LEARNING_RATE,\n",
        "                         loss_weights_compile={},\n",
        "                         loss_weights_train={}                         \n",
        "                         ):\n",
        "\n",
        "\n",
        "    if re_compile == True:\n",
        "        model.compile(\n",
        "            #optimizer=SGD(lr=1.3513402*0.1),\n",
        "            optimizer=SGD(lr=min_lr),\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            loss_weights=loss_weights_compile,\n",
        "            #weighted_metrics=[\"accuracy\"]\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        workers=4, \n",
        "        epochs=1,\n",
        "        verbose=1,\n",
        "        class_weight=loss_weights_train,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=generate_new_callbacks(steps_per_epoch=steps_per_epoch, min_lrm=in_lr, max_lr=max_lr, epoch_count=epoch_count)\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzipUJFJjYRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10=create_model()\n",
        "run_model_iterations(wrn_28_10,\n",
        "                    re_compile=True,\n",
        "                    epoch_count=1, \n",
        "                    steps_per_epoch=1, \n",
        "                    min_lr=LEARNING_RATE*0.01, \n",
        "                    max_lr=LEARNING_RATE,\n",
        "                    loss_weights_compile={},\n",
        "                    loss_weights_train={})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay1_bpi-aW_I",
        "colab_type": "code",
        "outputId": "8d5f5b5b-fd26-4076-fbe9-4e65a492b948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100\n",
            "Epoch 1/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 58.1959 - gender_output_loss: 0.7809 - image_quality_output_loss: 1.0712 - age_output_loss: 3.3250 - weight_output_loss: 3.1586 - bag_output_loss: 1.7701 - pose_output_loss: 1.8670 - footwear_output_loss: 1.4521 - emotion_output_loss: 3.3465 - gender_output_acc: 0.5539 - image_quality_output_acc: 0.4440 - age_output_acc: 0.3168 - weight_output_acc: 0.5776 - bag_output_acc: 0.4634 - pose_output_acc: 0.5948 - footwear_output_acc: 0.4418 - emotion_output_acc: 0.7004\n",
            "30/30 [==============================] - 30s 1s/step - loss: 57.5037 - gender_output_loss: 0.7785 - image_quality_output_loss: 1.0708 - age_output_loss: 3.3000 - weight_output_loss: 3.0777 - bag_output_loss: 1.7404 - pose_output_loss: 1.8278 - footwear_output_loss: 1.4480 - emotion_output_loss: 3.3142 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.4437 - age_output_acc: 0.3167 - weight_output_acc: 0.5875 - bag_output_acc: 0.4729 - pose_output_acc: 0.6062 - footwear_output_acc: 0.4417 - emotion_output_acc: 0.7021 - val_loss: 28.0711 - val_gender_output_loss: 0.6933 - val_image_quality_output_loss: 1.0056 - val_age_output_loss: 1.4338 - val_weight_output_loss: 1.0216 - val_bag_output_loss: 0.9297 - val_pose_output_loss: 0.9306 - val_footwear_output_loss: 1.0471 - val_emotion_output_loss: 0.9213 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4277 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 28.07107, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.001.h5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 23s 766ms/step - loss: 53.2489 - gender_output_loss: 0.7799 - image_quality_output_loss: 0.9793 - age_output_loss: 3.0756 - weight_output_loss: 2.8068 - bag_output_loss: 1.5869 - pose_output_loss: 1.6405 - footwear_output_loss: 1.4569 - emotion_output_loss: 2.9752 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4229 - weight_output_acc: 0.6021 - bag_output_acc: 0.5688 - pose_output_acc: 0.6500 - footwear_output_acc: 0.4542 - emotion_output_acc: 0.7021 - val_loss: 28.0218 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 0.9927 - val_age_output_loss: 1.4299 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.9368 - val_pose_output_loss: 0.9336 - val_footwear_output_loss: 1.0401 - val_emotion_output_loss: 0.9369 - val_gender_output_acc: 0.5433 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4301 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00002: val_loss improved from 28.07107 to 28.02183, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.002.h5\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 24s 794ms/step - loss: 50.6846 - gender_output_loss: 0.7805 - image_quality_output_loss: 1.0164 - age_output_loss: 2.8958 - weight_output_loss: 2.2649 - bag_output_loss: 1.6391 - pose_output_loss: 1.6316 - footwear_output_loss: 1.3544 - emotion_output_loss: 2.9219 - gender_output_acc: 0.5437 - image_quality_output_acc: 0.5188 - age_output_acc: 0.3646 - weight_output_acc: 0.6562 - bag_output_acc: 0.5396 - pose_output_acc: 0.6479 - footwear_output_acc: 0.4646 - emotion_output_acc: 0.7000 - val_loss: 28.2517 - val_gender_output_loss: 0.7122 - val_image_quality_output_loss: 0.9907 - val_age_output_loss: 1.4433 - val_weight_output_loss: 1.0272 - val_bag_output_loss: 0.9354 - val_pose_output_loss: 0.9420 - val_footwear_output_loss: 1.0321 - val_emotion_output_loss: 0.9487 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4306 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 28.02183\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 53.3861 - gender_output_loss: 0.7864 - image_quality_output_loss: 1.0117 - age_output_loss: 2.9464 - weight_output_loss: 2.9931 - bag_output_loss: 1.5935 - pose_output_loss: 1.8035 - footwear_output_loss: 1.4563 - emotion_output_loss: 2.8568 - gender_output_acc: 0.5250 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3917 - weight_output_acc: 0.5938 - bag_output_acc: 0.5729 - pose_output_acc: 0.6292 - footwear_output_acc: 0.4375 - emotion_output_acc: 0.7146 - val_loss: 27.9442 - val_gender_output_loss: 0.6949 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.4265 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 0.9348 - val_pose_output_loss: 0.9403 - val_footwear_output_loss: 1.0309 - val_emotion_output_loss: 0.9296 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4341 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 28.02183\n",
            "\n",
            "Epoch 00004: val_loss improved from 28.02183 to 27.94422, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.004.h5\n",
            "Epoch 5/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 54.7842 - gender_output_loss: 0.7725 - image_quality_output_loss: 0.9774 - age_output_loss: 3.2607 - weight_output_loss: 2.9147 - bag_output_loss: 1.8183 - pose_output_loss: 1.8913 - footwear_output_loss: 1.4505 - emotion_output_loss: 2.7463 - gender_output_acc: 0.5819 - image_quality_output_acc: 0.5690 - age_output_acc: 0.2996 - weight_output_acc: 0.6336 - bag_output_acc: 0.5366 - pose_output_acc: 0.5905 - footwear_output_acc: 0.4203 - emotion_output_acc: 0.7198\n",
            "Epoch 00004: val_loss improved from 28.02183 to 27.94422, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.004.h5\n",
            "30/30 [==============================] - 24s 798ms/step - loss: 54.5409 - gender_output_loss: 0.7751 - image_quality_output_loss: 0.9750 - age_output_loss: 3.2362 - weight_output_loss: 2.8931 - bag_output_loss: 1.8158 - pose_output_loss: 1.8804 - footwear_output_loss: 1.4498 - emotion_output_loss: 2.7366 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3083 - weight_output_acc: 0.6354 - bag_output_acc: 0.5375 - pose_output_acc: 0.5938 - footwear_output_acc: 0.4187 - emotion_output_acc: 0.7208 - val_loss: 27.9851 - val_gender_output_loss: 0.6893 - val_image_quality_output_loss: 0.9878 - val_age_output_loss: 1.4371 - val_weight_output_loss: 1.0028 - val_bag_output_loss: 0.9328 - val_pose_output_loss: 0.9404 - val_footwear_output_loss: 1.0245 - val_emotion_output_loss: 0.9323 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4158 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 27.94422\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 51.6722 - gender_output_loss: 0.7764 - image_quality_output_loss: 1.0069 - age_output_loss: 2.8112 - weight_output_loss: 2.9442 - bag_output_loss: 1.6380 - pose_output_loss: 1.7742 - footwear_output_loss: 1.4365 - emotion_output_loss: 2.6117 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5292 - age_output_acc: 0.3938 - weight_output_acc: 0.6271 - bag_output_acc: 0.5521 - pose_output_acc: 0.6167 - footwear_output_acc: 0.4437 - emotion_output_acc: 0.7146 - val_loss: 27.7514 - val_gender_output_loss: 0.6906 - val_image_quality_output_loss: 0.9872 - val_age_output_loss: 1.4203 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9285 - val_pose_output_loss: 0.9265 - val_footwear_output_loss: 1.0261 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4439 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00006: val_loss improved from 27.94422 to 27.75138, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.006.h5\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 24s 786ms/step - loss: 50.8076 - gender_output_loss: 0.7719 - image_quality_output_loss: 1.0182 - age_output_loss: 2.9813 - weight_output_loss: 2.4898 - bag_output_loss: 1.4448 - pose_output_loss: 1.8969 - footwear_output_loss: 1.3994 - emotion_output_loss: 2.6359 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5104 - age_output_acc: 0.3875 - weight_output_acc: 0.6396 - bag_output_acc: 0.5750 - pose_output_acc: 0.5771 - footwear_output_acc: 0.4521 - emotion_output_acc: 0.7167 - val_loss: 27.7375 - val_gender_output_loss: 0.6895 - val_image_quality_output_loss: 0.9888 - val_age_output_loss: 1.4198 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9239 - val_pose_output_loss: 0.9326 - val_footwear_output_loss: 1.0243 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4326 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00007: val_loss improved from 27.75138 to 27.73750, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.007.h5\n",
            "\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 24s 790ms/step - loss: 53.6796 - gender_output_loss: 0.7665 - image_quality_output_loss: 0.9811 - age_output_loss: 2.9027 - weight_output_loss: 2.6319 - bag_output_loss: 1.5401 - pose_output_loss: 1.7862 - footwear_output_loss: 1.3364 - emotion_output_loss: 3.3912 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4187 - weight_output_acc: 0.6167 - bag_output_acc: 0.5896 - pose_output_acc: 0.6021 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.6750 - val_loss: 27.7973 - val_gender_output_loss: 0.6907 - val_image_quality_output_loss: 0.9881 - val_age_output_loss: 1.4242 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.9234 - val_pose_output_loss: 0.9314 - val_footwear_output_loss: 1.0216 - val_emotion_output_loss: 0.9279 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4419 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 27.73750\n",
            "Epoch 9/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 51.2933 - gender_output_loss: 0.7750 - image_quality_output_loss: 0.9558 - age_output_loss: 2.8893 - weight_output_loss: 2.7339 - bag_output_loss: 1.6956 - pose_output_loss: 1.5724 - footwear_output_loss: 1.4080 - emotion_output_loss: 2.7480 - gender_output_acc: 0.5668 - image_quality_output_acc: 0.5754 - age_output_acc: 0.4095 - weight_output_acc: 0.6099 - bag_output_acc: 0.5776 - pose_output_acc: 0.6530 - footwear_output_acc: 0.4569 - emotion_output_acc: 0.7112\n",
            "Epoch 00008: val_loss did not improve from 27.73750\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 50.6260 - gender_output_loss: 0.7748 - image_quality_output_loss: 0.9558 - age_output_loss: 2.8410 - weight_output_loss: 2.6633 - bag_output_loss: 1.6715 - pose_output_loss: 1.5844 - footwear_output_loss: 1.4145 - emotion_output_loss: 2.6884 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4187 - weight_output_acc: 0.6208 - bag_output_acc: 0.5792 - pose_output_acc: 0.6479 - footwear_output_acc: 0.4521 - emotion_output_acc: 0.7167 - val_loss: 27.7302 - val_gender_output_loss: 0.6886 - val_image_quality_output_loss: 0.9877 - val_age_output_loss: 1.4251 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9254 - val_pose_output_loss: 0.9352 - val_footwear_output_loss: 1.0179 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4464 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00009: val_loss improved from 27.73750 to 27.73017, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.009.h5\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 24s 792ms/step - loss: 50.9117 - gender_output_loss: 0.7838 - image_quality_output_loss: 0.9936 - age_output_loss: 2.9296 - weight_output_loss: 2.6787 - bag_output_loss: 1.6235 - pose_output_loss: 1.7269 - footwear_output_loss: 1.4060 - emotion_output_loss: 2.5705 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3958 - weight_output_acc: 0.6250 - bag_output_acc: 0.5875 - pose_output_acc: 0.6208 - footwear_output_acc: 0.4437 - emotion_output_acc: 0.7354 - val_loss: 27.7571 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 0.9863 - val_age_output_loss: 1.4218 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9275 - val_pose_output_loss: 0.9319 - val_footwear_output_loss: 1.0230 - val_emotion_output_loss: 0.9204 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5502 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 27.73017\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 52.6611 - gender_output_loss: 0.7798 - image_quality_output_loss: 0.9456 - age_output_loss: 2.9829 - weight_output_loss: 2.2191 - bag_output_loss: 1.7859 - pose_output_loss: 1.8971 - footwear_output_loss: 1.3900 - emotion_output_loss: 3.0854 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.6042 - age_output_acc: 0.3688 - weight_output_acc: 0.6604 - bag_output_acc: 0.5312 - pose_output_acc: 0.5875 - footwear_output_acc: 0.4979 - emotion_output_acc: 0.6812 - val_loss: 27.7543 - val_gender_output_loss: 0.6853 - val_image_quality_output_loss: 0.9889 - val_age_output_loss: 1.4314 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9262 - val_pose_output_loss: 0.9242 - val_footwear_output_loss: 1.0182 - val_emotion_output_loss: 0.9256 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4336 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 27.73017\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 49.2485 - gender_output_loss: 0.7699 - image_quality_output_loss: 1.0329 - age_output_loss: 2.8259 - weight_output_loss: 2.2482 - bag_output_loss: 1.3951 - pose_output_loss: 1.6544 - footwear_output_loss: 1.3846 - emotion_output_loss: 2.8092 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5062 - age_output_acc: 0.3938 - weight_output_acc: 0.6438 - bag_output_acc: 0.5750 - pose_output_acc: 0.6417 - footwear_output_acc: 0.4646 - emotion_output_acc: 0.7146 - val_loss: 28.0322 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 1.0015 - val_age_output_loss: 1.4339 - val_weight_output_loss: 1.0247 - val_bag_output_loss: 0.9305 - val_pose_output_loss: 0.9300 - val_footwear_output_loss: 1.0367 - val_emotion_output_loss: 0.9384 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4345 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 27.73017\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 50.8520 - gender_output_loss: 0.7773 - image_quality_output_loss: 0.9927 - age_output_loss: 2.8069 - weight_output_loss: 2.7089 - bag_output_loss: 1.6792 - pose_output_loss: 1.6326 - footwear_output_loss: 1.4029 - emotion_output_loss: 2.6970 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4042 - weight_output_acc: 0.6396 - bag_output_acc: 0.5083 - pose_output_acc: 0.6479 - footwear_output_acc: 0.4479 - emotion_output_acc: 0.7229 - val_loss: 27.7514 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4237 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.9304 - val_pose_output_loss: 0.9311 - val_footwear_output_loss: 1.0186 - val_emotion_output_loss: 0.9216 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4532 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 27.73017\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 52.3529 - gender_output_loss: 0.7745 - image_quality_output_loss: 1.0180 - age_output_loss: 2.8143 - weight_output_loss: 2.4187 - bag_output_loss: 1.4666 - pose_output_loss: 1.7126 - footwear_output_loss: 1.3506 - emotion_output_loss: 3.3994 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5188 - age_output_acc: 0.4021 - weight_output_acc: 0.6750 - bag_output_acc: 0.5896 - pose_output_acc: 0.6292 - footwear_output_acc: 0.4583 - emotion_output_acc: 0.6625 - val_loss: 27.7373 - val_gender_output_loss: 0.6889 - val_image_quality_output_loss: 0.9879 - val_age_output_loss: 1.4184 - val_weight_output_loss: 0.9965 - val_bag_output_loss: 0.9251 - val_pose_output_loss: 0.9285 - val_footwear_output_loss: 1.0117 - val_emotion_output_loss: 0.9306 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5354 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 27.73017\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 52.3529 - gender_output_loss: 0.7745 - image_quality_output_loss: 1.0180 - age_output_loss: 2.8143 - weight_output_loss: 2.4187 - bag_output_loss: 1.4666 - pose_output_loss: 1.7126 - footwear_output_loss: 1.3506 - emotion_output_loss: 3.3994 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5188 - age_output_acc: 0.4021 - weight_output_acc: 0.6750 - bag_output_acc: 0.5896 - pose_output_acc: 0.6292 - footwear_output_acc: 0.4583 - emotion_output_acc: 0.6625 - val_loss: 27.7373 - val_gender_output_loss: 0.6889 - val_image_quality_output_loss: 0.9879 - val_age_output_loss: 1.4184 - val_weight_output_loss: 0.9965 - val_bag_output_loss: 0.9251 - val_pose_output_loss: 0.9285 - val_footwear_output_loss: 1.0117 - val_emotion_output_loss: 0.9306 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5354 - val_emotion_output_acc: 0.7062\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 51.4058 - gender_output_loss: 0.7966 - image_quality_output_loss: 1.0096 - age_output_loss: 2.9207 - weight_output_loss: 2.6420 - bag_output_loss: 1.7901 - pose_output_loss: 1.7219 - footwear_output_loss: 1.3762 - emotion_output_loss: 2.6211 - gender_output_acc: 0.5188 - image_quality_output_acc: 0.5271 - age_output_acc: 0.3625 - weight_output_acc: 0.6417 - bag_output_acc: 0.5271 - pose_output_acc: 0.6271 - footwear_output_acc: 0.4833 - emotion_output_acc: 0.7167 - val_loss: 27.6066 - val_gender_output_loss: 0.6876 - val_image_quality_output_loss: 0.9863 - val_age_output_loss: 1.4189 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.9213 - val_pose_output_loss: 0.9274 - val_footwear_output_loss: 1.0051 - val_emotion_output_loss: 0.9149 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5492 - val_emotion_output_acc: 0.7062\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss improved from 27.73017 to 27.60662, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.015.h5\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 51.6557 - gender_output_loss: 0.7689 - image_quality_output_loss: 0.9622 - age_output_loss: 2.8718 - weight_output_loss: 2.7064 - bag_output_loss: 1.6347 - pose_output_loss: 1.7258 - footwear_output_loss: 1.4054 - emotion_output_loss: 2.8217 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4500 - weight_output_acc: 0.6208 - bag_output_acc: 0.5729 - pose_output_acc: 0.6375 - footwear_output_acc: 0.4875 - emotion_output_acc: 0.7042 - val_loss: 27.6355 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.4234 - val_weight_output_loss: 0.9887 - val_bag_output_loss: 0.9226 - val_pose_output_loss: 0.9254 - val_footwear_output_loss: 1.0090 - val_emotion_output_loss: 0.9149 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4808 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 27.60662\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 24s 814ms/step - loss: 51.4018 - gender_output_loss: 0.7707 - image_quality_output_loss: 0.9973 - age_output_loss: 2.8115 - weight_output_loss: 2.7055 - bag_output_loss: 1.6468 - pose_output_loss: 1.8364 - footwear_output_loss: 1.3417 - emotion_output_loss: 2.7409 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5271 - age_output_acc: 0.3833 - weight_output_acc: 0.6375 - bag_output_acc: 0.5854 - pose_output_acc: 0.5979 - footwear_output_acc: 0.4917 - emotion_output_acc: 0.7125 - val_loss: 27.6103 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4179 - val_weight_output_loss: 0.9880 - val_bag_output_loss: 0.9218 - val_pose_output_loss: 0.9262 - val_footwear_output_loss: 1.0091 - val_emotion_output_loss: 0.9163 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4631 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 27.60662\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 24s 806ms/step - loss: 50.0993 - gender_output_loss: 0.7681 - image_quality_output_loss: 0.9620 - age_output_loss: 2.5544 - weight_output_loss: 2.7634 - bag_output_loss: 1.5828 - pose_output_loss: 1.8170 - footwear_output_loss: 1.3623 - emotion_output_loss: 2.7010 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4563 - weight_output_acc: 0.6271 - bag_output_acc: 0.5708 - pose_output_acc: 0.6021 - footwear_output_acc: 0.4625 - emotion_output_acc: 0.7104 - val_loss: 27.8348 - val_gender_output_loss: 0.6886 - val_image_quality_output_loss: 0.9976 - val_age_output_loss: 1.4584 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.9311 - val_pose_output_loss: 0.9255 - val_footwear_output_loss: 1.0035 - val_emotion_output_loss: 0.9239 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5295 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 27.60662\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 25s 818ms/step - loss: 50.1873 - gender_output_loss: 0.7639 - image_quality_output_loss: 0.9806 - age_output_loss: 2.8260 - weight_output_loss: 2.7106 - bag_output_loss: 1.7363 - pose_output_loss: 1.8618 - footwear_output_loss: 1.3133 - emotion_output_loss: 2.3612 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3833 - weight_output_acc: 0.6396 - bag_output_acc: 0.5583 - pose_output_acc: 0.5958 - footwear_output_acc: 0.5167 - emotion_output_acc: 0.7521 - val_loss: 27.6412 - val_gender_output_loss: 0.6888 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.9249 - val_pose_output_loss: 0.9285 - val_footwear_output_loss: 1.0141 - val_emotion_output_loss: 0.9195 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4400 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 27.60662\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 25s 831ms/step - loss: 50.2726 - gender_output_loss: 0.7769 - image_quality_output_loss: 0.9701 - age_output_loss: 2.8594 - weight_output_loss: 2.6608 - bag_output_loss: 1.4610 - pose_output_loss: 1.8156 - footwear_output_loss: 1.5027 - emotion_output_loss: 2.5340 - gender_output_acc: 0.5562 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4313 - weight_output_acc: 0.6375 - bag_output_acc: 0.5583 - pose_output_acc: 0.6062 - footwear_output_acc: 0.3979 - emotion_output_acc: 0.7417 - val_loss: 27.6566 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9894 - val_age_output_loss: 1.4180 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9217 - val_pose_output_loss: 0.9287 - val_footwear_output_loss: 1.0265 - val_emotion_output_loss: 0.9239 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4242 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 27.60662\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 25s 822ms/step - loss: 51.3122 - gender_output_loss: 0.7793 - image_quality_output_loss: 0.9784 - age_output_loss: 2.9680 - weight_output_loss: 2.8787 - bag_output_loss: 1.5396 - pose_output_loss: 1.8004 - footwear_output_loss: 1.3837 - emotion_output_loss: 2.5316 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3979 - weight_output_acc: 0.6438 - bag_output_acc: 0.5792 - pose_output_acc: 0.6104 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.7292 - val_loss: 27.7644 - val_gender_output_loss: 0.6879 - val_image_quality_output_loss: 0.9869 - val_age_output_loss: 1.4371 - val_weight_output_loss: 0.9974 - val_bag_output_loss: 0.9281 - val_pose_output_loss: 0.9272 - val_footwear_output_loss: 1.0265 - val_emotion_output_loss: 0.9214 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4104 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 27.60662\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 25s 824ms/step - loss: 48.8182 - gender_output_loss: 0.7754 - image_quality_output_loss: 0.9635 - age_output_loss: 2.7117 - weight_output_loss: 2.6405 - bag_output_loss: 1.4380 - pose_output_loss: 1.8450 - footwear_output_loss: 1.4446 - emotion_output_loss: 2.3671 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4146 - weight_output_acc: 0.6167 - bag_output_acc: 0.5854 - pose_output_acc: 0.5958 - footwear_output_acc: 0.4750 - emotion_output_acc: 0.7437 - val_loss: 27.6348 - val_gender_output_loss: 0.6769 - val_image_quality_output_loss: 0.9882 - val_age_output_loss: 1.4249 - val_weight_output_loss: 0.9919 - val_bag_output_loss: 0.9211 - val_pose_output_loss: 0.9308 - val_footwear_output_loss: 1.0125 - val_emotion_output_loss: 0.9217 - val_gender_output_acc: 0.5723 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5581 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5217 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 27.60662\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 25s 833ms/step - loss: 53.4833 - gender_output_loss: 0.7704 - image_quality_output_loss: 0.9480 - age_output_loss: 3.1782 - weight_output_loss: 2.7771 - bag_output_loss: 1.5548 - pose_output_loss: 1.8120 - footwear_output_loss: 1.4455 - emotion_output_loss: 2.9130 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5896 - age_output_acc: 0.3562 - weight_output_acc: 0.6438 - bag_output_acc: 0.5833 - pose_output_acc: 0.6021 - footwear_output_acc: 0.4292 - emotion_output_acc: 0.7063 - val_loss: 27.6272 - val_gender_output_loss: 0.6773 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.4228 - val_weight_output_loss: 0.9963 - val_bag_output_loss: 0.9196 - val_pose_output_loss: 0.9289 - val_footwear_output_loss: 1.0195 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.5497 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4808 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 27.60662\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 23s 761ms/step - loss: 51.1428 - gender_output_loss: 0.7789 - image_quality_output_loss: 0.9466 - age_output_loss: 2.8663 - weight_output_loss: 2.4483 - bag_output_loss: 1.6323 - pose_output_loss: 1.6884 - footwear_output_loss: 1.4211 - emotion_output_loss: 2.9305 - gender_output_acc: 0.5333 - image_quality_output_acc: 0.5896 - age_output_acc: 0.3979 - weight_output_acc: 0.6708 - bag_output_acc: 0.5188 - pose_output_acc: 0.6229 - footwear_output_acc: 0.4625 - emotion_output_acc: 0.6979 - val_loss: 27.5182 - val_gender_output_loss: 0.6722 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4129 - val_weight_output_loss: 0.9894 - val_bag_output_loss: 0.9164 - val_pose_output_loss: 0.9257 - val_footwear_output_loss: 1.0065 - val_emotion_output_loss: 0.9198 - val_gender_output_acc: 0.5782 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5157 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00024: val_loss improved from 27.60662 to 27.51815, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.024.h5\n",
            "Epoch 25/100\n",
            "\n",
            "30/30 [==============================] - 27s 894ms/step - loss: 49.1518 - gender_output_loss: 0.7634 - image_quality_output_loss: 0.9856 - age_output_loss: 2.8891 - weight_output_loss: 2.5013 - bag_output_loss: 1.4032 - pose_output_loss: 1.7846 - footwear_output_loss: 1.3691 - emotion_output_loss: 2.4844 - gender_output_acc: 0.5896 - image_quality_output_acc: 0.5437 - age_output_acc: 0.3917 - weight_output_acc: 0.6458 - bag_output_acc: 0.5854 - pose_output_acc: 0.6125 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.7250 - val_loss: 27.4585 - val_gender_output_loss: 0.6723 - val_image_quality_output_loss: 0.9880 - val_age_output_loss: 1.4122 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.9139 - val_pose_output_loss: 0.9238 - val_footwear_output_loss: 1.0026 - val_emotion_output_loss: 0.9158 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5162 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00025: val_loss improved from 27.51815 to 27.45850, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.025.h5\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 47.7816 - gender_output_loss: 0.7680 - image_quality_output_loss: 1.0151 - age_output_loss: 2.6016 - weight_output_loss: 2.5453 - bag_output_loss: 1.4831 - pose_output_loss: 1.6242 - footwear_output_loss: 1.3710 - emotion_output_loss: 2.4397 - gender_output_acc: 0.5833 - image_quality_output_acc: 0.5167 - age_output_acc: 0.4500 - weight_output_acc: 0.6396 - bag_output_acc: 0.5688 - pose_output_acc: 0.6479 - footwear_output_acc: 0.4896 - emotion_output_acc: 0.7312 - val_loss: 27.4454 - val_gender_output_loss: 0.6687 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.9144 - val_pose_output_loss: 0.9249 - val_footwear_output_loss: 1.0035 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.5684 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5591 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5148 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00026: val_loss improved from 27.45850 to 27.44542, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.026.h5\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 51.7079 - gender_output_loss: 0.7707 - image_quality_output_loss: 0.9546 - age_output_loss: 3.0398 - weight_output_loss: 2.6208 - bag_output_loss: 1.5035 - pose_output_loss: 1.9698 - footwear_output_loss: 1.4150 - emotion_output_loss: 2.6603 - gender_output_acc: 0.5875 - image_quality_output_acc: 0.5958 - age_output_acc: 0.3625 - weight_output_acc: 0.6167 - bag_output_acc: 0.5708 - pose_output_acc: 0.5708 - footwear_output_acc: 0.4563 - emotion_output_acc: 0.7250 - val_loss: 27.5694 - val_gender_output_loss: 0.6771 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.4217 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.9183 - val_pose_output_loss: 0.9305 - val_footwear_output_loss: 1.0154 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5010 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 27.44542\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 52.7273 - gender_output_loss: 0.7622 - image_quality_output_loss: 0.9809 - age_output_loss: 3.0512 - weight_output_loss: 3.0511 - bag_output_loss: 1.5774 - pose_output_loss: 1.6811 - footwear_output_loss: 1.4265 - emotion_output_loss: 2.7299 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3750 - weight_output_acc: 0.6458 - bag_output_acc: 0.5896 - pose_output_acc: 0.6313 - footwear_output_acc: 0.4479 - emotion_output_acc: 0.7021 - val_loss: 27.6446 - val_gender_output_loss: 0.6746 - val_image_quality_output_loss: 0.9875 - val_age_output_loss: 1.4180 - val_weight_output_loss: 1.0151 - val_bag_output_loss: 0.9206 - val_pose_output_loss: 0.9234 - val_footwear_output_loss: 1.0181 - val_emotion_output_loss: 0.9266 - val_gender_output_acc: 0.5571 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4892 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 27.44542\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 24s 804ms/step - loss: 52.5022 - gender_output_loss: 0.7656 - image_quality_output_loss: 0.9713 - age_output_loss: 2.9503 - weight_output_loss: 2.9599 - bag_output_loss: 1.5912 - pose_output_loss: 1.6951 - footwear_output_loss: 1.3546 - emotion_output_loss: 2.8641 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3938 - weight_output_acc: 0.5979 - bag_output_acc: 0.5521 - pose_output_acc: 0.6354 - footwear_output_acc: 0.4958 - emotion_output_acc: 0.6979 - val_loss: 27.5009 - val_gender_output_loss: 0.6802 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.9188 - val_pose_output_loss: 0.9246 - val_footwear_output_loss: 1.0077 - val_emotion_output_loss: 0.9200 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5103 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 27.44542\n",
            "Epoch 30/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 50.9396 - gender_output_loss: 0.7488 - image_quality_output_loss: 0.9345 - age_output_loss: 2.9284 - weight_output_loss: 2.5826 - bag_output_loss: 1.5642 - pose_output_loss: 1.7052 - footwear_output_loss: 1.4149 - emotion_output_loss: 2.7901 - gender_output_acc: 0.6034 - image_quality_output_acc: 0.6013 - age_output_acc: 0.4009 - weight_output_acc: 0.6207 - bag_output_acc: 0.6013 - pose_output_acc: 0.6315 - footwear_output_acc: 0.5065 - emotion_output_acc: 0.7091Epoch 30/100\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 27.44542\n",
            "30/30 [==============================] - 25s 818ms/step - loss: 50.7276 - gender_output_loss: 0.7509 - image_quality_output_loss: 0.9314 - age_output_loss: 2.9301 - weight_output_loss: 2.5547 - bag_output_loss: 1.5586 - pose_output_loss: 1.7455 - footwear_output_loss: 1.4219 - emotion_output_loss: 2.7274 - gender_output_acc: 0.6000 - image_quality_output_acc: 0.6062 - age_output_acc: 0.3958 - weight_output_acc: 0.6229 - bag_output_acc: 0.6000 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5042 - emotion_output_acc: 0.7167 - val_loss: 27.5243 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9891 - val_age_output_loss: 1.4198 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.9198 - val_pose_output_loss: 0.9262 - val_footwear_output_loss: 1.0112 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3883 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4592 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 27.44542\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 25s 817ms/step - loss: 52.9331 - gender_output_loss: 0.7745 - image_quality_output_loss: 0.9636 - age_output_loss: 2.7324 - weight_output_loss: 2.7084 - bag_output_loss: 1.6152 - pose_output_loss: 1.6383 - footwear_output_loss: 1.4281 - emotion_output_loss: 3.3698 - gender_output_acc: 0.5562 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4167 - weight_output_acc: 0.6438 - bag_output_acc: 0.5833 - pose_output_acc: 0.6354 - footwear_output_acc: 0.4604 - emotion_output_acc: 0.6708 - val_loss: 27.3920 - val_gender_output_loss: 0.6591 - val_image_quality_output_loss: 0.9889 - val_age_output_loss: 1.4050 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.9158 - val_pose_output_loss: 0.9216 - val_footwear_output_loss: 1.0013 - val_emotion_output_loss: 0.9281 - val_gender_output_acc: 0.5807 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4911 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00031: val_loss improved from 27.44542 to 27.39198, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.031.h5\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 24s 795ms/step - loss: 51.0351 - gender_output_loss: 0.7717 - image_quality_output_loss: 0.9548 - age_output_loss: 2.8974 - weight_output_loss: 2.7630 - bag_output_loss: 1.7465 - pose_output_loss: 1.8467 - footwear_output_loss: 1.3811 - emotion_output_loss: 2.4646 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3958 - weight_output_acc: 0.6292 - bag_output_acc: 0.5167 - pose_output_acc: 0.6000 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.7312 - val_loss: 27.3761 - val_gender_output_loss: 0.6676 - val_image_quality_output_loss: 0.9865 - val_age_output_loss: 1.4067 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9152 - val_pose_output_loss: 0.9219 - val_footwear_output_loss: 1.0032 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.5763 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5153 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00032: val_loss improved from 27.39198 to 27.37608, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.032.h5\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 49.2092 - gender_output_loss: 0.7719 - image_quality_output_loss: 1.0108 - age_output_loss: 2.7594 - weight_output_loss: 2.5603 - bag_output_loss: 1.4746 - pose_output_loss: 1.7835 - footwear_output_loss: 1.3621 - emotion_output_loss: 2.5312 - gender_output_acc: 0.5500 - image_quality_output_acc: 0.5250 - age_output_acc: 0.3938 - weight_output_acc: 0.6375 - bag_output_acc: 0.5958 - pose_output_acc: 0.6083 - footwear_output_acc: 0.5021 - emotion_output_acc: 0.7375 - val_loss: 27.3372 - val_gender_output_loss: 0.6648 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9130 - val_pose_output_loss: 0.9215 - val_footwear_output_loss: 0.9940 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.5792 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5315 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "\n",
            "Epoch 00033: val_loss improved from 27.37608 to 27.33722, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.033.h5\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 24s 800ms/step - loss: 51.8815 - gender_output_loss: 0.7498 - image_quality_output_loss: 0.9731 - age_output_loss: 3.1146 - weight_output_loss: 2.7048 - bag_output_loss: 1.6752 - pose_output_loss: 1.7229 - footwear_output_loss: 1.3325 - emotion_output_loss: 2.6762 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3812 - weight_output_acc: 0.6042 - bag_output_acc: 0.5437 - pose_output_acc: 0.6333 - footwear_output_acc: 0.5167 - emotion_output_acc: 0.7208 - val_loss: 27.4347 - val_gender_output_loss: 0.6814 - val_image_quality_output_loss: 1.0067 - val_age_output_loss: 1.4058 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9167 - val_pose_output_loss: 0.9258 - val_footwear_output_loss: 0.9980 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.5468 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4705 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 27.33722\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 48.9167 - gender_output_loss: 0.7507 - image_quality_output_loss: 1.0061 - age_output_loss: 2.6324 - weight_output_loss: 2.2543 - bag_output_loss: 1.6305 - pose_output_loss: 1.6464 - footwear_output_loss: 1.3287 - emotion_output_loss: 2.8326 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5354 - age_output_acc: 0.4437 - weight_output_acc: 0.6625 - bag_output_acc: 0.5208 - pose_output_acc: 0.6458 - footwear_output_acc: 0.5583 - emotion_output_acc: 0.7063 - val_loss: 27.3222 - val_gender_output_loss: 0.6602 - val_image_quality_output_loss: 0.9864 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.9154 - val_pose_output_loss: 0.9235 - val_footwear_output_loss: 0.9886 - val_emotion_output_loss: 0.9167 - val_gender_output_acc: 0.5846 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5079 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00035: val_loss improved from 27.33722 to 27.32216, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.035.h5\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 50.0754 - gender_output_loss: 0.7681 - image_quality_output_loss: 0.9799 - age_output_loss: 2.9946 - weight_output_loss: 2.3564 - bag_output_loss: 1.6227 - pose_output_loss: 1.7160 - footwear_output_loss: 1.3364 - emotion_output_loss: 2.6401 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5396 - age_output_acc: 0.3917 - weight_output_acc: 0.6417 - bag_output_acc: 0.5688 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5042 - emotion_output_acc: 0.7167 - val_loss: 28.0005 - val_gender_output_loss: 0.6741 - val_image_quality_output_loss: 1.0148 - val_age_output_loss: 1.4252 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.9589 - val_pose_output_loss: 0.9412 - val_footwear_output_loss: 1.0917 - val_emotion_output_loss: 0.9446 - val_gender_output_acc: 0.5753 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4070 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 27.32216\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 24s 811ms/step - loss: 53.7176 - gender_output_loss: 0.7689 - image_quality_output_loss: 0.9806 - age_output_loss: 2.9118 - weight_output_loss: 2.7802 - bag_output_loss: 1.4894 - pose_output_loss: 1.8267 - footwear_output_loss: 1.4453 - emotion_output_loss: 3.2802 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3625 - weight_output_acc: 0.6313 - bag_output_acc: 0.5437 - pose_output_acc: 0.5958 - footwear_output_acc: 0.4521 - emotion_output_acc: 0.6812 - val_loss: 27.4061 - val_gender_output_loss: 0.6764 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9165 - val_pose_output_loss: 0.9304 - val_footwear_output_loss: 0.9959 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.5482 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5487 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 27.32216\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 27.32216\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 24s 806ms/step - loss: 49.1139 - gender_output_loss: 0.7520 - image_quality_output_loss: 0.9803 - age_output_loss: 2.9410 - weight_output_loss: 2.2358 - bag_output_loss: 1.7382 - pose_output_loss: 1.7388 - footwear_output_loss: 1.2807 - emotion_output_loss: 2.4810 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3812 - weight_output_acc: 0.6729 - bag_output_acc: 0.5562 - pose_output_acc: 0.6187 - footwear_output_acc: 0.5083 - emotion_output_acc: 0.7417 - val_loss: 27.3734 - val_gender_output_loss: 0.6699 - val_image_quality_output_loss: 0.9851 - val_age_output_loss: 1.4172 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.9168 - val_pose_output_loss: 0.9226 - val_footwear_output_loss: 0.9995 - val_emotion_output_loss: 0.9194 - val_gender_output_acc: 0.5512 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5182 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 27.32216\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 50.0627 - gender_output_loss: 0.7725 - image_quality_output_loss: 1.0042 - age_output_loss: 2.7413 - weight_output_loss: 2.4049 - bag_output_loss: 1.5100 - pose_output_loss: 1.8224 - footwear_output_loss: 1.3901 - emotion_output_loss: 2.8243 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4375 - weight_output_acc: 0.6562 - bag_output_acc: 0.6062 - pose_output_acc: 0.6083 - footwear_output_acc: 0.5083 - emotion_output_acc: 0.6979 - val_loss: 27.2492 - val_gender_output_loss: 0.6549 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4040 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9189 - val_pose_output_loss: 0.9220 - val_footwear_output_loss: 0.9924 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.5881 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5128 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00039: val_loss improved from 27.32216 to 27.24917, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.039.h5\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 25s 831ms/step - loss: 51.5126 - gender_output_loss: 0.7728 - image_quality_output_loss: 1.0041 - age_output_loss: 2.8409 - weight_output_loss: 2.7486 - bag_output_loss: 1.7116 - pose_output_loss: 1.6845 - footwear_output_loss: 1.4112 - emotion_output_loss: 2.7718 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4042 - weight_output_acc: 0.6208 - bag_output_acc: 0.5729 - pose_output_acc: 0.6250 - footwear_output_acc: 0.4958 - emotion_output_acc: 0.6938 - val_loss: 27.3541 - val_gender_output_loss: 0.6667 - val_image_quality_output_loss: 0.9851 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.9225 - val_pose_output_loss: 0.9244 - val_footwear_output_loss: 1.0071 - val_emotion_output_loss: 0.9156 - val_gender_output_acc: 0.5684 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5153 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 27.24917\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 48.8744 - gender_output_loss: 0.7670 - image_quality_output_loss: 1.0400 - age_output_loss: 2.6463 - weight_output_loss: 2.5740 - bag_output_loss: 1.6143 - pose_output_loss: 1.6261 - footwear_output_loss: 1.3352 - emotion_output_loss: 2.5779 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.4875 - age_output_acc: 0.4292 - weight_output_acc: 0.6521 - bag_output_acc: 0.5688 - pose_output_acc: 0.6604 - footwear_output_acc: 0.5271 - emotion_output_acc: 0.7250 - val_loss: 27.2324 - val_gender_output_loss: 0.6539 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4034 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.9204 - val_pose_output_loss: 0.9213 - val_footwear_output_loss: 0.9838 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.6004 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5118 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00041: val_loss improved from 27.24917 to 27.23235, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.041.h5\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 24s 805ms/step - loss: 49.8531 - gender_output_loss: 0.7723 - image_quality_output_loss: 0.9976 - age_output_loss: 2.8161 - weight_output_loss: 2.5941 - bag_output_loss: 1.4825 - pose_output_loss: 1.8141 - footwear_output_loss: 1.4279 - emotion_output_loss: 2.5684 - gender_output_acc: 0.5833 - image_quality_output_acc: 0.5417 - age_output_acc: 0.4187 - weight_output_acc: 0.6396 - bag_output_acc: 0.5708 - pose_output_acc: 0.6000 - footwear_output_acc: 0.4812 - emotion_output_acc: 0.7333 - val_loss: 27.3451 - val_gender_output_loss: 0.6690 - val_image_quality_output_loss: 0.9854 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.9181 - val_pose_output_loss: 0.9247 - val_footwear_output_loss: 1.0024 - val_emotion_output_loss: 0.9170 - val_gender_output_acc: 0.5832 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5512 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 27.23235\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 24s 815ms/step - loss: 50.0595 - gender_output_loss: 0.7850 - image_quality_output_loss: 0.9933 - age_output_loss: 2.7399 - weight_output_loss: 2.5148 - bag_output_loss: 1.7233 - pose_output_loss: 1.6444 - footwear_output_loss: 1.3417 - emotion_output_loss: 2.7430 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4083 - weight_output_acc: 0.6542 - bag_output_acc: 0.5146 - pose_output_acc: 0.6375 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7125 - val_loss: 27.2259 - val_gender_output_loss: 0.6629 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4059 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.9133 - val_pose_output_loss: 0.9236 - val_footwear_output_loss: 0.9817 - val_emotion_output_loss: 0.9146 - val_gender_output_acc: 0.5915 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5586 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5600 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00043: val_loss improved from 27.23235 to 27.22586, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.043.h5\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 24s 798ms/step - loss: 52.5751 - gender_output_loss: 0.7612 - image_quality_output_loss: 0.9806 - age_output_loss: 3.0203 - weight_output_loss: 2.7105 - bag_output_loss: 1.5248 - pose_output_loss: 1.9523 - footwear_output_loss: 1.3477 - emotion_output_loss: 2.8802 - gender_output_acc: 0.5833 - image_quality_output_acc: 0.5458 - age_output_acc: 0.3646 - weight_output_acc: 0.6438 - bag_output_acc: 0.5771 - pose_output_acc: 0.5688 - footwear_output_acc: 0.5104 - emotion_output_acc: 0.7042 - val_loss: 27.3416 - val_gender_output_loss: 0.6675 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4315 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.9099 - val_pose_output_loss: 0.9280 - val_footwear_output_loss: 0.9758 - val_emotion_output_loss: 0.9199 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3637 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5659 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 27.22586\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 51.1174 - gender_output_loss: 0.7667 - image_quality_output_loss: 0.9923 - age_output_loss: 2.8690 - weight_output_loss: 2.8561 - bag_output_loss: 1.4726 - pose_output_loss: 1.7394 - footwear_output_loss: 1.3756 - emotion_output_loss: 2.7370 - gender_output_acc: 0.5521 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3896 - weight_output_acc: 0.6521 - bag_output_acc: 0.5854 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5083 - emotion_output_acc: 0.7188 - val_loss: 27.2879 - val_gender_output_loss: 0.6744 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4135 - val_weight_output_loss: 0.9858 - val_bag_output_loss: 0.9112 - val_pose_output_loss: 0.9242 - val_footwear_output_loss: 0.9860 - val_emotion_output_loss: 0.9170 - val_gender_output_acc: 0.5497 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5512 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 27.22586\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 51.9258 - gender_output_loss: 0.7568 - image_quality_output_loss: 0.9656 - age_output_loss: 3.1758 - weight_output_loss: 2.4908 - bag_output_loss: 1.6603 - pose_output_loss: 1.7464 - footwear_output_loss: 1.3632 - emotion_output_loss: 2.7872 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3771 - weight_output_acc: 0.6458 - bag_output_acc: 0.5271 - pose_output_acc: 0.6187 - footwear_output_acc: 0.4875 - emotion_output_acc: 0.7021 - val_loss: 27.2903 - val_gender_output_loss: 0.6721 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4149 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.9104 - val_pose_output_loss: 0.9239 - val_footwear_output_loss: 0.9791 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.5541 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5231 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 27.22586\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 51.5787 - gender_output_loss: 0.7583 - image_quality_output_loss: 0.9492 - age_output_loss: 2.7289 - weight_output_loss: 2.8071 - bag_output_loss: 1.6253 - pose_output_loss: 1.8552 - footwear_output_loss: 1.3028 - emotion_output_loss: 2.8940 - gender_output_acc: 0.5896 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4354 - weight_output_acc: 0.6292 - bag_output_acc: 0.5437 - pose_output_acc: 0.5896 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7063 - val_loss: 27.2165 - val_gender_output_loss: 0.6577 - val_image_quality_output_loss: 0.9869 - val_age_output_loss: 1.4119 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9127 - val_pose_output_loss: 0.9249 - val_footwear_output_loss: 0.9668 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.5925 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5463 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5295 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00047: val_loss improved from 27.22586 to 27.21652, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.047.h5\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 23s 765ms/step - loss: 53.5685 - gender_output_loss: 0.7758 - image_quality_output_loss: 0.9379 - age_output_loss: 3.2404 - weight_output_loss: 2.8420 - bag_output_loss: 1.6643 - pose_output_loss: 1.8121 - footwear_output_loss: 1.3906 - emotion_output_loss: 2.8108 - gender_output_acc: 0.5479 - image_quality_output_acc: 0.5979 - age_output_acc: 0.3646 - weight_output_acc: 0.6187 - bag_output_acc: 0.5542 - pose_output_acc: 0.6000 - footwear_output_acc: 0.5333 - emotion_output_acc: 0.7125 - val_loss: 27.1812 - val_gender_output_loss: 0.6587 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9082 - val_pose_output_loss: 0.9247 - val_footwear_output_loss: 0.9718 - val_emotion_output_loss: 0.9200 - val_gender_output_acc: 0.5989 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5536 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5344 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00048: val_loss improved from 27.21652 to 27.18120, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.048.h5\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 27s 895ms/step - loss: 50.6620 - gender_output_loss: 0.7322 - image_quality_output_loss: 0.9903 - age_output_loss: 2.8809 - weight_output_loss: 2.5926 - bag_output_loss: 1.5071 - pose_output_loss: 1.7846 - footwear_output_loss: 1.3002 - emotion_output_loss: 2.8102 - gender_output_acc: 0.6229 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3833 - weight_output_acc: 0.6146 - bag_output_acc: 0.5750 - pose_output_acc: 0.6062 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.7104 - val_loss: 27.2046 - val_gender_output_loss: 0.6672 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4071 - val_weight_output_loss: 0.9882 - val_bag_output_loss: 0.9091 - val_pose_output_loss: 0.9223 - val_footwear_output_loss: 0.9669 - val_emotion_output_loss: 0.9202 - val_gender_output_acc: 0.5714 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5487 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 27.18120\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 25s 826ms/step - loss: 51.4605 - gender_output_loss: 0.7584 - image_quality_output_loss: 0.9878 - age_output_loss: 3.2568 - weight_output_loss: 2.4797 - bag_output_loss: 1.6146 - pose_output_loss: 1.6073 - footwear_output_loss: 1.3917 - emotion_output_loss: 2.7141 - gender_output_acc: 0.5896 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3500 - weight_output_acc: 0.6479 - bag_output_acc: 0.5729 - pose_output_acc: 0.6521 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7042 - val_loss: 27.2900 - val_gender_output_loss: 0.6756 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4190 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9138 - val_pose_output_loss: 0.9238 - val_footwear_output_loss: 0.9806 - val_emotion_output_loss: 0.9207 - val_gender_output_acc: 0.5453 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5502 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 27.18120\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 24s 812ms/step - loss: 51.4714 - gender_output_loss: 0.7472 - image_quality_output_loss: 0.9832 - age_output_loss: 3.1932 - weight_output_loss: 2.3562 - bag_output_loss: 1.5011 - pose_output_loss: 1.6886 - footwear_output_loss: 1.3492 - emotion_output_loss: 2.9279 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3979 - weight_output_acc: 0.6604 - bag_output_acc: 0.5646 - pose_output_acc: 0.6313 - footwear_output_acc: 0.4938 - emotion_output_acc: 0.6979 - val_loss: 27.2697 - val_gender_output_loss: 0.6731 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.4146 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.9122 - val_pose_output_loss: 0.9217 - val_footwear_output_loss: 0.9758 - val_emotion_output_loss: 0.9280 - val_gender_output_acc: 0.5463 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5556 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5005 - val_emotion_output_acc: 0.7062\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 27.18120\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 25s 818ms/step - loss: 50.6776 - gender_output_loss: 0.7897 - image_quality_output_loss: 0.9870 - age_output_loss: 2.9117 - weight_output_loss: 2.3930 - bag_output_loss: 1.6533 - pose_output_loss: 1.7678 - footwear_output_loss: 1.3535 - emotion_output_loss: 2.7868 - gender_output_acc: 0.5479 - image_quality_output_acc: 0.5437 - age_output_acc: 0.3896 - weight_output_acc: 0.6771 - bag_output_acc: 0.5292 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7188 - val_loss: 27.3882 - val_gender_output_loss: 0.6821 - val_image_quality_output_loss: 0.9870 - val_age_output_loss: 1.4161 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.9227 - val_pose_output_loss: 0.9255 - val_footwear_output_loss: 1.0131 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4739 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 27.18120\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 25s 827ms/step - loss: 51.8109 - gender_output_loss: 0.7687 - image_quality_output_loss: 0.9873 - age_output_loss: 2.7901 - weight_output_loss: 2.7511 - bag_output_loss: 1.6353 - pose_output_loss: 1.7390 - footwear_output_loss: 1.3634 - emotion_output_loss: 2.9665 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5417 - age_output_acc: 0.4250 - weight_output_acc: 0.6375 - bag_output_acc: 0.5688 - pose_output_acc: 0.6250 - footwear_output_acc: 0.4771 - emotion_output_acc: 0.7021 - val_loss: 27.2800 - val_gender_output_loss: 0.6642 - val_image_quality_output_loss: 0.9860 - val_age_output_loss: 1.4159 - val_weight_output_loss: 0.9929 - val_bag_output_loss: 0.9190 - val_pose_output_loss: 0.9213 - val_footwear_output_loss: 0.9613 - val_emotion_output_loss: 0.9326 - val_gender_output_acc: 0.5901 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5167 - val_emotion_output_acc: 0.7052\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 27.18120\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 27.18120\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 25s 825ms/step - loss: 50.5339 - gender_output_loss: 0.7739 - image_quality_output_loss: 0.9635 - age_output_loss: 2.8486 - weight_output_loss: 2.6375 - bag_output_loss: 1.6832 - pose_output_loss: 1.9264 - footwear_output_loss: 1.3335 - emotion_output_loss: 2.5241 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5938 - age_output_acc: 0.4000 - weight_output_acc: 0.6479 - bag_output_acc: 0.5354 - pose_output_acc: 0.5771 - footwear_output_acc: 0.5646 - emotion_output_acc: 0.7250 - val_loss: 27.2632 - val_gender_output_loss: 0.6679 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.4054 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.9284 - val_pose_output_loss: 0.9254 - val_footwear_output_loss: 0.9745 - val_emotion_output_loss: 0.9261 - val_gender_output_acc: 0.5802 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5551 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 27.18120\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 50.1785 - gender_output_loss: 0.7678 - image_quality_output_loss: 0.9914 - age_output_loss: 2.7128 - weight_output_loss: 2.3144 - bag_output_loss: 1.5111 - pose_output_loss: 1.6615 - footwear_output_loss: 1.3961 - emotion_output_loss: 3.1006 - gender_output_acc: 0.5479 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4313 - weight_output_acc: 0.6542 - bag_output_acc: 0.5500 - pose_output_acc: 0.6417 - footwear_output_acc: 0.4958 - emotion_output_acc: 0.6854 - val_loss: 27.1432 - val_gender_output_loss: 0.6681 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4041 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.9133 - val_pose_output_loss: 0.9200 - val_footwear_output_loss: 0.9730 - val_emotion_output_loss: 0.9216 - val_gender_output_acc: 0.5576 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5472 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00055: val_loss improved from 27.18120 to 27.14325, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.055.h5\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 50.3732 - gender_output_loss: 0.7763 - image_quality_output_loss: 0.9595 - age_output_loss: 2.9036 - weight_output_loss: 3.0477 - bag_output_loss: 1.5036 - pose_output_loss: 1.8380 - footwear_output_loss: 1.2846 - emotion_output_loss: 2.3500 - gender_output_acc: 0.5396 - image_quality_output_acc: 0.5792 - age_output_acc: 0.3833 - weight_output_acc: 0.5896 - bag_output_acc: 0.5792 - pose_output_acc: 0.6104 - footwear_output_acc: 0.5604 - emotion_output_acc: 0.7437 - val_loss: 27.0909 - val_gender_output_loss: 0.6656 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4023 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.9107 - val_pose_output_loss: 0.9190 - val_footwear_output_loss: 0.9671 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.5561 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5477 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00056: val_loss improved from 27.14325 to 27.09092, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.056.h5\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 49.9864 - gender_output_loss: 0.7651 - image_quality_output_loss: 0.9768 - age_output_loss: 2.6646 - weight_output_loss: 2.6150 - bag_output_loss: 1.5080 - pose_output_loss: 1.7261 - footwear_output_loss: 1.3724 - emotion_output_loss: 2.8508 - gender_output_acc: 0.5542 - image_quality_output_acc: 0.5437 - age_output_acc: 0.3917 - weight_output_acc: 0.6292 - bag_output_acc: 0.5625 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7021 - val_loss: 27.0817 - val_gender_output_loss: 0.6595 - val_image_quality_output_loss: 0.9831 - val_age_output_loss: 1.4011 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9097 - val_pose_output_loss: 0.9182 - val_footwear_output_loss: 0.9684 - val_emotion_output_loss: 0.9164 - val_gender_output_acc: 0.5866 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5581 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5433 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00057: val_loss improved from 27.09092 to 27.08172, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.057.h5\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 49.7788 - gender_output_loss: 0.7663 - image_quality_output_loss: 0.9393 - age_output_loss: 2.7189 - weight_output_loss: 2.8349 - bag_output_loss: 1.6621 - pose_output_loss: 1.6998 - footwear_output_loss: 1.3427 - emotion_output_loss: 2.5175 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.6000 - age_output_acc: 0.4354 - weight_output_acc: 0.6292 - bag_output_acc: 0.5458 - pose_output_acc: 0.6271 - footwear_output_acc: 0.5167 - emotion_output_acc: 0.7375 - val_loss: 27.1477 - val_gender_output_loss: 0.6655 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9158 - val_pose_output_loss: 0.9190 - val_footwear_output_loss: 0.9804 - val_emotion_output_loss: 0.9176 - val_gender_output_acc: 0.5738 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5463 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 27.08172\n",
            "Epoch 59/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 52.2300 - gender_output_loss: 0.7542 - image_quality_output_loss: 0.9549 - age_output_loss: 3.0946 - weight_output_loss: 3.1305 - bag_output_loss: 1.4349 - pose_output_loss: 1.8963 - footwear_output_loss: 1.3652 - emotion_output_loss: 2.5444 - gender_output_acc: 0.6099 - image_quality_output_acc: 0.5647 - age_output_acc: 0.3362 - weight_output_acc: 0.5970 - bag_output_acc: 0.5711 - pose_output_acc: 0.5754 - footwear_output_acc: 0.4892 - emotion_output_acc: 0.7306\n",
            "30/30 [==============================] - 25s 824ms/step - loss: 52.2257 - gender_output_loss: 0.7550 - image_quality_output_loss: 0.9647 - age_output_loss: 3.1065 - weight_output_loss: 3.1335 - bag_output_loss: 1.4606 - pose_output_loss: 1.9059 - footwear_output_loss: 1.3580 - emotion_output_loss: 2.5011 - gender_output_acc: 0.6062 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3396 - weight_output_acc: 0.5979 - bag_output_acc: 0.5667 - pose_output_acc: 0.5708 - footwear_output_acc: 0.4958 - emotion_output_acc: 0.7354 - val_loss: 27.2954 - val_gender_output_loss: 0.6642 - val_image_quality_output_loss: 0.9842 - val_age_output_loss: 1.4132 - val_weight_output_loss: 1.0064 - val_bag_output_loss: 0.9122 - val_pose_output_loss: 0.9501 - val_footwear_output_loss: 0.9579 - val_emotion_output_loss: 0.9221 - val_gender_output_acc: 0.5517 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5546 - val_pose_output_acc: 0.6122 - val_footwear_output_acc: 0.5374 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 27.08172\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 50.5304 - gender_output_loss: 0.7429 - image_quality_output_loss: 0.9971 - age_output_loss: 3.0650 - weight_output_loss: 2.5848 - bag_output_loss: 1.5307 - pose_output_loss: 1.6892 - footwear_output_loss: 1.3211 - emotion_output_loss: 2.6508 - gender_output_acc: 0.6083 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3812 - weight_output_acc: 0.6146 - bag_output_acc: 0.6167 - pose_output_acc: 0.6313 - footwear_output_acc: 0.5042 - emotion_output_acc: 0.7104 - val_loss: 27.1133 - val_gender_output_loss: 0.6677 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4026 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.9203 - val_pose_output_loss: 0.9245 - val_footwear_output_loss: 0.9601 - val_emotion_output_loss: 0.9185 - val_gender_output_acc: 0.5507 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5566 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 27.08172\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 49.6118 - gender_output_loss: 0.7634 - image_quality_output_loss: 0.9895 - age_output_loss: 2.6421 - weight_output_loss: 2.3916 - bag_output_loss: 1.5213 - pose_output_loss: 1.7489 - footwear_output_loss: 1.3043 - emotion_output_loss: 2.9561 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4521 - weight_output_acc: 0.6854 - bag_output_acc: 0.5875 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5333 - emotion_output_acc: 0.6896 - val_loss: 27.2807 - val_gender_output_loss: 0.6739 - val_image_quality_output_loss: 0.9828 - val_age_output_loss: 1.4203 - val_weight_output_loss: 1.0046 - val_bag_output_loss: 0.9186 - val_pose_output_loss: 0.9254 - val_footwear_output_loss: 0.9735 - val_emotion_output_loss: 0.9199 - val_gender_output_acc: 0.5438 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5379 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 27.08172\n",
            "Epoch 62/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 52.2429 - gender_output_loss: 0.7770 - image_quality_output_loss: 0.9924 - age_output_loss: 2.8154 - weight_output_loss: 3.0218 - bag_output_loss: 1.6788 - pose_output_loss: 1.6609 - footwear_output_loss: 1.4061 - emotion_output_loss: 2.8589 - gender_output_acc: 0.5496 - image_quality_output_acc: 0.5431 - age_output_acc: 0.4095 - weight_output_acc: 0.6207 - bag_output_acc: 0.5323 - pose_output_acc: 0.6401 - footwear_output_acc: 0.4871 - emotion_output_acc: 0.7177Epoch 62/100\n",
            "30/30 [==============================] - 25s 823ms/step - loss: 52.3131 - gender_output_loss: 0.7779 - image_quality_output_loss: 0.9932 - age_output_loss: 2.8252 - weight_output_loss: 3.0339 - bag_output_loss: 1.6782 - pose_output_loss: 1.6754 - footwear_output_loss: 1.4125 - emotion_output_loss: 2.8431 - gender_output_acc: 0.5500 - image_quality_output_acc: 0.5417 - age_output_acc: 0.4042 - weight_output_acc: 0.6146 - bag_output_acc: 0.5333 - pose_output_acc: 0.6375 - footwear_output_acc: 0.4833 - emotion_output_acc: 0.7188 - val_loss: 27.1168 - val_gender_output_loss: 0.6617 - val_image_quality_output_loss: 0.9843 - val_age_output_loss: 1.4009 - val_weight_output_loss: 0.9938 - val_bag_output_loss: 0.9139 - val_pose_output_loss: 0.9204 - val_footwear_output_loss: 0.9759 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5551 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5487 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 27.08172\n",
            "Epoch 63/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 50.1157 - gender_output_loss: 0.7483 - image_quality_output_loss: 0.9741 - age_output_loss: 2.8215 - weight_output_loss: 2.5211 - bag_output_loss: 1.7029 - pose_output_loss: 1.6052 - footwear_output_loss: 1.3262 - emotion_output_loss: 2.7853 - gender_output_acc: 0.5841 - image_quality_output_acc: 0.5647 - age_output_acc: 0.4224 - weight_output_acc: 0.6293 - bag_output_acc: 0.5603 - pose_output_acc: 0.6466 - footwear_output_acc: 0.5474 - emotion_output_acc: 0.6961\n",
            "Epoch 00062: val_loss did not improve from 27.08172\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 50.0630 - gender_output_loss: 0.7500 - image_quality_output_loss: 0.9753 - age_output_loss: 2.7920 - weight_output_loss: 2.5510 - bag_output_loss: 1.6969 - pose_output_loss: 1.6086 - footwear_output_loss: 1.3263 - emotion_output_loss: 2.7796 - gender_output_acc: 0.5813 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4229 - weight_output_acc: 0.6271 - bag_output_acc: 0.5625 - pose_output_acc: 0.6458 - footwear_output_acc: 0.5479 - emotion_output_acc: 0.6979 - val_loss: 27.0115 - val_gender_output_loss: 0.6608 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9121 - val_pose_output_loss: 0.9190 - val_footwear_output_loss: 0.9508 - val_emotion_output_loss: 0.9206 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5536 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5620 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00063: val_loss improved from 27.08172 to 27.01145, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.063.h5\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 49.5436 - gender_output_loss: 0.7459 - image_quality_output_loss: 0.9933 - age_output_loss: 2.8135 - weight_output_loss: 2.4420 - bag_output_loss: 1.4631 - pose_output_loss: 1.7939 - footwear_output_loss: 1.3443 - emotion_output_loss: 2.7313 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4021 - weight_output_acc: 0.6354 - bag_output_acc: 0.5688 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7250 - val_loss: 27.0214 - val_gender_output_loss: 0.6596 - val_image_quality_output_loss: 0.9833 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9123 - val_pose_output_loss: 0.9188 - val_footwear_output_loss: 0.9547 - val_emotion_output_loss: 0.9207 - val_gender_output_acc: 0.5659 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5443 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 27.01145\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 25s 833ms/step - loss: 49.6127 - gender_output_loss: 0.7662 - image_quality_output_loss: 0.9539 - age_output_loss: 2.8455 - weight_output_loss: 2.3359 - bag_output_loss: 1.6803 - pose_output_loss: 1.6871 - footwear_output_loss: 1.2942 - emotion_output_loss: 2.7482 - gender_output_acc: 0.5583 - image_quality_output_acc: 0.5771 - age_output_acc: 0.3833 - weight_output_acc: 0.6438 - bag_output_acc: 0.5479 - pose_output_acc: 0.6292 - footwear_output_acc: 0.5354 - emotion_output_acc: 0.7271 - val_loss: 26.9750 - val_gender_output_loss: 0.6493 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9820 - val_bag_output_loss: 0.9120 - val_pose_output_loss: 0.9181 - val_footwear_output_loss: 0.9545 - val_emotion_output_loss: 0.9201 - val_gender_output_acc: 0.6024 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5128 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00065: val_loss improved from 27.01145 to 26.97496, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.065.h5\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 25s 822ms/step - loss: 52.4484 - gender_output_loss: 0.7591 - image_quality_output_loss: 0.9810 - age_output_loss: 3.1128 - weight_output_loss: 2.8598 - bag_output_loss: 1.4951 - pose_output_loss: 1.8755 - footwear_output_loss: 1.2605 - emotion_output_loss: 2.8021 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3833 - weight_output_acc: 0.6292 - bag_output_acc: 0.5958 - pose_output_acc: 0.5875 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7021 - val_loss: 26.9634 - val_gender_output_loss: 0.6545 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9088 - val_pose_output_loss: 0.9213 - val_footwear_output_loss: 0.9511 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.6132 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5586 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5394 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00066: val_loss improved from 26.97496 to 26.96339, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.066.h5\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 48.0045 - gender_output_loss: 0.7417 - image_quality_output_loss: 0.9796 - age_output_loss: 2.8859 - weight_output_loss: 2.2429 - bag_output_loss: 1.4398 - pose_output_loss: 1.6276 - footwear_output_loss: 1.2990 - emotion_output_loss: 2.5996 - gender_output_acc: 0.6083 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4313 - weight_output_acc: 0.6875 - bag_output_acc: 0.5854 - pose_output_acc: 0.6500 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7333 - val_loss: 27.2214 - val_gender_output_loss: 0.6785 - val_image_quality_output_loss: 0.9865 - val_age_output_loss: 1.4346 - val_weight_output_loss: 0.9939 - val_bag_output_loss: 0.9127 - val_pose_output_loss: 0.9195 - val_footwear_output_loss: 0.9509 - val_emotion_output_loss: 0.9217 - val_gender_output_acc: 0.5453 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5492 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5595 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 26.96339\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 25s 827ms/step - loss: 51.5089 - gender_output_loss: 0.7650 - image_quality_output_loss: 0.9957 - age_output_loss: 2.8880 - weight_output_loss: 2.5857 - bag_output_loss: 1.4633 - pose_output_loss: 1.9258 - footwear_output_loss: 1.3936 - emotion_output_loss: 2.9107 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3625 - weight_output_acc: 0.6354 - bag_output_acc: 0.5771 - pose_output_acc: 0.5750 - footwear_output_acc: 0.4979 - emotion_output_acc: 0.7083 - val_loss: 27.2500 - val_gender_output_loss: 0.6622 - val_image_quality_output_loss: 0.9833 - val_age_output_loss: 1.4288 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.9181 - val_pose_output_loss: 0.9515 - val_footwear_output_loss: 0.9518 - val_emotion_output_loss: 0.9220 - val_gender_output_acc: 0.5832 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3637 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5290 - val_pose_output_acc: 0.6058 - val_footwear_output_acc: 0.5374 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 26.96339\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 25s 825ms/step - loss: 52.9427 - gender_output_loss: 0.7478 - image_quality_output_loss: 0.9542 - age_output_loss: 3.0183 - weight_output_loss: 3.0402 - bag_output_loss: 1.6796 - pose_output_loss: 1.8330 - footwear_output_loss: 1.3253 - emotion_output_loss: 2.7716 - gender_output_acc: 0.6000 - image_quality_output_acc: 0.5792 - age_output_acc: 0.3896 - weight_output_acc: 0.6146 - bag_output_acc: 0.5562 - pose_output_acc: 0.6021 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7271 - val_loss: 27.1649 - val_gender_output_loss: 0.6767 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4134 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9134 - val_pose_output_loss: 0.9275 - val_footwear_output_loss: 0.9787 - val_emotion_output_loss: 0.9224 - val_gender_output_acc: 0.5536 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5384 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 26.96339\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 25s 832ms/step - loss: 49.6889 - gender_output_loss: 0.7768 - image_quality_output_loss: 0.9800 - age_output_loss: 2.4221 - weight_output_loss: 2.6061 - bag_output_loss: 1.6621 - pose_output_loss: 1.9285 - footwear_output_loss: 1.3784 - emotion_output_loss: 2.7697 - gender_output_acc: 0.5292 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4688 - weight_output_acc: 0.6792 - bag_output_acc: 0.5688 - pose_output_acc: 0.5771 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.7021 - val_loss: 27.0468 - val_gender_output_loss: 0.6627 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9157 - val_pose_output_loss: 0.9198 - val_footwear_output_loss: 0.9673 - val_emotion_output_loss: 0.9168 - val_gender_output_acc: 0.5773 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5384 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 26.96339\n",
            "Epoch 70/100\n",
            "Epoch 71/100\n",
            "\n",
            "30/30 [==============================] - 25s 833ms/step - loss: 52.0448 - gender_output_loss: 0.7630 - image_quality_output_loss: 1.0159 - age_output_loss: 2.9725 - weight_output_loss: 2.6907 - bag_output_loss: 1.7057 - pose_output_loss: 1.6614 - footwear_output_loss: 1.4635 - emotion_output_loss: 2.8605 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5250 - age_output_acc: 0.3521 - weight_output_acc: 0.6229 - bag_output_acc: 0.5625 - pose_output_acc: 0.6292 - footwear_output_acc: 0.5062 - emotion_output_acc: 0.6958 - val_loss: 27.0665 - val_gender_output_loss: 0.6671 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4062 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.9138 - val_pose_output_loss: 0.9210 - val_footwear_output_loss: 0.9800 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5576 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 26.96339\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 23s 781ms/step - loss: 50.9621 - gender_output_loss: 0.7739 - image_quality_output_loss: 0.9816 - age_output_loss: 2.9391 - weight_output_loss: 2.5136 - bag_output_loss: 1.7220 - pose_output_loss: 1.7016 - footwear_output_loss: 1.2959 - emotion_output_loss: 2.8099 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4042 - weight_output_acc: 0.6500 - bag_output_acc: 0.4917 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.7042 - val_loss: 26.9742 - val_gender_output_loss: 0.6599 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4024 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9083 - val_pose_output_loss: 0.9191 - val_footwear_output_loss: 0.9606 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.5817 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5443 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 26.96339\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 27s 909ms/step - loss: 48.5224 - gender_output_loss: 0.7602 - image_quality_output_loss: 0.9255 - age_output_loss: 2.7746 - weight_output_loss: 2.4329 - bag_output_loss: 1.5690 - pose_output_loss: 1.6825 - footwear_output_loss: 1.3515 - emotion_output_loss: 2.5614 - gender_output_acc: 0.5833 - image_quality_output_acc: 0.6125 - age_output_acc: 0.4062 - weight_output_acc: 0.6708 - bag_output_acc: 0.5521 - pose_output_acc: 0.6354 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.7375 - val_loss: 26.9289 - val_gender_output_loss: 0.6562 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.4004 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9063 - val_pose_output_loss: 0.9182 - val_footwear_output_loss: 0.9614 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.5773 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5556 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5581 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00073: val_loss improved from 26.96339 to 26.92893, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.073.h5\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 24s 805ms/step - loss: 54.3910 - gender_output_loss: 0.7786 - image_quality_output_loss: 0.9837 - age_output_loss: 3.2096 - weight_output_loss: 2.9679 - bag_output_loss: 1.6839 - pose_output_loss: 1.7822 - footwear_output_loss: 1.3456 - emotion_output_loss: 2.9968 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3625 - weight_output_acc: 0.6208 - bag_output_acc: 0.5208 - pose_output_acc: 0.6104 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.6875 - val_loss: 27.0580 - val_gender_output_loss: 0.6560 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.4290 - val_weight_output_loss: 0.9868 - val_bag_output_loss: 0.9063 - val_pose_output_loss: 0.9233 - val_footwear_output_loss: 0.9468 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.6038 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3858 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5541 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5310 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 26.92893\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 50.1467 - gender_output_loss: 0.7702 - image_quality_output_loss: 1.0094 - age_output_loss: 2.7322 - weight_output_loss: 2.3878 - bag_output_loss: 1.6320 - pose_output_loss: 1.7530 - footwear_output_loss: 1.3403 - emotion_output_loss: 2.9045 - gender_output_acc: 0.5833 - image_quality_output_acc: 0.5333 - age_output_acc: 0.4417 - weight_output_acc: 0.6500 - bag_output_acc: 0.5458 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7042 - val_loss: 27.0172 - val_gender_output_loss: 0.6625 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.4082 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.9088 - val_pose_output_loss: 0.9209 - val_footwear_output_loss: 0.9442 - val_emotion_output_loss: 0.9360 - val_gender_output_acc: 0.5827 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5541 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5310 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 26.92893\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 25s 827ms/step - loss: 53.0536 - gender_output_loss: 0.7625 - image_quality_output_loss: 0.9948 - age_output_loss: 3.1040 - weight_output_loss: 2.9442 - bag_output_loss: 1.6182 - pose_output_loss: 1.8786 - footwear_output_loss: 1.2659 - emotion_output_loss: 2.8088 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3417 - weight_output_acc: 0.5958 - bag_output_acc: 0.5542 - pose_output_acc: 0.5833 - footwear_output_acc: 0.5688 - emotion_output_acc: 0.7042 - val_loss: 27.2548 - val_gender_output_loss: 0.6781 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4191 - val_weight_output_loss: 0.9988 - val_bag_output_loss: 0.9177 - val_pose_output_loss: 0.9273 - val_footwear_output_loss: 0.9914 - val_emotion_output_loss: 0.9283 - val_gender_output_acc: 0.5453 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5556 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5281 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 26.92893\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 25s 840ms/step - loss: 51.5470 - gender_output_loss: 0.7552 - image_quality_output_loss: 0.9845 - age_output_loss: 2.8511 - weight_output_loss: 2.8033 - bag_output_loss: 1.5899 - pose_output_loss: 1.7195 - footwear_output_loss: 1.3979 - emotion_output_loss: 2.8770 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4229 - weight_output_acc: 0.6042 - bag_output_acc: 0.5750 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5083 - emotion_output_acc: 0.7000 - val_loss: 27.1560 - val_gender_output_loss: 0.6680 - val_image_quality_output_loss: 0.9833 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.9119 - val_pose_output_loss: 0.9245 - val_footwear_output_loss: 0.9842 - val_emotion_output_loss: 0.9345 - val_gender_output_acc: 0.5561 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5650 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 26.92893\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 25s 821ms/step - loss: 48.6791 - gender_output_loss: 0.7601 - image_quality_output_loss: 0.9964 - age_output_loss: 2.8980 - weight_output_loss: 2.4898 - bag_output_loss: 1.6215 - pose_output_loss: 1.8503 - footwear_output_loss: 1.2518 - emotion_output_loss: 2.2933 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5354 - age_output_acc: 0.3688 - weight_output_acc: 0.6354 - bag_output_acc: 0.5750 - pose_output_acc: 0.6104 - footwear_output_acc: 0.5417 - emotion_output_acc: 0.7479 - val_loss: 26.9612 - val_gender_output_loss: 0.6613 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4126 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.9080 - val_pose_output_loss: 0.9289 - val_footwear_output_loss: 0.9514 - val_emotion_output_loss: 0.9142 - val_gender_output_acc: 0.5689 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5541 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 26.92893\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 25s 820ms/step - loss: 51.3527 - gender_output_loss: 0.7600 - image_quality_output_loss: 0.9956 - age_output_loss: 2.8749 - weight_output_loss: 2.3433 - bag_output_loss: 1.5265 - pose_output_loss: 1.6545 - footwear_output_loss: 1.2546 - emotion_output_loss: 3.3133 - gender_output_acc: 0.5896 - image_quality_output_acc: 0.5250 - age_output_acc: 0.4062 - weight_output_acc: 0.6812 - bag_output_acc: 0.5896 - pose_output_acc: 0.6438 - footwear_output_acc: 0.5375 - emotion_output_acc: 0.6667 - val_loss: 26.9664 - val_gender_output_loss: 0.6638 - val_image_quality_output_loss: 0.9833 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9100 - val_pose_output_loss: 0.9189 - val_footwear_output_loss: 0.9661 - val_emotion_output_loss: 0.9205 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5625 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 26.92893\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 51.0619 - gender_output_loss: 0.7372 - image_quality_output_loss: 0.9461 - age_output_loss: 3.0565 - weight_output_loss: 2.4807 - bag_output_loss: 1.6140 - pose_output_loss: 1.5346 - footwear_output_loss: 1.3145 - emotion_output_loss: 2.9873 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.6000 - age_output_acc: 0.3854 - weight_output_acc: 0.6313 - bag_output_acc: 0.5688 - pose_output_acc: 0.6708 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.6979 - val_loss: 26.8894 - val_gender_output_loss: 0.6604 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4055 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9068 - val_pose_output_loss: 0.9194 - val_footwear_output_loss: 0.9524 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.5679 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5551 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5655 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00080: val_loss improved from 26.92893 to 26.88938, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.080.h5\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 49.2246 - gender_output_loss: 0.7611 - image_quality_output_loss: 0.9722 - age_output_loss: 2.8764 - weight_output_loss: 2.5104 - bag_output_loss: 1.6786 - pose_output_loss: 1.7186 - footwear_output_loss: 1.4406 - emotion_output_loss: 2.4116 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3896 - weight_output_acc: 0.6646 - bag_output_acc: 0.5417 - pose_output_acc: 0.6208 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.7437 - val_loss: 26.8308 - val_gender_output_loss: 0.6536 - val_image_quality_output_loss: 0.9826 - val_age_output_loss: 1.4057 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9026 - val_pose_output_loss: 0.9179 - val_footwear_output_loss: 0.9432 - val_emotion_output_loss: 0.9135 - val_gender_output_acc: 0.5787 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5448 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00081: val_loss improved from 26.88938 to 26.83082, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.081.h5\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 49.8152 - gender_output_loss: 0.7474 - image_quality_output_loss: 0.9533 - age_output_loss: 2.7346 - weight_output_loss: 2.5872 - bag_output_loss: 1.5751 - pose_output_loss: 1.7428 - footwear_output_loss: 1.3259 - emotion_output_loss: 2.7773 - gender_output_acc: 0.5896 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4292 - weight_output_acc: 0.6271 - bag_output_acc: 0.5521 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5354 - emotion_output_acc: 0.7000 - val_loss: 26.7699 - val_gender_output_loss: 0.6556 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.3986 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.9031 - val_pose_output_loss: 0.9166 - val_footwear_output_loss: 0.9338 - val_emotion_output_loss: 0.9134 - val_gender_output_acc: 0.5802 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5645 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5679 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00082: val_loss improved from 26.83082 to 26.76987, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.082.h5\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 24s 812ms/step - loss: 48.3298 - gender_output_loss: 0.7475 - image_quality_output_loss: 0.9673 - age_output_loss: 2.5307 - weight_output_loss: 2.3281 - bag_output_loss: 1.6140 - pose_output_loss: 1.7426 - footwear_output_loss: 1.2605 - emotion_output_loss: 2.8023 - gender_output_acc: 0.6208 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4146 - weight_output_acc: 0.6375 - bag_output_acc: 0.5500 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5437 - emotion_output_acc: 0.7042 - val_loss: 26.8276 - val_gender_output_loss: 0.6507 - val_image_quality_output_loss: 0.9842 - val_age_output_loss: 1.4075 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.9051 - val_pose_output_loss: 0.9156 - val_footwear_output_loss: 0.9436 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.6009 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5340 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00082: val_loss improved from 26.83082 to 26.76987, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.082.h5\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 26.76987\n",
            "Epoch 84/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 49.4679 - gender_output_loss: 0.7587 - image_quality_output_loss: 0.9888 - age_output_loss: 2.4155 - weight_output_loss: 3.0401 - bag_output_loss: 1.6843 - pose_output_loss: 1.8423 - footwear_output_loss: 1.2826 - emotion_output_loss: 2.5153 - gender_output_acc: 0.5884 - image_quality_output_acc: 0.5345 - age_output_acc: 0.4310 - weight_output_acc: 0.6250 - bag_output_acc: 0.5453 - pose_output_acc: 0.5862 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7392\n",
            "Epoch 00083: val_loss did not improve from 26.76987\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 49.2479 - gender_output_loss: 0.7608 - image_quality_output_loss: 0.9862 - age_output_loss: 2.3836 - weight_output_loss: 3.0406 - bag_output_loss: 1.7063 - pose_output_loss: 1.8416 - footwear_output_loss: 1.2819 - emotion_output_loss: 2.4765 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5375 - age_output_acc: 0.4396 - weight_output_acc: 0.6250 - bag_output_acc: 0.5396 - pose_output_acc: 0.5875 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7417 - val_loss: 27.2432 - val_gender_output_loss: 0.6435 - val_image_quality_output_loss: 0.9963 - val_age_output_loss: 1.4328 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.9668 - val_pose_output_loss: 0.9191 - val_footwear_output_loss: 0.9579 - val_emotion_output_loss: 0.9392 - val_gender_output_acc: 0.6206 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.4980 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5148 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 26.76987\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 25s 831ms/step - loss: 50.9022 - gender_output_loss: 0.7643 - image_quality_output_loss: 0.9621 - age_output_loss: 2.8782 - weight_output_loss: 2.6009 - bag_output_loss: 1.5418 - pose_output_loss: 1.7099 - footwear_output_loss: 1.3220 - emotion_output_loss: 2.9402 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4104 - weight_output_acc: 0.6375 - bag_output_acc: 0.5500 - pose_output_acc: 0.6104 - footwear_output_acc: 0.5146 - emotion_output_acc: 0.7000 - val_loss: 27.0528 - val_gender_output_loss: 0.6782 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9149 - val_pose_output_loss: 0.9300 - val_footwear_output_loss: 0.9725 - val_emotion_output_loss: 0.9198 - val_gender_output_acc: 0.5448 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5487 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 26.76987\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 25s 818ms/step - loss: 49.8810 - gender_output_loss: 0.7631 - image_quality_output_loss: 1.0083 - age_output_loss: 2.8579 - weight_output_loss: 2.5121 - bag_output_loss: 1.5199 - pose_output_loss: 1.7121 - footwear_output_loss: 1.2987 - emotion_output_loss: 2.7781 - gender_output_acc: 0.5771 - image_quality_output_acc: 0.5229 - age_output_acc: 0.4437 - weight_output_acc: 0.6500 - bag_output_acc: 0.5854 - pose_output_acc: 0.6271 - footwear_output_acc: 0.5375 - emotion_output_acc: 0.7083 - val_loss: 27.0127 - val_gender_output_loss: 0.6746 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4120 - val_weight_output_loss: 0.9858 - val_bag_output_loss: 0.9165 - val_pose_output_loss: 0.9256 - val_footwear_output_loss: 0.9666 - val_emotion_output_loss: 0.9168 - val_gender_output_acc: 0.5507 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5571 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5349 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 26.76987\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 25s 817ms/step - loss: 51.6462 - gender_output_loss: 0.7714 - image_quality_output_loss: 0.9910 - age_output_loss: 3.0159 - weight_output_loss: 2.8496 - bag_output_loss: 1.4789 - pose_output_loss: 1.5887 - footwear_output_loss: 1.4126 - emotion_output_loss: 2.8807 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3833 - weight_output_acc: 0.6125 - bag_output_acc: 0.5854 - pose_output_acc: 0.6625 - footwear_output_acc: 0.4896 - emotion_output_acc: 0.6938 - val_loss: 26.9882 - val_gender_output_loss: 0.6724 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4120 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.9107 - val_pose_output_loss: 0.9204 - val_footwear_output_loss: 0.9689 - val_emotion_output_loss: 0.9225 - val_gender_output_acc: 0.5546 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5320 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 26.76987\n",
            "Epoch 00087: val_loss did not improve from 26.76987\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 25s 834ms/step - loss: 49.4649 - gender_output_loss: 0.7666 - image_quality_output_loss: 0.9811 - age_output_loss: 2.8084 - weight_output_loss: 2.5171 - bag_output_loss: 1.4883 - pose_output_loss: 1.7642 - footwear_output_loss: 1.3082 - emotion_output_loss: 2.7139 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3875 - weight_output_acc: 0.6354 - bag_output_acc: 0.5625 - pose_output_acc: 0.6042 - footwear_output_acc: 0.5271 - emotion_output_acc: 0.7229 - val_loss: 26.9273 - val_gender_output_loss: 0.6688 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9098 - val_pose_output_loss: 0.9207 - val_footwear_output_loss: 0.9623 - val_emotion_output_loss: 0.9174 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5408 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 26.76987\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 25s 837ms/step - loss: 53.4928 - gender_output_loss: 0.7537 - image_quality_output_loss: 1.0015 - age_output_loss: 3.1092 - weight_output_loss: 2.7336 - bag_output_loss: 1.8702 - pose_output_loss: 1.7115 - footwear_output_loss: 1.3372 - emotion_output_loss: 2.9928 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.5292 - age_output_acc: 0.3771 - weight_output_acc: 0.6333 - bag_output_acc: 0.5542 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5604 - emotion_output_acc: 0.7042 - val_loss: 26.9268 - val_gender_output_loss: 0.6668 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4084 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9073 - val_pose_output_loss: 0.9191 - val_footwear_output_loss: 0.9698 - val_emotion_output_loss: 0.9193 - val_gender_output_acc: 0.5714 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5118 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 26.76987\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 25s 824ms/step - loss: 51.0171 - gender_output_loss: 0.7552 - image_quality_output_loss: 0.9481 - age_output_loss: 2.9243 - weight_output_loss: 3.0866 - bag_output_loss: 1.5113 - pose_output_loss: 1.7670 - footwear_output_loss: 1.3599 - emotion_output_loss: 2.5368 - gender_output_acc: 0.6292 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4083 - weight_output_acc: 0.6083 - bag_output_acc: 0.5854 - pose_output_acc: 0.6021 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7229 - val_loss: 26.9715 - val_gender_output_loss: 0.6602 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.9099 - val_pose_output_loss: 0.9253 - val_footwear_output_loss: 0.9679 - val_emotion_output_loss: 0.9202 - val_gender_output_acc: 0.5965 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5586 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5202 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 26.76987\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 25s 825ms/step - loss: 49.2847 - gender_output_loss: 0.7472 - image_quality_output_loss: 0.9833 - age_output_loss: 2.9502 - weight_output_loss: 2.3925 - bag_output_loss: 1.5555 - pose_output_loss: 1.6599 - footwear_output_loss: 1.2701 - emotion_output_loss: 2.6784 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3833 - weight_output_acc: 0.6562 - bag_output_acc: 0.5375 - pose_output_acc: 0.6542 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7083 - val_loss: 26.8176 - val_gender_output_loss: 0.6509 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9038 - val_pose_output_loss: 0.9199 - val_footwear_output_loss: 0.9455 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.5965 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5615 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5655 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 26.76987\n",
            "Epoch 92/100\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 48.7018 - gender_output_loss: 0.7585 - image_quality_output_loss: 0.9350 - age_output_loss: 2.8898 - weight_output_loss: 2.4917 - bag_output_loss: 1.6123 - pose_output_loss: 1.7972 - footwear_output_loss: 1.3755 - emotion_output_loss: 2.3410 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4000 - weight_output_acc: 0.6438 - bag_output_acc: 0.5271 - pose_output_acc: 0.6125 - footwear_output_acc: 0.5000 - emotion_output_acc: 0.7417 - val_loss: 26.8896 - val_gender_output_loss: 0.6521 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.9019 - val_pose_output_loss: 0.9161 - val_footwear_output_loss: 0.9865 - val_emotion_output_loss: 0.9169 - val_gender_output_acc: 0.5965 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5635 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4582 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 26.76987\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 48.8261 - gender_output_loss: 0.7465 - image_quality_output_loss: 0.9961 - age_output_loss: 2.7792 - weight_output_loss: 2.4138 - bag_output_loss: 1.5462 - pose_output_loss: 1.8243 - footwear_output_loss: 1.3706 - emotion_output_loss: 2.5510 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5354 - age_output_acc: 0.3854 - weight_output_acc: 0.6542 - bag_output_acc: 0.5938 - pose_output_acc: 0.5979 - footwear_output_acc: 0.5542 - emotion_output_acc: 0.7292 - val_loss: 26.9198 - val_gender_output_loss: 0.6691 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4094 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9101 - val_pose_output_loss: 0.9228 - val_footwear_output_loss: 0.9772 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.5512 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5315 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 26.76987\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 50.9570 - gender_output_loss: 0.7504 - image_quality_output_loss: 0.9895 - age_output_loss: 2.7693 - weight_output_loss: 2.8507 - bag_output_loss: 1.4275 - pose_output_loss: 1.7165 - footwear_output_loss: 1.3471 - emotion_output_loss: 2.9511 - gender_output_acc: 0.5917 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4125 - weight_output_acc: 0.6271 - bag_output_acc: 0.5771 - pose_output_acc: 0.6313 - footwear_output_acc: 0.5042 - emotion_output_acc: 0.7042 - val_loss: 26.8569 - val_gender_output_loss: 0.6543 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4054 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.9100 - val_pose_output_loss: 0.9175 - val_footwear_output_loss: 0.9618 - val_emotion_output_loss: 0.9201 - val_gender_output_acc: 0.5723 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5098 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 26.76987\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 50.4377 - gender_output_loss: 0.7541 - image_quality_output_loss: 0.9888 - age_output_loss: 3.0599 - weight_output_loss: 2.3887 - bag_output_loss: 1.4272 - pose_output_loss: 1.8155 - footwear_output_loss: 1.3243 - emotion_output_loss: 2.8147 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3604 - weight_output_acc: 0.6646 - bag_output_acc: 0.5708 - pose_output_acc: 0.5979 - footwear_output_acc: 0.5333 - emotion_output_acc: 0.7063 - val_loss: 26.6843 - val_gender_output_loss: 0.6406 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4023 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8982 - val_pose_output_loss: 0.9120 - val_footwear_output_loss: 0.9393 - val_emotion_output_loss: 0.9211 - val_gender_output_acc: 0.6122 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5561 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00095: val_loss improved from 26.76987 to 26.68430, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.095.h5\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 23s 758ms/step - loss: 50.0757 - gender_output_loss: 0.7185 - image_quality_output_loss: 0.9591 - age_output_loss: 2.8192 - weight_output_loss: 2.8121 - bag_output_loss: 1.4906 - pose_output_loss: 1.9296 - footwear_output_loss: 1.2870 - emotion_output_loss: 2.5662 - gender_output_acc: 0.6438 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3896 - weight_output_acc: 0.6396 - bag_output_acc: 0.5771 - pose_output_acc: 0.5729 - footwear_output_acc: 0.5562 - emotion_output_acc: 0.7271 - val_loss: 26.6700 - val_gender_output_loss: 0.6435 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.3980 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9007 - val_pose_output_loss: 0.9141 - val_footwear_output_loss: 0.9413 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.6063 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5610 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5389 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00096: val_loss improved from 26.68430 to 26.67004, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.096.h5\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 26s 878ms/step - loss: 50.5747 - gender_output_loss: 0.7481 - image_quality_output_loss: 0.9441 - age_output_loss: 2.7826 - weight_output_loss: 2.5316 - bag_output_loss: 1.5677 - pose_output_loss: 1.7604 - footwear_output_loss: 1.3020 - emotion_output_loss: 2.9924 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4229 - weight_output_acc: 0.6687 - bag_output_acc: 0.5854 - pose_output_acc: 0.6146 - footwear_output_acc: 0.5417 - emotion_output_acc: 0.6896 - val_loss: 26.6408 - val_gender_output_loss: 0.6443 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.3938 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8986 - val_pose_output_loss: 0.9123 - val_footwear_output_loss: 0.9407 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.6132 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5576 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00097: val_loss improved from 26.67004 to 26.64076, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 24s 803ms/step - loss: 50.6721 - gender_output_loss: 0.7755 - image_quality_output_loss: 0.9660 - age_output_loss: 3.1777 - weight_output_loss: 2.9066 - bag_output_loss: 1.5895 - pose_output_loss: 1.8365 - footwear_output_loss: 1.2752 - emotion_output_loss: 2.2566 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3375 - weight_output_acc: 0.6062 - bag_output_acc: 0.5625 - pose_output_acc: 0.6000 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7437 - val_loss: 26.8467 - val_gender_output_loss: 0.6486 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4049 - val_weight_output_loss: 0.9853 - val_bag_output_loss: 0.9077 - val_pose_output_loss: 0.9277 - val_footwear_output_loss: 0.9617 - val_emotion_output_loss: 0.9188 - val_gender_output_acc: 0.6083 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5531 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5448 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00097: val_loss improved from 26.67004 to 26.64076, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 26.64076\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 24s 799ms/step - loss: 51.2085 - gender_output_loss: 0.7179 - image_quality_output_loss: 0.9700 - age_output_loss: 2.9837 - weight_output_loss: 2.8157 - bag_output_loss: 1.6479 - pose_output_loss: 1.7373 - footwear_output_loss: 1.2630 - emotion_output_loss: 2.7177 - gender_output_acc: 0.6438 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3938 - weight_output_acc: 0.6104 - bag_output_acc: 0.5417 - pose_output_acc: 0.6208 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7146 - val_loss: 26.7638 - val_gender_output_loss: 0.6467 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.4121 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9044 - val_pose_output_loss: 0.9176 - val_footwear_output_loss: 0.9353 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.6019 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5635 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5620 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 26.64076\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 24s 798ms/step - loss: 50.6200 - gender_output_loss: 0.7591 - image_quality_output_loss: 0.9952 - age_output_loss: 2.8932 - weight_output_loss: 2.5100 - bag_output_loss: 1.5984 - pose_output_loss: 1.6770 - footwear_output_loss: 1.3463 - emotion_output_loss: 2.9000 - gender_output_acc: 0.5604 - image_quality_output_acc: 0.5271 - age_output_acc: 0.3917 - weight_output_acc: 0.6500 - bag_output_acc: 0.5792 - pose_output_acc: 0.6187 - footwear_output_acc: 0.5521 - emotion_output_acc: 0.7021 - val_loss: 27.1185 - val_gender_output_loss: 0.6642 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.4137 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9163 - val_pose_output_loss: 0.9238 - val_footwear_output_loss: 1.0269 - val_emotion_output_loss: 0.9405 - val_gender_output_acc: 0.6344 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5576 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4247 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 26.64076Epoch 100/100\n",
            "\n",
            "30/30 [==============================] - 24s 799ms/step - loss: 51.2085 - gender_output_loss: 0.7179 - image_quality_output_loss: 0.9700 - age_output_loss: 2.9837 - weight_output_loss: 2.8157 - bag_output_loss: 1.6479 - pose_output_loss: 1.7373 - footwear_output_loss: 1.2630 - emotion_output_loss: 2.7177 - gender_output_acc: 0.6438 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3938 - weight_output_acc: 0.6104 - bag_output_acc: 0.5417 - pose_output_acc: 0.6208 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7146 - val_loss: 26.7638 - val_gender_output_loss: 0.6467 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.4121 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9044 - val_pose_output_loss: 0.9176 - val_footwear_output_loss: 0.9353 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.6019 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5635 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5620 - val_emotion_output_acc: 0.7062\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7c89d6f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FvlpzZyyMUH",
        "colab_type": "code",
        "outputId": "6dd4d723-81e0-448a-96f5-fc59934a0559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPUfY_NmI35",
        "colab_type": "code",
        "outputId": "cf46b098-6f5b-4a2a-a713-9ea6500e4622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 4, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 3, \n",
        "#                         'pose_output': 3, \n",
        "#                         'footwear_output': 2, \n",
        "#                         'emotion_output': 4}\n",
        "\n",
        "\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n",
            "Returning new callback array with steps_per_epoch= 100 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json  to: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json1577524781_backup\n",
            "Epoch 1/100\n",
            "  2/100 [..............................] - ETA: 6:38 - loss: 66.1505 - gender_output_loss: 0.6945 - image_quality_output_loss: 0.9638 - age_output_loss: 3.0244 - weight_output_loss: 3.8865 - bag_output_loss: 1.9364 - pose_output_loss: 2.0708 - footwear_output_loss: 1.4651 - emotion_output_loss: 5.0548 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3438 - weight_output_acc: 0.5312 - bag_output_acc: 0.5312 - pose_output_acc: 0.5625 - footwear_output_acc: 0.4688 - emotion_output_acc: 0.5625 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.305855). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 99/100 [============================>.] - ETA: 0s - loss: 50.3115 - gender_output_loss: 0.7369 - image_quality_output_loss: 0.9715 - age_output_loss: 2.7393 - weight_output_loss: 2.5836 - bag_output_loss: 1.6235 - pose_output_loss: 1.7295 - footwear_output_loss: 1.3298 - emotion_output_loss: 2.8910 - gender_output_acc: 0.6193 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4059 - weight_output_acc: 0.6566 - bag_output_acc: 0.5732 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5354 - emotion_output_acc: 0.6995\n",
            "100/100 [==============================] - 56s 565ms/step - loss: 50.3948 - gender_output_loss: 0.7389 - image_quality_output_loss: 0.9720 - age_output_loss: 2.7361 - weight_output_loss: 2.5943 - bag_output_loss: 1.6506 - pose_output_loss: 1.7314 - footwear_output_loss: 1.3285 - emotion_output_loss: 2.8846 - gender_output_acc: 0.6169 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4050 - weight_output_acc: 0.6556 - bag_output_acc: 0.5706 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5356 - emotion_output_acc: 0.7000 - val_loss: 26.6551 - val_gender_output_loss: 0.6432 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3921 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9025 - val_pose_output_loss: 0.9135 - val_footwear_output_loss: 0.9425 - val_emotion_output_loss: 0.9174 - val_gender_output_acc: 0.6093 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5285 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 26.65510, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.001.h5\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 48.5350 - gender_output_loss: 0.7436 - image_quality_output_loss: 0.9860 - age_output_loss: 2.8463 - weight_output_loss: 2.6250 - bag_output_loss: 1.5721 - pose_output_loss: 1.7247 - footwear_output_loss: 1.2967 - emotion_output_loss: 2.3594 - gender_output_acc: 0.6150 - image_quality_output_acc: 0.5487 - age_output_acc: 0.4094 - weight_output_acc: 0.6300 - bag_output_acc: 0.5662 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5450 - emotion_output_acc: 0.7419 - val_loss: 26.6836 - val_gender_output_loss: 0.6386 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.9007 - val_pose_output_loss: 0.9136 - val_footwear_output_loss: 0.9384 - val_emotion_output_loss: 0.9186 - val_gender_output_acc: 0.6324 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5330 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 26.65510\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 48s 484ms/step - loss: 51.1102 - gender_output_loss: 0.7460 - image_quality_output_loss: 0.9742 - age_output_loss: 2.9985 - weight_output_loss: 2.6287 - bag_output_loss: 1.6254 - pose_output_loss: 1.7262 - footwear_output_loss: 1.2833 - emotion_output_loss: 2.8233 - gender_output_acc: 0.6112 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3787 - weight_output_acc: 0.6287 - bag_output_acc: 0.5450 - pose_output_acc: 0.6188 - footwear_output_acc: 0.5369 - emotion_output_acc: 0.7137 - val_loss: 26.8102 - val_gender_output_loss: 0.6469 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4203 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.9072 - val_pose_output_loss: 0.9226 - val_footwear_output_loss: 0.9457 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.5910 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5472 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5507 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 26.65510\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 48s 478ms/step - loss: 50.4400 - gender_output_loss: 0.7581 - image_quality_output_loss: 0.9631 - age_output_loss: 2.8477 - weight_output_loss: 2.7740 - bag_output_loss: 1.5434 - pose_output_loss: 1.6792 - footwear_output_loss: 1.3722 - emotion_output_loss: 2.7565 - gender_output_acc: 0.5863 - image_quality_output_acc: 0.5756 - age_output_acc: 0.3969 - weight_output_acc: 0.6175 - bag_output_acc: 0.5662 - pose_output_acc: 0.6331 - footwear_output_acc: 0.5200 - emotion_output_acc: 0.7144 - val_loss: 26.6947 - val_gender_output_loss: 0.6589 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.3997 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9039 - val_pose_output_loss: 0.9147 - val_footwear_output_loss: 0.9614 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.5787 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5586 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5689 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 26.65510\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 50.1715 - gender_output_loss: 0.7466 - image_quality_output_loss: 0.9693 - age_output_loss: 2.8342 - weight_output_loss: 2.5528 - bag_output_loss: 1.5850 - pose_output_loss: 1.7949 - footwear_output_loss: 1.2948 - emotion_output_loss: 2.8008 - gender_output_acc: 0.5975 - image_quality_output_acc: 0.5606 - age_output_acc: 0.3813 - weight_output_acc: 0.6344 - bag_output_acc: 0.5675 - pose_output_acc: 0.6019 - footwear_output_acc: 0.5544 - emotion_output_acc: 0.7100 - val_loss: 26.7461 - val_gender_output_loss: 0.6680 - val_image_quality_output_loss: 0.9825 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9035 - val_pose_output_loss: 0.9203 - val_footwear_output_loss: 0.9756 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.5522 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5472 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 26.65510\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 49.4018 - gender_output_loss: 0.7508 - image_quality_output_loss: 0.9703 - age_output_loss: 2.8868 - weight_output_loss: 2.4612 - bag_output_loss: 1.5484 - pose_output_loss: 1.7938 - footwear_output_loss: 1.3008 - emotion_output_loss: 2.6542 - gender_output_acc: 0.6006 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4088 - weight_output_acc: 0.6525 - bag_output_acc: 0.5681 - pose_output_acc: 0.6063 - footwear_output_acc: 0.5281 - emotion_output_acc: 0.7206 - val_loss: 26.6879 - val_gender_output_loss: 0.6475 - val_image_quality_output_loss: 0.9843 - val_age_output_loss: 1.4042 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.9101 - val_pose_output_loss: 0.9241 - val_footwear_output_loss: 0.9495 - val_emotion_output_loss: 0.9175 - val_gender_output_acc: 0.6166 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6166 - val_footwear_output_acc: 0.5556 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 26.65510\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 51.1792 - gender_output_loss: 0.7468 - image_quality_output_loss: 0.9928 - age_output_loss: 2.9420 - weight_output_loss: 2.5574 - bag_output_loss: 1.4935 - pose_output_loss: 1.7312 - footwear_output_loss: 1.2902 - emotion_output_loss: 3.0600 - gender_output_acc: 0.5981 - image_quality_output_acc: 0.5400 - age_output_acc: 0.3813 - weight_output_acc: 0.6512 - bag_output_acc: 0.5613 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5375 - emotion_output_acc: 0.6856 - val_loss: 26.5686 - val_gender_output_loss: 0.6492 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4000 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.9028 - val_pose_output_loss: 0.9077 - val_footwear_output_loss: 0.9439 - val_emotion_output_loss: 0.9193 - val_gender_output_acc: 0.5955 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5694 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00007: val_loss improved from 26.65510 to 26.56857, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.007.h5\n",
            "Epoch 8/100\n",
            " 20/100 [=====>........................] - ETA: 24s - loss: 51.5333 - gender_output_loss: 0.7388 - image_quality_output_loss: 0.9927 - age_output_loss: 2.9885 - weight_output_loss: 2.9688 - bag_output_loss: 1.4910 - pose_output_loss: 1.7793 - footwear_output_loss: 1.4466 - emotion_output_loss: 2.6870 - gender_output_acc: 0.6125 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4313 - weight_output_acc: 0.6344 - bag_output_acc: 0.5594 - pose_output_acc: 0.6094 - footwear_output_acc: 0.4719 - emotion_output_acc: 0.7281\n",
            "100/100 [==============================] - 48s 483ms/step - loss: 50.0535 - gender_output_loss: 0.7463 - image_quality_output_loss: 0.9646 - age_output_loss: 2.9931 - weight_output_loss: 2.7491 - bag_output_loss: 1.5932 - pose_output_loss: 1.7599 - footwear_output_loss: 1.3750 - emotion_output_loss: 2.4618 - gender_output_acc: 0.6100 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3875 - weight_output_acc: 0.6312 - bag_output_acc: 0.5525 - pose_output_acc: 0.6106 - footwear_output_acc: 0.5075 - emotion_output_acc: 0.7350 - val_loss: 26.4580 - val_gender_output_loss: 0.6331 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.3970 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8963 - val_pose_output_loss: 0.9017 - val_footwear_output_loss: 0.9421 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.6432 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5645 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5295 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00008: val_loss improved from 26.56857 to 26.45795, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.008.h5\n",
            "Epoch 9/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 50.4125 - gender_output_loss: 0.7389 - image_quality_output_loss: 0.9809 - age_output_loss: 2.8856 - weight_output_loss: 2.7594 - bag_output_loss: 1.5793 - pose_output_loss: 1.7471 - footwear_output_loss: 1.2600 - emotion_output_loss: 2.7253 - gender_output_acc: 0.6067 - image_quality_output_acc: 0.5511 - age_output_acc: 0.4104 - weight_output_acc: 0.6307 - bag_output_acc: 0.5593 - pose_output_acc: 0.6155 - footwear_output_acc: 0.5612 - emotion_output_acc: 0.7134\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 50.3769 - gender_output_loss: 0.7378 - image_quality_output_loss: 0.9816 - age_output_loss: 2.8918 - weight_output_loss: 2.7458 - bag_output_loss: 1.5735 - pose_output_loss: 1.7461 - footwear_output_loss: 1.2591 - emotion_output_loss: 2.7262 - gender_output_acc: 0.6075 - image_quality_output_acc: 0.5513 - age_output_acc: 0.4081 - weight_output_acc: 0.6306 - bag_output_acc: 0.5600 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5613 - emotion_output_acc: 0.7131 - val_loss: 26.5765 - val_gender_output_loss: 0.6348 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4025 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8974 - val_pose_output_loss: 0.9053 - val_footwear_output_loss: 0.9848 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.6417 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5669 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.4897 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 26.45795\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 49.7283 - gender_output_loss: 0.7342 - image_quality_output_loss: 0.9706 - age_output_loss: 2.8873 - weight_output_loss: 2.5211 - bag_output_loss: 1.5386 - pose_output_loss: 1.6805 - footwear_output_loss: 1.3194 - emotion_output_loss: 2.7920 - gender_output_acc: 0.6338 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3825 - weight_output_acc: 0.6469 - bag_output_acc: 0.5787 - pose_output_acc: 0.6331 - footwear_output_acc: 0.5319 - emotion_output_acc: 0.7100 - val_loss: 26.6652 - val_gender_output_loss: 0.6323 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4077 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.9113 - val_pose_output_loss: 0.9064 - val_footwear_output_loss: 0.9909 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.6363 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.5536 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.4838 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 26.45795\n",
            "Epoch 11/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 50.7285 - gender_output_loss: 0.7435 - image_quality_output_loss: 0.9827 - age_output_loss: 2.8546 - weight_output_loss: 2.8154 - bag_output_loss: 1.5902 - pose_output_loss: 1.6923 - footwear_output_loss: 1.3036 - emotion_output_loss: 2.8083 - gender_output_acc: 0.6111 - image_quality_output_acc: 0.5455 - age_output_acc: 0.3864 - weight_output_acc: 0.6199 - bag_output_acc: 0.5745 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5518 - emotion_output_acc: 0.7064\n",
            "Epoch 00010: val_loss did not improve from 26.45795\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 50.6928 - gender_output_loss: 0.7440 - image_quality_output_loss: 0.9823 - age_output_loss: 2.8638 - weight_output_loss: 2.8004 - bag_output_loss: 1.5860 - pose_output_loss: 1.6916 - footwear_output_loss: 1.3018 - emotion_output_loss: 2.8059 - gender_output_acc: 0.6100 - image_quality_output_acc: 0.5456 - age_output_acc: 0.3862 - weight_output_acc: 0.6219 - bag_output_acc: 0.5731 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5525 - emotion_output_acc: 0.7069 - val_loss: 26.5504 - val_gender_output_loss: 0.6381 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.9054 - val_pose_output_loss: 0.9047 - val_footwear_output_loss: 0.9481 - val_emotion_output_loss: 0.9186 - val_gender_output_acc: 0.6294 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5591 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5586 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 26.45795\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 50.6912 - gender_output_loss: 0.7515 - image_quality_output_loss: 0.9644 - age_output_loss: 2.8528 - weight_output_loss: 2.6549 - bag_output_loss: 1.6289 - pose_output_loss: 1.7678 - footwear_output_loss: 1.3072 - emotion_output_loss: 2.8454 - gender_output_acc: 0.6006 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3981 - weight_output_acc: 0.6238 - bag_output_acc: 0.5494 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5306 - emotion_output_acc: 0.7056 - val_loss: 26.5939 - val_gender_output_loss: 0.6547 - val_image_quality_output_loss: 0.9842 - val_age_output_loss: 1.4118 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9039 - val_pose_output_loss: 0.9137 - val_footwear_output_loss: 0.9582 - val_emotion_output_loss: 0.9211 - val_gender_output_acc: 0.5851 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3853 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5655 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5630 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 26.45795\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 50.0675 - gender_output_loss: 0.7494 - image_quality_output_loss: 0.9817 - age_output_loss: 2.8046 - weight_output_loss: 2.7098 - bag_output_loss: 1.5788 - pose_output_loss: 1.7818 - footwear_output_loss: 1.3230 - emotion_output_loss: 2.7165 - gender_output_acc: 0.6019 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4119 - weight_output_acc: 0.6469 - bag_output_acc: 0.5644 - pose_output_acc: 0.6069 - footwear_output_acc: 0.5319 - emotion_output_acc: 0.7119 - val_loss: 26.4649 - val_gender_output_loss: 0.6330 - val_image_quality_output_loss: 0.9822 - val_age_output_loss: 1.3932 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.8984 - val_pose_output_loss: 0.9112 - val_footwear_output_loss: 0.9498 - val_emotion_output_loss: 0.9315 - val_gender_output_acc: 0.6339 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5581 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5231 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 26.45795\n",
            "Epoch 14/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 50.2322 - gender_output_loss: 0.7330 - image_quality_output_loss: 0.9854 - age_output_loss: 2.8974 - weight_output_loss: 2.4605 - bag_output_loss: 1.4839 - pose_output_loss: 1.7338 - footwear_output_loss: 1.3216 - emotion_output_loss: 2.9727 - gender_output_acc: 0.6301 - image_quality_output_acc: 0.5511 - age_output_acc: 0.3838 - weight_output_acc: 0.6553 - bag_output_acc: 0.5947 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5335 - emotion_output_acc: 0.6982Epoch 14/100\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 50.2272 - gender_output_loss: 0.7325 - image_quality_output_loss: 0.9845 - age_output_loss: 2.9152 - weight_output_loss: 2.4489 - bag_output_loss: 1.4790 - pose_output_loss: 1.7423 - footwear_output_loss: 1.3181 - emotion_output_loss: 2.9623 - gender_output_acc: 0.6306 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3844 - weight_output_acc: 0.6569 - bag_output_acc: 0.5969 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5344 - emotion_output_acc: 0.6988 - val_loss: 26.4188 - val_gender_output_loss: 0.6455 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4006 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9064 - val_pose_output_loss: 0.9068 - val_footwear_output_loss: 0.9355 - val_emotion_output_loss: 0.9221 - val_gender_output_acc: 0.5979 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5605 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5595 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.45795 to 26.41880, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.014.h5\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 49.2796 - gender_output_loss: 0.7424 - image_quality_output_loss: 0.9779 - age_output_loss: 2.8134 - weight_output_loss: 2.5206 - bag_output_loss: 1.5436 - pose_output_loss: 1.7745 - footwear_output_loss: 1.2825 - emotion_output_loss: 2.7214 - gender_output_acc: 0.6131 - image_quality_output_acc: 0.5288 - age_output_acc: 0.4113 - weight_output_acc: 0.6325 - bag_output_acc: 0.5831 - pose_output_acc: 0.6056 - footwear_output_acc: 0.5475 - emotion_output_acc: 0.7169 - val_loss: 26.2477 - val_gender_output_loss: 0.6259 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.3905 - val_weight_output_loss: 0.9740 - val_bag_output_loss: 0.8929 - val_pose_output_loss: 0.8954 - val_footwear_output_loss: 0.9417 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.6467 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5694 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5261 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00015: val_loss improved from 26.41880 to 26.24769, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.015.h5\n",
            "Epoch 16/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 49.9076 - gender_output_loss: 0.7193 - image_quality_output_loss: 0.9749 - age_output_loss: 2.8550 - weight_output_loss: 2.5633 - bag_output_loss: 1.6150 - pose_output_loss: 1.7684 - footwear_output_loss: 1.2898 - emotion_output_loss: 2.7676 - gender_output_acc: 0.6231 - image_quality_output_acc: 0.5486 - age_output_acc: 0.4097 - weight_output_acc: 0.6471 - bag_output_acc: 0.5707 - pose_output_acc: 0.6067 - footwear_output_acc: 0.5467 - emotion_output_acc: 0.7140\n",
            "Epoch 00015: val_loss improved from 26.41880 to 26.24769, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.015.h5\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 49.9396 - gender_output_loss: 0.7198 - image_quality_output_loss: 0.9749 - age_output_loss: 2.8495 - weight_output_loss: 2.5791 - bag_output_loss: 1.6188 - pose_output_loss: 1.7585 - footwear_output_loss: 1.2870 - emotion_output_loss: 2.7749 - gender_output_acc: 0.6219 - image_quality_output_acc: 0.5487 - age_output_acc: 0.4088 - weight_output_acc: 0.6450 - bag_output_acc: 0.5694 - pose_output_acc: 0.6094 - footwear_output_acc: 0.5475 - emotion_output_acc: 0.7131 - val_loss: 26.1951 - val_gender_output_loss: 0.6284 - val_image_quality_output_loss: 0.9791 - val_age_output_loss: 1.3935 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.8960 - val_pose_output_loss: 0.8935 - val_footwear_output_loss: 0.9131 - val_emotion_output_loss: 0.9194 - val_gender_output_acc: 0.6289 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5699 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5620 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00016: val_loss improved from 26.24769 to 26.19514, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.016.h5\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 50.4656 - gender_output_loss: 0.7598 - image_quality_output_loss: 0.9551 - age_output_loss: 2.9454 - weight_output_loss: 2.6054 - bag_output_loss: 1.5496 - pose_output_loss: 1.7091 - footwear_output_loss: 1.3231 - emotion_output_loss: 2.8525 - gender_output_acc: 0.5931 - image_quality_output_acc: 0.5775 - age_output_acc: 0.3719 - weight_output_acc: 0.6244 - bag_output_acc: 0.5675 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5281 - emotion_output_acc: 0.7019 - val_loss: 26.2382 - val_gender_output_loss: 0.6207 - val_image_quality_output_loss: 0.9789 - val_age_output_loss: 1.3997 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8964 - val_pose_output_loss: 0.8941 - val_footwear_output_loss: 0.9312 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.6516 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5659 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5463 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 26.19514\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 49.7294 - gender_output_loss: 0.7299 - image_quality_output_loss: 0.9687 - age_output_loss: 2.8082 - weight_output_loss: 2.7286 - bag_output_loss: 1.5069 - pose_output_loss: 1.7368 - footwear_output_loss: 1.2780 - emotion_output_loss: 2.7575 - gender_output_acc: 0.6344 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3856 - weight_output_acc: 0.6238 - bag_output_acc: 0.5931 - pose_output_acc: 0.6138 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7100 - val_loss: 26.2098 - val_gender_output_loss: 0.6367 - val_image_quality_output_loss: 0.9796 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.8932 - val_pose_output_loss: 0.8949 - val_footwear_output_loss: 0.9124 - val_emotion_output_loss: 0.9178 - val_gender_output_acc: 0.6122 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5615 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5669 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 26.19514\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 46s 459ms/step - loss: 49.5797 - gender_output_loss: 0.7236 - image_quality_output_loss: 0.9687 - age_output_loss: 2.8464 - weight_output_loss: 2.7587 - bag_output_loss: 1.5347 - pose_output_loss: 1.7994 - footwear_output_loss: 1.2841 - emotion_output_loss: 2.5960 - gender_output_acc: 0.6306 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4000 - weight_output_acc: 0.6238 - bag_output_acc: 0.5687 - pose_output_acc: 0.5962 - footwear_output_acc: 0.5244 - emotion_output_acc: 0.7238 - val_loss: 26.3166 - val_gender_output_loss: 0.6368 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9010 - val_pose_output_loss: 0.9136 - val_footwear_output_loss: 0.9418 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.6137 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5664 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5566 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 26.19514\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 45s 455ms/step - loss: 48.3564 - gender_output_loss: 0.7334 - image_quality_output_loss: 0.9840 - age_output_loss: 2.7999 - weight_output_loss: 2.4549 - bag_output_loss: 1.5526 - pose_output_loss: 1.6831 - footwear_output_loss: 1.2959 - emotion_output_loss: 2.6262 - gender_output_acc: 0.6162 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4069 - weight_output_acc: 0.6556 - bag_output_acc: 0.5750 - pose_output_acc: 0.6312 - footwear_output_acc: 0.5381 - emotion_output_acc: 0.7219 - val_loss: 26.5596 - val_gender_output_loss: 0.6476 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4229 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.9149 - val_pose_output_loss: 0.9198 - val_footwear_output_loss: 0.9559 - val_emotion_output_loss: 0.9255 - val_gender_output_acc: 0.5945 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5438 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 26.19514\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 49.5050 - gender_output_loss: 0.7364 - image_quality_output_loss: 0.9792 - age_output_loss: 2.8583 - weight_output_loss: 2.6062 - bag_output_loss: 1.5885 - pose_output_loss: 1.6941 - footwear_output_loss: 1.3172 - emotion_output_loss: 2.7044 - gender_output_acc: 0.6287 - image_quality_output_acc: 0.5400 - age_output_acc: 0.3987 - weight_output_acc: 0.6488 - bag_output_acc: 0.5569 - pose_output_acc: 0.6338 - footwear_output_acc: 0.5400 - emotion_output_acc: 0.7163 - val_loss: 26.3415 - val_gender_output_loss: 0.6422 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.4059 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.9005 - val_pose_output_loss: 0.9061 - val_footwear_output_loss: 0.9575 - val_emotion_output_loss: 0.9161 - val_gender_output_acc: 0.5915 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5502 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 26.19514\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 49.6648 - gender_output_loss: 0.7298 - image_quality_output_loss: 0.9799 - age_output_loss: 2.8979 - weight_output_loss: 2.5721 - bag_output_loss: 1.5843 - pose_output_loss: 1.7511 - footwear_output_loss: 1.3406 - emotion_output_loss: 2.6884 - gender_output_acc: 0.6325 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4069 - weight_output_acc: 0.6475 - bag_output_acc: 0.5763 - pose_output_acc: 0.6081 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7150 - val_loss: 26.1760 - val_gender_output_loss: 0.6275 - val_image_quality_output_loss: 0.9786 - val_age_output_loss: 1.3962 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.8985 - val_pose_output_loss: 0.8956 - val_footwear_output_loss: 0.9413 - val_emotion_output_loss: 0.9207 - val_gender_output_acc: 0.6358 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5709 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5458 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00022: val_loss improved from 26.19514 to 26.17596, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.022.h5\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 50.3130 - gender_output_loss: 0.7159 - image_quality_output_loss: 0.9626 - age_output_loss: 2.9902 - weight_output_loss: 2.5798 - bag_output_loss: 1.6142 - pose_output_loss: 1.6970 - footwear_output_loss: 1.2279 - emotion_output_loss: 2.8469 - gender_output_acc: 0.6356 - image_quality_output_acc: 0.5706 - age_output_acc: 0.3806 - weight_output_acc: 0.6338 - bag_output_acc: 0.5694 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5769 - emotion_output_acc: 0.7106 - val_loss: 26.2090 - val_gender_output_loss: 0.6330 - val_image_quality_output_loss: 0.9804 - val_age_output_loss: 1.4055 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9033 - val_pose_output_loss: 0.8944 - val_footwear_output_loss: 0.9254 - val_emotion_output_loss: 0.9202 - val_gender_output_acc: 0.6230 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5645 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5645 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 26.17596\n",
            "Epoch 24/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 49.7560 - gender_output_loss: 0.7273 - image_quality_output_loss: 1.0044 - age_output_loss: 2.8475 - weight_output_loss: 2.7331 - bag_output_loss: 1.6405 - pose_output_loss: 1.6939 - footwear_output_loss: 1.3424 - emotion_output_loss: 2.6364 - gender_output_acc: 0.6402 - image_quality_output_acc: 0.5215 - age_output_acc: 0.4066 - weight_output_acc: 0.6332 - bag_output_acc: 0.5676 - pose_output_acc: 0.6269 - footwear_output_acc: 0.5354 - emotion_output_acc: 0.7134\n",
            "Epoch 00023: val_loss did not improve from 26.17596\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 49.7369 - gender_output_loss: 0.7277 - image_quality_output_loss: 1.0019 - age_output_loss: 2.8506 - weight_output_loss: 2.7281 - bag_output_loss: 1.6378 - pose_output_loss: 1.6923 - footwear_output_loss: 1.3392 - emotion_output_loss: 2.6382 - gender_output_acc: 0.6400 - image_quality_output_acc: 0.5244 - age_output_acc: 0.4062 - weight_output_acc: 0.6338 - bag_output_acc: 0.5694 - pose_output_acc: 0.6269 - footwear_output_acc: 0.5363 - emotion_output_acc: 0.7131 - val_loss: 26.0675 - val_gender_output_loss: 0.6173 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.3948 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.9007 - val_pose_output_loss: 0.8886 - val_footwear_output_loss: 0.9235 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.6506 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5645 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5507 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00024: val_loss improved from 26.17596 to 26.06746, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.024.h5\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 45s 451ms/step - loss: 49.0403 - gender_output_loss: 0.7333 - image_quality_output_loss: 0.9648 - age_output_loss: 2.8348 - weight_output_loss: 2.6979 - bag_output_loss: 1.4821 - pose_output_loss: 1.6509 - footwear_output_loss: 1.2733 - emotion_output_loss: 2.6997 - gender_output_acc: 0.6194 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4131 - weight_output_acc: 0.6319 - bag_output_acc: 0.5869 - pose_output_acc: 0.6356 - footwear_output_acc: 0.5687 - emotion_output_acc: 0.7206 - val_loss: 26.0254 - val_gender_output_loss: 0.6174 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.3893 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8953 - val_pose_output_loss: 0.8878 - val_footwear_output_loss: 0.9164 - val_emotion_output_loss: 0.9252 - val_gender_output_acc: 0.6486 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5773 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5615 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00025: val_loss improved from 26.06746 to 26.02542, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.025.h5\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 50.0917 - gender_output_loss: 0.7478 - image_quality_output_loss: 0.9696 - age_output_loss: 2.9735 - weight_output_loss: 2.5513 - bag_output_loss: 1.6022 - pose_output_loss: 1.7172 - footwear_output_loss: 1.2960 - emotion_output_loss: 2.7752 - gender_output_acc: 0.6213 - image_quality_output_acc: 0.5588 - age_output_acc: 0.3881 - weight_output_acc: 0.6419 - bag_output_acc: 0.5463 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5450 - emotion_output_acc: 0.7069 - val_loss: 26.0515 - val_gender_output_loss: 0.6237 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.3906 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9084 - val_pose_output_loss: 0.8871 - val_footwear_output_loss: 0.9193 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.6471 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5674 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5733 - val_emotion_output_acc: 0.7062\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00026: val_loss did not improve from 26.02542\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 49.4293 - gender_output_loss: 0.7296 - image_quality_output_loss: 0.9710 - age_output_loss: 2.8431 - weight_output_loss: 2.6429 - bag_output_loss: 1.4310 - pose_output_loss: 1.6748 - footwear_output_loss: 1.2912 - emotion_output_loss: 2.8465 - gender_output_acc: 0.6300 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4006 - weight_output_acc: 0.6375 - bag_output_acc: 0.5919 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5450 - emotion_output_acc: 0.7044 - val_loss: 26.1502 - val_gender_output_loss: 0.6344 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.3968 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.9112 - val_pose_output_loss: 0.8968 - val_footwear_output_loss: 0.9407 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.6255 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5610 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5723 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 26.02542\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 47s 469ms/step - loss: 49.6030 - gender_output_loss: 0.7285 - image_quality_output_loss: 0.9625 - age_output_loss: 2.7481 - weight_output_loss: 2.5893 - bag_output_loss: 1.5931 - pose_output_loss: 1.8385 - footwear_output_loss: 1.2715 - emotion_output_loss: 2.8017 - gender_output_acc: 0.6250 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4113 - weight_output_acc: 0.6356 - bag_output_acc: 0.5656 - pose_output_acc: 0.5875 - footwear_output_acc: 0.5613 - emotion_output_acc: 0.7069 - val_loss: 26.3527 - val_gender_output_loss: 0.6322 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4256 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.8989 - val_pose_output_loss: 0.9129 - val_footwear_output_loss: 0.9564 - val_emotion_output_loss: 0.9335 - val_gender_output_acc: 0.6265 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5586 - val_pose_output_acc: 0.6181 - val_footwear_output_acc: 0.5502 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 26.02542\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 49.0227 - gender_output_loss: 0.7401 - image_quality_output_loss: 0.9782 - age_output_loss: 2.8378 - weight_output_loss: 2.5808 - bag_output_loss: 1.5680 - pose_output_loss: 1.6988 - footwear_output_loss: 1.3363 - emotion_output_loss: 2.6582 - gender_output_acc: 0.6175 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3956 - weight_output_acc: 0.6462 - bag_output_acc: 0.5731 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5200 - emotion_output_acc: 0.7188 - val_loss: 26.1662 - val_gender_output_loss: 0.6428 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.4035 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.8998 - val_pose_output_loss: 0.9029 - val_footwear_output_loss: 0.9632 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.6378 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5679 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5645 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 26.02542\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 49.8437 - gender_output_loss: 0.7334 - image_quality_output_loss: 0.9702 - age_output_loss: 2.8144 - weight_output_loss: 2.6562 - bag_output_loss: 1.5819 - pose_output_loss: 1.7131 - footwear_output_loss: 1.2818 - emotion_output_loss: 2.8499 - gender_output_acc: 0.6231 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4319 - weight_output_acc: 0.6469 - bag_output_acc: 0.5725 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7062 - val_loss: 26.5144 - val_gender_output_loss: 0.6213 - val_image_quality_output_loss: 0.9918 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9923 - val_bag_output_loss: 0.8938 - val_pose_output_loss: 0.9184 - val_footwear_output_loss: 0.9723 - val_emotion_output_loss: 0.9862 - val_gender_output_acc: 0.6491 - val_image_quality_output_acc: 0.5364 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.5802 - val_pose_output_acc: 0.6201 - val_footwear_output_acc: 0.5276 - val_emotion_output_acc: 0.6919\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 26.02542\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 50.3918 - gender_output_loss: 0.7349 - image_quality_output_loss: 0.9759 - age_output_loss: 2.8659 - weight_output_loss: 2.6361 - bag_output_loss: 1.5605 - pose_output_loss: 1.7229 - footwear_output_loss: 1.3219 - emotion_output_loss: 2.9397 - gender_output_acc: 0.6356 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3881 - weight_output_acc: 0.6369 - bag_output_acc: 0.5769 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5387 - emotion_output_acc: 0.6931 - val_loss: 25.9911 - val_gender_output_loss: 0.6224 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3953 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8945 - val_pose_output_loss: 0.8901 - val_footwear_output_loss: 0.9361 - val_emotion_output_loss: 0.9231 - val_gender_output_acc: 0.6570 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5664 - val_pose_output_acc: 0.6230 - val_footwear_output_acc: 0.5443 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00031: val_loss improved from 26.02542 to 25.99112, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.031.h5\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 45s 452ms/step - loss: 49.3688 - gender_output_loss: 0.7218 - image_quality_output_loss: 0.9799 - age_output_loss: 2.8649 - weight_output_loss: 2.8011 - bag_output_loss: 1.5803 - pose_output_loss: 1.7160 - footwear_output_loss: 1.3001 - emotion_output_loss: 2.5691 - gender_output_acc: 0.6262 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3906 - weight_output_acc: 0.6056 - bag_output_acc: 0.5706 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5588 - emotion_output_acc: 0.7319 - val_loss: 25.8712 - val_gender_output_loss: 0.6190 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8879 - val_pose_output_loss: 0.8783 - val_footwear_output_loss: 0.9203 - val_emotion_output_loss: 0.9203 - val_gender_output_acc: 0.6496 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5817 - val_pose_output_acc: 0.6260 - val_footwear_output_acc: 0.5625 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00032: val_loss improved from 25.99112 to 25.87123, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.032.h5\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 45s 448ms/step - loss: 49.2102 - gender_output_loss: 0.7237 - image_quality_output_loss: 0.9672 - age_output_loss: 2.8488 - weight_output_loss: 2.7414 - bag_output_loss: 1.5474 - pose_output_loss: 1.6073 - footwear_output_loss: 1.3160 - emotion_output_loss: 2.6948 - gender_output_acc: 0.6456 - image_quality_output_acc: 0.5631 - age_output_acc: 0.3787 - weight_output_acc: 0.6275 - bag_output_acc: 0.5794 - pose_output_acc: 0.6306 - footwear_output_acc: 0.5363 - emotion_output_acc: 0.7100 - val_loss: 25.9806 - val_gender_output_loss: 0.6130 - val_image_quality_output_loss: 0.9915 - val_age_output_loss: 1.4012 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8864 - val_pose_output_loss: 0.8811 - val_footwear_output_loss: 0.9518 - val_emotion_output_loss: 0.9222 - val_gender_output_acc: 0.6644 - val_image_quality_output_acc: 0.5389 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5851 - val_pose_output_acc: 0.6230 - val_footwear_output_acc: 0.5221 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 25.87123\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 49.5350 - gender_output_loss: 0.7167 - image_quality_output_loss: 0.9861 - age_output_loss: 2.9362 - weight_output_loss: 2.4850 - bag_output_loss: 1.5168 - pose_output_loss: 1.7642 - footwear_output_loss: 1.2814 - emotion_output_loss: 2.7997 - gender_output_acc: 0.6613 - image_quality_output_acc: 0.5400 - age_output_acc: 0.3900 - weight_output_acc: 0.6556 - bag_output_acc: 0.5662 - pose_output_acc: 0.5981 - footwear_output_acc: 0.5575 - emotion_output_acc: 0.7113 - val_loss: 26.7494 - val_gender_output_loss: 0.6175 - val_image_quality_output_loss: 0.9916 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9946 - val_bag_output_loss: 0.9021 - val_pose_output_loss: 0.8901 - val_footwear_output_loss: 1.0989 - val_emotion_output_loss: 0.9941 - val_gender_output_acc: 0.6713 - val_image_quality_output_acc: 0.5364 - val_age_output_acc: 0.3922 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5674 - val_pose_output_acc: 0.6211 - val_footwear_output_acc: 0.4316 - val_emotion_output_acc: 0.6983\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 25.87123\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 49.7390 - gender_output_loss: 0.7286 - image_quality_output_loss: 0.9612 - age_output_loss: 2.9018 - weight_output_loss: 2.6724 - bag_output_loss: 1.5788 - pose_output_loss: 1.6828 - footwear_output_loss: 1.2895 - emotion_output_loss: 2.7654 - gender_output_acc: 0.6419 - image_quality_output_acc: 0.5731 - age_output_acc: 0.3862 - weight_output_acc: 0.6275 - bag_output_acc: 0.5794 - pose_output_acc: 0.6238 - footwear_output_acc: 0.5525 - emotion_output_acc: 0.7144 - val_loss: 25.8452 - val_gender_output_loss: 0.6184 - val_image_quality_output_loss: 0.9867 - val_age_output_loss: 1.3920 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8963 - val_pose_output_loss: 0.8793 - val_footwear_output_loss: 0.9148 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.6570 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5763 - val_pose_output_acc: 0.6245 - val_footwear_output_acc: 0.5576 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00035: val_loss improved from 25.87123 to 25.84525, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.035.h5\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 44s 441ms/step - loss: 48.5566 - gender_output_loss: 0.7286 - image_quality_output_loss: 0.9671 - age_output_loss: 2.8038 - weight_output_loss: 2.3926 - bag_output_loss: 1.5453 - pose_output_loss: 1.7604 - footwear_output_loss: 1.3439 - emotion_output_loss: 2.7204 - gender_output_acc: 0.6300 - image_quality_output_acc: 0.5613 - age_output_acc: 0.4075 - weight_output_acc: 0.6631 - bag_output_acc: 0.5656 - pose_output_acc: 0.6056 - footwear_output_acc: 0.5212 - emotion_output_acc: 0.7163 - val_loss: 25.9202 - val_gender_output_loss: 0.6158 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8871 - val_pose_output_loss: 0.8962 - val_footwear_output_loss: 0.9476 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.6594 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5758 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5404 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 25.84525\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 49.9549 - gender_output_loss: 0.7313 - image_quality_output_loss: 0.9797 - age_output_loss: 3.0411 - weight_output_loss: 2.4895 - bag_output_loss: 1.5300 - pose_output_loss: 1.7661 - footwear_output_loss: 1.2610 - emotion_output_loss: 2.8081 - gender_output_acc: 0.6344 - image_quality_output_acc: 0.5475 - age_output_acc: 0.3813 - weight_output_acc: 0.6338 - bag_output_acc: 0.5731 - pose_output_acc: 0.6038 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7113 - val_loss: 25.8803 - val_gender_output_loss: 0.6290 - val_image_quality_output_loss: 0.9776 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8880 - val_pose_output_loss: 0.8890 - val_footwear_output_loss: 0.9211 - val_emotion_output_loss: 0.9208 - val_gender_output_acc: 0.6294 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5723 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5842 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 25.84525\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 48.4478 - gender_output_loss: 0.7241 - image_quality_output_loss: 0.9537 - age_output_loss: 2.8589 - weight_output_loss: 2.5307 - bag_output_loss: 1.5748 - pose_output_loss: 1.6968 - footwear_output_loss: 1.3204 - emotion_output_loss: 2.5937 - gender_output_acc: 0.6331 - image_quality_output_acc: 0.5694 - age_output_acc: 0.3856 - weight_output_acc: 0.6544 - bag_output_acc: 0.5844 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5463 - emotion_output_acc: 0.7269 - val_loss: 25.8627 - val_gender_output_loss: 0.6411 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4060 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8905 - val_pose_output_loss: 0.8852 - val_footwear_output_loss: 0.9170 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.6004 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3893 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5699 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5778 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 25.84525\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 50.0045 - gender_output_loss: 0.7284 - image_quality_output_loss: 0.9777 - age_output_loss: 2.7821 - weight_output_loss: 2.7061 - bag_output_loss: 1.5616 - pose_output_loss: 1.6937 - footwear_output_loss: 1.2748 - emotion_output_loss: 2.9531 - gender_output_acc: 0.6319 - image_quality_output_acc: 0.5400 - age_output_acc: 0.4150 - weight_output_acc: 0.6525 - bag_output_acc: 0.5675 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5531 - emotion_output_acc: 0.6950 - val_loss: 25.7089 - val_gender_output_loss: 0.6215 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.3890 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.8865 - val_pose_output_loss: 0.8816 - val_footwear_output_loss: 0.9176 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.6481 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5837 - val_pose_output_acc: 0.6206 - val_footwear_output_acc: 0.5733 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00039: val_loss improved from 25.84525 to 25.70888, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.039.h5\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 49.7867 - gender_output_loss: 0.7204 - image_quality_output_loss: 0.9603 - age_output_loss: 2.8802 - weight_output_loss: 2.7378 - bag_output_loss: 1.4648 - pose_output_loss: 1.7277 - footwear_output_loss: 1.2660 - emotion_output_loss: 2.8430 - gender_output_acc: 0.6400 - image_quality_output_acc: 0.5687 - age_output_acc: 0.3963 - weight_output_acc: 0.6269 - bag_output_acc: 0.6050 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7075 - val_loss: 25.6469 - val_gender_output_loss: 0.6177 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.3845 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8878 - val_pose_output_loss: 0.8778 - val_footwear_output_loss: 0.9114 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.6526 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5748 - val_pose_output_acc: 0.6216 - val_footwear_output_acc: 0.5758 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00040: val_loss improved from 25.70888 to 25.64688, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.040.h5\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 45s 451ms/step - loss: 48.7232 - gender_output_loss: 0.7046 - image_quality_output_loss: 0.9762 - age_output_loss: 2.8582 - weight_output_loss: 2.5406 - bag_output_loss: 1.5171 - pose_output_loss: 1.7445 - footwear_output_loss: 1.2929 - emotion_output_loss: 2.6824 - gender_output_acc: 0.6688 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3875 - weight_output_acc: 0.6450 - bag_output_acc: 0.5938 - pose_output_acc: 0.6112 - footwear_output_acc: 0.5537 - emotion_output_acc: 0.7137 - val_loss: 25.6342 - val_gender_output_loss: 0.6134 - val_image_quality_output_loss: 0.9806 - val_age_output_loss: 1.3893 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8823 - val_pose_output_loss: 0.8725 - val_footwear_output_loss: 0.9172 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.6526 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5822 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5591 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00041: val_loss improved from 25.64688 to 25.63424, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.041.h5\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 48.7305 - gender_output_loss: 0.7181 - image_quality_output_loss: 0.9843 - age_output_loss: 2.6630 - weight_output_loss: 2.6453 - bag_output_loss: 1.6043 - pose_output_loss: 1.6979 - footwear_output_loss: 1.2804 - emotion_output_loss: 2.7679 - gender_output_acc: 0.6450 - image_quality_output_acc: 0.5406 - age_output_acc: 0.4106 - weight_output_acc: 0.6225 - bag_output_acc: 0.5619 - pose_output_acc: 0.6088 - footwear_output_acc: 0.5481 - emotion_output_acc: 0.7175 - val_loss: 25.6373 - val_gender_output_loss: 0.6132 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8826 - val_pose_output_loss: 0.8679 - val_footwear_output_loss: 0.9234 - val_emotion_output_loss: 0.9148 - val_gender_output_acc: 0.6516 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.5851 - val_pose_output_acc: 0.6255 - val_footwear_output_acc: 0.5512 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 25.63424\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 48.8279 - gender_output_loss: 0.7236 - image_quality_output_loss: 0.9674 - age_output_loss: 2.9257 - weight_output_loss: 2.6370 - bag_output_loss: 1.5696 - pose_output_loss: 1.6567 - footwear_output_loss: 1.2896 - emotion_output_loss: 2.5976 - gender_output_acc: 0.6406 - image_quality_output_acc: 0.5656 - age_output_acc: 0.3831 - weight_output_acc: 0.6356 - bag_output_acc: 0.5763 - pose_output_acc: 0.6294 - footwear_output_acc: 0.5556 - emotion_output_acc: 0.7169 - val_loss: 25.7129 - val_gender_output_loss: 0.6183 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.3914 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8886 - val_pose_output_loss: 0.8810 - val_footwear_output_loss: 0.9183 - val_emotion_output_loss: 0.9257 - val_gender_output_acc: 0.6476 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5802 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5753 - val_emotion_output_acc: 0.7062\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 25.63424\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 49.4804 - gender_output_loss: 0.7386 - image_quality_output_loss: 0.9902 - age_output_loss: 2.8129 - weight_output_loss: 2.5695 - bag_output_loss: 1.6435 - pose_output_loss: 1.6500 - footwear_output_loss: 1.2737 - emotion_output_loss: 2.8685 - gender_output_acc: 0.6188 - image_quality_output_acc: 0.5394 - age_output_acc: 0.3994 - weight_output_acc: 0.6350 - bag_output_acc: 0.5675 - pose_output_acc: 0.6287 - footwear_output_acc: 0.5831 - emotion_output_acc: 0.7031 - val_loss: 25.8563 - val_gender_output_loss: 0.6243 - val_image_quality_output_loss: 0.9780 - val_age_output_loss: 1.4007 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8962 - val_pose_output_loss: 0.8891 - val_footwear_output_loss: 0.9511 - val_emotion_output_loss: 0.9279 - val_gender_output_acc: 0.6442 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5684 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5630 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 25.63424\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 49.3330 - gender_output_loss: 0.7324 - image_quality_output_loss: 0.9726 - age_output_loss: 2.8419 - weight_output_loss: 2.6788 - bag_output_loss: 1.5596 - pose_output_loss: 1.7091 - footwear_output_loss: 1.2846 - emotion_output_loss: 2.7525 - gender_output_acc: 0.6300 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3931 - weight_output_acc: 0.6225 - bag_output_acc: 0.5687 - pose_output_acc: 0.6063 - footwear_output_acc: 0.5444 - emotion_output_acc: 0.7113 - val_loss: 25.5208 - val_gender_output_loss: 0.6159 - val_image_quality_output_loss: 0.9793 - val_age_output_loss: 1.3905 - val_weight_output_loss: 0.9724 - val_bag_output_loss: 0.8849 - val_pose_output_loss: 0.8688 - val_footwear_output_loss: 0.9092 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.6481 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5842 - val_pose_output_acc: 0.6280 - val_footwear_output_acc: 0.5797 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00045: val_loss improved from 25.63424 to 25.52081, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.045.h5\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 45s 453ms/step - loss: 48.9784 - gender_output_loss: 0.7155 - image_quality_output_loss: 0.9599 - age_output_loss: 2.9624 - weight_output_loss: 2.5280 - bag_output_loss: 1.6301 - pose_output_loss: 1.7846 - footwear_output_loss: 1.2819 - emotion_output_loss: 2.5688 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5725 - age_output_acc: 0.3837 - weight_output_acc: 0.6481 - bag_output_acc: 0.5588 - pose_output_acc: 0.6006 - footwear_output_acc: 0.5425 - emotion_output_acc: 0.7250 - val_loss: 25.6297 - val_gender_output_loss: 0.6146 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8897 - val_pose_output_loss: 0.8733 - val_footwear_output_loss: 0.9374 - val_emotion_output_loss: 0.9132 - val_gender_output_acc: 0.6432 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5723 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5556 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 25.52081\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 46s 459ms/step - loss: 50.2980 - gender_output_loss: 0.7219 - image_quality_output_loss: 0.9793 - age_output_loss: 3.0318 - weight_output_loss: 2.7628 - bag_output_loss: 1.5905 - pose_output_loss: 1.7194 - footwear_output_loss: 1.2626 - emotion_output_loss: 2.7324 - gender_output_acc: 0.6469 - image_quality_output_acc: 0.5394 - age_output_acc: 0.3875 - weight_output_acc: 0.6231 - bag_output_acc: 0.5656 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5594 - emotion_output_acc: 0.7200 - val_loss: 25.5300 - val_gender_output_loss: 0.6157 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.3971 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8880 - val_pose_output_loss: 0.8743 - val_footwear_output_loss: 0.8983 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.6432 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5832 - val_pose_output_acc: 0.6230 - val_footwear_output_acc: 0.5886 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 25.52081\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 50.8676 - gender_output_loss: 0.7227 - image_quality_output_loss: 0.9894 - age_output_loss: 2.9529 - weight_output_loss: 2.8473 - bag_output_loss: 1.5012 - pose_output_loss: 1.7067 - footwear_output_loss: 1.2602 - emotion_output_loss: 2.9646 - gender_output_acc: 0.6456 - image_quality_output_acc: 0.5325 - age_output_acc: 0.3875 - weight_output_acc: 0.6294 - bag_output_acc: 0.5869 - pose_output_acc: 0.6075 - footwear_output_acc: 0.5594 - emotion_output_acc: 0.6900 - val_loss: 25.4754 - val_gender_output_loss: 0.5963 - val_image_quality_output_loss: 0.9801 - val_age_output_loss: 1.3888 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8881 - val_pose_output_loss: 0.8604 - val_footwear_output_loss: 0.9070 - val_emotion_output_loss: 0.9180 - val_gender_output_acc: 0.6742 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5782 - val_pose_output_acc: 0.6304 - val_footwear_output_acc: 0.5738 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "\n",
            "Epoch 00048: val_loss improved from 25.52081 to 25.47544, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.048.h5\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 47.0380 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9729 - age_output_loss: 2.8003 - weight_output_loss: 2.3907 - bag_output_loss: 1.4397 - pose_output_loss: 1.6110 - footwear_output_loss: 1.3015 - emotion_output_loss: 2.6292 - gender_output_acc: 0.6769 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3856 - weight_output_acc: 0.6556 - bag_output_acc: 0.5919 - pose_output_acc: 0.6381 - footwear_output_acc: 0.5513 - emotion_output_acc: 0.7175 - val_loss: 25.3628 - val_gender_output_loss: 0.5996 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.3824 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8798 - val_pose_output_loss: 0.8590 - val_footwear_output_loss: 0.8901 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.6649 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5945 - val_pose_output_acc: 0.6280 - val_footwear_output_acc: 0.5822 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00049: val_loss improved from 25.47544 to 25.36276, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.049.h5\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 47s 472ms/step - loss: 48.2806 - gender_output_loss: 0.6932 - image_quality_output_loss: 0.9557 - age_output_loss: 2.7641 - weight_output_loss: 2.5060 - bag_output_loss: 1.5524 - pose_output_loss: 1.6654 - footwear_output_loss: 1.2621 - emotion_output_loss: 2.7884 - gender_output_acc: 0.6681 - image_quality_output_acc: 0.5637 - age_output_acc: 0.4119 - weight_output_acc: 0.6400 - bag_output_acc: 0.5800 - pose_output_acc: 0.6188 - footwear_output_acc: 0.5687 - emotion_output_acc: 0.7125 - val_loss: 25.4751 - val_gender_output_loss: 0.6082 - val_image_quality_output_loss: 0.9806 - val_age_output_loss: 1.3862 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8845 - val_pose_output_loss: 0.8659 - val_footwear_output_loss: 0.9116 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.6575 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5802 - val_pose_output_acc: 0.6265 - val_footwear_output_acc: 0.5891 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 25.36276\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 48s 476ms/step - loss: 47.3037 - gender_output_loss: 0.7137 - image_quality_output_loss: 0.9387 - age_output_loss: 2.7047 - weight_output_loss: 2.5451 - bag_output_loss: 1.5475 - pose_output_loss: 1.6935 - footwear_output_loss: 1.2576 - emotion_output_loss: 2.5610 - gender_output_acc: 0.6619 - image_quality_output_acc: 0.5875 - age_output_acc: 0.4056 - weight_output_acc: 0.6531 - bag_output_acc: 0.5725 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5662 - emotion_output_acc: 0.7294 - val_loss: 25.4879 - val_gender_output_loss: 0.6090 - val_image_quality_output_loss: 0.9806 - val_age_output_loss: 1.3991 - val_weight_output_loss: 0.9748 - val_bag_output_loss: 0.8796 - val_pose_output_loss: 0.8699 - val_footwear_output_loss: 0.9149 - val_emotion_output_loss: 0.9143 - val_gender_output_acc: 0.6555 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5856 - val_pose_output_acc: 0.6255 - val_footwear_output_acc: 0.5758 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 25.36276\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 46s 459ms/step - loss: 50.5541 - gender_output_loss: 0.7432 - image_quality_output_loss: 0.9658 - age_output_loss: 2.9989 - weight_output_loss: 2.6916 - bag_output_loss: 1.5401 - pose_output_loss: 1.7566 - footwear_output_loss: 1.2578 - emotion_output_loss: 2.9047 - gender_output_acc: 0.6287 - image_quality_output_acc: 0.5644 - age_output_acc: 0.3550 - weight_output_acc: 0.6356 - bag_output_acc: 0.5806 - pose_output_acc: 0.6044 - footwear_output_acc: 0.5806 - emotion_output_acc: 0.7044 - val_loss: 25.6122 - val_gender_output_loss: 0.6014 - val_image_quality_output_loss: 0.9822 - val_age_output_loss: 1.3941 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.8925 - val_pose_output_loss: 0.8748 - val_footwear_output_loss: 0.9031 - val_emotion_output_loss: 0.9353 - val_gender_output_acc: 0.6634 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5797 - val_pose_output_acc: 0.6211 - val_footwear_output_acc: 0.5807 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 25.36276\n",
            "Epoch 53/100\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 50.0769 - gender_output_loss: 0.7138 - image_quality_output_loss: 0.9607 - age_output_loss: 2.9621 - weight_output_loss: 2.8163 - bag_output_loss: 1.4943 - pose_output_loss: 1.6656 - footwear_output_loss: 1.2934 - emotion_output_loss: 2.8373 - gender_output_acc: 0.6481 - image_quality_output_acc: 0.5600 - age_output_acc: 0.3881 - weight_output_acc: 0.6262 - bag_output_acc: 0.5913 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5469 - emotion_output_acc: 0.7044 - val_loss: 25.5452 - val_gender_output_loss: 0.6089 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.3974 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.8835 - val_pose_output_loss: 0.8858 - val_footwear_output_loss: 0.8985 - val_emotion_output_loss: 0.9262 - val_gender_output_acc: 0.6481 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5817 - val_pose_output_acc: 0.6201 - val_footwear_output_acc: 0.5950 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 25.36276\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 48.4110 - gender_output_loss: 0.7115 - image_quality_output_loss: 0.9733 - age_output_loss: 2.8077 - weight_output_loss: 2.6185 - bag_output_loss: 1.4539 - pose_output_loss: 1.7472 - footwear_output_loss: 1.2866 - emotion_output_loss: 2.6964 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4156 - weight_output_acc: 0.6412 - bag_output_acc: 0.6012 - pose_output_acc: 0.6088 - footwear_output_acc: 0.5425 - emotion_output_acc: 0.7206 - val_loss: 25.3565 - val_gender_output_loss: 0.6073 - val_image_quality_output_loss: 0.9799 - val_age_output_loss: 1.3847 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.8861 - val_pose_output_loss: 0.8697 - val_footwear_output_loss: 0.8979 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6575 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4060 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5758 - val_pose_output_acc: 0.6294 - val_footwear_output_acc: 0.5837 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00054: val_loss improved from 25.36276 to 25.35653, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.054.h5\n",
            "Epoch 55/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 48.2930 - gender_output_loss: 0.7144 - image_quality_output_loss: 0.9848 - age_output_loss: 2.8509 - weight_output_loss: 2.4507 - bag_output_loss: 1.6434 - pose_output_loss: 1.6331 - footwear_output_loss: 1.2815 - emotion_output_loss: 2.6919 - gender_output_acc: 0.6515 - image_quality_output_acc: 0.5385 - age_output_acc: 0.4040 - weight_output_acc: 0.6383 - bag_output_acc: 0.5764 - pose_output_acc: 0.6256 - footwear_output_acc: 0.5574 - emotion_output_acc: 0.7140\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 48.3229 - gender_output_loss: 0.7147 - image_quality_output_loss: 0.9864 - age_output_loss: 2.8543 - weight_output_loss: 2.4454 - bag_output_loss: 1.6435 - pose_output_loss: 1.6392 - footwear_output_loss: 1.2814 - emotion_output_loss: 2.6945 - gender_output_acc: 0.6506 - image_quality_output_acc: 0.5381 - age_output_acc: 0.4012 - weight_output_acc: 0.6394 - bag_output_acc: 0.5756 - pose_output_acc: 0.6250 - footwear_output_acc: 0.5569 - emotion_output_acc: 0.7137 - val_loss: 25.2789 - val_gender_output_loss: 0.5934 - val_image_quality_output_loss: 0.9752 - val_age_output_loss: 1.3886 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.8848 - val_pose_output_loss: 0.8699 - val_footwear_output_loss: 0.8861 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6880 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5896 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5960 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00055: val_loss improved from 25.35653 to 25.27888, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.055.h5\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 48.1862 - gender_output_loss: 0.7092 - image_quality_output_loss: 0.9873 - age_output_loss: 2.8240 - weight_output_loss: 2.5595 - bag_output_loss: 1.5142 - pose_output_loss: 1.5983 - footwear_output_loss: 1.2482 - emotion_output_loss: 2.7534 - gender_output_acc: 0.6488 - image_quality_output_acc: 0.5375 - age_output_acc: 0.4069 - weight_output_acc: 0.6469 - bag_output_acc: 0.5806 - pose_output_acc: 0.6350 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7056 - val_loss: 25.2793 - val_gender_output_loss: 0.5890 - val_image_quality_output_loss: 0.9752 - val_age_output_loss: 1.3907 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.8826 - val_pose_output_loss: 0.8637 - val_footwear_output_loss: 0.8915 - val_emotion_output_loss: 0.9156 - val_gender_output_acc: 0.6860 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5886 - val_pose_output_acc: 0.6270 - val_footwear_output_acc: 0.5817 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 25.27888\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 48.4830 - gender_output_loss: 0.7189 - image_quality_output_loss: 0.9550 - age_output_loss: 2.7463 - weight_output_loss: 2.6397 - bag_output_loss: 1.5962 - pose_output_loss: 1.6929 - footwear_output_loss: 1.2612 - emotion_output_loss: 2.7182 - gender_output_acc: 0.6488 - image_quality_output_acc: 0.5687 - age_output_acc: 0.3937 - weight_output_acc: 0.6331 - bag_output_acc: 0.5681 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5694 - emotion_output_acc: 0.7131 - val_loss: 25.2963 - val_gender_output_loss: 0.5890 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.3909 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8822 - val_pose_output_loss: 0.8677 - val_footwear_output_loss: 0.8911 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.6816 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5871 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5969 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 25.27888\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 49.1047 - gender_output_loss: 0.6957 - image_quality_output_loss: 0.9809 - age_output_loss: 2.8421 - weight_output_loss: 2.6755 - bag_output_loss: 1.6013 - pose_output_loss: 1.7123 - footwear_output_loss: 1.3029 - emotion_output_loss: 2.7121 - gender_output_acc: 0.6775 - image_quality_output_acc: 0.5400 - age_output_acc: 0.4031 - weight_output_acc: 0.6281 - bag_output_acc: 0.5906 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5400 - emotion_output_acc: 0.7137 - val_loss: 25.5050 - val_gender_output_loss: 0.5920 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.3899 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8905 - val_pose_output_loss: 0.8712 - val_footwear_output_loss: 0.9434 - val_emotion_output_loss: 0.9335 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5940 - val_pose_output_acc: 0.6216 - val_footwear_output_acc: 0.5325 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 25.27888\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 49.1879 - gender_output_loss: 0.7192 - image_quality_output_loss: 0.9615 - age_output_loss: 2.9227 - weight_output_loss: 2.6223 - bag_output_loss: 1.4913 - pose_output_loss: 1.6906 - footwear_output_loss: 1.2753 - emotion_output_loss: 2.8063 - gender_output_acc: 0.6381 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4019 - weight_output_acc: 0.6431 - bag_output_acc: 0.5950 - pose_output_acc: 0.6156 - footwear_output_acc: 0.5537 - emotion_output_acc: 0.7125 - val_loss: 25.5386 - val_gender_output_loss: 0.6094 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.8908 - val_pose_output_loss: 0.8836 - val_footwear_output_loss: 0.9185 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.6545 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5807 - val_pose_output_acc: 0.6201 - val_footwear_output_acc: 0.5881 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 25.27888\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 48.7682 - gender_output_loss: 0.7127 - image_quality_output_loss: 0.9778 - age_output_loss: 2.9311 - weight_output_loss: 2.4092 - bag_output_loss: 1.6046 - pose_output_loss: 1.6473 - footwear_output_loss: 1.2914 - emotion_output_loss: 2.7927 - gender_output_acc: 0.6538 - image_quality_output_acc: 0.5469 - age_output_acc: 0.3900 - weight_output_acc: 0.6525 - bag_output_acc: 0.5875 - pose_output_acc: 0.6312 - footwear_output_acc: 0.5481 - emotion_output_acc: 0.7069 - val_loss: 25.7516 - val_gender_output_loss: 0.6122 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.3929 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.9631 - val_pose_output_loss: 0.8875 - val_footwear_output_loss: 0.9117 - val_emotion_output_loss: 0.9417 - val_gender_output_acc: 0.6732 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5763 - val_pose_output_acc: 0.6255 - val_footwear_output_acc: 0.5704 - val_emotion_output_acc: 0.6924\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 25.27888\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 49.5315 - gender_output_loss: 0.7315 - image_quality_output_loss: 0.9747 - age_output_loss: 2.8740 - weight_output_loss: 2.5873 - bag_output_loss: 1.5478 - pose_output_loss: 1.7070 - footwear_output_loss: 1.2803 - emotion_output_loss: 2.9088 - gender_output_acc: 0.6306 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3969 - weight_output_acc: 0.6450 - bag_output_acc: 0.5675 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5537 - emotion_output_acc: 0.6975 - val_loss: 25.5003 - val_gender_output_loss: 0.6082 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.4057 - val_weight_output_loss: 1.0000 - val_bag_output_loss: 0.8861 - val_pose_output_loss: 0.8872 - val_footwear_output_loss: 0.9057 - val_emotion_output_loss: 0.9200 - val_gender_output_acc: 0.6550 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.5915 - val_pose_output_acc: 0.6211 - val_footwear_output_acc: 0.5728 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 25.27888\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 48.5851 - gender_output_loss: 0.7093 - image_quality_output_loss: 0.9866 - age_output_loss: 2.8552 - weight_output_loss: 2.7785 - bag_output_loss: 1.4861 - pose_output_loss: 1.6198 - footwear_output_loss: 1.2579 - emotion_output_loss: 2.6806 - gender_output_acc: 0.6594 - image_quality_output_acc: 0.5506 - age_output_acc: 0.3756 - weight_output_acc: 0.6225 - bag_output_acc: 0.5756 - pose_output_acc: 0.6344 - footwear_output_acc: 0.5706 - emotion_output_acc: 0.7194 - val_loss: 25.1486 - val_gender_output_loss: 0.5950 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.3799 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.8866 - val_pose_output_loss: 0.8697 - val_footwear_output_loss: 0.8863 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6777 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5842 - val_pose_output_acc: 0.6255 - val_footwear_output_acc: 0.6053 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00062: val_loss improved from 25.27888 to 25.14860, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.062.h5\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 49.7538 - gender_output_loss: 0.7148 - image_quality_output_loss: 0.9550 - age_output_loss: 2.9097 - weight_output_loss: 2.8052 - bag_output_loss: 1.6532 - pose_output_loss: 1.7286 - footwear_output_loss: 1.2603 - emotion_output_loss: 2.7069 - gender_output_acc: 0.6506 - image_quality_output_acc: 0.5606 - age_output_acc: 0.4012 - weight_output_acc: 0.6188 - bag_output_acc: 0.5544 - pose_output_acc: 0.6000 - footwear_output_acc: 0.5644 - emotion_output_acc: 0.7156 - val_loss: 25.1557 - val_gender_output_loss: 0.5884 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.8842 - val_pose_output_loss: 0.8639 - val_footwear_output_loss: 0.8908 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.6718 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5822 - val_pose_output_acc: 0.6324 - val_footwear_output_acc: 0.5856 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 25.14860\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 46.6465 - gender_output_loss: 0.6842 - image_quality_output_loss: 0.9419 - age_output_loss: 2.6862 - weight_output_loss: 2.6065 - bag_output_loss: 1.5352 - pose_output_loss: 1.7378 - footwear_output_loss: 1.2874 - emotion_output_loss: 2.3942 - gender_output_acc: 0.6838 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4106 - weight_output_acc: 0.6381 - bag_output_acc: 0.5794 - pose_output_acc: 0.5969 - footwear_output_acc: 0.5662 - emotion_output_acc: 0.7344 - val_loss: 25.0486 - val_gender_output_loss: 0.5862 - val_image_quality_output_loss: 0.9784 - val_age_output_loss: 1.3812 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8812 - val_pose_output_loss: 0.8519 - val_footwear_output_loss: 0.8836 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.6767 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5906 - val_pose_output_acc: 0.6309 - val_footwear_output_acc: 0.5915 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00064: val_loss improved from 25.14860 to 25.04862, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.064.h5\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 47s 471ms/step - loss: 47.6447 - gender_output_loss: 0.6954 - image_quality_output_loss: 0.9854 - age_output_loss: 2.8022 - weight_output_loss: 2.4579 - bag_output_loss: 1.5216 - pose_output_loss: 1.6169 - footwear_output_loss: 1.2797 - emotion_output_loss: 2.7173 - gender_output_acc: 0.6762 - image_quality_output_acc: 0.5400 - age_output_acc: 0.4019 - weight_output_acc: 0.6437 - bag_output_acc: 0.5844 - pose_output_acc: 0.6331 - footwear_output_acc: 0.5537 - emotion_output_acc: 0.7113 - val_loss: 25.1123 - val_gender_output_loss: 0.5879 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.3834 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8834 - val_pose_output_loss: 0.8460 - val_footwear_output_loss: 0.8882 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.5886 - val_pose_output_acc: 0.6368 - val_footwear_output_acc: 0.5866 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 25.04862\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 46s 462ms/step - loss: 49.2599 - gender_output_loss: 0.6904 - image_quality_output_loss: 0.9571 - age_output_loss: 2.7922 - weight_output_loss: 2.5939 - bag_output_loss: 1.5036 - pose_output_loss: 1.7116 - footwear_output_loss: 1.2654 - emotion_output_loss: 2.9972 - gender_output_acc: 0.6663 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4119 - weight_output_acc: 0.6412 - bag_output_acc: 0.5725 - pose_output_acc: 0.6056 - footwear_output_acc: 0.5631 - emotion_output_acc: 0.6963 - val_loss: 25.2327 - val_gender_output_loss: 0.6009 - val_image_quality_output_loss: 0.9744 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8911 - val_pose_output_loss: 0.8648 - val_footwear_output_loss: 0.8988 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.6698 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5704 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.6014 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 25.04862\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 48.2572 - gender_output_loss: 0.7089 - image_quality_output_loss: 0.9711 - age_output_loss: 2.8978 - weight_output_loss: 2.5289 - bag_output_loss: 1.5695 - pose_output_loss: 1.6521 - footwear_output_loss: 1.2409 - emotion_output_loss: 2.6842 - gender_output_acc: 0.6544 - image_quality_output_acc: 0.5456 - age_output_acc: 0.3856 - weight_output_acc: 0.6531 - bag_output_acc: 0.5894 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7150 - val_loss: 25.3991 - val_gender_output_loss: 0.6176 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4022 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.8939 - val_pose_output_loss: 0.8743 - val_footwear_output_loss: 0.9235 - val_emotion_output_loss: 0.9163 - val_gender_output_acc: 0.6624 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3922 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5901 - val_pose_output_acc: 0.6216 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 25.04862\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 49.6797 - gender_output_loss: 0.7157 - image_quality_output_loss: 0.9639 - age_output_loss: 2.8885 - weight_output_loss: 2.6994 - bag_output_loss: 1.5523 - pose_output_loss: 1.6517 - footwear_output_loss: 1.3381 - emotion_output_loss: 2.8909 - gender_output_acc: 0.6594 - image_quality_output_acc: 0.5650 - age_output_acc: 0.3800 - weight_output_acc: 0.6250 - bag_output_acc: 0.5775 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5312 - emotion_output_acc: 0.7000 - val_loss: 25.5379 - val_gender_output_loss: 0.6011 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4031 - val_weight_output_loss: 1.0061 - val_bag_output_loss: 0.9464 - val_pose_output_loss: 0.8782 - val_footwear_output_loss: 0.9043 - val_emotion_output_loss: 0.9156 - val_gender_output_acc: 0.6575 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5664 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5773 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 25.04862\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 46s 463ms/step - loss: 49.0494 - gender_output_loss: 0.6948 - image_quality_output_loss: 0.9752 - age_output_loss: 2.8570 - weight_output_loss: 2.7383 - bag_output_loss: 1.5469 - pose_output_loss: 1.6164 - footwear_output_loss: 1.2492 - emotion_output_loss: 2.8214 - gender_output_acc: 0.6569 - image_quality_output_acc: 0.5563 - age_output_acc: 0.3894 - weight_output_acc: 0.6331 - bag_output_acc: 0.5925 - pose_output_acc: 0.6400 - footwear_output_acc: 0.5606 - emotion_output_acc: 0.7069 - val_loss: 25.2646 - val_gender_output_loss: 0.6113 - val_image_quality_output_loss: 0.9703 - val_age_output_loss: 1.3974 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8931 - val_pose_output_loss: 0.8861 - val_footwear_output_loss: 0.8888 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.6708 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5753 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.6088 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 25.04862\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 46.9407 - gender_output_loss: 0.7179 - image_quality_output_loss: 0.9765 - age_output_loss: 2.8674 - weight_output_loss: 2.5166 - bag_output_loss: 1.5529 - pose_output_loss: 1.7085 - footwear_output_loss: 1.2967 - emotion_output_loss: 2.3456 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5506 - age_output_acc: 0.3837 - weight_output_acc: 0.6481 - bag_output_acc: 0.5763 - pose_output_acc: 0.6112 - footwear_output_acc: 0.5600 - emotion_output_acc: 0.7425 - val_loss: 25.1156 - val_gender_output_loss: 0.5919 - val_image_quality_output_loss: 0.9707 - val_age_output_loss: 1.3941 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.8863 - val_pose_output_loss: 0.8604 - val_footwear_output_loss: 0.8941 - val_emotion_output_loss: 0.9173 - val_gender_output_acc: 0.6826 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5837 - val_pose_output_acc: 0.6260 - val_footwear_output_acc: 0.6166 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 25.04862\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 46s 465ms/step - loss: 48.6273 - gender_output_loss: 0.7100 - image_quality_output_loss: 0.9498 - age_output_loss: 2.9777 - weight_output_loss: 2.7376 - bag_output_loss: 1.4857 - pose_output_loss: 1.6744 - footwear_output_loss: 1.2338 - emotion_output_loss: 2.6192 - gender_output_acc: 0.6556 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4069 - weight_output_acc: 0.6219 - bag_output_acc: 0.5781 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7231 - val_loss: 25.0287 - val_gender_output_loss: 0.5841 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.3982 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8824 - val_pose_output_loss: 0.8444 - val_footwear_output_loss: 0.8843 - val_emotion_output_loss: 0.9164 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5846 - val_pose_output_acc: 0.6344 - val_footwear_output_acc: 0.6009 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00071: val_loss improved from 25.04862 to 25.02866, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.071.h5\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 46s 455ms/step - loss: 48.4430 - gender_output_loss: 0.7224 - image_quality_output_loss: 0.9820 - age_output_loss: 2.7226 - weight_output_loss: 2.6052 - bag_output_loss: 1.6151 - pose_output_loss: 1.7165 - footwear_output_loss: 1.2575 - emotion_output_loss: 2.7665 - gender_output_acc: 0.6444 - image_quality_output_acc: 0.5350 - age_output_acc: 0.4062 - weight_output_acc: 0.6369 - bag_output_acc: 0.5619 - pose_output_acc: 0.6112 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.7056 - val_loss: 24.8775 - val_gender_output_loss: 0.5828 - val_image_quality_output_loss: 0.9705 - val_age_output_loss: 1.3813 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8806 - val_pose_output_loss: 0.8419 - val_footwear_output_loss: 0.8760 - val_emotion_output_loss: 0.9104 - val_gender_output_acc: 0.6914 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5881 - val_pose_output_acc: 0.6309 - val_footwear_output_acc: 0.6058 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00072: val_loss improved from 25.02866 to 24.87746, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.072.h5\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 48.8543 - gender_output_loss: 0.7017 - image_quality_output_loss: 0.9662 - age_output_loss: 2.9447 - weight_output_loss: 2.6761 - bag_output_loss: 1.4423 - pose_output_loss: 1.6784 - footwear_output_loss: 1.2408 - emotion_output_loss: 2.7795 - gender_output_acc: 0.6456 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3725 - weight_output_acc: 0.6238 - bag_output_acc: 0.5925 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5869 - emotion_output_acc: 0.7037 - val_loss: 24.9031 - val_gender_output_loss: 0.5902 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.3881 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8766 - val_pose_output_loss: 0.8468 - val_footwear_output_loss: 0.8680 - val_emotion_output_loss: 0.9074 - val_gender_output_acc: 0.6742 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5856 - val_pose_output_acc: 0.6304 - val_footwear_output_acc: 0.6191 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 24.87746\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 47s 468ms/step - loss: 49.0465 - gender_output_loss: 0.7014 - image_quality_output_loss: 0.9765 - age_output_loss: 2.7982 - weight_output_loss: 2.8398 - bag_output_loss: 1.5609 - pose_output_loss: 1.6322 - footwear_output_loss: 1.2392 - emotion_output_loss: 2.7946 - gender_output_acc: 0.6637 - image_quality_output_acc: 0.5400 - age_output_acc: 0.4075 - weight_output_acc: 0.6312 - bag_output_acc: 0.5706 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5706 - emotion_output_acc: 0.7075 - val_loss: 25.0008 - val_gender_output_loss: 0.5924 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.3894 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8797 - val_pose_output_loss: 0.8514 - val_footwear_output_loss: 0.8803 - val_emotion_output_loss: 0.9099 - val_gender_output_acc: 0.6767 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6029 - val_pose_output_acc: 0.6324 - val_footwear_output_acc: 0.6043 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 24.87746\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 48.3737 - gender_output_loss: 0.7306 - image_quality_output_loss: 0.9570 - age_output_loss: 2.8269 - weight_output_loss: 2.4658 - bag_output_loss: 1.5913 - pose_output_loss: 1.6738 - footwear_output_loss: 1.2878 - emotion_output_loss: 2.7980 - gender_output_acc: 0.6338 - image_quality_output_acc: 0.5613 - age_output_acc: 0.3963 - weight_output_acc: 0.6569 - bag_output_acc: 0.5700 - pose_output_acc: 0.6144 - footwear_output_acc: 0.5431 - emotion_output_acc: 0.7062 - val_loss: 25.2431 - val_gender_output_loss: 0.6072 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3897 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9045 - val_pose_output_loss: 0.8736 - val_footwear_output_loss: 0.8988 - val_emotion_output_loss: 0.9290 - val_gender_output_acc: 0.6619 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5694 - val_pose_output_acc: 0.6245 - val_footwear_output_acc: 0.5802 - val_emotion_output_acc: 0.7052\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 24.87746\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 48.0344 - gender_output_loss: 0.7140 - image_quality_output_loss: 0.9726 - age_output_loss: 2.7998 - weight_output_loss: 2.4468 - bag_output_loss: 1.5404 - pose_output_loss: 1.6174 - footwear_output_loss: 1.2894 - emotion_output_loss: 2.8395 - gender_output_acc: 0.6344 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3994 - weight_output_acc: 0.6512 - bag_output_acc: 0.5713 - pose_output_acc: 0.6319 - footwear_output_acc: 0.5456 - emotion_output_acc: 0.7056 - val_loss: 25.3162 - val_gender_output_loss: 0.6207 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4048 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.8986 - val_pose_output_loss: 0.8880 - val_footwear_output_loss: 0.9373 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.6521 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5679 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5802 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 24.87746\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 48.5454 - gender_output_loss: 0.7107 - image_quality_output_loss: 0.9662 - age_output_loss: 2.9044 - weight_output_loss: 2.5779 - bag_output_loss: 1.6402 - pose_output_loss: 1.6750 - footwear_output_loss: 1.2582 - emotion_output_loss: 2.6724 - gender_output_acc: 0.6369 - image_quality_output_acc: 0.5613 - age_output_acc: 0.3994 - weight_output_acc: 0.6281 - bag_output_acc: 0.5569 - pose_output_acc: 0.6231 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7244 - val_loss: 25.3304 - val_gender_output_loss: 0.5967 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4286 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9065 - val_pose_output_loss: 0.8691 - val_footwear_output_loss: 0.9022 - val_emotion_output_loss: 0.9253 - val_gender_output_acc: 0.6752 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3632 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5674 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5861 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 24.87746\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 46s 455ms/step - loss: 47.7876 - gender_output_loss: 0.6937 - image_quality_output_loss: 0.9585 - age_output_loss: 2.9191 - weight_output_loss: 2.6251 - bag_output_loss: 1.4353 - pose_output_loss: 1.6501 - footwear_output_loss: 1.2185 - emotion_output_loss: 2.6420 - gender_output_acc: 0.6506 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3869 - weight_output_acc: 0.6369 - bag_output_acc: 0.5913 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5869 - emotion_output_acc: 0.7144 - val_loss: 24.9678 - val_gender_output_loss: 0.5966 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.3919 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.8872 - val_pose_output_loss: 0.8601 - val_footwear_output_loss: 0.8704 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.6860 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5856 - val_pose_output_acc: 0.6344 - val_footwear_output_acc: 0.6014 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 24.87746\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 46s 464ms/step - loss: 47.9806 - gender_output_loss: 0.7061 - image_quality_output_loss: 0.9643 - age_output_loss: 2.8217 - weight_output_loss: 2.6210 - bag_output_loss: 1.5342 - pose_output_loss: 1.6763 - footwear_output_loss: 1.2726 - emotion_output_loss: 2.6641 - gender_output_acc: 0.6587 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4144 - weight_output_acc: 0.6381 - bag_output_acc: 0.5869 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5556 - emotion_output_acc: 0.7175 - val_loss: 25.0386 - val_gender_output_loss: 0.6018 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.3938 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8911 - val_pose_output_loss: 0.8560 - val_footwear_output_loss: 0.8901 - val_emotion_output_loss: 0.9208 - val_gender_output_acc: 0.6949 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5832 - val_pose_output_acc: 0.6284 - val_footwear_output_acc: 0.5930 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 24.87746\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 47s 473ms/step - loss: 49.0240 - gender_output_loss: 0.6967 - image_quality_output_loss: 0.9598 - age_output_loss: 2.8457 - weight_output_loss: 2.7298 - bag_output_loss: 1.5906 - pose_output_loss: 1.5844 - footwear_output_loss: 1.2525 - emotion_output_loss: 2.8646 - gender_output_acc: 0.6650 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4062 - weight_output_acc: 0.6238 - bag_output_acc: 0.5694 - pose_output_acc: 0.6312 - footwear_output_acc: 0.5687 - emotion_output_acc: 0.7006 - val_loss: 24.7970 - val_gender_output_loss: 0.5802 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.3841 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8853 - val_pose_output_loss: 0.8395 - val_footwear_output_loss: 0.8800 - val_emotion_output_loss: 0.9102 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5822 - val_pose_output_acc: 0.6309 - val_footwear_output_acc: 0.5979 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00080: val_loss improved from 24.87746 to 24.79697, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.080.h5\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 48.5878 - gender_output_loss: 0.7018 - image_quality_output_loss: 0.9713 - age_output_loss: 2.7715 - weight_output_loss: 2.5794 - bag_output_loss: 1.6116 - pose_output_loss: 1.6630 - footwear_output_loss: 1.2566 - emotion_output_loss: 2.8580 - gender_output_acc: 0.6756 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3856 - weight_output_acc: 0.6306 - bag_output_acc: 0.5800 - pose_output_acc: 0.6150 - footwear_output_acc: 0.5863 - emotion_output_acc: 0.7019 - val_loss: 24.9051 - val_gender_output_loss: 0.5846 - val_image_quality_output_loss: 0.9758 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9808 - val_bag_output_loss: 0.8933 - val_pose_output_loss: 0.8434 - val_footwear_output_loss: 0.8866 - val_emotion_output_loss: 0.9179 - val_gender_output_acc: 0.6826 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.5827 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5856 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 24.79697\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 49.9008 - gender_output_loss: 0.7031 - image_quality_output_loss: 0.9627 - age_output_loss: 2.9404 - weight_output_loss: 2.7220 - bag_output_loss: 1.5171 - pose_output_loss: 1.6881 - footwear_output_loss: 1.2089 - emotion_output_loss: 2.9917 - gender_output_acc: 0.6613 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3956 - weight_output_acc: 0.6344 - bag_output_acc: 0.5981 - pose_output_acc: 0.6100 - footwear_output_acc: 0.5900 - emotion_output_acc: 0.6925 - val_loss: 24.8877 - val_gender_output_loss: 0.5867 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.3873 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.8815 - val_pose_output_loss: 0.8461 - val_footwear_output_loss: 0.8861 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.6752 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5925 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5930 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 24.79697\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 48.4191 - gender_output_loss: 0.7343 - image_quality_output_loss: 0.9824 - age_output_loss: 2.9527 - weight_output_loss: 2.6754 - bag_output_loss: 1.5750 - pose_output_loss: 1.5907 - footwear_output_loss: 1.2921 - emotion_output_loss: 2.6094 - gender_output_acc: 0.6188 - image_quality_output_acc: 0.5475 - age_output_acc: 0.3950 - weight_output_acc: 0.6287 - bag_output_acc: 0.5844 - pose_output_acc: 0.6312 - footwear_output_acc: 0.5537 - emotion_output_acc: 0.7169 - val_loss: 25.0128 - val_gender_output_loss: 0.5933 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.3937 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.8889 - val_pose_output_loss: 0.8579 - val_footwear_output_loss: 0.9063 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.6850 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.5896 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5861 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 24.79697\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 48.4571 - gender_output_loss: 0.7210 - image_quality_output_loss: 0.9659 - age_output_loss: 2.8333 - weight_output_loss: 2.6601 - bag_output_loss: 1.4762 - pose_output_loss: 1.6612 - footwear_output_loss: 1.3119 - emotion_output_loss: 2.7805 - gender_output_acc: 0.6319 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4006 - weight_output_acc: 0.6281 - bag_output_acc: 0.5894 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5550 - emotion_output_acc: 0.7119 - val_loss: 25.1585 - val_gender_output_loss: 0.6073 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4062 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8834 - val_pose_output_loss: 0.8733 - val_footwear_output_loss: 0.9324 - val_emotion_output_loss: 0.9209 - val_gender_output_acc: 0.6781 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5955 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5615 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 24.79697\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 47.9830 - gender_output_loss: 0.7115 - image_quality_output_loss: 0.9822 - age_output_loss: 2.8742 - weight_output_loss: 2.6569 - bag_output_loss: 1.6036 - pose_output_loss: 1.7258 - footwear_output_loss: 1.2520 - emotion_output_loss: 2.5114 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5481 - age_output_acc: 0.3731 - weight_output_acc: 0.6412 - bag_output_acc: 0.5725 - pose_output_acc: 0.5981 - footwear_output_acc: 0.5775 - emotion_output_acc: 0.7313 - val_loss: 25.2663 - val_gender_output_loss: 0.6372 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.4220 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9180 - val_pose_output_loss: 0.8630 - val_footwear_output_loss: 0.8858 - val_emotion_output_loss: 0.9222 - val_gender_output_acc: 0.6885 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5807 - val_pose_output_acc: 0.6407 - val_footwear_output_acc: 0.5797 - val_emotion_output_acc: 0.7018\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 24.79697\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 46.0980 - gender_output_loss: 0.6984 - image_quality_output_loss: 0.9642 - age_output_loss: 2.7347 - weight_output_loss: 2.3318 - bag_output_loss: 1.5603 - pose_output_loss: 1.6231 - footwear_output_loss: 1.2462 - emotion_output_loss: 2.5556 - gender_output_acc: 0.6737 - image_quality_output_acc: 0.5575 - age_output_acc: 0.4075 - weight_output_acc: 0.6675 - bag_output_acc: 0.5644 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7300 - val_loss: 24.8246 - val_gender_output_loss: 0.5903 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.3843 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.8833 - val_pose_output_loss: 0.8547 - val_footwear_output_loss: 0.9066 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5940 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5837 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 24.79697\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 47.2436 - gender_output_loss: 0.7008 - image_quality_output_loss: 0.9465 - age_output_loss: 2.7959 - weight_output_loss: 2.6571 - bag_output_loss: 1.4532 - pose_output_loss: 1.5706 - footwear_output_loss: 1.3015 - emotion_output_loss: 2.6397 - gender_output_acc: 0.6562 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4037 - weight_output_acc: 0.6350 - bag_output_acc: 0.6069 - pose_output_acc: 0.6481 - footwear_output_acc: 0.5550 - emotion_output_acc: 0.7200 - val_loss: 24.6961 - val_gender_output_loss: 0.5846 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3856 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.8796 - val_pose_output_loss: 0.8364 - val_footwear_output_loss: 0.8905 - val_emotion_output_loss: 0.9038 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5935 - val_pose_output_acc: 0.6383 - val_footwear_output_acc: 0.6024 - val_emotion_output_acc: 0.7047\n",
            "\n",
            "Epoch 00087: val_loss improved from 24.79697 to 24.69612, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.087.h5\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 46s 458ms/step - loss: 47.8578 - gender_output_loss: 0.7137 - image_quality_output_loss: 0.9726 - age_output_loss: 2.7237 - weight_output_loss: 2.6741 - bag_output_loss: 1.4968 - pose_output_loss: 1.5674 - footwear_output_loss: 1.2468 - emotion_output_loss: 2.8319 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4000 - weight_output_acc: 0.6450 - bag_output_acc: 0.5906 - pose_output_acc: 0.6419 - footwear_output_acc: 0.5694 - emotion_output_acc: 0.7113 - val_loss: 24.6174 - val_gender_output_loss: 0.5805 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.3796 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.8769 - val_pose_output_loss: 0.8370 - val_footwear_output_loss: 0.8743 - val_emotion_output_loss: 0.9041 - val_gender_output_acc: 0.6919 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5950 - val_pose_output_acc: 0.6363 - val_footwear_output_acc: 0.6112 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00088: val_loss improved from 24.69612 to 24.61737, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.088.h5\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 45s 455ms/step - loss: 49.1929 - gender_output_loss: 0.7035 - image_quality_output_loss: 0.9755 - age_output_loss: 2.9499 - weight_output_loss: 2.7118 - bag_output_loss: 1.6449 - pose_output_loss: 1.7738 - footwear_output_loss: 1.2526 - emotion_output_loss: 2.6467 - gender_output_acc: 0.6512 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3744 - weight_output_acc: 0.6319 - bag_output_acc: 0.5669 - pose_output_acc: 0.5962 - footwear_output_acc: 0.5644 - emotion_output_acc: 0.7144 - val_loss: 24.6320 - val_gender_output_loss: 0.5902 - val_image_quality_output_loss: 0.9693 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8803 - val_pose_output_loss: 0.8331 - val_footwear_output_loss: 0.8638 - val_emotion_output_loss: 0.9076 - val_gender_output_acc: 0.6978 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5994 - val_pose_output_acc: 0.6412 - val_footwear_output_acc: 0.6166 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 24.61737\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 46s 459ms/step - loss: 47.0606 - gender_output_loss: 0.6954 - image_quality_output_loss: 0.9655 - age_output_loss: 2.8604 - weight_output_loss: 2.3941 - bag_output_loss: 1.5821 - pose_output_loss: 1.5937 - footwear_output_loss: 1.2304 - emotion_output_loss: 2.6451 - gender_output_acc: 0.6600 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3994 - weight_output_acc: 0.6450 - bag_output_acc: 0.5931 - pose_output_acc: 0.6488 - footwear_output_acc: 0.5931 - emotion_output_acc: 0.7188 - val_loss: 24.7017 - val_gender_output_loss: 0.5904 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.3836 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8803 - val_pose_output_loss: 0.8400 - val_footwear_output_loss: 0.8997 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.6826 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5965 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5955 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 24.61737\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 48.4449 - gender_output_loss: 0.7000 - image_quality_output_loss: 0.9593 - age_output_loss: 2.9114 - weight_output_loss: 2.6894 - bag_output_loss: 1.5755 - pose_output_loss: 1.6380 - footwear_output_loss: 1.2645 - emotion_output_loss: 2.6771 - gender_output_acc: 0.6587 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3819 - weight_output_acc: 0.6319 - bag_output_acc: 0.5831 - pose_output_acc: 0.6144 - footwear_output_acc: 0.5613 - emotion_output_acc: 0.7194 - val_loss: 24.7889 - val_gender_output_loss: 0.5898 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3969 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8775 - val_pose_output_loss: 0.8432 - val_footwear_output_loss: 0.8883 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.6905 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6388 - val_footwear_output_acc: 0.6048 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 24.61737\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 46s 459ms/step - loss: 47.9370 - gender_output_loss: 0.7050 - image_quality_output_loss: 0.9731 - age_output_loss: 2.8427 - weight_output_loss: 2.6369 - bag_output_loss: 1.4393 - pose_output_loss: 1.6458 - footwear_output_loss: 1.2434 - emotion_output_loss: 2.7599 - gender_output_acc: 0.6606 - image_quality_output_acc: 0.5419 - age_output_acc: 0.3937 - weight_output_acc: 0.6300 - bag_output_acc: 0.6063 - pose_output_acc: 0.6294 - footwear_output_acc: 0.5850 - emotion_output_acc: 0.7056 - val_loss: 24.7579 - val_gender_output_loss: 0.6082 - val_image_quality_output_loss: 0.9696 - val_age_output_loss: 1.3941 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8790 - val_pose_output_loss: 0.8436 - val_footwear_output_loss: 0.8648 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.6816 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5866 - val_pose_output_acc: 0.6403 - val_footwear_output_acc: 0.6038 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 24.61737\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 47s 465ms/step - loss: 49.0936 - gender_output_loss: 0.7172 - image_quality_output_loss: 0.9589 - age_output_loss: 2.9177 - weight_output_loss: 2.7504 - bag_output_loss: 1.5112 - pose_output_loss: 1.7217 - footwear_output_loss: 1.2915 - emotion_output_loss: 2.7600 - gender_output_acc: 0.6512 - image_quality_output_acc: 0.5719 - age_output_acc: 0.3775 - weight_output_acc: 0.6306 - bag_output_acc: 0.5887 - pose_output_acc: 0.6063 - footwear_output_acc: 0.5456 - emotion_output_acc: 0.7069 - val_loss: 24.7908 - val_gender_output_loss: 0.5972 - val_image_quality_output_loss: 0.9774 - val_age_output_loss: 1.3973 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8859 - val_pose_output_loss: 0.8489 - val_footwear_output_loss: 0.8890 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.6703 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5787 - val_pose_output_acc: 0.6314 - val_footwear_output_acc: 0.5842 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 24.61737\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 47s 475ms/step - loss: 47.5539 - gender_output_loss: 0.7028 - image_quality_output_loss: 0.9542 - age_output_loss: 2.7906 - weight_output_loss: 2.5129 - bag_output_loss: 1.5594 - pose_output_loss: 1.5174 - footwear_output_loss: 1.2449 - emotion_output_loss: 2.8345 - gender_output_acc: 0.6656 - image_quality_output_acc: 0.5656 - age_output_acc: 0.3881 - weight_output_acc: 0.6469 - bag_output_acc: 0.5700 - pose_output_acc: 0.6450 - footwear_output_acc: 0.5719 - emotion_output_acc: 0.7106 - val_loss: 24.6426 - val_gender_output_loss: 0.5856 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.3849 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8789 - val_pose_output_loss: 0.8485 - val_footwear_output_loss: 0.8794 - val_emotion_output_loss: 0.9081 - val_gender_output_acc: 0.6806 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5896 - val_pose_output_acc: 0.6304 - val_footwear_output_acc: 0.6073 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 24.61737\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 46s 460ms/step - loss: 47.7207 - gender_output_loss: 0.6965 - image_quality_output_loss: 0.9650 - age_output_loss: 2.8486 - weight_output_loss: 2.6430 - bag_output_loss: 1.5713 - pose_output_loss: 1.6463 - footwear_output_loss: 1.2266 - emotion_output_loss: 2.6249 - gender_output_acc: 0.6669 - image_quality_output_acc: 0.5481 - age_output_acc: 0.4012 - weight_output_acc: 0.6338 - bag_output_acc: 0.5694 - pose_output_acc: 0.6269 - footwear_output_acc: 0.5900 - emotion_output_acc: 0.7175 - val_loss: 24.5396 - val_gender_output_loss: 0.5816 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.3883 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8804 - val_pose_output_loss: 0.8241 - val_footwear_output_loss: 0.8619 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.6969 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6403 - val_footwear_output_acc: 0.6211 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00095: val_loss improved from 24.61737 to 24.53955, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.095.h5\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 45s 453ms/step - loss: 46.9509 - gender_output_loss: 0.6867 - image_quality_output_loss: 0.9646 - age_output_loss: 2.7512 - weight_output_loss: 2.5010 - bag_output_loss: 1.5718 - pose_output_loss: 1.5645 - footwear_output_loss: 1.2500 - emotion_output_loss: 2.6922 - gender_output_acc: 0.6669 - image_quality_output_acc: 0.5588 - age_output_acc: 0.4219 - weight_output_acc: 0.6388 - bag_output_acc: 0.5925 - pose_output_acc: 0.6437 - footwear_output_acc: 0.5706 - emotion_output_acc: 0.7125 - val_loss: 24.4309 - val_gender_output_loss: 0.5743 - val_image_quality_output_loss: 0.9694 - val_age_output_loss: 1.3821 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8791 - val_pose_output_loss: 0.8133 - val_footwear_output_loss: 0.8568 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.6939 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6486 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00096: val_loss improved from 24.53955 to 24.43085, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.096.h5\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 45s 454ms/step - loss: 48.1777 - gender_output_loss: 0.6949 - image_quality_output_loss: 0.9974 - age_output_loss: 2.8501 - weight_output_loss: 2.6110 - bag_output_loss: 1.5469 - pose_output_loss: 1.5648 - footwear_output_loss: 1.2177 - emotion_output_loss: 2.8323 - gender_output_acc: 0.6625 - image_quality_output_acc: 0.5112 - age_output_acc: 0.3869 - weight_output_acc: 0.6312 - bag_output_acc: 0.5938 - pose_output_acc: 0.6475 - footwear_output_acc: 0.5819 - emotion_output_acc: 0.7031 - val_loss: 24.4762 - val_gender_output_loss: 0.5731 - val_image_quality_output_loss: 0.9687 - val_age_output_loss: 1.3818 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8846 - val_pose_output_loss: 0.8158 - val_footwear_output_loss: 0.8579 - val_emotion_output_loss: 0.9142 - val_gender_output_acc: 0.6924 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5856 - val_pose_output_acc: 0.6521 - val_footwear_output_acc: 0.6161 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 24.43085\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 46s 457ms/step - loss: 46.7578 - gender_output_loss: 0.6904 - image_quality_output_loss: 0.9506 - age_output_loss: 2.8567 - weight_output_loss: 2.3914 - bag_output_loss: 1.3649 - pose_output_loss: 1.6535 - footwear_output_loss: 1.2610 - emotion_output_loss: 2.7108 - gender_output_acc: 0.6744 - image_quality_output_acc: 0.5637 - age_output_acc: 0.3837 - weight_output_acc: 0.6538 - bag_output_acc: 0.6238 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5719 - emotion_output_acc: 0.7150 - val_loss: 24.6616 - val_gender_output_loss: 0.5862 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.4014 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.8984 - val_pose_output_loss: 0.8232 - val_footwear_output_loss: 0.8804 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.6954 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5763 - val_pose_output_acc: 0.6437 - val_footwear_output_acc: 0.5979 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 24.43085\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 46s 456ms/step - loss: 49.5843 - gender_output_loss: 0.7113 - image_quality_output_loss: 0.9547 - age_output_loss: 2.9384 - weight_output_loss: 2.6548 - bag_output_loss: 1.6119 - pose_output_loss: 1.5997 - footwear_output_loss: 1.2871 - emotion_output_loss: 2.9705 - gender_output_acc: 0.6519 - image_quality_output_acc: 0.5631 - age_output_acc: 0.3669 - weight_output_acc: 0.6456 - bag_output_acc: 0.5756 - pose_output_acc: 0.6331 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.6931 - val_loss: 24.9342 - val_gender_output_loss: 0.6107 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4089 - val_weight_output_loss: 1.0007 - val_bag_output_loss: 0.8923 - val_pose_output_loss: 0.8593 - val_footwear_output_loss: 0.8803 - val_emotion_output_loss: 0.9191 - val_gender_output_acc: 0.6786 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3632 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.5763 - val_pose_output_acc: 0.6363 - val_footwear_output_acc: 0.6033 - val_emotion_output_acc: 0.7047\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 24.43085\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 47s 467ms/step - loss: 48.2231 - gender_output_loss: 0.7162 - image_quality_output_loss: 0.9804 - age_output_loss: 2.8168 - weight_output_loss: 2.7444 - bag_output_loss: 1.5486 - pose_output_loss: 1.6596 - footwear_output_loss: 1.2980 - emotion_output_loss: 2.6702 - gender_output_acc: 0.6637 - image_quality_output_acc: 0.5413 - age_output_acc: 0.4069 - weight_output_acc: 0.6306 - bag_output_acc: 0.5769 - pose_output_acc: 0.6275 - footwear_output_acc: 0.5475 - emotion_output_acc: 0.7137 - val_loss: 25.0469 - val_gender_output_loss: 0.6031 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.8931 - val_pose_output_loss: 0.9403 - val_footwear_output_loss: 0.8909 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.6570 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.5699 - val_pose_output_acc: 0.5591 - val_footwear_output_acc: 0.5842 - val_emotion_output_acc: 0.7047\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 24.43085\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n",
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dls1akPMJKF",
        "colab_type": "code",
        "outputId": "5a64f4dc-4143-4bd8-9183-724fc1c05083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11dXe5JmI1I",
        "colab_type": "code",
        "outputId": "b4e44af0-3232-4dd6-caad-1758a232bd8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "STEPS_PER_EPOCH=200\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JSON path: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n",
            "Returning new callback array with steps_per_epoch= 200 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100\n",
            "Backing up history file: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json  to: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json1577531556_backup\n",
            "Epoch 1/100\n",
            "  2/200 [..............................] - ETA: 14:12 - loss: 45.5879 - gender_output_loss: 0.8115 - image_quality_output_loss: 0.9669 - age_output_loss: 2.5192 - weight_output_loss: 2.1223 - bag_output_loss: 1.9821 - pose_output_loss: 1.4867 - footwear_output_loss: 1.1765 - emotion_output_loss: 2.2305 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4062 - weight_output_acc: 0.7188 - bag_output_acc: 0.5625 - pose_output_acc: 0.6875 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7188"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.360489). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 90s 449ms/step - loss: 50.0542 - gender_output_loss: 0.7402 - image_quality_output_loss: 0.9742 - age_output_loss: 2.7943 - weight_output_loss: 2.7127 - bag_output_loss: 1.5901 - pose_output_loss: 1.7832 - footwear_output_loss: 1.3115 - emotion_output_loss: 2.6662 - gender_output_acc: 0.6122 - image_quality_output_acc: 0.5553 - age_output_acc: 0.4019 - weight_output_acc: 0.6278 - bag_output_acc: 0.5672 - pose_output_acc: 0.6081 - footwear_output_acc: 0.5438 - emotion_output_acc: 0.7194 - val_loss: 26.6649 - val_gender_output_loss: 0.6392 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4015 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8986 - val_pose_output_loss: 0.9117 - val_footwear_output_loss: 0.9372 - val_emotion_output_loss: 0.9178 - val_gender_output_acc: 0.6161 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5595 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5674 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 26.66487, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.001.h5\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 50.6463 - gender_output_loss: 0.7477 - image_quality_output_loss: 0.9695 - age_output_loss: 2.8522 - weight_output_loss: 2.7003 - bag_output_loss: 1.5681 - pose_output_loss: 1.7757 - footwear_output_loss: 1.2885 - emotion_output_loss: 2.8028 - gender_output_acc: 0.6063 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3953 - weight_output_acc: 0.6325 - bag_output_acc: 0.5594 - pose_output_acc: 0.6088 - footwear_output_acc: 0.5444 - emotion_output_acc: 0.7087 - val_loss: 26.6161 - val_gender_output_loss: 0.6391 - val_image_quality_output_loss: 0.9822 - val_age_output_loss: 1.3940 - val_weight_output_loss: 0.9775 - val_bag_output_loss: 0.8981 - val_pose_output_loss: 0.9112 - val_footwear_output_loss: 0.9471 - val_emotion_output_loss: 0.9191 - val_gender_output_acc: 0.6225 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5615 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5458 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00002: val_loss improved from 26.66487 to 26.61607, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.002.h5\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 79s 394ms/step - loss: 50.5162 - gender_output_loss: 0.7461 - image_quality_output_loss: 0.9824 - age_output_loss: 2.9266 - weight_output_loss: 2.4939 - bag_output_loss: 1.5965 - pose_output_loss: 1.6875 - footwear_output_loss: 1.3134 - emotion_output_loss: 2.8870 - gender_output_acc: 0.5978 - image_quality_output_acc: 0.5481 - age_output_acc: 0.3991 - weight_output_acc: 0.6456 - bag_output_acc: 0.5653 - pose_output_acc: 0.6284 - footwear_output_acc: 0.5413 - emotion_output_acc: 0.7006 - val_loss: 26.7477 - val_gender_output_loss: 0.6538 - val_image_quality_output_loss: 0.9848 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.9267 - val_pose_output_loss: 0.9057 - val_footwear_output_loss: 0.9173 - val_emotion_output_loss: 0.9321 - val_gender_output_acc: 0.5787 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5763 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 26.61607\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 48.8729 - gender_output_loss: 0.7436 - image_quality_output_loss: 0.9787 - age_output_loss: 2.9082 - weight_output_loss: 2.4802 - bag_output_loss: 1.5358 - pose_output_loss: 1.7002 - footwear_output_loss: 1.3322 - emotion_output_loss: 2.5487 - gender_output_acc: 0.6050 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3931 - weight_output_acc: 0.6484 - bag_output_acc: 0.5666 - pose_output_acc: 0.6278 - footwear_output_acc: 0.5203 - emotion_output_acc: 0.7256 - val_loss: 26.8920 - val_gender_output_loss: 0.6571 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.4187 - val_weight_output_loss: 1.0028 - val_bag_output_loss: 0.9097 - val_pose_output_loss: 0.9198 - val_footwear_output_loss: 0.9773 - val_emotion_output_loss: 0.9320 - val_gender_output_acc: 0.5773 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5266 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 26.61607\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 79s 393ms/step - loss: 51.1742 - gender_output_loss: 0.7501 - image_quality_output_loss: 0.9887 - age_output_loss: 2.8005 - weight_output_loss: 2.7254 - bag_output_loss: 1.6112 - pose_output_loss: 1.7791 - footwear_output_loss: 1.3302 - emotion_output_loss: 2.9415 - gender_output_acc: 0.6009 - image_quality_output_acc: 0.5400 - age_output_acc: 0.4025 - weight_output_acc: 0.6281 - bag_output_acc: 0.5713 - pose_output_acc: 0.6078 - footwear_output_acc: 0.5337 - emotion_output_acc: 0.6937 - val_loss: 26.5206 - val_gender_output_loss: 0.6481 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4141 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.9001 - val_pose_output_loss: 0.9129 - val_footwear_output_loss: 0.9338 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.5866 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5743 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00005: val_loss improved from 26.61607 to 26.52060, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.005.h5\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 78s 389ms/step - loss: 50.0193 - gender_output_loss: 0.7547 - image_quality_output_loss: 0.9737 - age_output_loss: 2.9209 - weight_output_loss: 2.6179 - bag_output_loss: 1.5482 - pose_output_loss: 1.7269 - footwear_output_loss: 1.3080 - emotion_output_loss: 2.7294 - gender_output_acc: 0.5975 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3887 - weight_output_acc: 0.6413 - bag_output_acc: 0.5650 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5459 - emotion_output_acc: 0.7150 - val_loss: 26.4883 - val_gender_output_loss: 0.6612 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4111 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9017 - val_pose_output_loss: 0.9089 - val_footwear_output_loss: 0.9373 - val_emotion_output_loss: 0.9153 - val_gender_output_acc: 0.5797 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5610 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5743 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00006: val_loss improved from 26.52060 to 26.48831, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.006.h5\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 77s 387ms/step - loss: 49.5700 - gender_output_loss: 0.7315 - image_quality_output_loss: 0.9671 - age_output_loss: 2.8746 - weight_output_loss: 2.6388 - bag_output_loss: 1.5714 - pose_output_loss: 1.7270 - footwear_output_loss: 1.2946 - emotion_output_loss: 2.6609 - gender_output_acc: 0.6244 - image_quality_output_acc: 0.5644 - age_output_acc: 0.3891 - weight_output_acc: 0.6369 - bag_output_acc: 0.5744 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5525 - emotion_output_acc: 0.7209 - val_loss: 26.4022 - val_gender_output_loss: 0.6374 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.4015 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8999 - val_pose_output_loss: 0.9038 - val_footwear_output_loss: 0.9401 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.6294 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5640 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5463 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00007: val_loss improved from 26.48831 to 26.40216, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.007.h5\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 79s 394ms/step - loss: 49.1682 - gender_output_loss: 0.7380 - image_quality_output_loss: 0.9792 - age_output_loss: 2.8450 - weight_output_loss: 2.5867 - bag_output_loss: 1.5267 - pose_output_loss: 1.6923 - footwear_output_loss: 1.3236 - emotion_output_loss: 2.6696 - gender_output_acc: 0.6175 - image_quality_output_acc: 0.5475 - age_output_acc: 0.4059 - weight_output_acc: 0.6453 - bag_output_acc: 0.5713 - pose_output_acc: 0.6256 - footwear_output_acc: 0.5272 - emotion_output_acc: 0.7159 - val_loss: 26.2852 - val_gender_output_loss: 0.6376 - val_image_quality_output_loss: 0.9794 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8955 - val_pose_output_loss: 0.8997 - val_footwear_output_loss: 0.9333 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6324 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5620 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5527 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00008: val_loss improved from 26.40216 to 26.28525, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.008.h5\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 49.4722 - gender_output_loss: 0.7326 - image_quality_output_loss: 0.9707 - age_output_loss: 2.9027 - weight_output_loss: 2.5996 - bag_output_loss: 1.5532 - pose_output_loss: 1.7651 - footwear_output_loss: 1.2863 - emotion_output_loss: 2.6311 - gender_output_acc: 0.6222 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3941 - weight_output_acc: 0.6266 - bag_output_acc: 0.5778 - pose_output_acc: 0.6072 - footwear_output_acc: 0.5484 - emotion_output_acc: 0.7244 - val_loss: 26.3822 - val_gender_output_loss: 0.6265 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9933 - val_bag_output_loss: 0.9034 - val_pose_output_loss: 0.9081 - val_footwear_output_loss: 0.9452 - val_emotion_output_loss: 0.9174 - val_gender_output_acc: 0.6535 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6112 - val_bag_output_acc: 0.5600 - val_pose_output_acc: 0.6142 - val_footwear_output_acc: 0.5261 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 26.28525\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 79s 393ms/step - loss: 49.8643 - gender_output_loss: 0.7249 - image_quality_output_loss: 0.9703 - age_output_loss: 2.8149 - weight_output_loss: 2.7064 - bag_output_loss: 1.5771 - pose_output_loss: 1.7086 - footwear_output_loss: 1.2720 - emotion_output_loss: 2.7769 - gender_output_acc: 0.6372 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3975 - weight_output_acc: 0.6362 - bag_output_acc: 0.5759 - pose_output_acc: 0.6206 - footwear_output_acc: 0.5531 - emotion_output_acc: 0.7081 - val_loss: 26.6105 - val_gender_output_loss: 0.6278 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4104 - val_weight_output_loss: 1.0085 - val_bag_output_loss: 0.9038 - val_pose_output_loss: 0.8966 - val_footwear_output_loss: 0.9887 - val_emotion_output_loss: 0.9441 - val_gender_output_acc: 0.6560 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5738 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5094 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 26.28525\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 80s 402ms/step - loss: 50.1048 - gender_output_loss: 0.7463 - image_quality_output_loss: 0.9695 - age_output_loss: 2.9153 - weight_output_loss: 2.5495 - bag_output_loss: 1.5832 - pose_output_loss: 1.7258 - footwear_output_loss: 1.3127 - emotion_output_loss: 2.8151 - gender_output_acc: 0.5988 - image_quality_output_acc: 0.5628 - age_output_acc: 0.3853 - weight_output_acc: 0.6441 - bag_output_acc: 0.5594 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5353 - emotion_output_acc: 0.7097 - val_loss: 26.2995 - val_gender_output_loss: 0.6391 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4096 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8970 - val_pose_output_loss: 0.9028 - val_footwear_output_loss: 0.9332 - val_emotion_output_loss: 0.9188 - val_gender_output_acc: 0.6171 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5659 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5659 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 26.28525\n",
            "Epoch 12/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 49.3364 - gender_output_loss: 0.7476 - image_quality_output_loss: 0.9645 - age_output_loss: 2.8034 - weight_output_loss: 2.6259 - bag_output_loss: 1.6495 - pose_output_loss: 1.7292 - footwear_output_loss: 1.2956 - emotion_output_loss: 2.6484 - gender_output_acc: 0.6046 - image_quality_output_acc: 0.5656 - age_output_acc: 0.4004 - weight_output_acc: 0.6401 - bag_output_acc: 0.5619 - pose_output_acc: 0.6215 - footwear_output_acc: 0.5415 - emotion_output_acc: 0.7173Epoch 12/100\n",
            "200/200 [==============================] - 79s 395ms/step - loss: 49.3188 - gender_output_loss: 0.7478 - image_quality_output_loss: 0.9651 - age_output_loss: 2.8015 - weight_output_loss: 2.6244 - bag_output_loss: 1.6548 - pose_output_loss: 1.7271 - footwear_output_loss: 1.2964 - emotion_output_loss: 2.6439 - gender_output_acc: 0.6044 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4003 - weight_output_acc: 0.6403 - bag_output_acc: 0.5622 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5416 - emotion_output_acc: 0.7178 - val_loss: 26.3329 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.4024 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9155 - val_pose_output_loss: 0.9081 - val_footwear_output_loss: 0.9649 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.5960 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5566 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5384 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 26.28525\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 79s 394ms/step - loss: 49.6439 - gender_output_loss: 0.7395 - image_quality_output_loss: 0.9750 - age_output_loss: 2.8858 - weight_output_loss: 2.6299 - bag_output_loss: 1.5000 - pose_output_loss: 1.7712 - footwear_output_loss: 1.3477 - emotion_output_loss: 2.7087 - gender_output_acc: 0.6159 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4122 - weight_output_acc: 0.6484 - bag_output_acc: 0.5837 - pose_output_acc: 0.6081 - footwear_output_acc: 0.5225 - emotion_output_acc: 0.7175 - val_loss: 26.2881 - val_gender_output_loss: 0.6252 - val_image_quality_output_loss: 0.9844 - val_age_output_loss: 1.4139 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8976 - val_pose_output_loss: 0.9044 - val_footwear_output_loss: 0.9575 - val_emotion_output_loss: 0.9305 - val_gender_output_acc: 0.6442 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5704 - val_pose_output_acc: 0.6201 - val_footwear_output_acc: 0.5207 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 26.28525\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 78s 389ms/step - loss: 50.2705 - gender_output_loss: 0.7394 - image_quality_output_loss: 0.9800 - age_output_loss: 2.8882 - weight_output_loss: 2.6067 - bag_output_loss: 1.5346 - pose_output_loss: 1.7303 - footwear_output_loss: 1.3048 - emotion_output_loss: 2.9165 - gender_output_acc: 0.6262 - image_quality_output_acc: 0.5438 - age_output_acc: 0.3869 - weight_output_acc: 0.6322 - bag_output_acc: 0.5706 - pose_output_acc: 0.6122 - footwear_output_acc: 0.5416 - emotion_output_acc: 0.7009 - val_loss: 26.1242 - val_gender_output_loss: 0.6239 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.4045 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8981 - val_pose_output_loss: 0.9002 - val_footwear_output_loss: 0.9562 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.6570 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5817 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5212 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.28525 to 26.12418, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.014.h5\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 79s 393ms/step - loss: 50.1854 - gender_output_loss: 0.7305 - image_quality_output_loss: 0.9810 - age_output_loss: 2.8592 - weight_output_loss: 2.7378 - bag_output_loss: 1.5608 - pose_output_loss: 1.7431 - footwear_output_loss: 1.2460 - emotion_output_loss: 2.8384 - gender_output_acc: 0.6253 - image_quality_output_acc: 0.5431 - age_output_acc: 0.3894 - weight_output_acc: 0.6325 - bag_output_acc: 0.5697 - pose_output_acc: 0.6138 - footwear_output_acc: 0.5794 - emotion_output_acc: 0.7091 - val_loss: 26.0240 - val_gender_output_loss: 0.6189 - val_image_quality_output_loss: 0.9896 - val_age_output_loss: 1.3906 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8884 - val_pose_output_loss: 0.8873 - val_footwear_output_loss: 0.9488 - val_emotion_output_loss: 0.9313 - val_gender_output_acc: 0.6555 - val_image_quality_output_acc: 0.5389 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5906 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5251 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00015: val_loss improved from 26.12418 to 26.02400, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.015.h5\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 48.7735 - gender_output_loss: 0.7371 - image_quality_output_loss: 0.9741 - age_output_loss: 2.8303 - weight_output_loss: 2.4784 - bag_output_loss: 1.5593 - pose_output_loss: 1.7280 - footwear_output_loss: 1.2887 - emotion_output_loss: 2.7044 - gender_output_acc: 0.6256 - image_quality_output_acc: 0.5578 - age_output_acc: 0.4091 - weight_output_acc: 0.6466 - bag_output_acc: 0.5719 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5463 - emotion_output_acc: 0.7147 - val_loss: 25.8658 - val_gender_output_loss: 0.6131 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.3902 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8879 - val_pose_output_loss: 0.8870 - val_footwear_output_loss: 0.9393 - val_emotion_output_loss: 0.9132 - val_gender_output_acc: 0.6575 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5797 - val_pose_output_acc: 0.6206 - val_footwear_output_acc: 0.5389 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00016: val_loss improved from 26.02400 to 25.86577, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.016.h5\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 76s 382ms/step - loss: 49.4154 - gender_output_loss: 0.7243 - image_quality_output_loss: 0.9687 - age_output_loss: 2.8509 - weight_output_loss: 2.5473 - bag_output_loss: 1.5943 - pose_output_loss: 1.6667 - footwear_output_loss: 1.3140 - emotion_output_loss: 2.8103 - gender_output_acc: 0.6319 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4037 - weight_output_acc: 0.6409 - bag_output_acc: 0.5725 - pose_output_acc: 0.6275 - footwear_output_acc: 0.5453 - emotion_output_acc: 0.7091 - val_loss: 25.9797 - val_gender_output_loss: 0.6181 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4030 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8951 - val_pose_output_loss: 0.8902 - val_footwear_output_loss: 0.9424 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.6545 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5714 - val_pose_output_acc: 0.6196 - val_footwear_output_acc: 0.5487 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 25.86577\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 76s 379ms/step - loss: 49.4636 - gender_output_loss: 0.7221 - image_quality_output_loss: 0.9682 - age_output_loss: 2.9195 - weight_output_loss: 2.7702 - bag_output_loss: 1.4968 - pose_output_loss: 1.7063 - footwear_output_loss: 1.2793 - emotion_output_loss: 2.6529 - gender_output_acc: 0.6413 - image_quality_output_acc: 0.5619 - age_output_acc: 0.3781 - weight_output_acc: 0.6238 - bag_output_acc: 0.5881 - pose_output_acc: 0.6212 - footwear_output_acc: 0.5497 - emotion_output_acc: 0.7147 - val_loss: 25.9104 - val_gender_output_loss: 0.6149 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4025 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8870 - val_pose_output_loss: 0.8887 - val_footwear_output_loss: 0.9325 - val_emotion_output_loss: 0.9158 - val_gender_output_acc: 0.6599 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5842 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5453 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 25.86577\n",
            "Epoch 19/100\n",
            "200/200 [==============================] - 80s 399ms/step - loss: 49.1016 - gender_output_loss: 0.7213 - image_quality_output_loss: 0.9707 - age_output_loss: 2.8913 - weight_output_loss: 2.7050 - bag_output_loss: 1.4933 - pose_output_loss: 1.7264 - footwear_output_loss: 1.2745 - emotion_output_loss: 2.6365 - gender_output_acc: 0.6372 - image_quality_output_acc: 0.5587 - age_output_acc: 0.3812 - weight_output_acc: 0.6297 - bag_output_acc: 0.5913 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5484 - emotion_output_acc: 0.7219 - val_loss: 26.1482 - val_gender_output_loss: 0.6518 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.3937 - val_weight_output_loss: 0.9741 - val_bag_output_loss: 0.8907 - val_pose_output_loss: 0.8929 - val_footwear_output_loss: 0.9294 - val_emotion_output_loss: 0.9769 - val_gender_output_acc: 0.5876 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5719 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5566 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 25.86577\n",
            "Epoch 20/100\n",
            "200/200 [==============================] - 78s 390ms/step - loss: 49.4493 - gender_output_loss: 0.7463 - image_quality_output_loss: 0.9702 - age_output_loss: 2.8651 - weight_output_loss: 2.4819 - bag_output_loss: 1.5976 - pose_output_loss: 1.7261 - footwear_output_loss: 1.3137 - emotion_output_loss: 2.8191 - gender_output_acc: 0.6088 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3988 - weight_output_acc: 0.6597 - bag_output_acc: 0.5587 - pose_output_acc: 0.6178 - footwear_output_acc: 0.5344 - emotion_output_acc: 0.7056 - val_loss: 25.9525 - val_gender_output_loss: 0.6347 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4018 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8970 - val_pose_output_loss: 0.9056 - val_footwear_output_loss: 0.9248 - val_emotion_output_loss: 0.9247 - val_gender_output_acc: 0.6314 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5650 - val_pose_output_acc: 0.6216 - val_footwear_output_acc: 0.5468 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 25.86577\n",
            "Epoch 21/100\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 49.6265 - gender_output_loss: 0.7393 - image_quality_output_loss: 0.9774 - age_output_loss: 2.8421 - weight_output_loss: 2.6406 - bag_output_loss: 1.5925 - pose_output_loss: 1.7692 - footwear_output_loss: 1.3249 - emotion_output_loss: 2.7472 - gender_output_acc: 0.6234 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4069 - weight_output_acc: 0.6306 - bag_output_acc: 0.5775 - pose_output_acc: 0.6069 - footwear_output_acc: 0.5416 - emotion_output_acc: 0.7163 - val_loss: 25.8217 - val_gender_output_loss: 0.6242 - val_image_quality_output_loss: 0.9806 - val_age_output_loss: 1.3973 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8981 - val_pose_output_loss: 0.8984 - val_footwear_output_loss: 0.9177 - val_emotion_output_loss: 0.9259 - val_gender_output_acc: 0.6491 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5674 - val_pose_output_acc: 0.6225 - val_footwear_output_acc: 0.5684 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00021: val_loss improved from 25.86577 to 25.82169, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.021.h5\n",
            "Epoch 22/100\n",
            "200/200 [==============================] - 78s 391ms/step - loss: 49.4355 - gender_output_loss: 0.7280 - image_quality_output_loss: 0.9665 - age_output_loss: 2.8745 - weight_output_loss: 2.6269 - bag_output_loss: 1.5535 - pose_output_loss: 1.6830 - footwear_output_loss: 1.2559 - emotion_output_loss: 2.8283 - gender_output_acc: 0.6303 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3944 - weight_output_acc: 0.6319 - bag_output_acc: 0.5653 - pose_output_acc: 0.6256 - footwear_output_acc: 0.5694 - emotion_output_acc: 0.7091 - val_loss: 25.7953 - val_gender_output_loss: 0.6173 - val_image_quality_output_loss: 0.9792 - val_age_output_loss: 1.4077 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8972 - val_pose_output_loss: 0.8928 - val_footwear_output_loss: 0.9370 - val_emotion_output_loss: 0.9204 - val_gender_output_acc: 0.6467 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4060 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5630 - val_pose_output_acc: 0.6211 - val_footwear_output_acc: 0.5541 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00022: val_loss improved from 25.82169 to 25.79534, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.022.h5\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 77s 383ms/step - loss: 47.7395 - gender_output_loss: 0.7189 - image_quality_output_loss: 0.9671 - age_output_loss: 2.8271 - weight_output_loss: 2.4997 - bag_output_loss: 1.5213 - pose_output_loss: 1.7355 - footwear_output_loss: 1.2912 - emotion_output_loss: 2.5263 - gender_output_acc: 0.6519 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3869 - weight_output_acc: 0.6472 - bag_output_acc: 0.5753 - pose_output_acc: 0.6109 - footwear_output_acc: 0.5566 - emotion_output_acc: 0.7241 - val_loss: 25.5340 - val_gender_output_loss: 0.5939 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.3922 - val_weight_output_loss: 0.9739 - val_bag_output_loss: 0.8812 - val_pose_output_loss: 0.8815 - val_footwear_output_loss: 0.9059 - val_emotion_output_loss: 0.9239 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4050 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5886 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.5659 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00023: val_loss improved from 25.79534 to 25.53403, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.023.h5\n",
            "Epoch 24/100\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 49.4021 - gender_output_loss: 0.7117 - image_quality_output_loss: 0.9703 - age_output_loss: 2.8683 - weight_output_loss: 2.7661 - bag_output_loss: 1.5622 - pose_output_loss: 1.6800 - footwear_output_loss: 1.2623 - emotion_output_loss: 2.7322 - gender_output_acc: 0.6459 - image_quality_output_acc: 0.5509 - age_output_acc: 0.4053 - weight_output_acc: 0.6219 - bag_output_acc: 0.5878 - pose_output_acc: 0.6184 - footwear_output_acc: 0.5656 - emotion_output_acc: 0.7141 - val_loss: 25.5036 - val_gender_output_loss: 0.5974 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.3907 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.8772 - val_footwear_output_loss: 0.9069 - val_emotion_output_loss: 0.9207 - val_gender_output_acc: 0.6708 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5950 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5709 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00024: val_loss improved from 25.53403 to 25.50359, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.024.h5\n",
            "Epoch 25/100\n",
            "200/200 [==============================] - 77s 384ms/step - loss: 49.5807 - gender_output_loss: 0.7208 - image_quality_output_loss: 0.9737 - age_output_loss: 2.8857 - weight_output_loss: 2.5699 - bag_output_loss: 1.5959 - pose_output_loss: 1.6698 - footwear_output_loss: 1.2779 - emotion_output_loss: 2.8762 - gender_output_acc: 0.6384 - image_quality_output_acc: 0.5441 - age_output_acc: 0.4009 - weight_output_acc: 0.6425 - bag_output_acc: 0.5734 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5522 - emotion_output_acc: 0.7012 - val_loss: 25.5275 - val_gender_output_loss: 0.5983 - val_image_quality_output_loss: 0.9749 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8853 - val_pose_output_loss: 0.8796 - val_footwear_output_loss: 0.9129 - val_emotion_output_loss: 0.9170 - val_gender_output_acc: 0.6668 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5866 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5689 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 25.50359\n",
            "Epoch 26/100\n",
            "200/200 [==============================] - 79s 394ms/step - loss: 49.0304 - gender_output_loss: 0.7199 - image_quality_output_loss: 0.9643 - age_output_loss: 2.8638 - weight_output_loss: 2.5698 - bag_output_loss: 1.5194 - pose_output_loss: 1.6769 - footwear_output_loss: 1.2688 - emotion_output_loss: 2.8263 - gender_output_acc: 0.6353 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3875 - weight_output_acc: 0.6428 - bag_output_acc: 0.5784 - pose_output_acc: 0.6200 - footwear_output_acc: 0.5659 - emotion_output_acc: 0.7028 - val_loss: 25.5599 - val_gender_output_loss: 0.6107 - val_image_quality_output_loss: 0.9788 - val_age_output_loss: 1.3941 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8864 - val_pose_output_loss: 0.8887 - val_footwear_output_loss: 0.9257 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.6560 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5797 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5576 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 25.50359\n",
            "Epoch 27/100\n",
            "200/200 [==============================] - 77s 383ms/step - loss: 49.4486 - gender_output_loss: 0.7290 - image_quality_output_loss: 0.9753 - age_output_loss: 2.8500 - weight_output_loss: 2.6180 - bag_output_loss: 1.5794 - pose_output_loss: 1.7399 - footwear_output_loss: 1.2828 - emotion_output_loss: 2.8065 - gender_output_acc: 0.6241 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3916 - weight_output_acc: 0.6234 - bag_output_acc: 0.5706 - pose_output_acc: 0.6034 - footwear_output_acc: 0.5616 - emotion_output_acc: 0.7106 - val_loss: 25.6734 - val_gender_output_loss: 0.6122 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4041 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8881 - val_pose_output_loss: 0.8821 - val_footwear_output_loss: 0.9597 - val_emotion_output_loss: 0.9208 - val_gender_output_acc: 0.6752 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.5866 - val_pose_output_acc: 0.6220 - val_footwear_output_acc: 0.5241 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 25.50359\n",
            "Epoch 28/100\n",
            "200/200 [==============================] - 80s 398ms/step - loss: 48.7987 - gender_output_loss: 0.7207 - image_quality_output_loss: 0.9657 - age_output_loss: 2.8831 - weight_output_loss: 2.6037 - bag_output_loss: 1.5532 - pose_output_loss: 1.6684 - footwear_output_loss: 1.2920 - emotion_output_loss: 2.7100 - gender_output_acc: 0.6372 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3856 - weight_output_acc: 0.6441 - bag_output_acc: 0.5775 - pose_output_acc: 0.6241 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.7122 - val_loss: 25.6881 - val_gender_output_loss: 0.6120 - val_image_quality_output_loss: 0.9809 - val_age_output_loss: 1.3948 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.8879 - val_pose_output_loss: 0.8918 - val_footwear_output_loss: 0.9405 - val_emotion_output_loss: 0.9455 - val_gender_output_acc: 0.6678 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5837 - val_pose_output_acc: 0.6201 - val_footwear_output_acc: 0.5344 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 25.50359\n",
            "Epoch 29/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 48.7427 - gender_output_loss: 0.7223 - image_quality_output_loss: 0.9782 - age_output_loss: 2.8340 - weight_output_loss: 2.7047 - bag_output_loss: 1.5349 - pose_output_loss: 1.6641 - footwear_output_loss: 1.2744 - emotion_output_loss: 2.7006 - gender_output_acc: 0.6341 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4072 - weight_output_acc: 0.6394 - bag_output_acc: 0.5831 - pose_output_acc: 0.6241 - footwear_output_acc: 0.5619 - emotion_output_acc: 0.7194 - val_loss: 25.3681 - val_gender_output_loss: 0.6102 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.3851 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8890 - val_pose_output_loss: 0.8811 - val_footwear_output_loss: 0.9053 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.6555 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5812 - val_pose_output_acc: 0.6348 - val_footwear_output_acc: 0.5699 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00029: val_loss improved from 25.50359 to 25.36808, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.029.h5\n",
            "Epoch 30/100\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 49.3840 - gender_output_loss: 0.7140 - image_quality_output_loss: 0.9735 - age_output_loss: 2.8718 - weight_output_loss: 2.7334 - bag_output_loss: 1.6069 - pose_output_loss: 1.6941 - footwear_output_loss: 1.2889 - emotion_output_loss: 2.7349 - gender_output_acc: 0.6441 - image_quality_output_acc: 0.5428 - age_output_acc: 0.3909 - weight_output_acc: 0.6325 - bag_output_acc: 0.5769 - pose_output_acc: 0.6194 - footwear_output_acc: 0.5534 - emotion_output_acc: 0.7125 - val_loss: 25.3418 - val_gender_output_loss: 0.5983 - val_image_quality_output_loss: 0.9798 - val_age_output_loss: 1.3947 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8870 - val_pose_output_loss: 0.8785 - val_footwear_output_loss: 0.9091 - val_emotion_output_loss: 0.9147 - val_gender_output_acc: 0.6821 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5871 - val_pose_output_acc: 0.6206 - val_footwear_output_acc: 0.5733 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00029: val_loss improved from 25.50359 to 25.36808, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.029.h5\n",
            "\n",
            "Epoch 00030: val_loss improved from 25.36808 to 25.34178, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.030.h5\n",
            "Epoch 31/100\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 48.5582 - gender_output_loss: 0.7067 - image_quality_output_loss: 0.9677 - age_output_loss: 2.8628 - weight_output_loss: 2.5646 - bag_output_loss: 1.4992 - pose_output_loss: 1.7138 - footwear_output_loss: 1.2561 - emotion_output_loss: 2.7599 - gender_output_acc: 0.6466 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3925 - weight_output_acc: 0.6378 - bag_output_acc: 0.5816 - pose_output_acc: 0.6091 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7116 - val_loss: 25.1609 - val_gender_output_loss: 0.5849 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.3882 - val_weight_output_loss: 0.9728 - val_bag_output_loss: 0.8827 - val_pose_output_loss: 0.8621 - val_footwear_output_loss: 0.9018 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5886 - val_pose_output_acc: 0.6240 - val_footwear_output_acc: 0.5778 - val_emotion_output_acc: 0.7062\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00031: val_loss improved from 25.34178 to 25.16092, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.031.h5\n",
            "Epoch 32/100\n",
            "200/200 [==============================] - 80s 402ms/step - loss: 48.0042 - gender_output_loss: 0.7017 - image_quality_output_loss: 0.9545 - age_output_loss: 2.8534 - weight_output_loss: 2.6091 - bag_output_loss: 1.5701 - pose_output_loss: 1.6609 - footwear_output_loss: 1.2314 - emotion_output_loss: 2.6091 - gender_output_acc: 0.6594 - image_quality_output_acc: 0.5691 - age_output_acc: 0.3944 - weight_output_acc: 0.6356 - bag_output_acc: 0.5844 - pose_output_acc: 0.6191 - footwear_output_acc: 0.5766 - emotion_output_acc: 0.7209 - val_loss: 25.1360 - val_gender_output_loss: 0.5844 - val_image_quality_output_loss: 0.9792 - val_age_output_loss: 1.3851 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8827 - val_pose_output_loss: 0.8596 - val_footwear_output_loss: 0.8963 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.6855 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5832 - val_pose_output_acc: 0.6250 - val_footwear_output_acc: 0.5763 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00032: val_loss improved from 25.16092 to 25.13595, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.032.h5\n",
            "Epoch 33/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 48.4927 - gender_output_loss: 0.7111 - image_quality_output_loss: 0.9667 - age_output_loss: 2.8281 - weight_output_loss: 2.6171 - bag_output_loss: 1.4790 - pose_output_loss: 1.6865 - footwear_output_loss: 1.2780 - emotion_output_loss: 2.7668 - gender_output_acc: 0.6559 - image_quality_output_acc: 0.5575 - age_output_acc: 0.4003 - weight_output_acc: 0.6500 - bag_output_acc: 0.5994 - pose_output_acc: 0.6119 - footwear_output_acc: 0.5503 - emotion_output_acc: 0.7097 - val_loss: 25.1263 - val_gender_output_loss: 0.5818 - val_image_quality_output_loss: 0.9710 - val_age_output_loss: 1.3850 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.8909 - val_pose_output_loss: 0.8626 - val_footwear_output_loss: 0.8910 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.6954 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5763 - val_pose_output_acc: 0.6265 - val_footwear_output_acc: 0.5979 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00033: val_loss improved from 25.13595 to 25.12629, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.033.h5\n",
            "Epoch 34/100\n",
            "200/200 [==============================] - 78s 392ms/step - loss: 49.3197 - gender_output_loss: 0.7073 - image_quality_output_loss: 0.9644 - age_output_loss: 2.9161 - weight_output_loss: 2.7585 - bag_output_loss: 1.5739 - pose_output_loss: 1.6810 - footwear_output_loss: 1.2852 - emotion_output_loss: 2.7153 - gender_output_acc: 0.6509 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4044 - weight_output_acc: 0.6134 - bag_output_acc: 0.5822 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5644 - emotion_output_acc: 0.7116 - val_loss: 25.2886 - val_gender_output_loss: 0.6145 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8857 - val_pose_output_loss: 0.8725 - val_footwear_output_loss: 0.8838 - val_emotion_output_loss: 0.9162 - val_gender_output_acc: 0.6742 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5935 - val_pose_output_acc: 0.6270 - val_footwear_output_acc: 0.5876 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 25.12629\n",
            "Epoch 35/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 48.5292 - gender_output_loss: 0.7052 - image_quality_output_loss: 0.9512 - age_output_loss: 2.8074 - weight_output_loss: 2.5746 - bag_output_loss: 1.5201 - pose_output_loss: 1.7038 - footwear_output_loss: 1.2485 - emotion_output_loss: 2.8202 - gender_output_acc: 0.6639 - image_quality_output_acc: 0.5678 - age_output_acc: 0.3982 - weight_output_acc: 0.6442 - bag_output_acc: 0.5804 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5675 - emotion_output_acc: 0.7051Epoch 35/100\n",
            "200/200 [==============================] - 78s 391ms/step - loss: 48.5075 - gender_output_loss: 0.7050 - image_quality_output_loss: 0.9511 - age_output_loss: 2.8092 - weight_output_loss: 2.5732 - bag_output_loss: 1.5226 - pose_output_loss: 1.7044 - footwear_output_loss: 1.2475 - emotion_output_loss: 2.8124 - gender_output_acc: 0.6637 - image_quality_output_acc: 0.5681 - age_output_acc: 0.3978 - weight_output_acc: 0.6441 - bag_output_acc: 0.5803 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5672 - emotion_output_acc: 0.7056 - val_loss: 25.2724 - val_gender_output_loss: 0.5880 - val_image_quality_output_loss: 0.9914 - val_age_output_loss: 1.4109 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.8891 - val_pose_output_loss: 0.8576 - val_footwear_output_loss: 0.9153 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.6831 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6122 - val_bag_output_acc: 0.5802 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5571 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 25.12629\n",
            "Epoch 36/100\n",
            "200/200 [==============================] - 80s 401ms/step - loss: 48.0076 - gender_output_loss: 0.7164 - image_quality_output_loss: 0.9872 - age_output_loss: 2.8065 - weight_output_loss: 2.5065 - bag_output_loss: 1.5676 - pose_output_loss: 1.6510 - footwear_output_loss: 1.2520 - emotion_output_loss: 2.7300 - gender_output_acc: 0.6447 - image_quality_output_acc: 0.5422 - age_output_acc: 0.3981 - weight_output_acc: 0.6472 - bag_output_acc: 0.5706 - pose_output_acc: 0.6244 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7191 - val_loss: 25.3172 - val_gender_output_loss: 0.6077 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3986 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8919 - val_pose_output_loss: 0.8729 - val_footwear_output_loss: 0.9289 - val_emotion_output_loss: 0.9264 - val_gender_output_acc: 0.6585 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5782 - val_pose_output_acc: 0.6275 - val_footwear_output_acc: 0.5561 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 25.12629\n",
            "Epoch 37/100\n",
            "200/200 [==============================] - 81s 404ms/step - loss: 49.2537 - gender_output_loss: 0.7251 - image_quality_output_loss: 0.9679 - age_output_loss: 2.9270 - weight_output_loss: 2.6522 - bag_output_loss: 1.6436 - pose_output_loss: 1.6913 - footwear_output_loss: 1.2698 - emotion_output_loss: 2.7327 - gender_output_acc: 0.6338 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3828 - weight_output_acc: 0.6369 - bag_output_acc: 0.5709 - pose_output_acc: 0.6153 - footwear_output_acc: 0.5666 - emotion_output_acc: 0.7097 - val_loss: 25.6803 - val_gender_output_loss: 0.6412 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4496 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9013 - val_pose_output_loss: 0.8920 - val_footwear_output_loss: 0.9551 - val_emotion_output_loss: 0.9246 - val_gender_output_acc: 0.6088 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5694 - val_pose_output_acc: 0.6191 - val_footwear_output_acc: 0.5615 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 25.12629\n",
            "Epoch 38/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 48.5156 - gender_output_loss: 0.7142 - image_quality_output_loss: 0.9642 - age_output_loss: 2.8846 - weight_output_loss: 2.5730 - bag_output_loss: 1.5514 - pose_output_loss: 1.6542 - footwear_output_loss: 1.3116 - emotion_output_loss: 2.7427 - gender_output_acc: 0.6504 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3960 - weight_output_acc: 0.6313 - bag_output_acc: 0.5851 - pose_output_acc: 0.6222 - footwear_output_acc: 0.5405 - emotion_output_acc: 0.7133Epoch 38/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 48.5323 - gender_output_loss: 0.7147 - image_quality_output_loss: 0.9649 - age_output_loss: 2.8855 - weight_output_loss: 2.5754 - bag_output_loss: 1.5533 - pose_output_loss: 1.6533 - footwear_output_loss: 1.3118 - emotion_output_loss: 2.7429 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3969 - weight_output_acc: 0.6306 - bag_output_acc: 0.5844 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5403 - emotion_output_acc: 0.7131 - val_loss: 25.1103 - val_gender_output_loss: 0.5978 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.3900 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.8858 - val_pose_output_loss: 0.8819 - val_footwear_output_loss: 0.9091 - val_emotion_output_loss: 0.9197 - val_gender_output_acc: 0.6796 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6206 - val_footwear_output_acc: 0.5984 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00038: val_loss improved from 25.12629 to 25.11034, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.038.h5\n",
            "Epoch 39/100\n",
            "200/200 [==============================] - 79s 395ms/step - loss: 48.1647 - gender_output_loss: 0.6920 - image_quality_output_loss: 0.9645 - age_output_loss: 2.7984 - weight_output_loss: 2.6835 - bag_output_loss: 1.4764 - pose_output_loss: 1.6703 - footwear_output_loss: 1.2572 - emotion_output_loss: 2.7471 - gender_output_acc: 0.6709 - image_quality_output_acc: 0.5606 - age_output_acc: 0.4088 - weight_output_acc: 0.6413 - bag_output_acc: 0.5900 - pose_output_acc: 0.6166 - footwear_output_acc: 0.5694 - emotion_output_acc: 0.7131 - val_loss: 24.9327 - val_gender_output_loss: 0.5807 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.8785 - val_pose_output_loss: 0.8579 - val_footwear_output_loss: 0.8892 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5881 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5974 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00039: val_loss improved from 25.11034 to 24.93272, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.039.h5\n",
            "Epoch 40/100\n",
            "200/200 [==============================] - 81s 403ms/step - loss: 47.8552 - gender_output_loss: 0.7101 - image_quality_output_loss: 0.9714 - age_output_loss: 2.7914 - weight_output_loss: 2.5842 - bag_output_loss: 1.5520 - pose_output_loss: 1.6420 - footwear_output_loss: 1.2324 - emotion_output_loss: 2.7189 - gender_output_acc: 0.6519 - image_quality_output_acc: 0.5469 - age_output_acc: 0.4009 - weight_output_acc: 0.6403 - bag_output_acc: 0.5866 - pose_output_acc: 0.6228 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7141 - val_loss: 24.9100 - val_gender_output_loss: 0.5738 - val_image_quality_output_loss: 0.9713 - val_age_output_loss: 1.3897 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8790 - val_pose_output_loss: 0.8544 - val_footwear_output_loss: 0.8825 - val_emotion_output_loss: 0.9223 - val_gender_output_acc: 0.6988 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3912 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5832 - val_pose_output_acc: 0.6299 - val_footwear_output_acc: 0.6014 - val_emotion_output_acc: 0.7047\n",
            "\n",
            "Epoch 00040: val_loss improved from 24.93272 to 24.91004, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.040.h5\n",
            "Epoch 41/100\n",
            "200/200 [==============================] - 78s 390ms/step - loss: 47.9663 - gender_output_loss: 0.6948 - image_quality_output_loss: 0.9602 - age_output_loss: 2.7847 - weight_output_loss: 2.6474 - bag_output_loss: 1.4615 - pose_output_loss: 1.6601 - footwear_output_loss: 1.2662 - emotion_output_loss: 2.7579 - gender_output_acc: 0.6600 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4047 - weight_output_acc: 0.6328 - bag_output_acc: 0.5975 - pose_output_acc: 0.6162 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7122 - val_loss: 24.9528 - val_gender_output_loss: 0.5751 - val_image_quality_output_loss: 0.9696 - val_age_output_loss: 1.3915 - val_weight_output_loss: 0.9982 - val_bag_output_loss: 0.8795 - val_pose_output_loss: 0.8500 - val_footwear_output_loss: 0.8803 - val_emotion_output_loss: 0.9279 - val_gender_output_acc: 0.6949 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6063 - val_bag_output_acc: 0.5846 - val_pose_output_acc: 0.6314 - val_footwear_output_acc: 0.6014 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 24.91004\n",
            "Epoch 42/100\n",
            "200/200 [==============================] - 79s 396ms/step - loss: 47.9742 - gender_output_loss: 0.6903 - image_quality_output_loss: 0.9560 - age_output_loss: 2.8435 - weight_output_loss: 2.6396 - bag_output_loss: 1.5877 - pose_output_loss: 1.6182 - footwear_output_loss: 1.2603 - emotion_output_loss: 2.6542 - gender_output_acc: 0.6697 - image_quality_output_acc: 0.5687 - age_output_acc: 0.3891 - weight_output_acc: 0.6334 - bag_output_acc: 0.5831 - pose_output_acc: 0.6272 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7159 - val_loss: 24.9776 - val_gender_output_loss: 0.5856 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.3867 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.8894 - val_pose_output_loss: 0.8661 - val_footwear_output_loss: 0.8885 - val_emotion_output_loss: 0.9242 - val_gender_output_acc: 0.6959 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5822 - val_pose_output_acc: 0.6235 - val_footwear_output_acc: 0.6068 - val_emotion_output_acc: 0.7062\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 24.91004\n",
            "Epoch 43/100\n",
            "200/200 [==============================] - 80s 400ms/step - loss: 48.7238 - gender_output_loss: 0.7099 - image_quality_output_loss: 0.9661 - age_output_loss: 2.9501 - weight_output_loss: 2.5162 - bag_output_loss: 1.5373 - pose_output_loss: 1.7130 - footwear_output_loss: 1.2440 - emotion_output_loss: 2.7937 - gender_output_acc: 0.6569 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3941 - weight_output_acc: 0.6509 - bag_output_acc: 0.5869 - pose_output_acc: 0.6044 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7069 - val_loss: 25.0411 - val_gender_output_loss: 0.5980 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.3996 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.8811 - val_pose_output_loss: 0.8589 - val_footwear_output_loss: 0.8977 - val_emotion_output_loss: 0.9386 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5915 - val_pose_output_acc: 0.6294 - val_footwear_output_acc: 0.5861 - val_emotion_output_acc: 0.7008\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 24.91004\n",
            "Epoch 44/100\n",
            "200/200 [==============================] - 81s 404ms/step - loss: 48.4953 - gender_output_loss: 0.7143 - image_quality_output_loss: 0.9804 - age_output_loss: 2.8395 - weight_output_loss: 2.5602 - bag_output_loss: 1.5758 - pose_output_loss: 1.6852 - footwear_output_loss: 1.2379 - emotion_output_loss: 2.8086 - gender_output_acc: 0.6478 - image_quality_output_acc: 0.5450 - age_output_acc: 0.3956 - weight_output_acc: 0.6366 - bag_output_acc: 0.5634 - pose_output_acc: 0.6197 - footwear_output_acc: 0.5675 - emotion_output_acc: 0.7022 - val_loss: 24.9668 - val_gender_output_loss: 0.5878 - val_image_quality_output_loss: 0.9697 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 0.8903 - val_pose_output_loss: 0.8639 - val_footwear_output_loss: 0.9136 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.6836 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5881 - val_pose_output_acc: 0.6270 - val_footwear_output_acc: 0.5950 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 24.91004\n",
            "Epoch 45/100\n",
            "200/200 [==============================] - 80s 401ms/step - loss: 48.1394 - gender_output_loss: 0.7050 - image_quality_output_loss: 0.9603 - age_output_loss: 2.8512 - weight_output_loss: 2.6061 - bag_output_loss: 1.5459 - pose_output_loss: 1.6583 - footwear_output_loss: 1.2854 - emotion_output_loss: 2.7175 - gender_output_acc: 0.6550 - image_quality_output_acc: 0.5572 - age_output_acc: 0.3959 - weight_output_acc: 0.6350 - bag_output_acc: 0.5853 - pose_output_acc: 0.6225 - footwear_output_acc: 0.5525 - emotion_output_acc: 0.7134 - val_loss: 24.9655 - val_gender_output_loss: 0.5876 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3955 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8855 - val_pose_output_loss: 0.8530 - val_footwear_output_loss: 0.9323 - val_emotion_output_loss: 0.9295 - val_gender_output_acc: 0.6718 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5787 - val_pose_output_acc: 0.6275 - val_footwear_output_acc: 0.5655 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 24.91004\n",
            "Epoch 46/100\n",
            "200/200 [==============================] - 82s 409ms/step - loss: 48.2886 - gender_output_loss: 0.7020 - image_quality_output_loss: 0.9556 - age_output_loss: 2.8761 - weight_output_loss: 2.6634 - bag_output_loss: 1.5994 - pose_output_loss: 1.6191 - footwear_output_loss: 1.2629 - emotion_output_loss: 2.7000 - gender_output_acc: 0.6569 - image_quality_output_acc: 0.5647 - age_output_acc: 0.3941 - weight_output_acc: 0.6384 - bag_output_acc: 0.5766 - pose_output_acc: 0.6284 - footwear_output_acc: 0.5666 - emotion_output_acc: 0.7163 - val_loss: 24.7973 - val_gender_output_loss: 0.5758 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.3923 - val_weight_output_loss: 0.9961 - val_bag_output_loss: 0.8888 - val_pose_output_loss: 0.8479 - val_footwear_output_loss: 0.8955 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.6919 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.5935 - val_pose_output_acc: 0.6422 - val_footwear_output_acc: 0.5846 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00046: val_loss improved from 24.91004 to 24.79730, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.046.h5\n",
            "Epoch 47/100\n",
            "200/200 [==============================] - 82s 409ms/step - loss: 47.6693 - gender_output_loss: 0.7025 - image_quality_output_loss: 0.9682 - age_output_loss: 2.8632 - weight_output_loss: 2.6137 - bag_output_loss: 1.4834 - pose_output_loss: 1.6198 - footwear_output_loss: 1.2538 - emotion_output_loss: 2.6857 - gender_output_acc: 0.6659 - image_quality_output_acc: 0.5478 - age_output_acc: 0.3997 - weight_output_acc: 0.6416 - bag_output_acc: 0.5938 - pose_output_acc: 0.6238 - footwear_output_acc: 0.5684 - emotion_output_acc: 0.7188 - val_loss: 24.6983 - val_gender_output_loss: 0.5710 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.3858 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8913 - val_pose_output_loss: 0.8328 - val_footwear_output_loss: 0.8862 - val_emotion_output_loss: 0.9230 - val_gender_output_acc: 0.7047 - val_image_quality_output_acc: 0.5305 - val_age_output_acc: 0.3888 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5842 - val_pose_output_acc: 0.6334 - val_footwear_output_acc: 0.5969 - val_emotion_output_acc: 0.7047\n",
            "\n",
            "Epoch 00047: val_loss improved from 24.79730 to 24.69832, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.047.h5\n",
            "Epoch 48/100\n",
            "200/200 [==============================] - 80s 398ms/step - loss: 47.3558 - gender_output_loss: 0.6792 - image_quality_output_loss: 0.9557 - age_output_loss: 2.7673 - weight_output_loss: 2.6376 - bag_output_loss: 1.4791 - pose_output_loss: 1.5925 - footwear_output_loss: 1.2537 - emotion_output_loss: 2.7300 - gender_output_acc: 0.6750 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4053 - weight_output_acc: 0.6384 - bag_output_acc: 0.6038 - pose_output_acc: 0.6278 - footwear_output_acc: 0.5706 - emotion_output_acc: 0.7113 - val_loss: 24.5833 - val_gender_output_loss: 0.5778 - val_image_quality_output_loss: 0.9715 - val_age_output_loss: 1.3888 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.8762 - val_pose_output_loss: 0.8271 - val_footwear_output_loss: 0.8733 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.6831 - val_image_quality_output_acc: 0.5394 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.5876 - val_pose_output_acc: 0.6348 - val_footwear_output_acc: 0.6063 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00048: val_loss improved from 24.69832 to 24.58327, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.048.h5\n",
            "Epoch 49/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 48.4954 - gender_output_loss: 0.6989 - image_quality_output_loss: 0.9636 - age_output_loss: 2.9233 - weight_output_loss: 2.7389 - bag_output_loss: 1.5085 - pose_output_loss: 1.6077 - footwear_output_loss: 1.2411 - emotion_output_loss: 2.7430 - gender_output_acc: 0.6559 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3950 - weight_output_acc: 0.6275 - bag_output_acc: 0.5969 - pose_output_acc: 0.6294 - footwear_output_acc: 0.5809 - emotion_output_acc: 0.7094 - val_loss: 24.8147 - val_gender_output_loss: 0.5873 - val_image_quality_output_loss: 0.9791 - val_age_output_loss: 1.3896 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.9092 - val_pose_output_loss: 0.8308 - val_footwear_output_loss: 0.8871 - val_emotion_output_loss: 0.9293 - val_gender_output_acc: 0.7042 - val_image_quality_output_acc: 0.5394 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5714 - val_pose_output_acc: 0.6452 - val_footwear_output_acc: 0.5969 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 24.58327\n",
            "Epoch 50/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 47.9796 - gender_output_loss: 0.7055 - image_quality_output_loss: 0.9639 - age_output_loss: 2.8684 - weight_output_loss: 2.5770 - bag_output_loss: 1.5974 - pose_output_loss: 1.5896 - footwear_output_loss: 1.2326 - emotion_output_loss: 2.7410 - gender_output_acc: 0.6592 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3860 - weight_output_acc: 0.6385 - bag_output_acc: 0.5675 - pose_output_acc: 0.6300 - footwear_output_acc: 0.5606 - emotion_output_acc: 0.7092Epoch 50/100\n",
            "200/200 [==============================] - 80s 402ms/step - loss: 47.9415 - gender_output_loss: 0.7057 - image_quality_output_loss: 0.9640 - age_output_loss: 2.8703 - weight_output_loss: 2.5704 - bag_output_loss: 1.5974 - pose_output_loss: 1.5909 - footwear_output_loss: 1.2320 - emotion_output_loss: 2.7338 - gender_output_acc: 0.6587 - image_quality_output_acc: 0.5553 - age_output_acc: 0.3866 - weight_output_acc: 0.6394 - bag_output_acc: 0.5672 - pose_output_acc: 0.6303 - footwear_output_acc: 0.5609 - emotion_output_acc: 0.7097 - val_loss: 24.5291 - val_gender_output_loss: 0.5738 - val_image_quality_output_loss: 0.9661 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.8836 - val_pose_output_loss: 0.8194 - val_footwear_output_loss: 0.8983 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5837 - val_pose_output_acc: 0.6511 - val_footwear_output_acc: 0.5906 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00050: val_loss improved from 24.58327 to 24.52909, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.050.h5\n",
            "Epoch 51/100\n",
            "200/200 [==============================] - 82s 412ms/step - loss: 47.4013 - gender_output_loss: 0.6882 - image_quality_output_loss: 0.9556 - age_output_loss: 2.8354 - weight_output_loss: 2.5092 - bag_output_loss: 1.5483 - pose_output_loss: 1.6974 - footwear_output_loss: 1.2612 - emotion_output_loss: 2.6403 - gender_output_acc: 0.6678 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3916 - weight_output_acc: 0.6500 - bag_output_acc: 0.5853 - pose_output_acc: 0.6191 - footwear_output_acc: 0.5637 - emotion_output_acc: 0.7212 - val_loss: 25.0794 - val_gender_output_loss: 0.5848 - val_image_quality_output_loss: 0.9936 - val_age_output_loss: 1.4109 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9157 - val_pose_output_loss: 0.8622 - val_footwear_output_loss: 0.9520 - val_emotion_output_loss: 0.9219 - val_gender_output_acc: 0.6850 - val_image_quality_output_acc: 0.5315 - val_age_output_acc: 0.3853 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5797 - val_pose_output_acc: 0.6280 - val_footwear_output_acc: 0.5394 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 24.52909\n",
            "Epoch 52/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 47.7072 - gender_output_loss: 0.7121 - image_quality_output_loss: 0.9619 - age_output_loss: 2.8480 - weight_output_loss: 2.5222 - bag_output_loss: 1.5805 - pose_output_loss: 1.6395 - footwear_output_loss: 1.2795 - emotion_output_loss: 2.6972 - gender_output_acc: 0.6495 - image_quality_output_acc: 0.5590 - age_output_acc: 0.3951 - weight_output_acc: 0.6489 - bag_output_acc: 0.5776 - pose_output_acc: 0.6275 - footwear_output_acc: 0.5534 - emotion_output_acc: 0.7205Epoch 52/100\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 47.7602 - gender_output_loss: 0.7112 - image_quality_output_loss: 0.9621 - age_output_loss: 2.8479 - weight_output_loss: 2.5348 - bag_output_loss: 1.5788 - pose_output_loss: 1.6412 - footwear_output_loss: 1.2793 - emotion_output_loss: 2.7015 - gender_output_acc: 0.6503 - image_quality_output_acc: 0.5587 - age_output_acc: 0.3953 - weight_output_acc: 0.6475 - bag_output_acc: 0.5781 - pose_output_acc: 0.6275 - footwear_output_acc: 0.5534 - emotion_output_acc: 0.7200 - val_loss: 24.7918 - val_gender_output_loss: 0.5970 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.3947 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8931 - val_pose_output_loss: 0.8388 - val_footwear_output_loss: 0.9061 - val_emotion_output_loss: 0.9343 - val_gender_output_acc: 0.6658 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5837 - val_pose_output_acc: 0.6289 - val_footwear_output_acc: 0.5832 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 24.52909\n",
            "Epoch 53/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 48.2948 - gender_output_loss: 0.7068 - image_quality_output_loss: 0.9758 - age_output_loss: 2.8733 - weight_output_loss: 2.7939 - bag_output_loss: 1.5188 - pose_output_loss: 1.5713 - footwear_output_loss: 1.2890 - emotion_output_loss: 2.7126 - gender_output_acc: 0.6533 - image_quality_output_acc: 0.5468 - age_output_acc: 0.3923 - weight_output_acc: 0.6203 - bag_output_acc: 0.5857 - pose_output_acc: 0.6363 - footwear_output_acc: 0.5572 - emotion_output_acc: 0.7133\n",
            "200/200 [==============================] - 87s 437ms/step - loss: 48.3327 - gender_output_loss: 0.7063 - image_quality_output_loss: 0.9760 - age_output_loss: 2.8755 - weight_output_loss: 2.7868 - bag_output_loss: 1.5193 - pose_output_loss: 1.5683 - footwear_output_loss: 1.2872 - emotion_output_loss: 2.7282 - gender_output_acc: 0.6544 - image_quality_output_acc: 0.5463 - age_output_acc: 0.3919 - weight_output_acc: 0.6212 - bag_output_acc: 0.5859 - pose_output_acc: 0.6372 - footwear_output_acc: 0.5581 - emotion_output_acc: 0.7119 - val_loss: 25.1585 - val_gender_output_loss: 0.6295 - val_image_quality_output_loss: 0.9692 - val_age_output_loss: 1.4316 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8988 - val_pose_output_loss: 0.8672 - val_footwear_output_loss: 0.9141 - val_emotion_output_loss: 0.9522 - val_gender_output_acc: 0.6796 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5758 - val_pose_output_acc: 0.6314 - val_footwear_output_acc: 0.5910 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 24.52909\n",
            "Epoch 54/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 48.3217 - gender_output_loss: 0.6899 - image_quality_output_loss: 0.9529 - age_output_loss: 2.8323 - weight_output_loss: 2.6473 - bag_output_loss: 1.5648 - pose_output_loss: 1.6267 - footwear_output_loss: 1.2253 - emotion_output_loss: 2.8540 - gender_output_acc: 0.6643 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3888 - weight_output_acc: 0.6338 - bag_output_acc: 0.5807 - pose_output_acc: 0.6256 - footwear_output_acc: 0.5892 - emotion_output_acc: 0.7054\n",
            "200/200 [==============================] - 81s 406ms/step - loss: 48.3298 - gender_output_loss: 0.6901 - image_quality_output_loss: 0.9515 - age_output_loss: 2.8323 - weight_output_loss: 2.6403 - bag_output_loss: 1.5658 - pose_output_loss: 1.6263 - footwear_output_loss: 1.2246 - emotion_output_loss: 2.8617 - gender_output_acc: 0.6644 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3897 - weight_output_acc: 0.6341 - bag_output_acc: 0.5806 - pose_output_acc: 0.6253 - footwear_output_acc: 0.5897 - emotion_output_acc: 0.7047 - val_loss: 24.5285 - val_gender_output_loss: 0.5867 - val_image_quality_output_loss: 0.9642 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8819 - val_pose_output_loss: 0.8231 - val_footwear_output_loss: 0.8925 - val_emotion_output_loss: 0.9188 - val_gender_output_acc: 0.6757 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5979 - val_pose_output_acc: 0.6358 - val_footwear_output_acc: 0.5960 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00054: val_loss improved from 24.52909 to 24.52846, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.054.h5\n",
            "Epoch 55/100\n",
            "200/200 [==============================] - 82s 411ms/step - loss: 47.3545 - gender_output_loss: 0.7042 - image_quality_output_loss: 0.9653 - age_output_loss: 2.8380 - weight_output_loss: 2.6184 - bag_output_loss: 1.4989 - pose_output_loss: 1.6249 - footwear_output_loss: 1.2281 - emotion_output_loss: 2.6694 - gender_output_acc: 0.6556 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3959 - weight_output_acc: 0.6372 - bag_output_acc: 0.5950 - pose_output_acc: 0.6297 - footwear_output_acc: 0.5797 - emotion_output_acc: 0.7137 - val_loss: 24.4772 - val_gender_output_loss: 0.5766 - val_image_quality_output_loss: 0.9848 - val_age_output_loss: 1.3924 - val_weight_output_loss: 0.9800 - val_bag_output_loss: 0.8820 - val_pose_output_loss: 0.8227 - val_footwear_output_loss: 0.8784 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.6880 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.5925 - val_pose_output_acc: 0.6447 - val_footwear_output_acc: 0.6033 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "Epoch 00055: val_loss improved from 24.52846 to 24.47717, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.055.h5\n",
            "Epoch 56/100\n",
            "200/200 [==============================] - 81s 404ms/step - loss: 46.9748 - gender_output_loss: 0.6706 - image_quality_output_loss: 0.9525 - age_output_loss: 2.8509 - weight_output_loss: 2.5934 - bag_output_loss: 1.5500 - pose_output_loss: 1.5057 - footwear_output_loss: 1.2569 - emotion_output_loss: 2.6430 - gender_output_acc: 0.6816 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3975 - weight_output_acc: 0.6353 - bag_output_acc: 0.5787 - pose_output_acc: 0.6562 - footwear_output_acc: 0.5663 - emotion_output_acc: 0.7166 - val_loss: 24.2259 - val_gender_output_loss: 0.5685 - val_image_quality_output_loss: 0.9605 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8750 - val_pose_output_loss: 0.7857 - val_footwear_output_loss: 0.8742 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.7131 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6575 - val_footwear_output_acc: 0.5984 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00056: val_loss improved from 24.47717 to 24.22590, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.056.h5\n",
            "Epoch 57/100\n",
            "200/200 [==============================] - 80s 401ms/step - loss: 47.1430 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9601 - age_output_loss: 2.7600 - weight_output_loss: 2.5607 - bag_output_loss: 1.5343 - pose_output_loss: 1.5373 - footwear_output_loss: 1.2364 - emotion_output_loss: 2.7884 - gender_output_acc: 0.6741 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4091 - weight_output_acc: 0.6384 - bag_output_acc: 0.5966 - pose_output_acc: 0.6494 - footwear_output_acc: 0.5681 - emotion_output_acc: 0.7066 - val_loss: 24.2967 - val_gender_output_loss: 0.5683 - val_image_quality_output_loss: 0.9559 - val_age_output_loss: 1.3920 - val_weight_output_loss: 0.9841 - val_bag_output_loss: 0.8748 - val_pose_output_loss: 0.8074 - val_footwear_output_loss: 0.8784 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.7037 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5940 - val_pose_output_acc: 0.6486 - val_footwear_output_acc: 0.6152 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 24.22590\n",
            "Epoch 58/100\n",
            "200/200 [==============================] - 81s 406ms/step - loss: 47.5876 - gender_output_loss: 0.6800 - image_quality_output_loss: 0.9626 - age_output_loss: 2.8671 - weight_output_loss: 2.5440 - bag_output_loss: 1.5054 - pose_output_loss: 1.5723 - footwear_output_loss: 1.2320 - emotion_output_loss: 2.8069 - gender_output_acc: 0.6769 - image_quality_output_acc: 0.5481 - age_output_acc: 0.3912 - weight_output_acc: 0.6403 - bag_output_acc: 0.5856 - pose_output_acc: 0.6409 - footwear_output_acc: 0.5775 - emotion_output_acc: 0.7078 - val_loss: 24.5387 - val_gender_output_loss: 0.5866 - val_image_quality_output_loss: 0.9662 - val_age_output_loss: 1.4043 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.8801 - val_pose_output_loss: 0.8112 - val_footwear_output_loss: 0.8730 - val_emotion_output_loss: 0.9296 - val_gender_output_acc: 0.6722 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5881 - val_pose_output_acc: 0.6344 - val_footwear_output_acc: 0.6161 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 24.22590\n",
            "Epoch 59/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 47.7192 - gender_output_loss: 0.6937 - image_quality_output_loss: 0.9597 - age_output_loss: 2.9395 - weight_output_loss: 2.6154 - bag_output_loss: 1.5437 - pose_output_loss: 1.5076 - footwear_output_loss: 1.2658 - emotion_output_loss: 2.7163 - gender_output_acc: 0.6630 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3932 - weight_output_acc: 0.6391 - bag_output_acc: 0.5948 - pose_output_acc: 0.6520 - footwear_output_acc: 0.5609 - emotion_output_acc: 0.7120\n",
            "Epoch 00058: val_loss did not improve from 24.22590\n",
            "200/200 [==============================] - 81s 403ms/step - loss: 47.6659 - gender_output_loss: 0.6936 - image_quality_output_loss: 0.9593 - age_output_loss: 2.9400 - weight_output_loss: 2.6129 - bag_output_loss: 1.5406 - pose_output_loss: 1.5071 - footwear_output_loss: 1.2655 - emotion_output_loss: 2.7073 - gender_output_acc: 0.6631 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3931 - weight_output_acc: 0.6387 - bag_output_acc: 0.5950 - pose_output_acc: 0.6522 - footwear_output_acc: 0.5613 - emotion_output_acc: 0.7128 - val_loss: 24.3450 - val_gender_output_loss: 0.5846 - val_image_quality_output_loss: 0.9629 - val_age_output_loss: 1.3875 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8822 - val_pose_output_loss: 0.8009 - val_footwear_output_loss: 0.8995 - val_emotion_output_loss: 0.9199 - val_gender_output_acc: 0.7082 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.5881 - val_pose_output_acc: 0.6535 - val_footwear_output_acc: 0.5896 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 24.22590\n",
            "Epoch 60/100\n",
            "200/200 [==============================] - 81s 407ms/step - loss: 48.8384 - gender_output_loss: 0.7062 - image_quality_output_loss: 0.9540 - age_output_loss: 2.8822 - weight_output_loss: 2.7962 - bag_output_loss: 1.5332 - pose_output_loss: 1.5915 - footwear_output_loss: 1.2371 - emotion_output_loss: 2.8804 - gender_output_acc: 0.6538 - image_quality_output_acc: 0.5641 - age_output_acc: 0.3869 - weight_output_acc: 0.6216 - bag_output_acc: 0.5828 - pose_output_acc: 0.6366 - footwear_output_acc: 0.5619 - emotion_output_acc: 0.7012 - val_loss: 24.9687 - val_gender_output_loss: 0.6030 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3942 - val_weight_output_loss: 1.0049 - val_bag_output_loss: 0.8995 - val_pose_output_loss: 0.9188 - val_footwear_output_loss: 0.9403 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.6673 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5719 - val_pose_output_acc: 0.5827 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 24.22590\n",
            "Epoch 61/100\n",
            "200/200 [==============================] - 81s 404ms/step - loss: 47.1472 - gender_output_loss: 0.7027 - image_quality_output_loss: 0.9675 - age_output_loss: 2.7767 - weight_output_loss: 2.6074 - bag_output_loss: 1.5531 - pose_output_loss: 1.6627 - footwear_output_loss: 1.2939 - emotion_output_loss: 2.6110 - gender_output_acc: 0.6556 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3988 - weight_output_acc: 0.6491 - bag_output_acc: 0.5791 - pose_output_acc: 0.6169 - footwear_output_acc: 0.5531 - emotion_output_acc: 0.7244 - val_loss: 24.4134 - val_gender_output_loss: 0.5859 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.3980 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8908 - val_pose_output_loss: 0.8180 - val_footwear_output_loss: 0.8915 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.6806 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.6432 - val_footwear_output_acc: 0.5999 - val_emotion_output_acc: 0.7057\n",
            "200/200 [==============================]\n",
            "Epoch 00061: val_loss did not improve from 24.22590\n",
            "Epoch 62/100\n",
            "200/200 [==============================] - 82s 409ms/step - loss: 47.0288 - gender_output_loss: 0.6849 - image_quality_output_loss: 0.9533 - age_output_loss: 2.8669 - weight_output_loss: 2.5148 - bag_output_loss: 1.4398 - pose_output_loss: 1.6180 - footwear_output_loss: 1.2508 - emotion_output_loss: 2.7236 - gender_output_acc: 0.6766 - image_quality_output_acc: 0.5641 - age_output_acc: 0.3959 - weight_output_acc: 0.6366 - bag_output_acc: 0.5991 - pose_output_acc: 0.6241 - footwear_output_acc: 0.5694 - emotion_output_acc: 0.7106 - val_loss: 24.0212 - val_gender_output_loss: 0.5766 - val_image_quality_output_loss: 0.9626 - val_age_output_loss: 1.3836 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8755 - val_pose_output_loss: 0.7744 - val_footwear_output_loss: 0.8767 - val_emotion_output_loss: 0.9059 - val_gender_output_acc: 0.6801 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5974 - val_pose_output_acc: 0.6703 - val_footwear_output_acc: 0.6196 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00062: val_loss improved from 24.22590 to 24.02123, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.062.h5\n",
            "Epoch 63/100\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 47.2173 - gender_output_loss: 0.6900 - image_quality_output_loss: 0.9602 - age_output_loss: 2.8522 - weight_output_loss: 2.6418 - bag_output_loss: 1.5378 - pose_output_loss: 1.5450 - footwear_output_loss: 1.2545 - emotion_output_loss: 2.6684 - gender_output_acc: 0.6609 - image_quality_output_acc: 0.5559 - age_output_acc: 0.3969 - weight_output_acc: 0.6334 - bag_output_acc: 0.6009 - pose_output_acc: 0.6481 - footwear_output_acc: 0.5706 - emotion_output_acc: 0.7147 - val_loss: 24.0265 - val_gender_output_loss: 0.5744 - val_image_quality_output_loss: 0.9685 - val_age_output_loss: 1.3873 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.8852 - val_pose_output_loss: 0.7621 - val_footwear_output_loss: 0.8661 - val_emotion_output_loss: 0.9106 - val_gender_output_acc: 0.6791 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3858 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5945 - val_pose_output_acc: 0.6806 - val_footwear_output_acc: 0.6117 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 24.02123\n",
            "Epoch 64/100\n",
            "200/200 [==============================] - 82s 412ms/step - loss: 47.0715 - gender_output_loss: 0.6886 - image_quality_output_loss: 0.9443 - age_output_loss: 2.8419 - weight_output_loss: 2.5626 - bag_output_loss: 1.5883 - pose_output_loss: 1.5293 - footwear_output_loss: 1.2222 - emotion_output_loss: 2.7029 - gender_output_acc: 0.6797 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3878 - weight_output_acc: 0.6375 - bag_output_acc: 0.5875 - pose_output_acc: 0.6519 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7144 - val_loss: 23.8852 - val_gender_output_loss: 0.5583 - val_image_quality_output_loss: 0.9588 - val_age_output_loss: 1.3806 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.8787 - val_pose_output_loss: 0.7519 - val_footwear_output_loss: 0.8598 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.7062 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.5896 - val_pose_output_acc: 0.6777 - val_footwear_output_acc: 0.6191 - val_emotion_output_acc: 0.7028\n",
            "\n",
            "Epoch 00064: val_loss improved from 24.02123 to 23.88515, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.064.h5\n",
            "Epoch 65/100\n",
            "200/200 [==============================] - 82s 410ms/step - loss: 47.1470 - gender_output_loss: 0.6724 - image_quality_output_loss: 0.9658 - age_output_loss: 2.7945 - weight_output_loss: 2.5807 - bag_output_loss: 1.5698 - pose_output_loss: 1.5079 - footwear_output_loss: 1.2532 - emotion_output_loss: 2.7683 - gender_output_acc: 0.6884 - image_quality_output_acc: 0.5409 - age_output_acc: 0.4016 - weight_output_acc: 0.6488 - bag_output_acc: 0.5825 - pose_output_acc: 0.6566 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7113 - val_loss: 23.9050 - val_gender_output_loss: 0.5610 - val_image_quality_output_loss: 0.9582 - val_age_output_loss: 1.3957 - val_weight_output_loss: 0.9888 - val_bag_output_loss: 0.8776 - val_pose_output_loss: 0.7348 - val_footwear_output_loss: 0.8596 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.7175 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5955 - val_pose_output_acc: 0.6919 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 23.88515\n",
            "Epoch 66/100\n",
            "200/200 [==============================] - 81s 407ms/step - loss: 46.4866 - gender_output_loss: 0.6773 - image_quality_output_loss: 0.9515 - age_output_loss: 2.7643 - weight_output_loss: 2.6990 - bag_output_loss: 1.5358 - pose_output_loss: 1.5090 - footwear_output_loss: 1.2354 - emotion_output_loss: 2.5854 - gender_output_acc: 0.6866 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4028 - weight_output_acc: 0.6319 - bag_output_acc: 0.5856 - pose_output_acc: 0.6550 - footwear_output_acc: 0.5700 - emotion_output_acc: 0.7200 - val_loss: 24.1494 - val_gender_output_loss: 0.5786 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8865 - val_pose_output_loss: 0.7861 - val_footwear_output_loss: 0.8749 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.6924 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.5861 - val_pose_output_acc: 0.6737 - val_footwear_output_acc: 0.6009 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 23.88515\n",
            "Epoch 67/100\n",
            "200/200 [==============================] - 82s 412ms/step - loss: 47.6810 - gender_output_loss: 0.6919 - image_quality_output_loss: 0.9635 - age_output_loss: 2.9350 - weight_output_loss: 2.5948 - bag_output_loss: 1.5258 - pose_output_loss: 1.5507 - footwear_output_loss: 1.2423 - emotion_output_loss: 2.7552 - gender_output_acc: 0.6750 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3891 - weight_output_acc: 0.6359 - bag_output_acc: 0.5944 - pose_output_acc: 0.6375 - footwear_output_acc: 0.5687 - emotion_output_acc: 0.7041 - val_loss: 24.0877 - val_gender_output_loss: 0.5771 - val_image_quality_output_loss: 0.9585 - val_age_output_loss: 1.3857 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8826 - val_pose_output_loss: 0.7960 - val_footwear_output_loss: 0.9009 - val_emotion_output_loss: 0.9034 - val_gender_output_acc: 0.6909 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.5901 - val_pose_output_acc: 0.6629 - val_footwear_output_acc: 0.5891 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 23.88515\n",
            "Epoch 68/100\n",
            "200/200 [==============================] - 83s 415ms/step - loss: 47.5495 - gender_output_loss: 0.6985 - image_quality_output_loss: 0.9578 - age_output_loss: 2.8414 - weight_output_loss: 2.5763 - bag_output_loss: 1.5401 - pose_output_loss: 1.6055 - footwear_output_loss: 1.2472 - emotion_output_loss: 2.7814 - gender_output_acc: 0.6678 - image_quality_output_acc: 0.5563 - age_output_acc: 0.3906 - weight_output_acc: 0.6413 - bag_output_acc: 0.5769 - pose_output_acc: 0.6353 - footwear_output_acc: 0.5691 - emotion_output_acc: 0.7128 - val_loss: 24.2288 - val_gender_output_loss: 0.5974 - val_image_quality_output_loss: 0.9636 - val_age_output_loss: 1.3965 - val_weight_output_loss: 0.9988 - val_bag_output_loss: 0.8953 - val_pose_output_loss: 0.7842 - val_footwear_output_loss: 0.9000 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.6614 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.5704 - val_pose_output_acc: 0.6565 - val_footwear_output_acc: 0.5984 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 23.88515\n",
            "Epoch 69/100\n",
            "200/200 [==============================] - 83s 414ms/step - loss: 47.3692 - gender_output_loss: 0.6864 - image_quality_output_loss: 0.9556 - age_output_loss: 2.8862 - weight_output_loss: 2.5261 - bag_output_loss: 1.5611 - pose_output_loss: 1.5018 - footwear_output_loss: 1.2763 - emotion_output_loss: 2.7908 - gender_output_acc: 0.6781 - image_quality_output_acc: 0.5628 - age_output_acc: 0.3903 - weight_output_acc: 0.6441 - bag_output_acc: 0.5750 - pose_output_acc: 0.6591 - footwear_output_acc: 0.5628 - emotion_output_acc: 0.7084 - val_loss: 24.2898 - val_gender_output_loss: 0.5966 - val_image_quality_output_loss: 0.9771 - val_age_output_loss: 1.4060 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9007 - val_pose_output_loss: 0.7811 - val_footwear_output_loss: 0.8697 - val_emotion_output_loss: 0.9393 - val_gender_output_acc: 0.6978 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5807 - val_pose_output_acc: 0.6727 - val_footwear_output_acc: 0.6088 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 23.88515\n",
            "Epoch 70/100\n",
            "200/200 [==============================] - 82s 409ms/step - loss: 47.9735 - gender_output_loss: 0.6911 - image_quality_output_loss: 0.9752 - age_output_loss: 2.8504 - weight_output_loss: 2.6743 - bag_output_loss: 1.5413 - pose_output_loss: 1.5226 - footwear_output_loss: 1.2549 - emotion_output_loss: 2.8701 - gender_output_acc: 0.6775 - image_quality_output_acc: 0.5363 - age_output_acc: 0.3953 - weight_output_acc: 0.6344 - bag_output_acc: 0.5816 - pose_output_acc: 0.6503 - footwear_output_acc: 0.5594 - emotion_output_acc: 0.7009 - val_loss: 23.9836 - val_gender_output_loss: 0.5625 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.3969 - val_weight_output_loss: 1.0104 - val_bag_output_loss: 0.8775 - val_pose_output_loss: 0.7458 - val_footwear_output_loss: 0.8637 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.7013 - val_image_quality_output_acc: 0.5374 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6088 - val_bag_output_acc: 0.5974 - val_pose_output_acc: 0.6880 - val_footwear_output_acc: 0.6024 - val_emotion_output_acc: 0.6998\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 23.88515\n",
            "Epoch 71/100\n",
            "200/200 [==============================] - 83s 417ms/step - loss: 45.7193 - gender_output_loss: 0.6723 - image_quality_output_loss: 0.9430 - age_output_loss: 2.7837 - weight_output_loss: 2.5582 - bag_output_loss: 1.5382 - pose_output_loss: 1.4246 - footwear_output_loss: 1.2473 - emotion_output_loss: 2.5697 - gender_output_acc: 0.6809 - image_quality_output_acc: 0.5691 - age_output_acc: 0.3950 - weight_output_acc: 0.6387 - bag_output_acc: 0.5859 - pose_output_acc: 0.6750 - footwear_output_acc: 0.5806 - emotion_output_acc: 0.7222 - val_loss: 23.6855 - val_gender_output_loss: 0.5583 - val_image_quality_output_loss: 0.9601 - val_age_output_loss: 1.3875 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8807 - val_pose_output_loss: 0.7215 - val_footwear_output_loss: 0.8620 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.7136 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3853 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5955 - val_pose_output_acc: 0.6900 - val_footwear_output_acc: 0.6127 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00071: val_loss improved from 23.88515 to 23.68548, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.071.h5\n",
            "Epoch 72/100\n",
            "200/200 [==============================] - 82s 410ms/step - loss: 46.7981 - gender_output_loss: 0.6686 - image_quality_output_loss: 0.9529 - age_output_loss: 2.8446 - weight_output_loss: 2.6936 - bag_output_loss: 1.5236 - pose_output_loss: 1.4625 - footwear_output_loss: 1.2290 - emotion_output_loss: 2.6678 - gender_output_acc: 0.6906 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3878 - weight_output_acc: 0.6284 - bag_output_acc: 0.5903 - pose_output_acc: 0.6587 - footwear_output_acc: 0.5772 - emotion_output_acc: 0.7137 - val_loss: 23.5167 - val_gender_output_loss: 0.5439 - val_image_quality_output_loss: 0.9527 - val_age_output_loss: 1.3843 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.8706 - val_pose_output_loss: 0.6921 - val_footwear_output_loss: 0.8672 - val_emotion_output_loss: 0.8991 - val_gender_output_acc: 0.7254 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6043 - val_pose_output_acc: 0.7106 - val_footwear_output_acc: 0.6166 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00072: val_loss improved from 23.68548 to 23.51671, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.072.h5\n",
            "Epoch 73/100\n",
            "200/200 [==============================] - 83s 415ms/step - loss: 46.2378 - gender_output_loss: 0.6573 - image_quality_output_loss: 0.9485 - age_output_loss: 2.8262 - weight_output_loss: 2.5764 - bag_output_loss: 1.4718 - pose_output_loss: 1.4125 - footwear_output_loss: 1.2008 - emotion_output_loss: 2.7333 - gender_output_acc: 0.7006 - image_quality_output_acc: 0.5509 - age_output_acc: 0.3959 - weight_output_acc: 0.6316 - bag_output_acc: 0.6091 - pose_output_acc: 0.6672 - footwear_output_acc: 0.5909 - emotion_output_acc: 0.7081 - val_loss: 24.2327 - val_gender_output_loss: 0.5571 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4055 - val_weight_output_loss: 1.0087 - val_bag_output_loss: 0.8768 - val_pose_output_loss: 0.7845 - val_footwear_output_loss: 0.8593 - val_emotion_output_loss: 0.9497 - val_gender_output_acc: 0.7096 - val_image_quality_output_acc: 0.5295 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.6014 - val_pose_output_acc: 0.6747 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6781\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00073: val_loss did not improve from 23.51671\n",
            "Epoch 74/100\n",
            "200/200 [==============================] - 81s 407ms/step - loss: 46.5630 - gender_output_loss: 0.6670 - image_quality_output_loss: 0.9540 - age_output_loss: 2.8324 - weight_output_loss: 2.6431 - bag_output_loss: 1.5130 - pose_output_loss: 1.4565 - footwear_output_loss: 1.2380 - emotion_output_loss: 2.6703 - gender_output_acc: 0.6944 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4081 - weight_output_acc: 0.6387 - bag_output_acc: 0.5969 - pose_output_acc: 0.6603 - footwear_output_acc: 0.5669 - emotion_output_acc: 0.7109 - val_loss: 23.8192 - val_gender_output_loss: 0.5575 - val_image_quality_output_loss: 0.9612 - val_age_output_loss: 1.3951 - val_weight_output_loss: 1.0076 - val_bag_output_loss: 0.8819 - val_pose_output_loss: 0.7295 - val_footwear_output_loss: 0.8642 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.6988 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6127 - val_bag_output_acc: 0.5965 - val_pose_output_acc: 0.6865 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 23.51671\n",
            "Epoch 75/100\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 47.3540 - gender_output_loss: 0.6774 - image_quality_output_loss: 0.9624 - age_output_loss: 2.8529 - weight_output_loss: 2.6284 - bag_output_loss: 1.5573 - pose_output_loss: 1.4721 - footwear_output_loss: 1.2667 - emotion_output_loss: 2.7938 - gender_output_acc: 0.6863 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3884 - weight_output_acc: 0.6391 - bag_output_acc: 0.5794 - pose_output_acc: 0.6609 - footwear_output_acc: 0.5644 - emotion_output_acc: 0.7103 - val_loss: 24.1422 - val_gender_output_loss: 0.5832 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.3969 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.8938 - val_pose_output_loss: 0.7857 - val_footwear_output_loss: 0.9407 - val_emotion_output_loss: 0.9034 - val_gender_output_acc: 0.6836 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.5748 - val_pose_output_acc: 0.6737 - val_footwear_output_acc: 0.5389 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 23.51671\n",
            "Epoch 76/100\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 45.8732 - gender_output_loss: 0.6848 - image_quality_output_loss: 0.9571 - age_output_loss: 2.7297 - weight_output_loss: 2.5220 - bag_output_loss: 1.5506 - pose_output_loss: 1.5104 - footwear_output_loss: 1.2706 - emotion_output_loss: 2.6050 - gender_output_acc: 0.6716 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4128 - weight_output_acc: 0.6422 - bag_output_acc: 0.5837 - pose_output_acc: 0.6506 - footwear_output_acc: 0.5603 - emotion_output_acc: 0.7225 - val_loss: 24.6654 - val_gender_output_loss: 0.5637 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.4592 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8785 - val_pose_output_loss: 0.9049 - val_footwear_output_loss: 0.9216 - val_emotion_output_loss: 0.9253 - val_gender_output_acc: 0.7008 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5866 - val_pose_output_acc: 0.6014 - val_footwear_output_acc: 0.5758 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 23.51671\n",
            "Epoch 77/100\n",
            "200/200 [==============================] - 82s 408ms/step - loss: 47.5926 - gender_output_loss: 0.6883 - image_quality_output_loss: 0.9531 - age_output_loss: 2.9279 - weight_output_loss: 2.6585 - bag_output_loss: 1.5430 - pose_output_loss: 1.4717 - footwear_output_loss: 1.2607 - emotion_output_loss: 2.7803 - gender_output_acc: 0.6712 - image_quality_output_acc: 0.5678 - age_output_acc: 0.3931 - weight_output_acc: 0.6353 - bag_output_acc: 0.5837 - pose_output_acc: 0.6641 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7081 - val_loss: 23.6527 - val_gender_output_loss: 0.5719 - val_image_quality_output_loss: 0.9549 - val_age_output_loss: 1.3874 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8722 - val_pose_output_loss: 0.7019 - val_footwear_output_loss: 0.9147 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.6954 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6097 - val_bag_output_acc: 0.6073 - val_pose_output_acc: 0.7052 - val_footwear_output_acc: 0.5733 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 23.51671\n",
            "Epoch 78/100\n",
            "200/200 [==============================] - 83s 413ms/step - loss: 46.6402 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9713 - age_output_loss: 2.8474 - weight_output_loss: 2.5786 - bag_output_loss: 1.5684 - pose_output_loss: 1.5025 - footwear_output_loss: 1.2228 - emotion_output_loss: 2.6594 - gender_output_acc: 0.6712 - image_quality_output_acc: 0.5397 - age_output_acc: 0.4000 - weight_output_acc: 0.6413 - bag_output_acc: 0.5881 - pose_output_acc: 0.6578 - footwear_output_acc: 0.5812 - emotion_output_acc: 0.7209 - val_loss: 23.4337 - val_gender_output_loss: 0.5656 - val_image_quality_output_loss: 0.9536 - val_age_output_loss: 1.3841 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.8760 - val_pose_output_loss: 0.6794 - val_footwear_output_loss: 0.8792 - val_emotion_output_loss: 0.8981 - val_gender_output_acc: 0.7087 - val_image_quality_output_acc: 0.5497 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6053 - val_pose_output_acc: 0.7151 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.7037\n",
            "200/200 [==============================] - 82s 408ms/step - loss: 47.5926 - gender_output_loss: 0.6883 - image_quality_output_loss: 0.9531 - age_output_loss: 2.9279 - weight_output_loss: 2.6585 - bag_output_loss: 1.5430 - pose_output_loss: 1.4717 - footwear_output_loss: 1.2607 - emotion_output_loss: 2.7803 - gender_output_acc: 0.6712 - image_quality_output_acc: 0.5678 - age_output_acc: 0.3931 - weight_output_acc: 0.6353 - bag_output_acc: 0.5837 - pose_output_acc: 0.6641 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.7081 - val_loss: 23.6527 - val_gender_output_loss: 0.5719 - val_image_quality_output_loss: 0.9549 - val_age_output_loss: 1.3874 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8722 - val_pose_output_loss: 0.7019 - val_footwear_output_loss: 0.9147 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.6954 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6097 - val_bag_output_acc: 0.6073 - val_pose_output_acc: 0.7052 - val_footwear_output_acc: 0.5733 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00078: val_loss improved from 23.51671 to 23.43375, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.078.h5\n",
            "Epoch 79/100\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 46.6913 - gender_output_loss: 0.6450 - image_quality_output_loss: 0.9513 - age_output_loss: 2.8291 - weight_output_loss: 2.6470 - bag_output_loss: 1.4980 - pose_output_loss: 1.4470 - footwear_output_loss: 1.2437 - emotion_output_loss: 2.7548 - gender_output_acc: 0.7047 - image_quality_output_acc: 0.5572 - age_output_acc: 0.3859 - weight_output_acc: 0.6303 - bag_output_acc: 0.6009 - pose_output_acc: 0.6728 - footwear_output_acc: 0.5737 - emotion_output_acc: 0.7044 - val_loss: 23.3947 - val_gender_output_loss: 0.5409 - val_image_quality_output_loss: 0.9480 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9925 - val_bag_output_loss: 0.8740 - val_pose_output_loss: 0.6628 - val_footwear_output_loss: 0.8713 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.7219 - val_image_quality_output_acc: 0.5399 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.5846 - val_bag_output_acc: 0.6088 - val_pose_output_acc: 0.7205 - val_footwear_output_acc: 0.6122 - val_emotion_output_acc: 0.6978\n",
            "\n",
            "Epoch 00079: val_loss improved from 23.43375 to 23.39469, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.079.h5\n",
            "Epoch 80/100\n",
            "200/200 [==============================] - 83s 414ms/step - loss: 45.6712 - gender_output_loss: 0.6638 - image_quality_output_loss: 0.9611 - age_output_loss: 2.8369 - weight_output_loss: 2.6050 - bag_output_loss: 1.4851 - pose_output_loss: 1.3752 - footwear_output_loss: 1.2137 - emotion_output_loss: 2.5898 - gender_output_acc: 0.6916 - image_quality_output_acc: 0.5397 - age_output_acc: 0.3953 - weight_output_acc: 0.6384 - bag_output_acc: 0.6059 - pose_output_acc: 0.6828 - footwear_output_acc: 0.5794 - emotion_output_acc: 0.7147 - val_loss: 23.3243 - val_gender_output_loss: 0.5429 - val_image_quality_output_loss: 0.9618 - val_age_output_loss: 1.3894 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8733 - val_pose_output_loss: 0.6479 - val_footwear_output_loss: 0.8592 - val_emotion_output_loss: 0.9081 - val_gender_output_acc: 0.7338 - val_image_quality_output_acc: 0.5285 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6033 - val_bag_output_acc: 0.6029 - val_pose_output_acc: 0.7338 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00080: val_loss improved from 23.39469 to 23.32431, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.080.h5\n",
            "Epoch 81/100\n",
            "200/200 [==============================] - 82s 409ms/step - loss: 46.0095 - gender_output_loss: 0.6380 - image_quality_output_loss: 0.9486 - age_output_loss: 2.7921 - weight_output_loss: 2.6617 - bag_output_loss: 1.5504 - pose_output_loss: 1.3066 - footwear_output_loss: 1.2491 - emotion_output_loss: 2.6816 - gender_output_acc: 0.7022 - image_quality_output_acc: 0.5569 - age_output_acc: 0.3997 - weight_output_acc: 0.6328 - bag_output_acc: 0.5887 - pose_output_acc: 0.6972 - footwear_output_acc: 0.5681 - emotion_output_acc: 0.7134 - val_loss: 23.3315 - val_gender_output_loss: 0.5386 - val_image_quality_output_loss: 0.9457 - val_age_output_loss: 1.3904 - val_weight_output_loss: 0.9970 - val_bag_output_loss: 0.8673 - val_pose_output_loss: 0.6671 - val_footwear_output_loss: 0.8575 - val_emotion_output_loss: 0.9018 - val_gender_output_acc: 0.7259 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6019 - val_bag_output_acc: 0.6097 - val_pose_output_acc: 0.7003 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7013\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 23.32431\n",
            "Epoch 82/100\n",
            "200/200 [==============================] - 83s 414ms/step - loss: 46.0170 - gender_output_loss: 0.6510 - image_quality_output_loss: 0.9556 - age_output_loss: 2.7935 - weight_output_loss: 2.5752 - bag_output_loss: 1.4648 - pose_output_loss: 1.4554 - footwear_output_loss: 1.2071 - emotion_output_loss: 2.7123 - gender_output_acc: 0.7006 - image_quality_output_acc: 0.5603 - age_output_acc: 0.3975 - weight_output_acc: 0.6353 - bag_output_acc: 0.6016 - pose_output_acc: 0.6675 - footwear_output_acc: 0.5844 - emotion_output_acc: 0.7103 - val_loss: 23.3317 - val_gender_output_loss: 0.5635 - val_image_quality_output_loss: 0.9478 - val_age_output_loss: 1.3824 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8701 - val_pose_output_loss: 0.6733 - val_footwear_output_loss: 0.8690 - val_emotion_output_loss: 0.9010 - val_gender_output_acc: 0.7028 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6014 - val_pose_output_acc: 0.7092 - val_footwear_output_acc: 0.6186 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 23.32431\n",
            "Epoch 83/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 46.4253 - gender_output_loss: 0.6620 - image_quality_output_loss: 0.9398 - age_output_loss: 2.8571 - weight_output_loss: 2.5424 - bag_output_loss: 1.5635 - pose_output_loss: 1.4038 - footwear_output_loss: 1.2372 - emotion_output_loss: 2.7309 - gender_output_acc: 0.6960 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3904 - weight_output_acc: 0.6379 - bag_output_acc: 0.5895 - pose_output_acc: 0.6831 - footwear_output_acc: 0.5751 - emotion_output_acc: 0.7073\n",
            "200/200 [==============================] - 85s 426ms/step - loss: 46.4234 - gender_output_loss: 0.6616 - image_quality_output_loss: 0.9399 - age_output_loss: 2.8511 - weight_output_loss: 2.5405 - bag_output_loss: 1.5615 - pose_output_loss: 1.4071 - footwear_output_loss: 1.2354 - emotion_output_loss: 2.7379 - gender_output_acc: 0.6966 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3909 - weight_output_acc: 0.6381 - bag_output_acc: 0.5897 - pose_output_acc: 0.6828 - footwear_output_acc: 0.5763 - emotion_output_acc: 0.7066 - val_loss: 23.6554 - val_gender_output_loss: 0.5540 - val_image_quality_output_loss: 0.9708 - val_age_output_loss: 1.3882 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.8691 - val_pose_output_loss: 0.6924 - val_footwear_output_loss: 0.8689 - val_emotion_output_loss: 0.9484 - val_gender_output_acc: 0.7101 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.5940 - val_pose_output_acc: 0.7047 - val_footwear_output_acc: 0.6097 - val_emotion_output_acc: 0.6762\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 23.32431\n",
            "Epoch 84/100\n",
            "200/200 [==============================] - 83s 414ms/step - loss: 46.3289 - gender_output_loss: 0.6616 - image_quality_output_loss: 0.9535 - age_output_loss: 2.8425 - weight_output_loss: 2.5125 - bag_output_loss: 1.5046 - pose_output_loss: 1.4787 - footwear_output_loss: 1.2578 - emotion_output_loss: 2.7193 - gender_output_acc: 0.6828 - image_quality_output_acc: 0.5587 - age_output_acc: 0.4050 - weight_output_acc: 0.6419 - bag_output_acc: 0.5922 - pose_output_acc: 0.6613 - footwear_output_acc: 0.5709 - emotion_output_acc: 0.7128 - val_loss: 23.8027 - val_gender_output_loss: 0.5677 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.8690 - val_pose_output_loss: 0.7620 - val_footwear_output_loss: 0.8781 - val_emotion_output_loss: 0.9104 - val_gender_output_acc: 0.6973 - val_image_quality_output_acc: 0.5497 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6014 - val_pose_output_acc: 0.6703 - val_footwear_output_acc: 0.6073 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 23.32431\n",
            "Epoch 85/100\n",
            "200/200 [==============================] - 82s 412ms/step - loss: 46.2222 - gender_output_loss: 0.6761 - image_quality_output_loss: 0.9533 - age_output_loss: 2.8484 - weight_output_loss: 2.5469 - bag_output_loss: 1.4913 - pose_output_loss: 1.4783 - footwear_output_loss: 1.2704 - emotion_output_loss: 2.6629 - gender_output_acc: 0.6897 - image_quality_output_acc: 0.5559 - age_output_acc: 0.3919 - weight_output_acc: 0.6375 - bag_output_acc: 0.5991 - pose_output_acc: 0.6681 - footwear_output_acc: 0.5716 - emotion_output_acc: 0.7141 - val_loss: 24.1788 - val_gender_output_loss: 0.5430 - val_image_quality_output_loss: 0.9759 - val_age_output_loss: 1.3897 - val_weight_output_loss: 1.0207 - val_bag_output_loss: 0.8738 - val_pose_output_loss: 0.8239 - val_footwear_output_loss: 0.8616 - val_emotion_output_loss: 0.9715 - val_gender_output_acc: 0.7190 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6112 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.6973 - val_footwear_output_acc: 0.6107 - val_emotion_output_acc: 0.6501\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 23.32431\n",
            "Epoch 86/100\n",
            "200/200 [==============================] - 84s 418ms/step - loss: 47.2333 - gender_output_loss: 0.6540 - image_quality_output_loss: 0.9559 - age_output_loss: 2.8567 - weight_output_loss: 2.7815 - bag_output_loss: 1.5505 - pose_output_loss: 1.4037 - footwear_output_loss: 1.2069 - emotion_output_loss: 2.7889 - gender_output_acc: 0.7006 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3887 - weight_output_acc: 0.6316 - bag_output_acc: 0.5734 - pose_output_acc: 0.6753 - footwear_output_acc: 0.5897 - emotion_output_acc: 0.7097 - val_loss: 23.4863 - val_gender_output_loss: 0.5621 - val_image_quality_output_loss: 0.9440 - val_age_output_loss: 1.3941 - val_weight_output_loss: 1.0011 - val_bag_output_loss: 0.8710 - val_pose_output_loss: 0.6888 - val_footwear_output_loss: 0.8781 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.7126 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.6122 - val_bag_output_acc: 0.6033 - val_pose_output_acc: 0.7156 - val_footwear_output_acc: 0.6063 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 23.32431\n",
            "Epoch 87/100\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 46.7317 - gender_output_loss: 0.6434 - image_quality_output_loss: 0.9526 - age_output_loss: 2.8120 - weight_output_loss: 2.6654 - bag_output_loss: 1.6126 - pose_output_loss: 1.3559 - footwear_output_loss: 1.2496 - emotion_output_loss: 2.7735 - gender_output_acc: 0.7034 - image_quality_output_acc: 0.5447 - age_output_acc: 0.3944 - weight_output_acc: 0.6319 - bag_output_acc: 0.5897 - pose_output_acc: 0.6897 - footwear_output_acc: 0.5769 - emotion_output_acc: 0.7059 - val_loss: 23.3598 - val_gender_output_loss: 0.5237 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4176 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8844 - val_pose_output_loss: 0.6477 - val_footwear_output_loss: 0.8591 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.7416 - val_image_quality_output_acc: 0.5330 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.5984 - val_pose_output_acc: 0.7259 - val_footwear_output_acc: 0.5950 - val_emotion_output_acc: 0.6924\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 23.32431\n",
            "Epoch 88/100\n",
            "199/200 [============================>.] - ETA: 0s - loss: 45.7339 - gender_output_loss: 0.6165 - image_quality_output_loss: 0.9394 - age_output_loss: 2.8383 - weight_output_loss: 2.7090 - bag_output_loss: 1.4657 - pose_output_loss: 1.3384 - footwear_output_loss: 1.1959 - emotion_output_loss: 2.6373 - gender_output_acc: 0.7296 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3979 - weight_output_acc: 0.6300 - bag_output_acc: 0.6171 - pose_output_acc: 0.6960 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7198\n",
            "Epoch 00087: val_loss did not improve from 23.32431\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 45.7019 - gender_output_loss: 0.6173 - image_quality_output_loss: 0.9384 - age_output_loss: 2.8382 - weight_output_loss: 2.7046 - bag_output_loss: 1.4642 - pose_output_loss: 1.3397 - footwear_output_loss: 1.1943 - emotion_output_loss: 2.6337 - gender_output_acc: 0.7288 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3984 - weight_output_acc: 0.6300 - bag_output_acc: 0.6178 - pose_output_acc: 0.6956 - footwear_output_acc: 0.5919 - emotion_output_acc: 0.7200 - val_loss: 22.9226 - val_gender_output_loss: 0.5081 - val_image_quality_output_loss: 0.9592 - val_age_output_loss: 1.3783 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.8622 - val_pose_output_loss: 0.6213 - val_footwear_output_loss: 0.8456 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.7451 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.4104 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.6093 - val_pose_output_acc: 0.7367 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00088: val_loss improved from 23.32431 to 22.92262, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.088.h5\n",
            "Epoch 89/100\n",
            "200/200 [==============================] - 82s 411ms/step - loss: 45.4749 - gender_output_loss: 0.6122 - image_quality_output_loss: 0.9431 - age_output_loss: 2.8690 - weight_output_loss: 2.4987 - bag_output_loss: 1.4755 - pose_output_loss: 1.2856 - footwear_output_loss: 1.2228 - emotion_output_loss: 2.7193 - gender_output_acc: 0.7206 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3928 - weight_output_acc: 0.6425 - bag_output_acc: 0.6162 - pose_output_acc: 0.7038 - footwear_output_acc: 0.5728 - emotion_output_acc: 0.7075 - val_loss: 22.9868 - val_gender_output_loss: 0.5123 - val_image_quality_output_loss: 0.9527 - val_age_output_loss: 1.3785 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.8625 - val_pose_output_loss: 0.6353 - val_footwear_output_loss: 0.8441 - val_emotion_output_loss: 0.9052 - val_gender_output_acc: 0.7402 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7328 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 22.92262\n",
            "Epoch 90/100\n",
            "200/200 [==============================] - 84s 422ms/step - loss: 45.2463 - gender_output_loss: 0.6211 - image_quality_output_loss: 0.9573 - age_output_loss: 2.7457 - weight_output_loss: 2.5153 - bag_output_loss: 1.5111 - pose_output_loss: 1.3526 - footwear_output_loss: 1.2139 - emotion_output_loss: 2.6908 - gender_output_acc: 0.7113 - image_quality_output_acc: 0.5475 - age_output_acc: 0.4019 - weight_output_acc: 0.6431 - bag_output_acc: 0.5919 - pose_output_acc: 0.6944 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7119 - val_loss: 22.9962 - val_gender_output_loss: 0.5261 - val_image_quality_output_loss: 0.9358 - val_age_output_loss: 1.3940 - val_weight_output_loss: 0.9688 - val_bag_output_loss: 0.8757 - val_pose_output_loss: 0.6326 - val_footwear_output_loss: 0.8678 - val_emotion_output_loss: 0.8891 - val_gender_output_acc: 0.7347 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.6142 - val_bag_output_acc: 0.5989 - val_pose_output_acc: 0.7402 - val_footwear_output_acc: 0.6142 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 22.92262\n",
            "Epoch 91/100\n",
            "200/200 [==============================] - 84s 422ms/step - loss: 46.1473 - gender_output_loss: 0.6390 - image_quality_output_loss: 0.9508 - age_output_loss: 2.8593 - weight_output_loss: 2.6494 - bag_output_loss: 1.5442 - pose_output_loss: 1.3551 - footwear_output_loss: 1.2429 - emotion_output_loss: 2.6579 - gender_output_acc: 0.7072 - image_quality_output_acc: 0.5613 - age_output_acc: 0.3731 - weight_output_acc: 0.6425 - bag_output_acc: 0.5953 - pose_output_acc: 0.6944 - footwear_output_acc: 0.5872 - emotion_output_acc: 0.7072 - val_loss: 23.4517 - val_gender_output_loss: 0.5495 - val_image_quality_output_loss: 0.9388 - val_age_output_loss: 1.3922 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9180 - val_pose_output_loss: 0.7014 - val_footwear_output_loss: 0.8834 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7254 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5625 - val_pose_output_acc: 0.7116 - val_footwear_output_acc: 0.6171 - val_emotion_output_acc: 0.7028\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 22.92262\n",
            "Epoch 92/100\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 45.4594 - gender_output_loss: 0.6539 - image_quality_output_loss: 0.9566 - age_output_loss: 2.7632 - weight_output_loss: 2.5938 - bag_output_loss: 1.4760 - pose_output_loss: 1.4005 - footwear_output_loss: 1.2483 - emotion_output_loss: 2.6315 - gender_output_acc: 0.6925 - image_quality_output_acc: 0.5409 - age_output_acc: 0.4034 - weight_output_acc: 0.6350 - bag_output_acc: 0.5928 - pose_output_acc: 0.6756 - footwear_output_acc: 0.5691 - emotion_output_acc: 0.7172 - val_loss: 23.4722 - val_gender_output_loss: 0.5536 - val_image_quality_output_loss: 0.9539 - val_age_output_loss: 1.3874 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8726 - val_pose_output_loss: 0.7079 - val_footwear_output_loss: 0.9013 - val_emotion_output_loss: 0.9177 - val_gender_output_acc: 0.7234 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5979 - val_pose_output_acc: 0.6905 - val_footwear_output_acc: 0.5940 - val_emotion_output_acc: 0.7003\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 22.92262\n",
            "Epoch 93/100\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 47.1232 - gender_output_loss: 0.6486 - image_quality_output_loss: 0.9586 - age_output_loss: 2.8743 - weight_output_loss: 2.6516 - bag_output_loss: 1.5277 - pose_output_loss: 1.4013 - footwear_output_loss: 1.2340 - emotion_output_loss: 2.8667 - gender_output_acc: 0.6925 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3950 - weight_output_acc: 0.6303 - bag_output_acc: 0.5916 - pose_output_acc: 0.6837 - footwear_output_acc: 0.5844 - emotion_output_acc: 0.7006 - val_loss: 23.1989 - val_gender_output_loss: 0.5470 - val_image_quality_output_loss: 0.9371 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 0.8818 - val_pose_output_loss: 0.6695 - val_footwear_output_loss: 0.8539 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.7121 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.5950 - val_pose_output_acc: 0.7126 - val_footwear_output_acc: 0.6211 - val_emotion_output_acc: 0.7008\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 22.92262\n",
            "Epoch 94/100\n",
            "200/200 [==============================] - 85s 427ms/step - loss: 45.7018 - gender_output_loss: 0.6409 - image_quality_output_loss: 0.9331 - age_output_loss: 2.8620 - weight_output_loss: 2.5448 - bag_output_loss: 1.5580 - pose_output_loss: 1.3314 - footwear_output_loss: 1.2253 - emotion_output_loss: 2.6584 - gender_output_acc: 0.7097 - image_quality_output_acc: 0.5644 - age_output_acc: 0.3772 - weight_output_acc: 0.6434 - bag_output_acc: 0.5944 - pose_output_acc: 0.6937 - footwear_output_acc: 0.5897 - emotion_output_acc: 0.7113 - val_loss: 23.4110 - val_gender_output_loss: 0.5397 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8787 - val_pose_output_loss: 0.6861 - val_footwear_output_loss: 0.8428 - val_emotion_output_loss: 0.9289 - val_gender_output_acc: 0.7323 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3652 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6137 - val_pose_output_acc: 0.7131 - val_footwear_output_acc: 0.6142 - val_emotion_output_acc: 0.6732\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 22.92262\n",
            "Epoch 95/100\n",
            "200/200 [==============================] - 83s 413ms/step - loss: 46.1415 - gender_output_loss: 0.6065 - image_quality_output_loss: 0.9446 - age_output_loss: 2.8504 - weight_output_loss: 2.6301 - bag_output_loss: 1.5097 - pose_output_loss: 1.3569 - footwear_output_loss: 1.2010 - emotion_output_loss: 2.7595 - gender_output_acc: 0.7306 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3934 - weight_output_acc: 0.6328 - bag_output_acc: 0.5959 - pose_output_acc: 0.6994 - footwear_output_acc: 0.5941 - emotion_output_acc: 0.7056 - val_loss: 22.9510 - val_gender_output_loss: 0.5240 - val_image_quality_output_loss: 0.9369 - val_age_output_loss: 1.3938 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.8669 - val_pose_output_loss: 0.6139 - val_footwear_output_loss: 0.8833 - val_emotion_output_loss: 0.8962 - val_gender_output_acc: 0.7426 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6156 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.5812 - val_emotion_output_acc: 0.6993\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 22.92262\n",
            "Epoch 96/100\n",
            "200/200 [==============================] - 83s 414ms/step - loss: 44.8048 - gender_output_loss: 0.6023 - image_quality_output_loss: 0.9439 - age_output_loss: 2.7998 - weight_output_loss: 2.5265 - bag_output_loss: 1.4628 - pose_output_loss: 1.2866 - footwear_output_loss: 1.2333 - emotion_output_loss: 2.6297 - gender_output_acc: 0.7256 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4122 - weight_output_acc: 0.6459 - bag_output_acc: 0.6178 - pose_output_acc: 0.7113 - footwear_output_acc: 0.5794 - emotion_output_acc: 0.7131 - val_loss: 22.6709 - val_gender_output_loss: 0.4945 - val_image_quality_output_loss: 0.9396 - val_age_output_loss: 1.3786 - val_weight_output_loss: 0.9879 - val_bag_output_loss: 0.8571 - val_pose_output_loss: 0.5910 - val_footwear_output_loss: 0.8505 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.7539 - val_image_quality_output_acc: 0.5502 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7510 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.6998\n",
            "\n",
            "Epoch 00096: val_loss improved from 22.92262 to 22.67092, saving model to /content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.096.h5\n",
            "Epoch 97/100\n",
            "200/200 [==============================] - 82s 412ms/step - loss: 44.9399 - gender_output_loss: 0.6048 - image_quality_output_loss: 0.9357 - age_output_loss: 2.7666 - weight_output_loss: 2.7044 - bag_output_loss: 1.4914 - pose_output_loss: 1.2144 - footwear_output_loss: 1.1904 - emotion_output_loss: 2.6210 - gender_output_acc: 0.7281 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4066 - weight_output_acc: 0.6259 - bag_output_acc: 0.6159 - pose_output_acc: 0.7219 - footwear_output_acc: 0.5869 - emotion_output_acc: 0.7156 - val_loss: 22.7163 - val_gender_output_loss: 0.5039 - val_image_quality_output_loss: 0.9459 - val_age_output_loss: 1.3772 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.8635 - val_pose_output_loss: 0.5868 - val_footwear_output_loss: 0.8502 - val_emotion_output_loss: 0.8960 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.5512 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6152 - val_pose_output_acc: 0.7480 - val_footwear_output_acc: 0.6201 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 22.67092\n",
            "Epoch 98/100\n",
            "200/200 [==============================] - 84s 422ms/step - loss: 45.0739 - gender_output_loss: 0.6110 - image_quality_output_loss: 0.9387 - age_output_loss: 2.7833 - weight_output_loss: 2.5294 - bag_output_loss: 1.5261 - pose_output_loss: 1.2830 - footwear_output_loss: 1.2096 - emotion_output_loss: 2.6790 - gender_output_acc: 0.7350 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3925 - weight_output_acc: 0.6459 - bag_output_acc: 0.5997 - pose_output_acc: 0.7044 - footwear_output_acc: 0.5800 - emotion_output_acc: 0.7113 - val_loss: 22.8630 - val_gender_output_loss: 0.5075 - val_image_quality_output_loss: 0.9462 - val_age_output_loss: 1.3894 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.8567 - val_pose_output_loss: 0.6338 - val_footwear_output_loss: 0.8588 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6152 - val_pose_output_acc: 0.7367 - val_footwear_output_acc: 0.6058 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 22.67092\n",
            "Epoch 99/100\n",
            "200/200 [==============================] - 83s 416ms/step - loss: 45.4627 - gender_output_loss: 0.6181 - image_quality_output_loss: 0.9431 - age_output_loss: 2.8167 - weight_output_loss: 2.6284 - bag_output_loss: 1.4950 - pose_output_loss: 1.3472 - footwear_output_loss: 1.2103 - emotion_output_loss: 2.6402 - gender_output_acc: 0.7247 - image_quality_output_acc: 0.5600 - age_output_acc: 0.3925 - weight_output_acc: 0.6275 - bag_output_acc: 0.5991 - pose_output_acc: 0.7009 - footwear_output_acc: 0.5847 - emotion_output_acc: 0.7128 - val_loss: 22.9459 - val_gender_output_loss: 0.5148 - val_image_quality_output_loss: 0.9505 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.8558 - val_pose_output_loss: 0.6141 - val_footwear_output_loss: 0.8914 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.7500 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3922 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6191 - val_pose_output_acc: 0.7352 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 22.67092\n",
            "Epoch 100/100\n",
            "200/200 [==============================] - 83s 417ms/step - loss: 46.4366 - gender_output_loss: 0.6445 - image_quality_output_loss: 0.9514 - age_output_loss: 2.8699 - weight_output_loss: 2.6653 - bag_output_loss: 1.5009 - pose_output_loss: 1.3813 - footwear_output_loss: 1.2392 - emotion_output_loss: 2.7442 - gender_output_acc: 0.7041 - image_quality_output_acc: 0.5456 - age_output_acc: 0.3844 - weight_output_acc: 0.6356 - bag_output_acc: 0.5934 - pose_output_acc: 0.6788 - footwear_output_acc: 0.5759 - emotion_output_acc: 0.7053 - val_loss: 25.9893 - val_gender_output_loss: 0.8327 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.6611 - val_weight_output_loss: 1.0124 - val_bag_output_loss: 0.8871 - val_pose_output_loss: 0.7779 - val_footwear_output_loss: 0.9179 - val_emotion_output_loss: 1.0624 - val_gender_output_acc: 0.6560 - val_image_quality_output_acc: 0.5522 - val_age_output_acc: 0.2697 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.5960 - val_pose_output_acc: 0.6772 - val_footwear_output_acc: 0.5546 - val_emotion_output_acc: 0.5846\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 22.67092\n",
            "Saved JSON PATH: /content/gdrive/My Drive/json_wrn2_rkg_fresh_wrn_1577517949.json\n",
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5umiyWoNL91y",
        "colab_type": "code",
        "outputId": "0bd7e379-60cd-4881-ce75-f47be6d3ca49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=30\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.096.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577543864.json\n",
            "Returning new callback array with steps_per_epoch= 721 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 30 patience= 25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "720/721 [============================>.] - ETA: 0s - loss: 45.0902 - gender_output_loss: 0.6023 - image_quality_output_loss: 0.9377 - age_output_loss: 2.8013 - weight_output_loss: 2.5914 - bag_output_loss: 1.5011 - pose_output_loss: 1.2646 - footwear_output_loss: 1.1996 - emotion_output_loss: 2.6601 - gender_output_acc: 0.7300 - image_quality_output_acc: 0.5571 - age_output_acc: 0.4013 - weight_output_acc: 0.6383 - bag_output_acc: 0.6075 - pose_output_acc: 0.7147 - footwear_output_acc: 0.5900 - emotion_output_acc: 0.7120Epoch 1/30\n",
            "721/721 [==============================] - 255s 354ms/step - loss: 45.0893 - gender_output_loss: 0.6022 - image_quality_output_loss: 0.9381 - age_output_loss: 2.8012 - weight_output_loss: 2.5911 - bag_output_loss: 1.5010 - pose_output_loss: 1.2641 - footwear_output_loss: 1.1996 - emotion_output_loss: 2.6607 - gender_output_acc: 0.7301 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4013 - weight_output_acc: 0.6384 - bag_output_acc: 0.6073 - pose_output_acc: 0.7146 - footwear_output_acc: 0.5898 - emotion_output_acc: 0.7119 - val_loss: 22.6609 - val_gender_output_loss: 0.4913 - val_image_quality_output_loss: 0.9354 - val_age_output_loss: 1.3775 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8602 - val_pose_output_loss: 0.6047 - val_footwear_output_loss: 0.8437 - val_emotion_output_loss: 0.8954 - val_gender_output_acc: 0.7544 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3903 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7411 - val_footwear_output_acc: 0.6206 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.66086, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.001.h5\n",
            "Epoch 2/30\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 45.3040 - gender_output_loss: 0.6048 - image_quality_output_loss: 0.9438 - age_output_loss: 2.8089 - weight_output_loss: 2.5888 - bag_output_loss: 1.4992 - pose_output_loss: 1.2883 - footwear_output_loss: 1.2068 - emotion_output_loss: 2.6897 - gender_output_acc: 0.7322 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4001 - weight_output_acc: 0.6373 - bag_output_acc: 0.6063 - pose_output_acc: 0.7127 - footwear_output_acc: 0.5880 - emotion_output_acc: 0.7119 - val_loss: 22.9716 - val_gender_output_loss: 0.5175 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.4131 - val_weight_output_loss: 1.0011 - val_bag_output_loss: 0.8593 - val_pose_output_loss: 0.6089 - val_footwear_output_loss: 0.8742 - val_emotion_output_loss: 0.8987 - val_gender_output_acc: 0.7441 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6073 - val_pose_output_acc: 0.7343 - val_footwear_output_acc: 0.6029 - val_emotion_output_acc: 0.7062\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 22.66086\n",
            "Epoch 3/30\n",
            "721/721 [==============================] - 242s 336ms/step - loss: 45.5990 - gender_output_loss: 0.6168 - image_quality_output_loss: 0.9457 - age_output_loss: 2.8175 - weight_output_loss: 2.6100 - bag_output_loss: 1.5089 - pose_output_loss: 1.3223 - footwear_output_loss: 1.2205 - emotion_output_loss: 2.7013 - gender_output_acc: 0.7191 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3923 - weight_output_acc: 0.6361 - bag_output_acc: 0.5994 - pose_output_acc: 0.6895 - footwear_output_acc: 0.5844 - emotion_output_acc: 0.7109 - val_loss: 23.2921 - val_gender_output_loss: 0.5812 - val_image_quality_output_loss: 0.9493 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8882 - val_pose_output_loss: 0.6702 - val_footwear_output_loss: 0.8944 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.7254 - val_image_quality_output_acc: 0.5399 - val_age_output_acc: 0.3858 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.5930 - val_pose_output_acc: 0.7067 - val_footwear_output_acc: 0.5689 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 22.66086\n",
            "Epoch 4/30\n",
            "721/721 [==============================] - 244s 339ms/step - loss: 45.7123 - gender_output_loss: 0.6221 - image_quality_output_loss: 0.9484 - age_output_loss: 2.8364 - weight_output_loss: 2.6101 - bag_output_loss: 1.5140 - pose_output_loss: 1.3316 - footwear_output_loss: 1.2248 - emotion_output_loss: 2.7044 - gender_output_acc: 0.7119 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3914 - weight_output_acc: 0.6345 - bag_output_acc: 0.5960 - pose_output_acc: 0.6966 - footwear_output_acc: 0.5821 - emotion_output_acc: 0.7108 - val_loss: 23.9590 - val_gender_output_loss: 0.5668 - val_image_quality_output_loss: 0.9412 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9919 - val_bag_output_loss: 0.8755 - val_pose_output_loss: 0.7602 - val_footwear_output_loss: 0.8874 - val_emotion_output_loss: 1.0377 - val_gender_output_acc: 0.7269 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3848 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6019 - val_pose_output_acc: 0.7028 - val_footwear_output_acc: 0.6117 - val_emotion_output_acc: 0.6033\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 22.66086\n",
            "Epoch 5/30\n",
            "721/721 [==============================] - 244s 338ms/step - loss: 45.4938 - gender_output_loss: 0.6078 - image_quality_output_loss: 0.9463 - age_output_loss: 2.8295 - weight_output_loss: 2.6052 - bag_output_loss: 1.5126 - pose_output_loss: 1.3022 - footwear_output_loss: 1.2249 - emotion_output_loss: 2.7032 - gender_output_acc: 0.7223 - image_quality_output_acc: 0.5554 - age_output_acc: 0.3943 - weight_output_acc: 0.6354 - bag_output_acc: 0.5986 - pose_output_acc: 0.6999 - footwear_output_acc: 0.5803 - emotion_output_acc: 0.7091 - val_loss: 22.6730 - val_gender_output_loss: 0.5079 - val_image_quality_output_loss: 0.9538 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8644 - val_pose_output_loss: 0.5913 - val_footwear_output_loss: 0.8516 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.7387 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.3888 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6206 - val_pose_output_acc: 0.7544 - val_footwear_output_acc: 0.6161 - val_emotion_output_acc: 0.7028\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 22.66086\n",
            "Epoch 6/30\n",
            "721/721 [==============================] - 245s 340ms/step - loss: 45.1280 - gender_output_loss: 0.6012 - image_quality_output_loss: 0.9412 - age_output_loss: 2.8172 - weight_output_loss: 2.6090 - bag_output_loss: 1.5109 - pose_output_loss: 1.2564 - footwear_output_loss: 1.2056 - emotion_output_loss: 2.6828 - gender_output_acc: 0.7350 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3967 - weight_output_acc: 0.6357 - bag_output_acc: 0.5977 - pose_output_acc: 0.7179 - footwear_output_acc: 0.5886 - emotion_output_acc: 0.7098 - val_loss: 22.3319 - val_gender_output_loss: 0.4780 - val_image_quality_output_loss: 0.9303 - val_age_output_loss: 1.3754 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8448 - val_pose_output_loss: 0.5712 - val_footwear_output_loss: 0.8400 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5527 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6127 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7520 - val_footwear_output_acc: 0.6309 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00006: val_loss improved from 22.66086 to 22.33188, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.006.h5\n",
            "Epoch 7/30\n",
            "721/721 [==============================] - 245s 339ms/step - loss: 44.5031 - gender_output_loss: 0.5715 - image_quality_output_loss: 0.9351 - age_output_loss: 2.7944 - weight_output_loss: 2.5793 - bag_output_loss: 1.4880 - pose_output_loss: 1.1951 - footwear_output_loss: 1.1885 - emotion_output_loss: 2.6696 - gender_output_acc: 0.7451 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4004 - weight_output_acc: 0.6370 - bag_output_acc: 0.6158 - pose_output_acc: 0.7323 - footwear_output_acc: 0.5951 - emotion_output_acc: 0.7104 - val_loss: 22.1783 - val_gender_output_loss: 0.4544 - val_image_quality_output_loss: 0.9169 - val_age_output_loss: 1.3796 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8533 - val_pose_output_loss: 0.5497 - val_footwear_output_loss: 0.8490 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.7923 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7613 - val_footwear_output_acc: 0.6284 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00007: val_loss improved from 22.33188 to 22.17835, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.007.h5\n",
            "Epoch 8/30\n",
            "721/721 [==============================] - 243s 337ms/step - loss: 43.9166 - gender_output_loss: 0.5528 - image_quality_output_loss: 0.9282 - age_output_loss: 2.7727 - weight_output_loss: 2.5717 - bag_output_loss: 1.4680 - pose_output_loss: 1.1392 - footwear_output_loss: 1.1809 - emotion_output_loss: 2.6289 - gender_output_acc: 0.7575 - image_quality_output_acc: 0.5612 - age_output_acc: 0.4015 - weight_output_acc: 0.6388 - bag_output_acc: 0.6187 - pose_output_acc: 0.7474 - footwear_output_acc: 0.5984 - emotion_output_acc: 0.7124 - val_loss: 22.0765 - val_gender_output_loss: 0.4469 - val_image_quality_output_loss: 0.9473 - val_age_output_loss: 1.3747 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.8469 - val_pose_output_loss: 0.5268 - val_footwear_output_loss: 0.8271 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.7963 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00008: val_loss improved from 22.17835 to 22.07645, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.008.h5\n",
            "Epoch 9/30\n",
            "721/721 [==============================] - 245s 339ms/step - loss: 43.9264 - gender_output_loss: 0.5497 - image_quality_output_loss: 0.9268 - age_output_loss: 2.7647 - weight_output_loss: 2.5674 - bag_output_loss: 1.4652 - pose_output_loss: 1.1474 - footwear_output_loss: 1.1672 - emotion_output_loss: 2.6496 - gender_output_acc: 0.7646 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4019 - weight_output_acc: 0.6378 - bag_output_acc: 0.6223 - pose_output_acc: 0.7427 - footwear_output_acc: 0.6019 - emotion_output_acc: 0.7114 - val_loss: 22.4588 - val_gender_output_loss: 0.4514 - val_image_quality_output_loss: 1.0160 - val_age_output_loss: 1.3721 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8906 - val_pose_output_loss: 0.5472 - val_footwear_output_loss: 0.8280 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.4961 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.5965 - val_pose_output_acc: 0.7736 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6757\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 22.07645\n",
            "Epoch 10/30\n",
            "721/721 [==============================] - 247s 342ms/step - loss: 44.2476 - gender_output_loss: 0.5665 - image_quality_output_loss: 0.9325 - age_output_loss: 2.7821 - weight_output_loss: 2.5712 - bag_output_loss: 1.4782 - pose_output_loss: 1.1823 - footwear_output_loss: 1.1801 - emotion_output_loss: 2.6603 - gender_output_acc: 0.7535 - image_quality_output_acc: 0.5573 - age_output_acc: 0.4029 - weight_output_acc: 0.6384 - bag_output_acc: 0.6149 - pose_output_acc: 0.7421 - footwear_output_acc: 0.6013 - emotion_output_acc: 0.7111 - val_loss: 22.3609 - val_gender_output_loss: 0.4561 - val_image_quality_output_loss: 0.9324 - val_age_output_loss: 1.3739 - val_weight_output_loss: 0.9872 - val_bag_output_loss: 0.8560 - val_pose_output_loss: 0.5715 - val_footwear_output_loss: 0.8356 - val_emotion_output_loss: 0.9399 - val_gender_output_acc: 0.7869 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6196 - val_pose_output_acc: 0.7648 - val_footwear_output_acc: 0.6417 - val_emotion_output_acc: 0.6658\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 22.07645\n",
            "Epoch 11/30\n",
            "720/721 [============================>.] - ETA: 0s - loss: 44.5829 - gender_output_loss: 0.5803 - image_quality_output_loss: 0.9392 - age_output_loss: 2.8065 - weight_output_loss: 2.5769 - bag_output_loss: 1.4920 - pose_output_loss: 1.2153 - footwear_output_loss: 1.1947 - emotion_output_loss: 2.6685 - gender_output_acc: 0.7468 - image_quality_output_acc: 0.5587 - age_output_acc: 0.3965 - weight_output_acc: 0.6378 - bag_output_acc: 0.6140 - pose_output_acc: 0.7271 - footwear_output_acc: 0.5957 - emotion_output_acc: 0.7100\n",
            "721/721 [==============================] - 249s 345ms/step - loss: 44.5709 - gender_output_loss: 0.5807 - image_quality_output_loss: 0.9394 - age_output_loss: 2.8060 - weight_output_loss: 2.5754 - bag_output_loss: 1.4922 - pose_output_loss: 1.2160 - footwear_output_loss: 1.1944 - emotion_output_loss: 2.6664 - gender_output_acc: 0.7464 - image_quality_output_acc: 0.5585 - age_output_acc: 0.3967 - weight_output_acc: 0.6381 - bag_output_acc: 0.6140 - pose_output_acc: 0.7269 - footwear_output_acc: 0.5960 - emotion_output_acc: 0.7102 - val_loss: 22.4602 - val_gender_output_loss: 0.5180 - val_image_quality_output_loss: 0.9272 - val_age_output_loss: 1.3874 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.8839 - val_pose_output_loss: 0.5552 - val_footwear_output_loss: 0.8341 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.7485 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3681 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6033 - val_pose_output_acc: 0.7584 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.07645\n",
            "Epoch 12/30\n",
            "721/721 [==============================] - 248s 343ms/step - loss: 44.9608 - gender_output_loss: 0.5919 - image_quality_output_loss: 0.9428 - age_output_loss: 2.8181 - weight_output_loss: 2.6045 - bag_output_loss: 1.4991 - pose_output_loss: 1.2535 - footwear_output_loss: 1.2079 - emotion_output_loss: 2.6883 - gender_output_acc: 0.7425 - image_quality_output_acc: 0.5553 - age_output_acc: 0.3974 - weight_output_acc: 0.6342 - bag_output_acc: 0.6063 - pose_output_acc: 0.7176 - footwear_output_acc: 0.5923 - emotion_output_acc: 0.7100 - val_loss: 22.5231 - val_gender_output_loss: 0.4739 - val_image_quality_output_loss: 0.9937 - val_age_output_loss: 1.3815 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.8629 - val_pose_output_loss: 0.5760 - val_footwear_output_loss: 0.8981 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.7687 - val_image_quality_output_acc: 0.5276 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6142 - val_pose_output_acc: 0.7490 - val_footwear_output_acc: 0.5817 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 22.07645\n",
            "Epoch 13/30\n",
            "721/721 [==============================] - 248s 344ms/step - loss: 44.8364 - gender_output_loss: 0.5892 - image_quality_output_loss: 0.9397 - age_output_loss: 2.8159 - weight_output_loss: 2.5863 - bag_output_loss: 1.5044 - pose_output_loss: 1.2420 - footwear_output_loss: 1.2008 - emotion_output_loss: 2.6907 - gender_output_acc: 0.7421 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3953 - weight_output_acc: 0.6351 - bag_output_acc: 0.6096 - pose_output_acc: 0.7226 - footwear_output_acc: 0.5950 - emotion_output_acc: 0.7110 - val_loss: 22.1110 - val_gender_output_loss: 0.4547 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8407 - val_pose_output_loss: 0.5600 - val_footwear_output_loss: 0.8190 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.7835 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3371 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6211 - val_pose_output_acc: 0.7574 - val_footwear_output_acc: 0.6407 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 22.07645\n",
            "Epoch 14/30\n",
            "721/721 [==============================] - 250s 347ms/step - loss: 44.2717 - gender_output_loss: 0.5689 - image_quality_output_loss: 0.9352 - age_output_loss: 2.7877 - weight_output_loss: 2.5821 - bag_output_loss: 1.4762 - pose_output_loss: 1.1800 - footwear_output_loss: 1.1811 - emotion_output_loss: 2.6771 - gender_output_acc: 0.7513 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3964 - weight_output_acc: 0.6347 - bag_output_acc: 0.6124 - pose_output_acc: 0.7406 - footwear_output_acc: 0.6033 - emotion_output_acc: 0.7090 - val_loss: 21.8502 - val_gender_output_loss: 0.4497 - val_image_quality_output_loss: 0.9149 - val_age_output_loss: 1.3784 - val_weight_output_loss: 0.9885 - val_bag_output_loss: 0.8314 - val_pose_output_loss: 0.5254 - val_footwear_output_loss: 0.8269 - val_emotion_output_loss: 0.9006 - val_gender_output_acc: 0.7874 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6299 - val_pose_output_acc: 0.7923 - val_footwear_output_acc: 0.6457 - val_emotion_output_acc: 0.6983\n",
            "\n",
            "Epoch 00014: val_loss improved from 22.07645 to 21.85019, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.014.h5\n",
            "Epoch 15/30\n",
            "721/721 [==============================] - 248s 345ms/step - loss: 43.6627 - gender_output_loss: 0.5430 - image_quality_output_loss: 0.9287 - age_output_loss: 2.7676 - weight_output_loss: 2.5668 - bag_output_loss: 1.4654 - pose_output_loss: 1.1250 - footwear_output_loss: 1.1710 - emotion_output_loss: 2.6329 - gender_output_acc: 0.7645 - image_quality_output_acc: 0.5609 - age_output_acc: 0.4021 - weight_output_acc: 0.6362 - bag_output_acc: 0.6201 - pose_output_acc: 0.7560 - footwear_output_acc: 0.6005 - emotion_output_acc: 0.7100 - val_loss: 21.7698 - val_gender_output_loss: 0.4318 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3603 - val_weight_output_loss: 0.9953 - val_bag_output_loss: 0.8465 - val_pose_output_loss: 0.5021 - val_footwear_output_loss: 0.8118 - val_emotion_output_loss: 0.9106 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7928 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00015: val_loss improved from 21.85019 to 21.76977, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.015.h5\n",
            "Epoch 16/30\n",
            "721/721 [==============================] - 249s 345ms/step - loss: 43.2367 - gender_output_loss: 0.5225 - image_quality_output_loss: 0.9218 - age_output_loss: 2.7449 - weight_output_loss: 2.5640 - bag_output_loss: 1.4441 - pose_output_loss: 1.0806 - footwear_output_loss: 1.1630 - emotion_output_loss: 2.6222 - gender_output_acc: 0.7786 - image_quality_output_acc: 0.5633 - age_output_acc: 0.4098 - weight_output_acc: 0.6378 - bag_output_acc: 0.6332 - pose_output_acc: 0.7667 - footwear_output_acc: 0.6057 - emotion_output_acc: 0.7102 - val_loss: 21.5875 - val_gender_output_loss: 0.4234 - val_image_quality_output_loss: 0.9142 - val_age_output_loss: 1.3588 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.8290 - val_pose_output_loss: 0.4881 - val_footwear_output_loss: 0.8182 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.8076 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6462 - val_pose_output_acc: 0.8056 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00016: val_loss improved from 21.76977 to 21.58746, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.016.h5\n",
            "Epoch 17/30\n",
            "721/721 [==============================] - 248s 345ms/step - loss: 43.0868 - gender_output_loss: 0.5155 - image_quality_output_loss: 0.9189 - age_output_loss: 2.7385 - weight_output_loss: 2.5523 - bag_output_loss: 1.4401 - pose_output_loss: 1.0639 - footwear_output_loss: 1.1577 - emotion_output_loss: 2.6247 - gender_output_acc: 0.7805 - image_quality_output_acc: 0.5635 - age_output_acc: 0.4072 - weight_output_acc: 0.6370 - bag_output_acc: 0.6342 - pose_output_acc: 0.7723 - footwear_output_acc: 0.6099 - emotion_output_acc: 0.7097 - val_loss: 22.0964 - val_gender_output_loss: 0.4188 - val_image_quality_output_loss: 1.0802 - val_age_output_loss: 1.3609 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.8521 - val_pose_output_loss: 0.4960 - val_footwear_output_loss: 0.8197 - val_emotion_output_loss: 0.9285 - val_gender_output_acc: 0.8086 - val_image_quality_output_acc: 0.4793 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6024 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.8056 - val_footwear_output_acc: 0.6476 - val_emotion_output_acc: 0.6742\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 21.58746\n",
            "Epoch 18/30\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 43.6045 - gender_output_loss: 0.5391 - image_quality_output_loss: 0.9244 - age_output_loss: 2.7554 - weight_output_loss: 2.5759 - bag_output_loss: 1.4620 - pose_output_loss: 1.1063 - footwear_output_loss: 1.1699 - emotion_output_loss: 2.6542 - gender_output_acc: 0.7680 - image_quality_output_acc: 0.5627 - age_output_acc: 0.4079 - weight_output_acc: 0.6359 - bag_output_acc: 0.6272 - pose_output_acc: 0.7584 - footwear_output_acc: 0.6069 - emotion_output_acc: 0.7100 - val_loss: 22.4729 - val_gender_output_loss: 0.4838 - val_image_quality_output_loss: 0.9591 - val_age_output_loss: 1.3983 - val_weight_output_loss: 1.0494 - val_bag_output_loss: 0.8656 - val_pose_output_loss: 0.5681 - val_footwear_output_loss: 0.8183 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.7830 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.5404 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7776 - val_footwear_output_acc: 0.6496 - val_emotion_output_acc: 0.7028\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 21.58746\n",
            "Epoch 19/30\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 43.9504 - gender_output_loss: 0.5505 - image_quality_output_loss: 0.9311 - age_output_loss: 2.7721 - weight_output_loss: 2.5881 - bag_output_loss: 1.4729 - pose_output_loss: 1.1441 - footwear_output_loss: 1.1825 - emotion_output_loss: 2.6663 - gender_output_acc: 0.7620 - image_quality_output_acc: 0.5603 - age_output_acc: 0.3988 - weight_output_acc: 0.6367 - bag_output_acc: 0.6188 - pose_output_acc: 0.7505 - footwear_output_acc: 0.6019 - emotion_output_acc: 0.7098 - val_loss: 22.8044 - val_gender_output_loss: 0.5725 - val_image_quality_output_loss: 0.9557 - val_age_output_loss: 1.3866 - val_weight_output_loss: 1.0132 - val_bag_output_loss: 0.9015 - val_pose_output_loss: 0.5930 - val_footwear_output_loss: 0.8457 - val_emotion_output_loss: 0.9350 - val_gender_output_acc: 0.7574 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3903 - val_weight_output_acc: 0.5645 - val_bag_output_acc: 0.6024 - val_pose_output_acc: 0.7692 - val_footwear_output_acc: 0.6284 - val_emotion_output_acc: 0.6683\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 21.58746\n",
            "Epoch 20/30\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 44.1932 - gender_output_loss: 0.5621 - image_quality_output_loss: 0.9349 - age_output_loss: 2.7930 - weight_output_loss: 2.5836 - bag_output_loss: 1.4863 - pose_output_loss: 1.1734 - footwear_output_loss: 1.1890 - emotion_output_loss: 2.6689 - gender_output_acc: 0.7526 - image_quality_output_acc: 0.5588 - age_output_acc: 0.3925 - weight_output_acc: 0.6356 - bag_output_acc: 0.6111 - pose_output_acc: 0.7325 - footwear_output_acc: 0.6012 - emotion_output_acc: 0.7095 - val_loss: 22.3272 - val_gender_output_loss: 0.4883 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4078 - val_weight_output_loss: 1.0020 - val_bag_output_loss: 0.8465 - val_pose_output_loss: 0.5565 - val_footwear_output_loss: 0.8335 - val_emotion_output_loss: 0.9011 - val_gender_output_acc: 0.7662 - val_image_quality_output_acc: 0.5148 - val_age_output_acc: 0.3647 - val_weight_output_acc: 0.5792 - val_bag_output_acc: 0.6255 - val_pose_output_acc: 0.7702 - val_footwear_output_acc: 0.6442 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 21.58746\n",
            "Epoch 21/30\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 44.2103 - gender_output_loss: 0.5578 - image_quality_output_loss: 0.9337 - age_output_loss: 2.7998 - weight_output_loss: 2.5864 - bag_output_loss: 1.4794 - pose_output_loss: 1.1738 - footwear_output_loss: 1.1814 - emotion_output_loss: 2.6775 - gender_output_acc: 0.7616 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3963 - weight_output_acc: 0.6355 - bag_output_acc: 0.6223 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6004 - emotion_output_acc: 0.7105 - val_loss: 21.7565 - val_gender_output_loss: 0.4298 - val_image_quality_output_loss: 0.9324 - val_age_output_loss: 1.3520 - val_weight_output_loss: 1.0094 - val_bag_output_loss: 0.8329 - val_pose_output_loss: 0.5338 - val_footwear_output_loss: 0.8163 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.7972 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7918 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.7013\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 21.58746\n",
            "Epoch 22/30\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 43.6526 - gender_output_loss: 0.5456 - image_quality_output_loss: 0.9283 - age_output_loss: 2.7640 - weight_output_loss: 2.5733 - bag_output_loss: 1.4607 - pose_output_loss: 1.1125 - footwear_output_loss: 1.1723 - emotion_output_loss: 2.6610 - gender_output_acc: 0.7714 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4021 - weight_output_acc: 0.6366 - bag_output_acc: 0.6245 - pose_output_acc: 0.7523 - footwear_output_acc: 0.6038 - emotion_output_acc: 0.7084 - val_loss: 21.3647 - val_gender_output_loss: 0.4100 - val_image_quality_output_loss: 0.9152 - val_age_output_loss: 1.3505 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8276 - val_pose_output_loss: 0.5011 - val_footwear_output_loss: 0.8065 - val_emotion_output_loss: 0.8905 - val_gender_output_acc: 0.8233 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.6427 - val_pose_output_acc: 0.8081 - val_footwear_output_acc: 0.6580 - val_emotion_output_acc: 0.7077\n",
            "\n",
            "Epoch 00022: val_loss improved from 21.58746 to 21.36473, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\n",
            "Epoch 23/30\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 43.1408 - gender_output_loss: 0.5233 - image_quality_output_loss: 0.9257 - age_output_loss: 2.7405 - weight_output_loss: 2.5598 - bag_output_loss: 1.4454 - pose_output_loss: 1.0694 - footwear_output_loss: 1.1515 - emotion_output_loss: 2.6378 - gender_output_acc: 0.7771 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4057 - weight_output_acc: 0.6357 - bag_output_acc: 0.6274 - pose_output_acc: 0.7712 - footwear_output_acc: 0.6154 - emotion_output_acc: 0.7098 - val_loss: 21.6200 - val_gender_output_loss: 0.4247 - val_image_quality_output_loss: 0.9427 - val_age_output_loss: 1.3596 - val_weight_output_loss: 1.0016 - val_bag_output_loss: 0.8492 - val_pose_output_loss: 0.4919 - val_footwear_output_loss: 0.8124 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5561 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.5891 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.8174 - val_footwear_output_acc: 0.6398 - val_emotion_output_acc: 0.7018\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 21.36473\n",
            "Epoch 24/30\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 42.5972 - gender_output_loss: 0.4966 - image_quality_output_loss: 0.9157 - age_output_loss: 2.7138 - weight_output_loss: 2.5411 - bag_output_loss: 1.4306 - pose_output_loss: 1.0149 - footwear_output_loss: 1.1390 - emotion_output_loss: 2.6226 - gender_output_acc: 0.7918 - image_quality_output_acc: 0.5669 - age_output_acc: 0.4079 - weight_output_acc: 0.6363 - bag_output_acc: 0.6377 - pose_output_acc: 0.7766 - footwear_output_acc: 0.6162 - emotion_output_acc: 0.7112 - val_loss: 21.6570 - val_gender_output_loss: 0.4091 - val_image_quality_output_loss: 0.9483 - val_age_output_loss: 1.3521 - val_weight_output_loss: 1.0009 - val_bag_output_loss: 0.8401 - val_pose_output_loss: 0.4935 - val_footwear_output_loss: 0.8258 - val_emotion_output_loss: 0.9231 - val_gender_output_acc: 0.8223 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.5935 - val_bag_output_acc: 0.6491 - val_pose_output_acc: 0.8292 - val_footwear_output_acc: 0.6545 - val_emotion_output_acc: 0.6757\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 21.36473\n",
            "Epoch 25/30\n",
            "721/721 [==============================] - 242s 336ms/step - loss: 42.3827 - gender_output_loss: 0.4790 - image_quality_output_loss: 0.9178 - age_output_loss: 2.7021 - weight_output_loss: 2.5451 - bag_output_loss: 1.4283 - pose_output_loss: 0.9951 - footwear_output_loss: 1.1323 - emotion_output_loss: 2.6069 - gender_output_acc: 0.8051 - image_quality_output_acc: 0.5654 - age_output_acc: 0.4100 - weight_output_acc: 0.6374 - bag_output_acc: 0.6405 - pose_output_acc: 0.7848 - footwear_output_acc: 0.6201 - emotion_output_acc: 0.7119 - val_loss: 21.8200 - val_gender_output_loss: 0.4334 - val_image_quality_output_loss: 0.9628 - val_age_output_loss: 1.3925 - val_weight_output_loss: 1.0067 - val_bag_output_loss: 0.8522 - val_pose_output_loss: 0.4729 - val_footwear_output_loss: 0.8195 - val_emotion_output_loss: 0.9112 - val_gender_output_acc: 0.8076 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.3804 - val_weight_output_acc: 0.5871 - val_bag_output_acc: 0.6403 - val_pose_output_acc: 0.8297 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 21.36473\n",
            "Epoch 26/30\n",
            "720/721 [============================>.] - ETA: 0s - loss: 42.9541 - gender_output_loss: 0.5028 - image_quality_output_loss: 0.9187 - age_output_loss: 2.7400 - weight_output_loss: 2.5603 - bag_output_loss: 1.4413 - pose_output_loss: 1.0354 - footwear_output_loss: 1.1495 - emotion_output_loss: 2.6421 - gender_output_acc: 0.7926 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4070 - weight_output_acc: 0.6361 - bag_output_acc: 0.6316 - pose_output_acc: 0.7758 - footwear_output_acc: 0.6152 - emotion_output_acc: 0.7112\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 42.9457 - gender_output_loss: 0.5030 - image_quality_output_loss: 0.9189 - age_output_loss: 2.7394 - weight_output_loss: 2.5592 - bag_output_loss: 1.4417 - pose_output_loss: 1.0367 - footwear_output_loss: 1.1490 - emotion_output_loss: 2.6402 - gender_output_acc: 0.7926 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4068 - weight_output_acc: 0.6361 - bag_output_acc: 0.6315 - pose_output_acc: 0.7757 - footwear_output_acc: 0.6153 - emotion_output_acc: 0.7114 - val_loss: 21.6155 - val_gender_output_loss: 0.4626 - val_image_quality_output_loss: 0.9446 - val_age_output_loss: 1.3691 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.8312 - val_pose_output_loss: 0.4914 - val_footwear_output_loss: 0.8106 - val_emotion_output_loss: 0.9043 - val_gender_output_acc: 0.7795 - val_image_quality_output_acc: 0.5389 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.8095 - val_footwear_output_acc: 0.6481 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 21.36473\n",
            "Epoch 27/30\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 43.3340 - gender_output_loss: 0.5167 - image_quality_output_loss: 0.9226 - age_output_loss: 2.7453 - weight_output_loss: 2.5752 - bag_output_loss: 1.4559 - pose_output_loss: 1.0887 - footwear_output_loss: 1.1533 - emotion_output_loss: 2.6605 - gender_output_acc: 0.7813 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4081 - weight_output_acc: 0.6364 - bag_output_acc: 0.6267 - pose_output_acc: 0.7659 - footwear_output_acc: 0.6123 - emotion_output_acc: 0.7086 - val_loss: 21.6693 - val_gender_output_loss: 0.4359 - val_image_quality_output_loss: 0.9650 - val_age_output_loss: 1.3453 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.8415 - val_pose_output_loss: 0.5152 - val_footwear_output_loss: 0.8076 - val_emotion_output_loss: 0.9257 - val_gender_output_acc: 0.8056 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.4050 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6565 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 21.36473\n",
            "Epoch 28/30\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 43.7486 - gender_output_loss: 0.5416 - image_quality_output_loss: 0.9298 - age_output_loss: 2.7715 - weight_output_loss: 2.5827 - bag_output_loss: 1.4724 - pose_output_loss: 1.1237 - footwear_output_loss: 1.1710 - emotion_output_loss: 2.6678 - gender_output_acc: 0.7686 - image_quality_output_acc: 0.5612 - age_output_acc: 0.3998 - weight_output_acc: 0.6362 - bag_output_acc: 0.6221 - pose_output_acc: 0.7548 - footwear_output_acc: 0.6045 - emotion_output_acc: 0.7096 - val_loss: 21.8876 - val_gender_output_loss: 0.4361 - val_image_quality_output_loss: 0.9995 - val_age_output_loss: 1.3587 - val_weight_output_loss: 0.9851 - val_bag_output_loss: 0.8466 - val_pose_output_loss: 0.5435 - val_footwear_output_loss: 0.8146 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6132 - val_pose_output_acc: 0.7781 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 21.36473\n",
            "Epoch 29/30\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 43.7440 - gender_output_loss: 0.5396 - image_quality_output_loss: 0.9336 - age_output_loss: 2.7730 - weight_output_loss: 2.5880 - bag_output_loss: 1.4675 - pose_output_loss: 1.1214 - footwear_output_loss: 1.1751 - emotion_output_loss: 2.6614 - gender_output_acc: 0.7686 - image_quality_output_acc: 0.5595 - age_output_acc: 0.4012 - weight_output_acc: 0.6344 - bag_output_acc: 0.6241 - pose_output_acc: 0.7567 - footwear_output_acc: 0.6034 - emotion_output_acc: 0.7106 - val_loss: 22.0561 - val_gender_output_loss: 0.5268 - val_image_quality_output_loss: 0.9165 - val_age_output_loss: 1.3548 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8785 - val_pose_output_loss: 0.5373 - val_footwear_output_loss: 0.8523 - val_emotion_output_loss: 0.9201 - val_gender_output_acc: 0.7820 - val_image_quality_output_acc: 0.5591 - val_age_output_acc: 0.4094 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.8061 - val_footwear_output_acc: 0.6211 - val_emotion_output_acc: 0.6841\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 21.36473\n",
            "Epoch 30/30\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 43.4156 - gender_output_loss: 0.5145 - image_quality_output_loss: 0.9251 - age_output_loss: 2.7577 - weight_output_loss: 2.5775 - bag_output_loss: 1.4666 - pose_output_loss: 1.0782 - footwear_output_loss: 1.1603 - emotion_output_loss: 2.6616 - gender_output_acc: 0.7867 - image_quality_output_acc: 0.5636 - age_output_acc: 0.4061 - weight_output_acc: 0.6365 - bag_output_acc: 0.6231 - pose_output_acc: 0.7680 - footwear_output_acc: 0.6105 - emotion_output_acc: 0.7101 - val_loss: 21.6670 - val_gender_output_loss: 0.4177 - val_image_quality_output_loss: 0.9701 - val_age_output_loss: 1.3450 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8293 - val_pose_output_loss: 0.5493 - val_footwear_output_loss: 0.8227 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.8150 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.4144 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.7923 - val_footwear_output_acc: 0.6432 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 21.36473\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577543864.json\n",
            "End of EPOCHS= 30  STEPS_PER_EPOCH= 721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTglbv2aaZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(\"/content/gdrive/My Drive/WRN_Extend/model_8233acc.h5py\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2cc5fxtr_v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_gen_array = [ ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=30,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.5,\n",
        "                             height_shift_range=0.5,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,3.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,3.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.8,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,4.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "]\n",
        "           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzs8k2MswOUC",
        "outputId": "b732812a-c7f8-4921-a0dd-04c529a61de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=50\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "# aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "#                              vertical_flip=False,\n",
        "#                              rotation_range=5,\n",
        "#                              width_shift_range=0.1,\n",
        "#                              height_shift_range=0.1,\n",
        "#                              zoom_range=[0.5,2.5],\n",
        "#                              shear_range=0.2,\n",
        "#                              #zca_whitening=True,\n",
        "#                              brightness_range=[0.5,2.5],\n",
        "#                              #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "#                              )\n",
        "from keras.models import load_model\n",
        "for aug_gen_count in range(len(aug_gen_array)):\n",
        "    print(\"Executing augmentation\",aug_gen_count)\n",
        "    new_model = create_model()\n",
        "    new_model.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "    \n",
        "    train_gen = PersonDataGenerator(train_df, \n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    normalize=True,\n",
        "                                    aug_flow=aug_gen_array[aug_gen_count])\n",
        "    valid_gen = PersonDataGenerator(val_df, \n",
        "                                    batch_size=BATCH_SIZE, \n",
        "                                    shuffle=False,normalize=True)\n",
        "\n",
        "    #wrn_28_10.load_weights()\n",
        "\n",
        "    loss_weights_compile = {'gender_output': 2, \n",
        "                            'image_quality_output': 2, \n",
        "                            'age_output': 4, \n",
        "                            'weight_output': 3, \n",
        "                            'bag_output': 3, \n",
        "                            'pose_output': 3, \n",
        "                            'footwear_output': 2, \n",
        "                            'emotion_output': 4}\n",
        "\n",
        "    new_model.compile(\n",
        "        #optimizer=SGD(lr=1.3513402*0.1),\n",
        "        optimizer=SGD(lr=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        loss_weights=loss_weights_compile,\n",
        "        #weighted_metrics=[\"accuracy\"]\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                        epoch_count=EPOCHS,\n",
        "                                        min_lr=LEARNING_RATE*0.01, \n",
        "                                        max_lr=LEARNING_RATE,\n",
        "                                        patience=10, check_point=False)\n",
        "    #print(callbacks)\n",
        "    new_model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        workers=4, \n",
        "        epochs=EPOCHS,\n",
        "        verbose=1,\n",
        "        class_weight=loss_weights_train,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    del new_model\n",
        "    print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing augmentation 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577560164_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 32s 1s/step - loss: 39.1244 - gender_output_loss: 0.5328 - image_quality_output_loss: 0.9263 - age_output_loss: 2.5475 - weight_output_loss: 2.1897 - bag_output_loss: 1.4486 - pose_output_loss: 1.0365 - footwear_output_loss: 1.1665 - emotion_output_loss: 2.1119 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4292 - weight_output_acc: 0.6812 - bag_output_acc: 0.6375 - pose_output_acc: 0.7937 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7396 - val_loss: 21.5024 - val_gender_output_loss: 0.4111 - val_image_quality_output_loss: 0.9370 - val_age_output_loss: 1.3576 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8296 - val_pose_output_loss: 0.5145 - val_footwear_output_loss: 0.8019 - val_emotion_output_loss: 0.8957 - val_gender_output_acc: 0.8209 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6093 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.8007 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.7057\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 25s 823ms/step - loss: 45.3003 - gender_output_loss: 0.4924 - image_quality_output_loss: 0.9468 - age_output_loss: 2.8539 - weight_output_loss: 3.0698 - bag_output_loss: 1.3616 - pose_output_loss: 1.0307 - footwear_output_loss: 1.1949 - emotion_output_loss: 2.7548 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5354 - age_output_acc: 0.4104 - weight_output_acc: 0.5854 - bag_output_acc: 0.6375 - pose_output_acc: 0.7896 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.6792 - val_loss: 23.8532 - val_gender_output_loss: 0.4747 - val_image_quality_output_loss: 1.0791 - val_age_output_loss: 1.4640 - val_weight_output_loss: 1.1161 - val_bag_output_loss: 0.8480 - val_pose_output_loss: 0.6204 - val_footwear_output_loss: 0.8370 - val_emotion_output_loss: 1.0629 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.5039 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.7756 - val_footwear_output_acc: 0.6348 - val_emotion_output_acc: 0.5635\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 26s 859ms/step - loss: 42.9870 - gender_output_loss: 0.4973 - image_quality_output_loss: 0.9245 - age_output_loss: 2.7849 - weight_output_loss: 2.5864 - bag_output_loss: 1.5088 - pose_output_loss: 1.0598 - footwear_output_loss: 1.2336 - emotion_output_loss: 2.4652 - gender_output_acc: 0.7917 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3979 - weight_output_acc: 0.6146 - bag_output_acc: 0.6438 - pose_output_acc: 0.7896 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7208 - val_loss: 21.7945 - val_gender_output_loss: 0.4368 - val_image_quality_output_loss: 0.9862 - val_age_output_loss: 1.3638 - val_weight_output_loss: 0.9917 - val_bag_output_loss: 0.8370 - val_pose_output_loss: 0.5045 - val_footwear_output_loss: 0.8302 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.8027 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6206 - val_emotion_output_acc: 0.6959\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 25s 831ms/step - loss: 44.9696 - gender_output_loss: 0.5528 - image_quality_output_loss: 0.9346 - age_output_loss: 2.8198 - weight_output_loss: 2.6200 - bag_output_loss: 1.4750 - pose_output_loss: 1.2794 - footwear_output_loss: 1.1740 - emotion_output_loss: 2.7586 - gender_output_acc: 0.7750 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4104 - weight_output_acc: 0.6333 - bag_output_acc: 0.5896 - pose_output_acc: 0.7396 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7021 - val_loss: 21.6821 - val_gender_output_loss: 0.4449 - val_image_quality_output_loss: 0.9158 - val_age_output_loss: 1.3542 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8420 - val_pose_output_loss: 0.5462 - val_footwear_output_loss: 0.8192 - val_emotion_output_loss: 0.8982 - val_gender_output_acc: 0.7948 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4055 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7717 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.7028\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 47.2901 - gender_output_loss: 0.5628 - image_quality_output_loss: 0.9289 - age_output_loss: 3.0091 - weight_output_loss: 2.9198 - bag_output_loss: 1.5189 - pose_output_loss: 1.2606 - footwear_output_loss: 1.2667 - emotion_output_loss: 2.8570 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3646 - weight_output_acc: 0.6208 - bag_output_acc: 0.6271 - pose_output_acc: 0.7125 - footwear_output_acc: 0.5521 - emotion_output_acc: 0.7021 - val_loss: 22.4343 - val_gender_output_loss: 0.4392 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.3737 - val_weight_output_loss: 1.0270 - val_bag_output_loss: 0.8469 - val_pose_output_loss: 0.5797 - val_footwear_output_loss: 0.8890 - val_emotion_output_loss: 0.9428 - val_gender_output_acc: 0.8041 - val_image_quality_output_acc: 0.5295 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.5876 - val_bag_output_acc: 0.6206 - val_pose_output_acc: 0.7726 - val_footwear_output_acc: 0.5812 - val_emotion_output_acc: 0.6880\n",
            "Epoch 5/50\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 25s 839ms/step - loss: 43.5564 - gender_output_loss: 0.5529 - image_quality_output_loss: 0.9240 - age_output_loss: 2.8901 - weight_output_loss: 2.8184 - bag_output_loss: 1.3981 - pose_output_loss: 0.9013 - footwear_output_loss: 1.1426 - emotion_output_loss: 2.5483 - gender_output_acc: 0.7375 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3938 - weight_output_acc: 0.6375 - bag_output_acc: 0.6062 - pose_output_acc: 0.8000 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7208 - val_loss: 21.8560 - val_gender_output_loss: 0.4272 - val_image_quality_output_loss: 0.9548 - val_age_output_loss: 1.3552 - val_weight_output_loss: 1.0156 - val_bag_output_loss: 0.8319 - val_pose_output_loss: 0.5340 - val_footwear_output_loss: 0.8125 - val_emotion_output_loss: 0.9229 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6038 - val_bag_output_acc: 0.6457 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6407 - val_emotion_output_acc: 0.6865\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 25s 831ms/step - loss: 44.4480 - gender_output_loss: 0.5165 - image_quality_output_loss: 0.8688 - age_output_loss: 2.6928 - weight_output_loss: 2.8145 - bag_output_loss: 1.4291 - pose_output_loss: 1.0907 - footwear_output_loss: 1.1399 - emotion_output_loss: 2.8534 - gender_output_acc: 0.7854 - image_quality_output_acc: 0.6104 - age_output_acc: 0.4313 - weight_output_acc: 0.6292 - bag_output_acc: 0.6375 - pose_output_acc: 0.7875 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.6938 - val_loss: 21.8375 - val_gender_output_loss: 0.4317 - val_image_quality_output_loss: 0.9288 - val_age_output_loss: 1.3632 - val_weight_output_loss: 1.0025 - val_bag_output_loss: 0.8582 - val_pose_output_loss: 0.5078 - val_footwear_output_loss: 0.8074 - val_emotion_output_loss: 0.9334 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6235 - val_pose_output_acc: 0.8061 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.6683\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 25s 836ms/step - loss: 42.1463 - gender_output_loss: 0.5010 - image_quality_output_loss: 0.9732 - age_output_loss: 2.6389 - weight_output_loss: 2.6564 - bag_output_loss: 1.4502 - pose_output_loss: 0.9306 - footwear_output_loss: 1.1947 - emotion_output_loss: 2.4830 - gender_output_acc: 0.8000 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4375 - weight_output_acc: 0.6313 - bag_output_acc: 0.6333 - pose_output_acc: 0.8021 - footwear_output_acc: 0.5708 - emotion_output_acc: 0.7188 - val_loss: 21.6275 - val_gender_output_loss: 0.4195 - val_image_quality_output_loss: 0.9425 - val_age_output_loss: 1.3526 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.8407 - val_pose_output_loss: 0.4955 - val_footwear_output_loss: 0.8014 - val_emotion_output_loss: 0.9217 - val_gender_output_acc: 0.8051 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6073 - val_bag_output_acc: 0.6284 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6580 - val_emotion_output_acc: 0.6767\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 25s 838ms/step - loss: 42.2214 - gender_output_loss: 0.5187 - image_quality_output_loss: 0.8935 - age_output_loss: 2.8038 - weight_output_loss: 2.4291 - bag_output_loss: 1.4694 - pose_output_loss: 1.0261 - footwear_output_loss: 1.1591 - emotion_output_loss: 2.4702 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.6083 - age_output_acc: 0.4208 - weight_output_acc: 0.6250 - bag_output_acc: 0.6104 - pose_output_acc: 0.7563 - footwear_output_acc: 0.6083 - emotion_output_acc: 0.7292 - val_loss: 21.6581 - val_gender_output_loss: 0.4186 - val_image_quality_output_loss: 0.9382 - val_age_output_loss: 1.3610 - val_weight_output_loss: 0.9972 - val_bag_output_loss: 0.8389 - val_pose_output_loss: 0.5044 - val_footwear_output_loss: 0.8101 - val_emotion_output_loss: 0.9125 - val_gender_output_acc: 0.8174 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.6033 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.8105 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6826\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 25s 837ms/step - loss: 43.0491 - gender_output_loss: 0.5384 - image_quality_output_loss: 0.9573 - age_output_loss: 2.7157 - weight_output_loss: 2.9793 - bag_output_loss: 1.3755 - pose_output_loss: 0.9805 - footwear_output_loss: 1.1329 - emotion_output_loss: 2.4286 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3896 - weight_output_acc: 0.6000 - bag_output_acc: 0.6208 - pose_output_acc: 0.7833 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7417 - val_loss: 22.1135 - val_gender_output_loss: 0.4135 - val_image_quality_output_loss: 0.9673 - val_age_output_loss: 1.3726 - val_weight_output_loss: 1.0228 - val_bag_output_loss: 0.8376 - val_pose_output_loss: 0.5418 - val_footwear_output_loss: 0.8056 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.8184 - val_image_quality_output_acc: 0.5305 - val_age_output_acc: 0.3893 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.6432 - val_pose_output_acc: 0.7997 - val_footwear_output_acc: 0.6501 - val_emotion_output_acc: 0.6378\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 25s 820ms/step - loss: 44.7486 - gender_output_loss: 0.5480 - image_quality_output_loss: 0.9370 - age_output_loss: 2.9769 - weight_output_loss: 2.6819 - bag_output_loss: 1.5147 - pose_output_loss: 1.0144 - footwear_output_loss: 1.1242 - emotion_output_loss: 2.6954 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3688 - weight_output_acc: 0.6062 - bag_output_acc: 0.6229 - pose_output_acc: 0.7833 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7042 - val_loss: 22.3498 - val_gender_output_loss: 0.4276 - val_image_quality_output_loss: 0.9561 - val_age_output_loss: 1.3620 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8446 - val_pose_output_loss: 0.6489 - val_footwear_output_loss: 0.8387 - val_emotion_output_loss: 0.9491 - val_gender_output_acc: 0.8066 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.4114 - val_weight_output_acc: 0.5955 - val_bag_output_acc: 0.6393 - val_pose_output_acc: 0.7712 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6609\n",
            "30/30 [==============================] - 25s 837ms/step - loss: 43.0491 - gender_output_loss: 0.5384 - image_quality_output_loss: 0.9573 - age_output_loss: 2.7157 - weight_output_loss: 2.9793 - bag_output_loss: 1.3755 - pose_output_loss: 0.9805 - footwear_output_loss: 1.1329 - emotion_output_loss: 2.4286 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3896 - weight_output_acc: 0.6000 - bag_output_acc: 0.6208 - pose_output_acc: 0.7833 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7417 - val_loss: 22.1135 - val_gender_output_loss: 0.4135 - val_image_quality_output_loss: 0.9673 - val_age_output_loss: 1.3726 - val_weight_output_loss: 1.0228 - val_bag_output_loss: 0.8376 - val_pose_output_loss: 0.5418 - val_footwear_output_loss: 0.8056 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.8184 - val_image_quality_output_acc: 0.5305 - val_age_output_acc: 0.3893 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.6432 - val_pose_output_acc: 0.7997 - val_footwear_output_acc: 0.6501 - val_emotion_output_acc: 0.6378\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577560453_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577560456_backup\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 30s 989ms/step - loss: 44.6495 - gender_output_loss: 0.6268 - image_quality_output_loss: 0.9016 - age_output_loss: 2.7483 - weight_output_loss: 2.1701 - bag_output_loss: 1.5656 - pose_output_loss: 1.3120 - footwear_output_loss: 1.1509 - emotion_output_loss: 2.9858 - gender_output_acc: 0.7063 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4250 - weight_output_acc: 0.6583 - bag_output_acc: 0.6104 - pose_output_acc: 0.6667 - footwear_output_acc: 0.5958 - emotion_output_acc: 0.6687 - val_loss: 21.6412 - val_gender_output_loss: 0.4267 - val_image_quality_output_loss: 0.9237 - val_age_output_loss: 1.3568 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8290 - val_pose_output_loss: 0.5548 - val_footwear_output_loss: 0.8029 - val_emotion_output_loss: 0.8987 - val_gender_output_acc: 0.8041 - val_image_quality_output_acc: 0.5527 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.5999 - val_bag_output_acc: 0.6471 - val_pose_output_acc: 0.7820 - val_footwear_output_acc: 0.6550 - val_emotion_output_acc: 0.6973\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 24s 789ms/step - loss: 43.9588 - gender_output_loss: 0.5842 - image_quality_output_loss: 0.9617 - age_output_loss: 2.7803 - weight_output_loss: 2.6936 - bag_output_loss: 1.6038 - pose_output_loss: 1.3240 - footwear_output_loss: 1.2987 - emotion_output_loss: 2.2684 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3979 - weight_output_acc: 0.6521 - bag_output_acc: 0.5771 - pose_output_acc: 0.7250 - footwear_output_acc: 0.5417 - emotion_output_acc: 0.7542 - val_loss: 21.9309 - val_gender_output_loss: 0.4157 - val_image_quality_output_loss: 0.9475 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.8557 - val_pose_output_loss: 0.5930 - val_footwear_output_loss: 0.8294 - val_emotion_output_loss: 0.9010 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6019 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7697 - val_footwear_output_acc: 0.6398 - val_emotion_output_acc: 0.6959\n",
            "Epoch 3/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 44.7679 - gender_output_loss: 0.6040 - image_quality_output_loss: 0.9415 - age_output_loss: 2.6914 - weight_output_loss: 2.5159 - bag_output_loss: 1.4957 - pose_output_loss: 1.3627 - footwear_output_loss: 1.2354 - emotion_output_loss: 2.7768 - gender_output_acc: 0.7155 - image_quality_output_acc: 0.5366 - age_output_acc: 0.3815 - weight_output_acc: 0.6659 - bag_output_acc: 0.5819 - pose_output_acc: 0.6918 - footwear_output_acc: 0.5819 - emotion_output_acc: 0.7026Epoch 3/50\n",
            "30/30 [==============================] - 24s 812ms/step - loss: 44.8836 - gender_output_loss: 0.6036 - image_quality_output_loss: 0.9386 - age_output_loss: 2.6849 - weight_output_loss: 2.5410 - bag_output_loss: 1.4933 - pose_output_loss: 1.3604 - footwear_output_loss: 1.2239 - emotion_output_loss: 2.8044 - gender_output_acc: 0.7146 - image_quality_output_acc: 0.5396 - age_output_acc: 0.3854 - weight_output_acc: 0.6646 - bag_output_acc: 0.5813 - pose_output_acc: 0.6938 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7000 - val_loss: 22.0313 - val_gender_output_loss: 0.4274 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.3756 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.8427 - val_pose_output_loss: 0.5646 - val_footwear_output_loss: 0.8213 - val_emotion_output_loss: 0.9212 - val_gender_output_acc: 0.8041 - val_image_quality_output_acc: 0.5246 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.7687 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6909\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 45.7216 - gender_output_loss: 0.6239 - image_quality_output_loss: 0.9511 - age_output_loss: 2.8007 - weight_output_loss: 2.5156 - bag_output_loss: 1.5120 - pose_output_loss: 1.2349 - footwear_output_loss: 1.2346 - emotion_output_loss: 2.9756 - gender_output_acc: 0.7083 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4062 - weight_output_acc: 0.6292 - bag_output_acc: 0.6062 - pose_output_acc: 0.7021 - footwear_output_acc: 0.5625 - emotion_output_acc: 0.6833 - val_loss: 22.5857 - val_gender_output_loss: 0.4360 - val_image_quality_output_loss: 0.9148 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.8443 - val_pose_output_loss: 0.7803 - val_footwear_output_loss: 0.8320 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.7894 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.7195 - val_footwear_output_acc: 0.6427 - val_emotion_output_acc: 0.6959\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 25s 823ms/step - loss: 44.1887 - gender_output_loss: 0.5868 - image_quality_output_loss: 0.9261 - age_output_loss: 2.8588 - weight_output_loss: 2.5302 - bag_output_loss: 1.4779 - pose_output_loss: 1.3658 - footwear_output_loss: 1.1488 - emotion_output_loss: 2.5246 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5771 - age_output_acc: 0.3771 - weight_output_acc: 0.6417 - bag_output_acc: 0.6208 - pose_output_acc: 0.6854 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7333 - val_loss: 22.4539 - val_gender_output_loss: 0.4838 - val_image_quality_output_loss: 0.9553 - val_age_output_loss: 1.4121 - val_weight_output_loss: 1.0364 - val_bag_output_loss: 0.8946 - val_pose_output_loss: 0.5421 - val_footwear_output_loss: 0.8291 - val_emotion_output_loss: 0.9098 - val_gender_output_acc: 0.7726 - val_image_quality_output_acc: 0.5364 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.5497 - val_bag_output_acc: 0.6152 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.7062\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 24s 814ms/step - loss: 45.6463 - gender_output_loss: 0.5811 - image_quality_output_loss: 0.9398 - age_output_loss: 2.8033 - weight_output_loss: 2.4375 - bag_output_loss: 1.5277 - pose_output_loss: 1.2597 - footwear_output_loss: 1.3009 - emotion_output_loss: 2.9762 - gender_output_acc: 0.7333 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4042 - weight_output_acc: 0.6646 - bag_output_acc: 0.6271 - pose_output_acc: 0.7250 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.6896 - val_loss: 22.1304 - val_gender_output_loss: 0.4310 - val_image_quality_output_loss: 0.9660 - val_age_output_loss: 1.3617 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.8636 - val_pose_output_loss: 0.5564 - val_footwear_output_loss: 0.8242 - val_emotion_output_loss: 0.9500 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6284 - val_pose_output_acc: 0.7741 - val_footwear_output_acc: 0.6427 - val_emotion_output_acc: 0.6737\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 24s 815ms/step - loss: 42.7428 - gender_output_loss: 0.6052 - image_quality_output_loss: 0.8788 - age_output_loss: 2.4698 - weight_output_loss: 2.7808 - bag_output_loss: 1.3976 - pose_output_loss: 1.3381 - footwear_output_loss: 1.2014 - emotion_output_loss: 2.4335 - gender_output_acc: 0.7063 - image_quality_output_acc: 0.6104 - age_output_acc: 0.4375 - weight_output_acc: 0.6187 - bag_output_acc: 0.6167 - pose_output_acc: 0.7083 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7354 - val_loss: 21.5867 - val_gender_output_loss: 0.4243 - val_image_quality_output_loss: 0.9160 - val_age_output_loss: 1.3619 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8434 - val_pose_output_loss: 0.5337 - val_footwear_output_loss: 0.7979 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.7997 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6097 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.7825 - val_footwear_output_acc: 0.6683 - val_emotion_output_acc: 0.7028\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 25s 817ms/step - loss: 43.7792 - gender_output_loss: 0.5590 - image_quality_output_loss: 0.9258 - age_output_loss: 2.9143 - weight_output_loss: 2.4717 - bag_output_loss: 1.4047 - pose_output_loss: 1.2699 - footwear_output_loss: 1.1530 - emotion_output_loss: 2.5497 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3583 - weight_output_acc: 0.6187 - bag_output_acc: 0.5792 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.7271 - val_loss: 21.5231 - val_gender_output_loss: 0.4285 - val_image_quality_output_loss: 0.9128 - val_age_output_loss: 1.3577 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8376 - val_pose_output_loss: 0.5129 - val_footwear_output_loss: 0.8170 - val_emotion_output_loss: 0.8913 - val_gender_output_acc: 0.7982 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.4075 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.6383 - val_pose_output_acc: 0.7923 - val_footwear_output_acc: 0.6506 - val_emotion_output_acc: 0.7037\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 24s 815ms/step - loss: 42.8959 - gender_output_loss: 0.6213 - image_quality_output_loss: 0.9388 - age_output_loss: 2.5410 - weight_output_loss: 2.2993 - bag_output_loss: 1.4629 - pose_output_loss: 1.4239 - footwear_output_loss: 1.1500 - emotion_output_loss: 2.6363 - gender_output_acc: 0.7167 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4167 - weight_output_acc: 0.6604 - bag_output_acc: 0.6333 - pose_output_acc: 0.6792 - footwear_output_acc: 0.5708 - emotion_output_acc: 0.7083 - val_loss: 21.7954 - val_gender_output_loss: 0.4265 - val_image_quality_output_loss: 0.9388 - val_age_output_loss: 1.3869 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8417 - val_pose_output_loss: 0.5294 - val_footwear_output_loss: 0.8107 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6358 - val_pose_output_acc: 0.7948 - val_footwear_output_acc: 0.6535 - val_emotion_output_acc: 0.6969\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 45.2191 - gender_output_loss: 0.6054 - image_quality_output_loss: 0.9257 - age_output_loss: 2.9470 - weight_output_loss: 2.3362 - bag_output_loss: 1.5355 - pose_output_loss: 1.4222 - footwear_output_loss: 1.1176 - emotion_output_loss: 2.7611 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5771 - age_output_acc: 0.3812 - weight_output_acc: 0.6667 - bag_output_acc: 0.5833 - pose_output_acc: 0.6812 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7125 - val_loss: 21.4573 - val_gender_output_loss: 0.4288 - val_image_quality_output_loss: 0.9148 - val_age_output_loss: 1.3507 - val_weight_output_loss: 0.9711 - val_bag_output_loss: 0.8387 - val_pose_output_loss: 0.5258 - val_footwear_output_loss: 0.7896 - val_emotion_output_loss: 0.8934 - val_gender_output_acc: 0.7987 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.7958 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.7062\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 25s 821ms/step - loss: 45.1376 - gender_output_loss: 0.5975 - image_quality_output_loss: 0.9178 - age_output_loss: 2.7946 - weight_output_loss: 2.7445 - bag_output_loss: 1.6321 - pose_output_loss: 1.4014 - footwear_output_loss: 1.1657 - emotion_output_loss: 2.5142 - gender_output_acc: 0.7437 - image_quality_output_acc: 0.5938 - age_output_acc: 0.3792 - weight_output_acc: 0.6146 - bag_output_acc: 0.5938 - pose_output_acc: 0.7125 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7208 - val_loss: 22.1195 - val_gender_output_loss: 0.4576 - val_image_quality_output_loss: 0.9560 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8508 - val_pose_output_loss: 0.5976 - val_footwear_output_loss: 0.8116 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.7844 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7539 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.7052\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 43.6858 - gender_output_loss: 0.6338 - image_quality_output_loss: 0.9535 - age_output_loss: 2.7256 - weight_output_loss: 2.6205 - bag_output_loss: 1.5621 - pose_output_loss: 1.3805 - footwear_output_loss: 1.2445 - emotion_output_loss: 2.3059 - gender_output_acc: 0.7021 - image_quality_output_acc: 0.5479 - age_output_acc: 0.4229 - weight_output_acc: 0.6458 - bag_output_acc: 0.6104 - pose_output_acc: 0.6896 - footwear_output_acc: 0.5646 - emotion_output_acc: 0.7417 - val_loss: 21.8482 - val_gender_output_loss: 0.4743 - val_image_quality_output_loss: 0.9148 - val_age_output_loss: 1.3701 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8427 - val_pose_output_loss: 0.5584 - val_footwear_output_loss: 0.8191 - val_emotion_output_loss: 0.8986 - val_gender_output_acc: 0.7746 - val_image_quality_output_acc: 0.5576 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.7731 - val_footwear_output_acc: 0.6452 - val_emotion_output_acc: 0.7062\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 25s 821ms/step - loss: 45.1376 - gender_output_loss: 0.5975 - image_quality_output_loss: 0.9178 - age_output_loss: 2.7946 - weight_output_loss: 2.7445 - bag_output_loss: 1.6321 - pose_output_loss: 1.4014 - footwear_output_loss: 1.1657 - emotion_output_loss: 2.5142 - gender_output_acc: 0.7437 - image_quality_output_acc: 0.5938 - age_output_acc: 0.3792 - weight_output_acc: 0.6146 - bag_output_acc: 0.5938 - pose_output_acc: 0.7125 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7208 - val_loss: 22.1195 - val_gender_output_loss: 0.4576 - val_image_quality_output_loss: 0.9560 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8508 - val_pose_output_loss: 0.5976 - val_footwear_output_loss: 0.8116 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.7844 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.7539 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.7052\n",
            "30/30 [==============================] - 25s 835ms/step - loss: 45.2489 - gender_output_loss: 0.5843 - image_quality_output_loss: 0.9597 - age_output_loss: 2.8537 - weight_output_loss: 2.5710 - bag_output_loss: 1.4839 - pose_output_loss: 1.1963 - footwear_output_loss: 1.2935 - emotion_output_loss: 2.7997 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3958 - weight_output_acc: 0.6521 - bag_output_acc: 0.6125 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5542 - emotion_output_acc: 0.7083 - val_loss: 24.0811 - val_gender_output_loss: 0.6983 - val_image_quality_output_loss: 0.9746 - val_age_output_loss: 1.4204 - val_weight_output_loss: 1.0023 - val_bag_output_loss: 0.9401 - val_pose_output_loss: 0.8429 - val_footwear_output_loss: 0.8264 - val_emotion_output_loss: 0.9596 - val_gender_output_acc: 0.6905 - val_image_quality_output_acc: 0.5354 - val_age_output_acc: 0.3681 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.5920 - val_pose_output_acc: 0.7318 - val_footwear_output_acc: 0.6412 - val_emotion_output_acc: 0.6609\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 25s 836ms/step - loss: 45.4827 - gender_output_loss: 0.5867 - image_quality_output_loss: 0.9438 - age_output_loss: 2.6005 - weight_output_loss: 2.8293 - bag_output_loss: 1.5749 - pose_output_loss: 1.2798 - footwear_output_loss: 1.1934 - emotion_output_loss: 2.8438 - gender_output_acc: 0.7167 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4521 - weight_output_acc: 0.6062 - bag_output_acc: 0.5917 - pose_output_acc: 0.7229 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.6938 - val_loss: 22.1989 - val_gender_output_loss: 0.5012 - val_image_quality_output_loss: 0.9180 - val_age_output_loss: 1.3696 - val_weight_output_loss: 1.0305 - val_bag_output_loss: 0.8546 - val_pose_output_loss: 0.5636 - val_footwear_output_loss: 0.8158 - val_emotion_output_loss: 0.9247 - val_gender_output_acc: 0.7643 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.5581 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.7712 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.6885\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 25s 838ms/step - loss: 42.2599 - gender_output_loss: 0.5660 - image_quality_output_loss: 0.9384 - age_output_loss: 2.4712 - weight_output_loss: 2.5793 - bag_output_loss: 1.3849 - pose_output_loss: 1.0841 - footwear_output_loss: 1.2891 - emotion_output_loss: 2.6096 - gender_output_acc: 0.7646 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4375 - weight_output_acc: 0.6125 - bag_output_acc: 0.6167 - pose_output_acc: 0.7688 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.6958 - val_loss: 22.2054 - val_gender_output_loss: 0.4794 - val_image_quality_output_loss: 0.9319 - val_age_output_loss: 1.3924 - val_weight_output_loss: 0.9822 - val_bag_output_loss: 0.8509 - val_pose_output_loss: 0.5784 - val_footwear_output_loss: 0.8749 - val_emotion_output_loss: 0.9060 - val_gender_output_acc: 0.7746 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6309 - val_pose_output_acc: 0.7741 - val_footwear_output_acc: 0.6038 - val_emotion_output_acc: 0.6900\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 25s 842ms/step - loss: 46.9386 - gender_output_loss: 0.5916 - image_quality_output_loss: 0.9045 - age_output_loss: 3.0710 - weight_output_loss: 3.0812 - bag_output_loss: 1.5759 - pose_output_loss: 1.1736 - footwear_output_loss: 1.1909 - emotion_output_loss: 2.6459 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5771 - age_output_acc: 0.3500 - weight_output_acc: 0.5979 - bag_output_acc: 0.6062 - pose_output_acc: 0.7354 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7021 - val_loss: 21.5747 - val_gender_output_loss: 0.4417 - val_image_quality_output_loss: 0.9380 - val_age_output_loss: 1.3628 - val_weight_output_loss: 0.9943 - val_bag_output_loss: 0.8365 - val_pose_output_loss: 0.5040 - val_footwear_output_loss: 0.7986 - val_emotion_output_loss: 0.8895 - val_gender_output_acc: 0.7889 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6398 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6526 - val_emotion_output_acc: 0.7062\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 25s 849ms/step - loss: 45.3740 - gender_output_loss: 0.5755 - image_quality_output_loss: 0.9616 - age_output_loss: 3.1059 - weight_output_loss: 2.6141 - bag_output_loss: 1.5874 - pose_output_loss: 1.1338 - footwear_output_loss: 1.1293 - emotion_output_loss: 2.6018 - gender_output_acc: 0.7312 - image_quality_output_acc: 0.5375 - age_output_acc: 0.4000 - weight_output_acc: 0.6417 - bag_output_acc: 0.5750 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7146 - val_loss: 21.9611 - val_gender_output_loss: 0.4748 - val_image_quality_output_loss: 0.9245 - val_age_output_loss: 1.3791 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8503 - val_pose_output_loss: 0.5257 - val_footwear_output_loss: 0.8596 - val_emotion_output_loss: 0.9080 - val_gender_output_acc: 0.7751 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.6142 - val_bag_output_acc: 0.6299 - val_pose_output_acc: 0.7958 - val_footwear_output_acc: 0.6220 - val_emotion_output_acc: 0.7008\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 25s 847ms/step - loss: 41.5634 - gender_output_loss: 0.5883 - image_quality_output_loss: 0.9259 - age_output_loss: 2.7696 - weight_output_loss: 2.7166 - bag_output_loss: 1.3563 - pose_output_loss: 1.3048 - footwear_output_loss: 1.1708 - emotion_output_loss: 1.9446 - gender_output_acc: 0.7375 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4042 - weight_output_acc: 0.6292 - bag_output_acc: 0.6250 - pose_output_acc: 0.7208 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7771 - val_loss: 21.6792 - val_gender_output_loss: 0.4541 - val_image_quality_output_loss: 0.9223 - val_age_output_loss: 1.3687 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.8386 - val_pose_output_loss: 0.5140 - val_footwear_output_loss: 0.8100 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.7859 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6378 - val_pose_output_acc: 0.7963 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.7067\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 25s 848ms/step - loss: 46.0394 - gender_output_loss: 0.6083 - image_quality_output_loss: 0.9571 - age_output_loss: 3.0299 - weight_output_loss: 2.4604 - bag_output_loss: 1.4291 - pose_output_loss: 1.3069 - footwear_output_loss: 1.3674 - emotion_output_loss: 2.8155 - gender_output_acc: 0.7104 - image_quality_output_acc: 0.5229 - age_output_acc: 0.3479 - weight_output_acc: 0.6542 - bag_output_acc: 0.6062 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5229 - emotion_output_acc: 0.7021 - val_loss: 21.9518 - val_gender_output_loss: 0.4702 - val_image_quality_output_loss: 0.9108 - val_age_output_loss: 1.3681 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.8498 - val_pose_output_loss: 0.5441 - val_footwear_output_loss: 0.8936 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.7717 - val_image_quality_output_acc: 0.5561 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7682 - val_footwear_output_acc: 0.6068 - val_emotion_output_acc: 0.6983\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 25s 842ms/step - loss: 45.9964 - gender_output_loss: 0.6011 - image_quality_output_loss: 0.9572 - age_output_loss: 2.7925 - weight_output_loss: 2.9906 - bag_output_loss: 1.4682 - pose_output_loss: 1.3404 - footwear_output_loss: 1.2242 - emotion_output_loss: 2.6654 - gender_output_acc: 0.7063 - image_quality_output_acc: 0.5292 - age_output_acc: 0.3812 - weight_output_acc: 0.6021 - bag_output_acc: 0.6062 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7167 - val_loss: 22.7136 - val_gender_output_loss: 0.5042 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.4197 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.8733 - val_pose_output_loss: 0.6051 - val_footwear_output_loss: 0.9063 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.7736 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3922 - val_weight_output_acc: 0.6097 - val_bag_output_acc: 0.6206 - val_pose_output_acc: 0.7613 - val_footwear_output_acc: 0.6024 - val_emotion_output_acc: 0.6964\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577560963_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577560966_backup\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 31s 1s/step - loss: 49.6529 - gender_output_loss: 0.7112 - image_quality_output_loss: 0.9455 - age_output_loss: 2.8625 - weight_output_loss: 2.8561 - bag_output_loss: 1.4186 - pose_output_loss: 1.5531 - footwear_output_loss: 1.3323 - emotion_output_loss: 3.3825 - gender_output_acc: 0.6271 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3958 - weight_output_acc: 0.6333 - bag_output_acc: 0.5708 - pose_output_acc: 0.6250 - footwear_output_acc: 0.4833 - emotion_output_acc: 0.6625 - val_loss: 21.7786 - val_gender_output_loss: 0.4326 - val_image_quality_output_loss: 0.9613 - val_age_output_loss: 1.3605 - val_weight_output_loss: 1.0010 - val_bag_output_loss: 0.8502 - val_pose_output_loss: 0.5123 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.9041 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.5891 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.8110 - val_footwear_output_acc: 0.6531 - val_emotion_output_acc: 0.6944\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 46.7686 - gender_output_loss: 0.7059 - image_quality_output_loss: 0.9569 - age_output_loss: 3.0051 - weight_output_loss: 2.6936 - bag_output_loss: 1.6892 - pose_output_loss: 1.4027 - footwear_output_loss: 1.3593 - emotion_output_loss: 2.5342 - gender_output_acc: 0.6479 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3875 - weight_output_acc: 0.6438 - bag_output_acc: 0.5583 - pose_output_acc: 0.6625 - footwear_output_acc: 0.4938 - emotion_output_acc: 0.7312 - val_loss: 22.2568 - val_gender_output_loss: 0.4789 - val_image_quality_output_loss: 0.9303 - val_age_output_loss: 1.3684 - val_weight_output_loss: 1.0318 - val_bag_output_loss: 0.8454 - val_pose_output_loss: 0.5889 - val_footwear_output_loss: 0.8664 - val_emotion_output_loss: 0.9060 - val_gender_output_acc: 0.7825 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.5659 - val_bag_output_acc: 0.6393 - val_pose_output_acc: 0.7972 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.6929\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 3/50\n",
            "30/30 [==============================] - 25s 848ms/step - loss: 46.7567 - gender_output_loss: 0.6532 - image_quality_output_loss: 0.9249 - age_output_loss: 2.8291 - weight_output_loss: 2.5588 - bag_output_loss: 1.3281 - pose_output_loss: 1.5972 - footwear_output_loss: 1.3435 - emotion_output_loss: 2.9838 - gender_output_acc: 0.6833 - image_quality_output_acc: 0.6021 - age_output_acc: 0.4417 - weight_output_acc: 0.6292 - bag_output_acc: 0.6333 - pose_output_acc: 0.6583 - footwear_output_acc: 0.5458 - emotion_output_acc: 0.7000 - val_loss: 21.7459 - val_gender_output_loss: 0.4445 - val_image_quality_output_loss: 0.9166 - val_age_output_loss: 1.3765 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8271 - val_pose_output_loss: 0.5345 - val_footwear_output_loss: 0.8358 - val_emotion_output_loss: 0.8892 - val_gender_output_acc: 0.7889 - val_image_quality_output_acc: 0.5566 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6373 - val_pose_output_acc: 0.7918 - val_footwear_output_acc: 0.6309 - val_emotion_output_acc: 0.7062\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 4/50\n",
            "30/30 [==============================] - 25s 836ms/step - loss: 46.2327 - gender_output_loss: 0.7279 - image_quality_output_loss: 0.9676 - age_output_loss: 2.6840 - weight_output_loss: 2.5930 - bag_output_loss: 1.7141 - pose_output_loss: 1.4527 - footwear_output_loss: 1.3073 - emotion_output_loss: 2.7509 - gender_output_acc: 0.6333 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3979 - weight_output_acc: 0.6458 - bag_output_acc: 0.5729 - pose_output_acc: 0.6625 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7083 - val_loss: 21.8978 - val_gender_output_loss: 0.4359 - val_image_quality_output_loss: 0.9196 - val_age_output_loss: 1.3770 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8488 - val_pose_output_loss: 0.5596 - val_footwear_output_loss: 0.8670 - val_emotion_output_loss: 0.8955 - val_gender_output_acc: 0.8120 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7712 - val_footwear_output_acc: 0.6216 - val_emotion_output_acc: 0.7062\n",
            "Epoch 5/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.9326 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9411 - age_output_loss: 2.9156 - weight_output_loss: 2.4174 - bag_output_loss: 1.6121 - pose_output_loss: 1.5698 - footwear_output_loss: 1.3735 - emotion_output_loss: 2.5686 - gender_output_acc: 0.6509 - image_quality_output_acc: 0.5884 - age_output_acc: 0.3642 - weight_output_acc: 0.6207 - bag_output_acc: 0.5862 - pose_output_acc: 0.6487 - footwear_output_acc: 0.5086 - emotion_output_acc: 0.7241\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "30/30 [==============================] - 25s 850ms/step - loss: 45.6029 - gender_output_loss: 0.6795 - image_quality_output_loss: 0.9464 - age_output_loss: 2.9063 - weight_output_loss: 2.3840 - bag_output_loss: 1.6056 - pose_output_loss: 1.5598 - footwear_output_loss: 1.3609 - emotion_output_loss: 2.5374 - gender_output_acc: 0.6521 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3604 - weight_output_acc: 0.6271 - bag_output_acc: 0.5917 - pose_output_acc: 0.6479 - footwear_output_acc: 0.5125 - emotion_output_acc: 0.7271 - val_loss: 27.7842 - val_gender_output_loss: 0.6847 - val_image_quality_output_loss: 1.1096 - val_age_output_loss: 1.4624 - val_weight_output_loss: 1.0807 - val_bag_output_loss: 0.9843 - val_pose_output_loss: 1.6022 - val_footwear_output_loss: 0.9275 - val_emotion_output_loss: 1.0709 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.5630 - val_bag_output_acc: 0.6088 - val_pose_output_acc: 0.7082 - val_footwear_output_acc: 0.6053 - val_emotion_output_acc: 0.6811\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 25s 842ms/step - loss: 47.0267 - gender_output_loss: 0.7212 - image_quality_output_loss: 0.9689 - age_output_loss: 2.8955 - weight_output_loss: 2.4429 - bag_output_loss: 1.4440 - pose_output_loss: 1.7247 - footwear_output_loss: 1.3270 - emotion_output_loss: 2.8426 - gender_output_acc: 0.6167 - image_quality_output_acc: 0.5500 - age_output_acc: 0.4021 - weight_output_acc: 0.6438 - bag_output_acc: 0.5896 - pose_output_acc: 0.6042 - footwear_output_acc: 0.5396 - emotion_output_acc: 0.7063 - val_loss: 22.4153 - val_gender_output_loss: 0.4573 - val_image_quality_output_loss: 0.9502 - val_age_output_loss: 1.3727 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.8461 - val_pose_output_loss: 0.6782 - val_footwear_output_loss: 0.8700 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.7795 - val_image_quality_output_acc: 0.5354 - val_age_output_acc: 0.4050 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6373 - val_pose_output_acc: 0.7215 - val_footwear_output_acc: 0.6048 - val_emotion_output_acc: 0.7067\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 26s 852ms/step - loss: 50.2173 - gender_output_loss: 0.6659 - image_quality_output_loss: 0.9793 - age_output_loss: 3.0430 - weight_output_loss: 2.7928 - bag_output_loss: 1.6846 - pose_output_loss: 1.3092 - footwear_output_loss: 1.3238 - emotion_output_loss: 3.3859 - gender_output_acc: 0.6750 - image_quality_output_acc: 0.5333 - age_output_acc: 0.4042 - weight_output_acc: 0.6438 - bag_output_acc: 0.6083 - pose_output_acc: 0.7021 - footwear_output_acc: 0.5167 - emotion_output_acc: 0.6604 - val_loss: 21.8709 - val_gender_output_loss: 0.4389 - val_image_quality_output_loss: 0.9552 - val_age_output_loss: 1.3603 - val_weight_output_loss: 0.9965 - val_bag_output_loss: 0.8370 - val_pose_output_loss: 0.5326 - val_footwear_output_loss: 0.8281 - val_emotion_output_loss: 0.9208 - val_gender_output_acc: 0.8007 - val_image_quality_output_acc: 0.5354 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.5974 - val_bag_output_acc: 0.6462 - val_pose_output_acc: 0.7928 - val_footwear_output_acc: 0.6383 - val_emotion_output_acc: 0.6880\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 25s 848ms/step - loss: 45.9969 - gender_output_loss: 0.6912 - image_quality_output_loss: 0.9484 - age_output_loss: 2.8407 - weight_output_loss: 2.4603 - bag_output_loss: 1.6876 - pose_output_loss: 1.6328 - footwear_output_loss: 1.2571 - emotion_output_loss: 2.5737 - gender_output_acc: 0.6667 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3979 - weight_output_acc: 0.6583 - bag_output_acc: 0.5208 - pose_output_acc: 0.6417 - footwear_output_acc: 0.5521 - emotion_output_acc: 0.7250 - val_loss: 21.8181 - val_gender_output_loss: 0.4383 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.3586 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.8435 - val_pose_output_loss: 0.5348 - val_footwear_output_loss: 0.8357 - val_emotion_output_loss: 0.9031 - val_gender_output_acc: 0.8022 - val_image_quality_output_acc: 0.5369 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.5940 - val_bag_output_acc: 0.6432 - val_pose_output_acc: 0.7923 - val_footwear_output_acc: 0.6348 - val_emotion_output_acc: 0.6973\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 25s 847ms/step - loss: 47.5309 - gender_output_loss: 0.7163 - image_quality_output_loss: 0.9662 - age_output_loss: 2.8481 - weight_output_loss: 2.4457 - bag_output_loss: 1.6625 - pose_output_loss: 1.5571 - footwear_output_loss: 1.3288 - emotion_output_loss: 2.9792 - gender_output_acc: 0.6229 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3792 - weight_output_acc: 0.6396 - bag_output_acc: 0.5562 - pose_output_acc: 0.6500 - footwear_output_acc: 0.5083 - emotion_output_acc: 0.6917 - val_loss: 21.7031 - val_gender_output_loss: 0.4452 - val_image_quality_output_loss: 0.9234 - val_age_output_loss: 1.3519 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8386 - val_pose_output_loss: 0.5349 - val_footwear_output_loss: 0.8536 - val_emotion_output_loss: 0.8936 - val_gender_output_acc: 0.7958 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6348 - val_pose_output_acc: 0.7918 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.7052\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 26s 852ms/step - loss: 44.2568 - gender_output_loss: 0.6663 - image_quality_output_loss: 0.9279 - age_output_loss: 2.8976 - weight_output_loss: 2.3878 - bag_output_loss: 1.5654 - pose_output_loss: 1.2685 - footwear_output_loss: 1.2810 - emotion_output_loss: 2.5121 - gender_output_acc: 0.6417 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4125 - weight_output_acc: 0.6646 - bag_output_acc: 0.5958 - pose_output_acc: 0.7125 - footwear_output_acc: 0.5646 - emotion_output_acc: 0.7333 - val_loss: 22.1831 - val_gender_output_loss: 0.4959 - val_image_quality_output_loss: 0.9290 - val_age_output_loss: 1.3772 - val_weight_output_loss: 1.0042 - val_bag_output_loss: 0.8574 - val_pose_output_loss: 0.5340 - val_footwear_output_loss: 0.8643 - val_emotion_output_loss: 0.9267 - val_gender_output_acc: 0.7751 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.5891 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7918 - val_footwear_output_acc: 0.6206 - val_emotion_output_acc: 0.6737\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 25s 841ms/step - loss: 46.3629 - gender_output_loss: 0.7100 - image_quality_output_loss: 0.9709 - age_output_loss: 2.6226 - weight_output_loss: 2.4655 - bag_output_loss: 1.8076 - pose_output_loss: 1.4367 - footwear_output_loss: 1.3060 - emotion_output_loss: 2.8921 - gender_output_acc: 0.6313 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3917 - weight_output_acc: 0.6542 - bag_output_acc: 0.5521 - pose_output_acc: 0.6812 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.7021 - val_loss: 22.5739 - val_gender_output_loss: 0.4445 - val_image_quality_output_loss: 0.9415 - val_age_output_loss: 1.3822 - val_weight_output_loss: 0.9950 - val_bag_output_loss: 0.8684 - val_pose_output_loss: 0.7152 - val_footwear_output_loss: 0.8314 - val_emotion_output_loss: 0.9185 - val_gender_output_acc: 0.8071 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6024 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7037 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.7062\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 43.1107 - gender_output_loss: 0.7183 - image_quality_output_loss: 0.9588 - age_output_loss: 2.9341 - weight_output_loss: 2.2183 - bag_output_loss: 1.5116 - pose_output_loss: 1.5877 - footwear_output_loss: 1.3194 - emotion_output_loss: 2.0573 - gender_output_acc: 0.6062 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3750 - weight_output_acc: 0.6750 - bag_output_acc: 0.5875 - pose_output_acc: 0.6375 - footwear_output_acc: 0.5333 - emotion_output_acc: 0.7604 - val_loss: 23.1548 - val_gender_output_loss: 0.7018 - val_image_quality_output_loss: 0.9209 - val_age_output_loss: 1.3714 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.9244 - val_pose_output_loss: 0.6663 - val_footwear_output_loss: 0.9048 - val_emotion_output_loss: 0.9130 - val_gender_output_acc: 0.6900 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.5728 - val_pose_output_acc: 0.7377 - val_footwear_output_acc: 0.5822 - val_emotion_output_acc: 0.6826\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 25s 845ms/step - loss: 48.7064 - gender_output_loss: 0.7158 - image_quality_output_loss: 0.9598 - age_output_loss: 2.8502 - weight_output_loss: 3.2715 - bag_output_loss: 1.5524 - pose_output_loss: 1.6609 - footwear_output_loss: 1.3593 - emotion_output_loss: 2.6461 - gender_output_acc: 0.6125 - image_quality_output_acc: 0.5458 - age_output_acc: 0.3917 - weight_output_acc: 0.5750 - bag_output_acc: 0.5562 - pose_output_acc: 0.6062 - footwear_output_acc: 0.5188 - emotion_output_acc: 0.7146 - val_loss: 24.7758 - val_gender_output_loss: 0.6575 - val_image_quality_output_loss: 0.9484 - val_age_output_loss: 1.4668 - val_weight_output_loss: 1.1049 - val_bag_output_loss: 0.8950 - val_pose_output_loss: 0.8349 - val_footwear_output_loss: 0.9539 - val_emotion_output_loss: 1.0219 - val_gender_output_acc: 0.7215 - val_image_quality_output_acc: 0.5394 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.5177 - val_bag_output_acc: 0.6201 - val_pose_output_acc: 0.7096 - val_footwear_output_acc: 0.5645 - val_emotion_output_acc: 0.5994\n",
            "Epoch 13/50Epoch 14/50\n",
            "30/30 [==============================] - 25s 847ms/step - loss: 50.1321 - gender_output_loss: 0.6922 - image_quality_output_loss: 0.9375 - age_output_loss: 3.0979 - weight_output_loss: 3.0981 - bag_output_loss: 1.5105 - pose_output_loss: 1.5812 - footwear_output_loss: 1.3001 - emotion_output_loss: 3.0288 - gender_output_acc: 0.6417 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3771 - weight_output_acc: 0.5938 - bag_output_acc: 0.5875 - pose_output_acc: 0.6458 - footwear_output_acc: 0.5437 - emotion_output_acc: 0.6875 - val_loss: 22.6489 - val_gender_output_loss: 0.4853 - val_image_quality_output_loss: 0.9256 - val_age_output_loss: 1.3731 - val_weight_output_loss: 1.0294 - val_bag_output_loss: 0.8564 - val_pose_output_loss: 0.6309 - val_footwear_output_loss: 0.8766 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.7849 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.5886 - val_bag_output_acc: 0.6383 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.6093 - val_emotion_output_acc: 0.6590\n",
            "Epoch 14/50\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 26s 858ms/step - loss: 45.7409 - gender_output_loss: 0.6953 - image_quality_output_loss: 0.9815 - age_output_loss: 2.6897 - weight_output_loss: 2.9252 - bag_output_loss: 1.5914 - pose_output_loss: 1.4515 - footwear_output_loss: 1.2539 - emotion_output_loss: 2.5056 - gender_output_acc: 0.6271 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3875 - weight_output_acc: 0.6042 - bag_output_acc: 0.5479 - pose_output_acc: 0.6417 - footwear_output_acc: 0.5583 - emotion_output_acc: 0.7271 - val_loss: 22.0274 - val_gender_output_loss: 0.4852 - val_image_quality_output_loss: 0.9336 - val_age_output_loss: 1.3676 - val_weight_output_loss: 1.0092 - val_bag_output_loss: 0.8501 - val_pose_output_loss: 0.5370 - val_footwear_output_loss: 0.8369 - val_emotion_output_loss: 0.9156 - val_gender_output_acc: 0.7840 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.5945 - val_bag_output_acc: 0.6358 - val_pose_output_acc: 0.7844 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6831\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 25s 847ms/step - loss: 44.8552 - gender_output_loss: 0.7033 - image_quality_output_loss: 0.9531 - age_output_loss: 2.8507 - weight_output_loss: 2.4417 - bag_output_loss: 1.4426 - pose_output_loss: 1.6087 - footwear_output_loss: 1.3258 - emotion_output_loss: 2.4539 - gender_output_acc: 0.6750 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3771 - weight_output_acc: 0.6625 - bag_output_acc: 0.5854 - pose_output_acc: 0.6167 - footwear_output_acc: 0.5271 - emotion_output_acc: 0.7271 - val_loss: 21.9388 - val_gender_output_loss: 0.4537 - val_image_quality_output_loss: 0.9406 - val_age_output_loss: 1.3698 - val_weight_output_loss: 1.0082 - val_bag_output_loss: 0.8475 - val_pose_output_loss: 0.5400 - val_footwear_output_loss: 0.8352 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.7972 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6014 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.7874 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.6924\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 26s 859ms/step - loss: 46.4292 - gender_output_loss: 0.6647 - image_quality_output_loss: 0.9588 - age_output_loss: 2.8771 - weight_output_loss: 2.9140 - bag_output_loss: 1.4703 - pose_output_loss: 1.4379 - footwear_output_loss: 1.2921 - emotion_output_loss: 2.6074 - gender_output_acc: 0.6667 - image_quality_output_acc: 0.5479 - age_output_acc: 0.4083 - weight_output_acc: 0.6000 - bag_output_acc: 0.6417 - pose_output_acc: 0.6979 - footwear_output_acc: 0.5104 - emotion_output_acc: 0.7292 - val_loss: 22.4607 - val_gender_output_loss: 0.4858 - val_image_quality_output_loss: 0.9482 - val_age_output_loss: 1.3914 - val_weight_output_loss: 1.0270 - val_bag_output_loss: 0.8605 - val_pose_output_loss: 0.6299 - val_footwear_output_loss: 0.8323 - val_emotion_output_loss: 0.9042 - val_gender_output_acc: 0.7766 - val_image_quality_output_acc: 0.5349 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.5876 - val_bag_output_acc: 0.6235 - val_pose_output_acc: 0.7387 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6998\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 25s 848ms/step - loss: 45.7458 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9336 - age_output_loss: 2.8363 - weight_output_loss: 2.4988 - bag_output_loss: 1.5106 - pose_output_loss: 1.5251 - footwear_output_loss: 1.3125 - emotion_output_loss: 2.6873 - gender_output_acc: 0.6458 - image_quality_output_acc: 0.5875 - age_output_acc: 0.4062 - weight_output_acc: 0.6187 - bag_output_acc: 0.5938 - pose_output_acc: 0.6646 - footwear_output_acc: 0.5250 - emotion_output_acc: 0.7104 - val_loss: 22.3134 - val_gender_output_loss: 0.4622 - val_image_quality_output_loss: 0.9220 - val_age_output_loss: 1.3803 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.8490 - val_pose_output_loss: 0.6482 - val_footwear_output_loss: 0.8626 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6078 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.7495 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.7028\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 26s 856ms/step - loss: 45.6147 - gender_output_loss: 0.6919 - image_quality_output_loss: 0.9560 - age_output_loss: 2.8327 - weight_output_loss: 2.3742 - bag_output_loss: 1.6600 - pose_output_loss: 1.4572 - footwear_output_loss: 1.2596 - emotion_output_loss: 2.7008 - gender_output_acc: 0.6667 - image_quality_output_acc: 0.5437 - age_output_acc: 0.4417 - weight_output_acc: 0.6687 - bag_output_acc: 0.5750 - pose_output_acc: 0.6729 - footwear_output_acc: 0.5583 - emotion_output_acc: 0.7125 - val_loss: 23.4975 - val_gender_output_loss: 0.5120 - val_image_quality_output_loss: 0.9608 - val_age_output_loss: 1.4133 - val_weight_output_loss: 0.9958 - val_bag_output_loss: 0.8746 - val_pose_output_loss: 0.8834 - val_footwear_output_loss: 0.8393 - val_emotion_output_loss: 0.9420 - val_gender_output_acc: 0.7741 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6289 - val_pose_output_acc: 0.6462 - val_footwear_output_acc: 0.6265 - val_emotion_output_acc: 0.7028\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577561463_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577561466_backup\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 32s 1s/step - loss: 45.0069 - gender_output_loss: 0.5238 - image_quality_output_loss: 0.9309 - age_output_loss: 2.9842 - weight_output_loss: 2.6719 - bag_output_loss: 1.5194 - pose_output_loss: 1.2010 - footwear_output_loss: 1.2212 - emotion_output_loss: 2.5826 - gender_output_acc: 0.7896 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3917 - weight_output_acc: 0.6458 - bag_output_acc: 0.6187 - pose_output_acc: 0.7500 - footwear_output_acc: 0.5708 - emotion_output_acc: 0.7042 - val_loss: 22.8442 - val_gender_output_loss: 0.4618 - val_image_quality_output_loss: 1.0793 - val_age_output_loss: 1.4181 - val_weight_output_loss: 1.0630 - val_bag_output_loss: 0.8607 - val_pose_output_loss: 0.5950 - val_footwear_output_loss: 0.8187 - val_emotion_output_loss: 0.9212 - val_gender_output_acc: 0.7972 - val_image_quality_output_acc: 0.5128 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.5379 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7820 - val_footwear_output_acc: 0.6476 - val_emotion_output_acc: 0.6841\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 25s 822ms/step - loss: 42.3333 - gender_output_loss: 0.5755 - image_quality_output_loss: 0.8977 - age_output_loss: 2.5196 - weight_output_loss: 2.5976 - bag_output_loss: 1.4281 - pose_output_loss: 1.3259 - footwear_output_loss: 1.1991 - emotion_output_loss: 2.4112 - gender_output_acc: 0.7458 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4208 - weight_output_acc: 0.6229 - bag_output_acc: 0.6229 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7292 - val_loss: 22.3241 - val_gender_output_loss: 0.4554 - val_image_quality_output_loss: 1.0170 - val_age_output_loss: 1.3861 - val_weight_output_loss: 1.0112 - val_bag_output_loss: 0.8640 - val_pose_output_loss: 0.5669 - val_footwear_output_loss: 0.8120 - val_emotion_output_loss: 0.9186 - val_gender_output_acc: 0.8012 - val_image_quality_output_acc: 0.5374 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.5694 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7815 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.6983\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 26s 869ms/step - loss: 45.1515 - gender_output_loss: 0.6174 - image_quality_output_loss: 0.9089 - age_output_loss: 2.8105 - weight_output_loss: 2.8424 - bag_output_loss: 1.4018 - pose_output_loss: 1.2290 - footwear_output_loss: 1.1451 - emotion_output_loss: 2.7342 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5896 - age_output_acc: 0.3917 - weight_output_acc: 0.6375 - bag_output_acc: 0.6458 - pose_output_acc: 0.7292 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7063 - val_loss: 21.9824 - val_gender_output_loss: 0.4268 - val_image_quality_output_loss: 0.9178 - val_age_output_loss: 1.3715 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.8374 - val_pose_output_loss: 0.6094 - val_footwear_output_loss: 0.8416 - val_emotion_output_loss: 0.9018 - val_gender_output_acc: 0.8086 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7662 - val_footwear_output_acc: 0.6422 - val_emotion_output_acc: 0.6983\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 26s 860ms/step - loss: 44.6657 - gender_output_loss: 0.5565 - image_quality_output_loss: 0.9710 - age_output_loss: 2.6649 - weight_output_loss: 2.6764 - bag_output_loss: 1.5351 - pose_output_loss: 1.2667 - footwear_output_loss: 1.1885 - emotion_output_loss: 2.7322 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5417 - age_output_acc: 0.4333 - weight_output_acc: 0.6375 - bag_output_acc: 0.6146 - pose_output_acc: 0.7542 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7000 - val_loss: 24.1863 - val_gender_output_loss: 0.8896 - val_image_quality_output_loss: 0.9463 - val_age_output_loss: 1.4539 - val_weight_output_loss: 1.0216 - val_bag_output_loss: 0.9479 - val_pose_output_loss: 0.6165 - val_footwear_output_loss: 0.9258 - val_emotion_output_loss: 0.9695 - val_gender_output_acc: 0.6757 - val_image_quality_output_acc: 0.5522 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.6043 - val_bag_output_acc: 0.6083 - val_pose_output_acc: 0.7869 - val_footwear_output_acc: 0.6196 - val_emotion_output_acc: 0.6855\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 26s 850ms/step - loss: 43.2274 - gender_output_loss: 0.5821 - image_quality_output_loss: 0.9550 - age_output_loss: 2.8340 - weight_output_loss: 2.7334 - bag_output_loss: 1.2707 - pose_output_loss: 1.0657 - footwear_output_loss: 1.2215 - emotion_output_loss: 2.4883 - gender_output_acc: 0.7312 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3854 - weight_output_acc: 0.6375 - bag_output_acc: 0.6333 - pose_output_acc: 0.7583 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.7188 - val_loss: 21.9886 - val_gender_output_loss: 0.4445 - val_image_quality_output_loss: 0.9171 - val_age_output_loss: 1.3815 - val_weight_output_loss: 0.9916 - val_bag_output_loss: 0.8428 - val_pose_output_loss: 0.5752 - val_footwear_output_loss: 0.8320 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.7992 - val_image_quality_output_acc: 0.5586 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.7672 - val_footwear_output_acc: 0.6294 - val_emotion_output_acc: 0.6949\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 26s 860ms/step - loss: 43.2922 - gender_output_loss: 0.5773 - image_quality_output_loss: 0.9076 - age_output_loss: 2.8557 - weight_output_loss: 2.5584 - bag_output_loss: 1.3979 - pose_output_loss: 1.1514 - footwear_output_loss: 1.1933 - emotion_output_loss: 2.4946 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3771 - weight_output_acc: 0.6354 - bag_output_acc: 0.6125 - pose_output_acc: 0.7479 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7188 - val_loss: 22.6992 - val_gender_output_loss: 0.4615 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.3899 - val_weight_output_loss: 1.0083 - val_bag_output_loss: 0.8665 - val_pose_output_loss: 0.6817 - val_footwear_output_loss: 0.8441 - val_emotion_output_loss: 0.9202 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.4055 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.7416 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6998\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 26s 863ms/step - loss: 43.6607 - gender_output_loss: 0.5596 - image_quality_output_loss: 0.9869 - age_output_loss: 2.6641 - weight_output_loss: 2.5026 - bag_output_loss: 1.7391 - pose_output_loss: 1.0108 - footwear_output_loss: 1.1463 - emotion_output_loss: 2.6626 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4187 - weight_output_acc: 0.6208 - bag_output_acc: 0.5625 - pose_output_acc: 0.7750 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7125 - val_loss: 21.7786 - val_gender_output_loss: 0.4381 - val_image_quality_output_loss: 0.9184 - val_age_output_loss: 1.3706 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8484 - val_pose_output_loss: 0.5433 - val_footwear_output_loss: 0.8157 - val_emotion_output_loss: 0.8984 - val_gender_output_acc: 0.7943 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6255 - val_pose_output_acc: 0.7908 - val_footwear_output_acc: 0.6412 - val_emotion_output_acc: 0.6998\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 26s 855ms/step - loss: 42.1896 - gender_output_loss: 0.5526 - image_quality_output_loss: 0.9318 - age_output_loss: 2.8058 - weight_output_loss: 2.3791 - bag_output_loss: 1.3930 - pose_output_loss: 1.1090 - footwear_output_loss: 1.0554 - emotion_output_loss: 2.5085 - gender_output_acc: 0.7771 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4208 - weight_output_acc: 0.6812 - bag_output_acc: 0.6229 - pose_output_acc: 0.7667 - footwear_output_acc: 0.6646 - emotion_output_acc: 0.7167 - val_loss: 22.4600 - val_gender_output_loss: 0.4629 - val_image_quality_output_loss: 0.9220 - val_age_output_loss: 1.3769 - val_weight_output_loss: 1.0006 - val_bag_output_loss: 0.8602 - val_pose_output_loss: 0.6104 - val_footwear_output_loss: 0.9257 - val_emotion_output_loss: 0.9270 - val_gender_output_acc: 0.7968 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7638 - val_footwear_output_acc: 0.6048 - val_emotion_output_acc: 0.7032\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 26s 857ms/step - loss: 40.6425 - gender_output_loss: 0.5402 - image_quality_output_loss: 0.9309 - age_output_loss: 2.6095 - weight_output_loss: 2.3252 - bag_output_loss: 1.4827 - pose_output_loss: 1.2253 - footwear_output_loss: 1.1914 - emotion_output_loss: 2.1426 - gender_output_acc: 0.7896 - image_quality_output_acc: 0.5750 - age_output_acc: 0.3729 - weight_output_acc: 0.6542 - bag_output_acc: 0.6292 - pose_output_acc: 0.7125 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7479 - val_loss: 22.4526 - val_gender_output_loss: 0.4457 - val_image_quality_output_loss: 0.9227 - val_age_output_loss: 1.4286 - val_weight_output_loss: 1.0139 - val_bag_output_loss: 0.8583 - val_pose_output_loss: 0.5634 - val_footwear_output_loss: 0.9037 - val_emotion_output_loss: 0.9195 - val_gender_output_acc: 0.7997 - val_image_quality_output_acc: 0.5571 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6344 - val_pose_output_acc: 0.7854 - val_footwear_output_acc: 0.6058 - val_emotion_output_acc: 0.7028\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 26s 861ms/step - loss: 45.0096 - gender_output_loss: 0.5881 - image_quality_output_loss: 0.9450 - age_output_loss: 2.9537 - weight_output_loss: 2.5443 - bag_output_loss: 1.5367 - pose_output_loss: 1.0617 - footwear_output_loss: 1.1860 - emotion_output_loss: 2.7799 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5437 - age_output_acc: 0.3750 - weight_output_acc: 0.6417 - bag_output_acc: 0.5896 - pose_output_acc: 0.7563 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.7042 - val_loss: 22.6222 - val_gender_output_loss: 0.4930 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4215 - val_weight_output_loss: 1.0197 - val_bag_output_loss: 0.9037 - val_pose_output_loss: 0.5810 - val_footwear_output_loss: 0.8272 - val_emotion_output_loss: 0.9074 - val_gender_output_acc: 0.7741 - val_image_quality_output_acc: 0.5364 - val_age_output_acc: 0.3642 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6127 - val_pose_output_acc: 0.7761 - val_footwear_output_acc: 0.6442 - val_emotion_output_acc: 0.7042\n",
            "Epoch 10/50\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 26s 861ms/step - loss: 41.7975 - gender_output_loss: 0.5200 - image_quality_output_loss: 0.9387 - age_output_loss: 2.5225 - weight_output_loss: 2.5268 - bag_output_loss: 1.3764 - pose_output_loss: 1.2074 - footwear_output_loss: 1.2226 - emotion_output_loss: 2.4512 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4021 - weight_output_acc: 0.6521 - bag_output_acc: 0.6313 - pose_output_acc: 0.7417 - footwear_output_acc: 0.5688 - emotion_output_acc: 0.7312 - val_loss: 21.6786 - val_gender_output_loss: 0.4311 - val_image_quality_output_loss: 0.9333 - val_age_output_loss: 1.3660 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8394 - val_pose_output_loss: 0.5337 - val_footwear_output_loss: 0.8184 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.7982 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7790 - val_footwear_output_acc: 0.6442 - val_emotion_output_acc: 0.7072\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 26s 857ms/step - loss: 44.5579 - gender_output_loss: 0.5446 - image_quality_output_loss: 0.9416 - age_output_loss: 2.6760 - weight_output_loss: 2.5923 - bag_output_loss: 1.5885 - pose_output_loss: 1.1625 - footwear_output_loss: 1.2365 - emotion_output_loss: 2.7927 - gender_output_acc: 0.7896 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4042 - weight_output_acc: 0.6125 - bag_output_acc: 0.5854 - pose_output_acc: 0.7479 - footwear_output_acc: 0.5771 - emotion_output_acc: 0.7083 - val_loss: 21.9064 - val_gender_output_loss: 0.4649 - val_image_quality_output_loss: 0.9364 - val_age_output_loss: 1.3588 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8637 - val_pose_output_loss: 0.5595 - val_footwear_output_loss: 0.8280 - val_emotion_output_loss: 0.8967 - val_gender_output_acc: 0.7741 - val_image_quality_output_acc: 0.5591 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6112 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.7682 - val_footwear_output_acc: 0.6348 - val_emotion_output_acc: 0.7057\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 25s 850ms/step - loss: 42.6010 - gender_output_loss: 0.5486 - image_quality_output_loss: 0.9807 - age_output_loss: 2.4968 - weight_output_loss: 2.4346 - bag_output_loss: 1.5068 - pose_output_loss: 1.3078 - footwear_output_loss: 1.1719 - emotion_output_loss: 2.5640 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.5271 - age_output_acc: 0.4229 - weight_output_acc: 0.6083 - bag_output_acc: 0.6104 - pose_output_acc: 0.7437 - footwear_output_acc: 0.5854 - emotion_output_acc: 0.7229 - val_loss: 23.8619 - val_gender_output_loss: 0.5508 - val_image_quality_output_loss: 0.9356 - val_age_output_loss: 1.4077 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.8736 - val_pose_output_loss: 1.0508 - val_footwear_output_loss: 0.8519 - val_emotion_output_loss: 0.9005 - val_gender_output_acc: 0.7343 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3883 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.5969 - val_pose_output_acc: 0.5344 - val_footwear_output_acc: 0.6156 - val_emotion_output_acc: 0.7087\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 26s 857ms/step - loss: 44.6699 - gender_output_loss: 0.5767 - image_quality_output_loss: 0.9388 - age_output_loss: 2.9072 - weight_output_loss: 2.6290 - bag_output_loss: 1.4262 - pose_output_loss: 1.1228 - footwear_output_loss: 1.1588 - emotion_output_loss: 2.7376 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3896 - weight_output_acc: 0.6208 - bag_output_acc: 0.6292 - pose_output_acc: 0.7563 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.7146 - val_loss: 22.7668 - val_gender_output_loss: 0.5025 - val_image_quality_output_loss: 0.9651 - val_age_output_loss: 1.4068 - val_weight_output_loss: 1.0006 - val_bag_output_loss: 0.8824 - val_pose_output_loss: 0.6629 - val_footwear_output_loss: 0.8420 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.7662 - val_image_quality_output_acc: 0.5192 - val_age_output_acc: 0.3912 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7333 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.7018\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 26s 856ms/step - loss: 44.4129 - gender_output_loss: 0.5502 - image_quality_output_loss: 0.9525 - age_output_loss: 2.7334 - weight_output_loss: 2.5356 - bag_output_loss: 1.4079 - pose_output_loss: 1.1396 - footwear_output_loss: 1.1366 - emotion_output_loss: 2.9359 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4354 - weight_output_acc: 0.6375 - bag_output_acc: 0.6125 - pose_output_acc: 0.7542 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.6854 - val_loss: 22.0186 - val_gender_output_loss: 0.4501 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.3828 - val_weight_output_loss: 0.9925 - val_bag_output_loss: 0.8485 - val_pose_output_loss: 0.5324 - val_footwear_output_loss: 0.8143 - val_emotion_output_loss: 0.9167 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5221 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.6112 - val_bag_output_acc: 0.6403 - val_pose_output_acc: 0.7908 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.6900\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 16/50\n",
            "30/30 [==============================] - 26s 859ms/step - loss: 44.8838 - gender_output_loss: 0.5450 - image_quality_output_loss: 0.9433 - age_output_loss: 2.9079 - weight_output_loss: 2.5474 - bag_output_loss: 1.5624 - pose_output_loss: 1.2124 - footwear_output_loss: 1.1223 - emotion_output_loss: 2.7144 - gender_output_acc: 0.7896 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3979 - weight_output_acc: 0.6542 - bag_output_acc: 0.6271 - pose_output_acc: 0.7375 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.6958 - val_loss: 21.7880 - val_gender_output_loss: 0.4472 - val_image_quality_output_loss: 0.9431 - val_age_output_loss: 1.3750 - val_weight_output_loss: 0.9933 - val_bag_output_loss: 0.8434 - val_pose_output_loss: 0.5180 - val_footwear_output_loss: 0.8270 - val_emotion_output_loss: 0.8957 - val_gender_output_acc: 0.7879 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.7948 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.7028\n",
            "Epoch 17/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 44.2459 - gender_output_loss: 0.5135 - image_quality_output_loss: 0.8870 - age_output_loss: 2.9734 - weight_output_loss: 2.8418 - bag_output_loss: 1.4545 - pose_output_loss: 1.0462 - footwear_output_loss: 1.1505 - emotion_output_loss: 2.5040 - gender_output_acc: 0.7909 - image_quality_output_acc: 0.5991 - age_output_acc: 0.3922 - weight_output_acc: 0.6358 - bag_output_acc: 0.6401 - pose_output_acc: 0.7759 - footwear_output_acc: 0.6013 - emotion_output_acc: 0.7091Epoch 17/50\n",
            "30/30 [==============================] - 26s 857ms/step - loss: 44.2861 - gender_output_loss: 0.5123 - image_quality_output_loss: 0.8894 - age_output_loss: 3.0138 - weight_output_loss: 2.7806 - bag_output_loss: 1.4444 - pose_output_loss: 1.0547 - footwear_output_loss: 1.1556 - emotion_output_loss: 2.5177 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5958 - age_output_acc: 0.3917 - weight_output_acc: 0.6396 - bag_output_acc: 0.6354 - pose_output_acc: 0.7750 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7063 - val_loss: 21.7883 - val_gender_output_loss: 0.4482 - val_image_quality_output_loss: 0.9504 - val_age_output_loss: 1.3756 - val_weight_output_loss: 0.9967 - val_bag_output_loss: 0.8437 - val_pose_output_loss: 0.5133 - val_footwear_output_loss: 0.8185 - val_emotion_output_loss: 0.8962 - val_gender_output_acc: 0.7849 - val_image_quality_output_acc: 0.5364 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6442 - val_pose_output_acc: 0.8012 - val_footwear_output_acc: 0.6432 - val_emotion_output_acc: 0.7032\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 26s 862ms/step - loss: 42.3968 - gender_output_loss: 0.5515 - image_quality_output_loss: 0.9222 - age_output_loss: 2.7209 - weight_output_loss: 2.5993 - bag_output_loss: 1.3781 - pose_output_loss: 1.1984 - footwear_output_loss: 1.1596 - emotion_output_loss: 2.3784 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4313 - weight_output_acc: 0.6083 - bag_output_acc: 0.6167 - pose_output_acc: 0.7479 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7312 - val_loss: 22.4258 - val_gender_output_loss: 0.5012 - val_image_quality_output_loss: 0.9780 - val_age_output_loss: 1.4074 - val_weight_output_loss: 1.0168 - val_bag_output_loss: 0.8761 - val_pose_output_loss: 0.5321 - val_footwear_output_loss: 0.8692 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.7859 - val_image_quality_output_acc: 0.5335 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.7992 - val_footwear_output_acc: 0.6112 - val_emotion_output_acc: 0.6998\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 26s 857ms/step - loss: 42.9954 - gender_output_loss: 0.5395 - image_quality_output_loss: 0.9440 - age_output_loss: 2.8651 - weight_output_loss: 2.2609 - bag_output_loss: 1.6242 - pose_output_loss: 1.0561 - footwear_output_loss: 1.1401 - emotion_output_loss: 2.5648 - gender_output_acc: 0.7979 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4104 - weight_output_acc: 0.6687 - bag_output_acc: 0.6083 - pose_output_acc: 0.7646 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7229 - val_loss: 23.3522 - val_gender_output_loss: 0.6355 - val_image_quality_output_loss: 0.9246 - val_age_output_loss: 1.4993 - val_weight_output_loss: 1.0244 - val_bag_output_loss: 0.9112 - val_pose_output_loss: 0.6025 - val_footwear_output_loss: 0.8911 - val_emotion_output_loss: 0.9083 - val_gender_output_acc: 0.7279 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3524 - val_weight_output_acc: 0.5861 - val_bag_output_acc: 0.6009 - val_pose_output_acc: 0.7781 - val_footwear_output_acc: 0.6024 - val_emotion_output_acc: 0.6978\n",
            "Epoch 20/50\n",
            "\n",
            "30/30 [==============================] - 26s 862ms/step - loss: 46.3373 - gender_output_loss: 0.5709 - image_quality_output_loss: 0.9232 - age_output_loss: 2.8329 - weight_output_loss: 2.9269 - bag_output_loss: 1.4234 - pose_output_loss: 1.2870 - footwear_output_loss: 1.1938 - emotion_output_loss: 2.8782 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3875 - weight_output_acc: 0.6208 - bag_output_acc: 0.5958 - pose_output_acc: 0.7208 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.6958 - val_loss: 22.2218 - val_gender_output_loss: 0.4518 - val_image_quality_output_loss: 0.9980 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9928 - val_bag_output_loss: 0.8651 - val_pose_output_loss: 0.5243 - val_footwear_output_loss: 0.8360 - val_emotion_output_loss: 0.9311 - val_gender_output_acc: 0.7864 - val_image_quality_output_acc: 0.5098 - val_age_output_acc: 0.3848 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6171 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6206 - val_emotion_output_acc: 0.6796\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 26s 866ms/step - loss: 44.4570 - gender_output_loss: 0.5450 - image_quality_output_loss: 0.9186 - age_output_loss: 2.6300 - weight_output_loss: 2.5725 - bag_output_loss: 1.4161 - pose_output_loss: 1.2521 - footwear_output_loss: 1.1005 - emotion_output_loss: 2.9702 - gender_output_acc: 0.7708 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4271 - weight_output_acc: 0.6125 - bag_output_acc: 0.6208 - pose_output_acc: 0.7354 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.6854 - val_loss: 22.9480 - val_gender_output_loss: 0.6110 - val_image_quality_output_loss: 0.9823 - val_age_output_loss: 1.4032 - val_weight_output_loss: 0.9984 - val_bag_output_loss: 0.8601 - val_pose_output_loss: 0.6520 - val_footwear_output_loss: 0.8314 - val_emotion_output_loss: 0.9370 - val_gender_output_acc: 0.7338 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7416 - val_footwear_output_acc: 0.6407 - val_emotion_output_acc: 0.6791\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577562023_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577562027_backup\n",
            "Epoch 1/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 45.2104 - gender_output_loss: 0.5822 - image_quality_output_loss: 0.9042 - age_output_loss: 2.8944 - weight_output_loss: 2.3631 - bag_output_loss: 1.5898 - pose_output_loss: 1.0662 - footwear_output_loss: 1.2498 - emotion_output_loss: 2.9729 - gender_output_acc: 0.7478 - image_quality_output_acc: 0.5819 - age_output_acc: 0.4073 - weight_output_acc: 0.6466 - bag_output_acc: 0.5690 - pose_output_acc: 0.7586 - footwear_output_acc: 0.5754 - emotion_output_acc: 0.6832\n",
            "30/30 [==============================] - 33s 1s/step - loss: 44.9443 - gender_output_loss: 0.5756 - image_quality_output_loss: 0.9097 - age_output_loss: 2.9368 - weight_output_loss: 2.3411 - bag_output_loss: 1.5718 - pose_output_loss: 1.0564 - footwear_output_loss: 1.2450 - emotion_output_loss: 2.9043 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4000 - weight_output_acc: 0.6479 - bag_output_acc: 0.5771 - pose_output_acc: 0.7604 - footwear_output_acc: 0.5771 - emotion_output_acc: 0.6896 - val_loss: 22.0545 - val_gender_output_loss: 0.4488 - val_image_quality_output_loss: 1.0214 - val_age_output_loss: 1.3741 - val_weight_output_loss: 1.0072 - val_bag_output_loss: 0.8400 - val_pose_output_loss: 0.5229 - val_footwear_output_loss: 0.8313 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.8036 - val_image_quality_output_acc: 0.5305 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.5817 - val_bag_output_acc: 0.6398 - val_pose_output_acc: 0.8169 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6939\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 42.8166 - gender_output_loss: 0.5781 - image_quality_output_loss: 0.9512 - age_output_loss: 2.7099 - weight_output_loss: 2.5656 - bag_output_loss: 1.5683 - pose_output_loss: 1.3038 - footwear_output_loss: 1.1325 - emotion_output_loss: 2.2824 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4125 - weight_output_acc: 0.6542 - bag_output_acc: 0.6083 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7563 - val_loss: 22.5550 - val_gender_output_loss: 0.4605 - val_image_quality_output_loss: 1.0388 - val_age_output_loss: 1.3911 - val_weight_output_loss: 1.0348 - val_bag_output_loss: 0.8581 - val_pose_output_loss: 0.5909 - val_footwear_output_loss: 0.8287 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.7948 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5659 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.7790 - val_footwear_output_acc: 0.6550 - val_emotion_output_acc: 0.6959\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 26s 864ms/step - loss: 43.6979 - gender_output_loss: 0.5239 - image_quality_output_loss: 0.9603 - age_output_loss: 2.6077 - weight_output_loss: 2.7974 - bag_output_loss: 1.4957 - pose_output_loss: 1.3499 - footwear_output_loss: 1.1487 - emotion_output_loss: 2.4655 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4229 - weight_output_acc: 0.6458 - bag_output_acc: 0.6062 - pose_output_acc: 0.7271 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7167 - val_loss: 22.0286 - val_gender_output_loss: 0.4493 - val_image_quality_output_loss: 0.9506 - val_age_output_loss: 1.3701 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8773 - val_pose_output_loss: 0.5491 - val_footwear_output_loss: 0.8303 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.7884 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6097 - val_pose_output_acc: 0.7913 - val_footwear_output_acc: 0.6417 - val_emotion_output_acc: 0.6934\n",
            "30/30 [==============================] - 25s 819ms/step - loss: 42.8166 - gender_output_loss: 0.5781 - image_quality_output_loss: 0.9512 - age_output_loss: 2.7099 - weight_output_loss: 2.5656 - bag_output_loss: 1.5683 - pose_output_loss: 1.3038 - footwear_output_loss: 1.1325 - emotion_output_loss: 2.2824 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4125 - weight_output_acc: 0.6542 - bag_output_acc: 0.6083 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7563 - val_loss: 22.5550 - val_gender_output_loss: 0.4605 - val_image_quality_output_loss: 1.0388 - val_age_output_loss: 1.3911 - val_weight_output_loss: 1.0348 - val_bag_output_loss: 0.8581 - val_pose_output_loss: 0.5909 - val_footwear_output_loss: 0.8287 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.7948 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5659 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.7790 - val_footwear_output_acc: 0.6550 - val_emotion_output_acc: 0.6959\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 26s 873ms/step - loss: 44.5086 - gender_output_loss: 0.6457 - image_quality_output_loss: 0.9390 - age_output_loss: 2.8980 - weight_output_loss: 2.6355 - bag_output_loss: 1.4944 - pose_output_loss: 1.1359 - footwear_output_loss: 1.1963 - emotion_output_loss: 2.5867 - gender_output_acc: 0.7083 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3729 - weight_output_acc: 0.6292 - bag_output_acc: 0.5813 - pose_output_acc: 0.7417 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.7063 - val_loss: 22.7868 - val_gender_output_loss: 0.4504 - val_image_quality_output_loss: 0.9561 - val_age_output_loss: 1.4307 - val_weight_output_loss: 1.0083 - val_bag_output_loss: 0.8473 - val_pose_output_loss: 0.6962 - val_footwear_output_loss: 0.8479 - val_emotion_output_loss: 0.9223 - val_gender_output_acc: 0.7948 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6093 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7608 - val_footwear_output_acc: 0.6378 - val_emotion_output_acc: 0.6934\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 26s 864ms/step - loss: 45.1539 - gender_output_loss: 0.5974 - image_quality_output_loss: 0.9343 - age_output_loss: 2.8686 - weight_output_loss: 2.4715 - bag_output_loss: 1.4811 - pose_output_loss: 1.1314 - footwear_output_loss: 1.0999 - emotion_output_loss: 2.9883 - gender_output_acc: 0.7250 - image_quality_output_acc: 0.5396 - age_output_acc: 0.3938 - weight_output_acc: 0.6333 - bag_output_acc: 0.6146 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.6896 - val_loss: 22.6043 - val_gender_output_loss: 0.4672 - val_image_quality_output_loss: 0.9695 - val_age_output_loss: 1.4256 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.8479 - val_pose_output_loss: 0.6249 - val_footwear_output_loss: 0.8227 - val_emotion_output_loss: 0.9413 - val_gender_output_acc: 0.7904 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6294 - val_pose_output_acc: 0.7840 - val_footwear_output_acc: 0.6452 - val_emotion_output_acc: 0.6594\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 26s 859ms/step - loss: 45.0573 - gender_output_loss: 0.6340 - image_quality_output_loss: 0.9160 - age_output_loss: 2.7201 - weight_output_loss: 2.5526 - bag_output_loss: 1.5210 - pose_output_loss: 1.2292 - footwear_output_loss: 1.2225 - emotion_output_loss: 2.8782 - gender_output_acc: 0.7146 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3812 - weight_output_acc: 0.6438 - bag_output_acc: 0.5979 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.6875 - val_loss: 22.3321 - val_gender_output_loss: 0.5323 - val_image_quality_output_loss: 0.9593 - val_age_output_loss: 1.3786 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.8665 - val_pose_output_loss: 0.5733 - val_footwear_output_loss: 0.8266 - val_emotion_output_loss: 0.9149 - val_gender_output_acc: 0.7687 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7790 - val_footwear_output_acc: 0.6550 - val_emotion_output_acc: 0.7008\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 25s 841ms/step - loss: 43.2561 - gender_output_loss: 0.5581 - image_quality_output_loss: 0.8950 - age_output_loss: 2.6894 - weight_output_loss: 2.6646 - bag_output_loss: 1.5284 - pose_output_loss: 1.0096 - footwear_output_loss: 1.2278 - emotion_output_loss: 2.5798 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4083 - weight_output_acc: 0.5979 - bag_output_acc: 0.6000 - pose_output_acc: 0.7646 - footwear_output_acc: 0.5813 - emotion_output_acc: 0.7229 - val_loss: 22.1168 - val_gender_output_loss: 0.4381 - val_image_quality_output_loss: 0.9956 - val_age_output_loss: 1.3772 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.8591 - val_pose_output_loss: 0.5440 - val_footwear_output_loss: 0.8114 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.7968 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.4035 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6535 - val_emotion_output_acc: 0.6855\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 25s 843ms/step - loss: 40.2349 - gender_output_loss: 0.5384 - image_quality_output_loss: 0.9392 - age_output_loss: 2.5876 - weight_output_loss: 2.2852 - bag_output_loss: 1.3994 - pose_output_loss: 1.1272 - footwear_output_loss: 1.2257 - emotion_output_loss: 2.2083 - gender_output_acc: 0.7729 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4187 - weight_output_acc: 0.6687 - bag_output_acc: 0.6396 - pose_output_acc: 0.7500 - footwear_output_acc: 0.5667 - emotion_output_acc: 0.7479 - val_loss: 21.9944 - val_gender_output_loss: 0.4501 - val_image_quality_output_loss: 0.9328 - val_age_output_loss: 1.3799 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.8604 - val_pose_output_loss: 0.5315 - val_footwear_output_loss: 0.8381 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6196 - val_pose_output_acc: 0.7889 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6959\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 25s 843ms/step - loss: 43.9992 - gender_output_loss: 0.5505 - image_quality_output_loss: 0.9282 - age_output_loss: 2.6514 - weight_output_loss: 2.5108 - bag_output_loss: 1.5722 - pose_output_loss: 1.1002 - footwear_output_loss: 1.0997 - emotion_output_loss: 2.8695 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4208 - weight_output_acc: 0.6458 - bag_output_acc: 0.6062 - pose_output_acc: 0.7729 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.6917 - val_loss: 22.3844 - val_gender_output_loss: 0.4598 - val_image_quality_output_loss: 1.0063 - val_age_output_loss: 1.3977 - val_weight_output_loss: 1.0214 - val_bag_output_loss: 0.8672 - val_pose_output_loss: 0.5493 - val_footwear_output_loss: 0.8302 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5325 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.5861 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.7864 - val_footwear_output_acc: 0.6422 - val_emotion_output_acc: 0.6944\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 44.0103 - gender_output_loss: 0.4878 - image_quality_output_loss: 0.9332 - age_output_loss: 2.7568 - weight_output_loss: 2.4390 - bag_output_loss: 1.3276 - pose_output_loss: 1.2288 - footwear_output_loss: 1.1803 - emotion_output_loss: 2.8965 - gender_output_acc: 0.8146 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4083 - weight_output_acc: 0.6354 - bag_output_acc: 0.6542 - pose_output_acc: 0.7521 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7021 - val_loss: 22.7922 - val_gender_output_loss: 0.4844 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.3805 - val_weight_output_loss: 1.0122 - val_bag_output_loss: 0.8919 - val_pose_output_loss: 0.6458 - val_footwear_output_loss: 0.8947 - val_emotion_output_loss: 0.9226 - val_gender_output_acc: 0.7864 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.4050 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6152 - val_pose_output_acc: 0.7456 - val_footwear_output_acc: 0.6166 - val_emotion_output_acc: 0.7003\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 25s 847ms/step - loss: 44.1024 - gender_output_loss: 0.5119 - image_quality_output_loss: 0.9416 - age_output_loss: 2.5297 - weight_output_loss: 3.0069 - bag_output_loss: 1.6491 - pose_output_loss: 1.1226 - footwear_output_loss: 1.1152 - emotion_output_loss: 2.5757 - gender_output_acc: 0.7937 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4479 - weight_output_acc: 0.6062 - bag_output_acc: 0.6333 - pose_output_acc: 0.7646 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7146 - val_loss: 22.4794 - val_gender_output_loss: 0.4624 - val_image_quality_output_loss: 0.9517 - val_age_output_loss: 1.3832 - val_weight_output_loss: 1.0095 - val_bag_output_loss: 0.9323 - val_pose_output_loss: 0.5629 - val_footwear_output_loss: 0.8855 - val_emotion_output_loss: 0.9064 - val_gender_output_acc: 0.7844 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6029 - val_bag_output_acc: 0.5906 - val_pose_output_acc: 0.7830 - val_footwear_output_acc: 0.6211 - val_emotion_output_acc: 0.7037\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 25s 845ms/step - loss: 43.5288 - gender_output_loss: 0.5502 - image_quality_output_loss: 0.9400 - age_output_loss: 2.9496 - weight_output_loss: 2.2942 - bag_output_loss: 1.3613 - pose_output_loss: 1.2237 - footwear_output_loss: 1.1543 - emotion_output_loss: 2.6490 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3792 - weight_output_acc: 0.6854 - bag_output_acc: 0.6354 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7146 - val_loss: 22.7867 - val_gender_output_loss: 0.4315 - val_image_quality_output_loss: 1.0059 - val_age_output_loss: 1.3745 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 0.9166 - val_pose_output_loss: 0.6299 - val_footwear_output_loss: 0.8646 - val_emotion_output_loss: 0.9693 - val_gender_output_acc: 0.8027 - val_image_quality_output_acc: 0.5089 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.5856 - val_pose_output_acc: 0.7500 - val_footwear_output_acc: 0.6019 - val_emotion_output_acc: 0.6531\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 26s 855ms/step - loss: 43.4996 - gender_output_loss: 0.5591 - image_quality_output_loss: 0.9482 - age_output_loss: 2.7437 - weight_output_loss: 2.3314 - bag_output_loss: 1.5058 - pose_output_loss: 1.2045 - footwear_output_loss: 1.1980 - emotion_output_loss: 2.6953 - gender_output_acc: 0.7917 - image_quality_output_acc: 0.5437 - age_output_acc: 0.3979 - weight_output_acc: 0.6542 - bag_output_acc: 0.6104 - pose_output_acc: 0.7479 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.7083 - val_loss: 22.6373 - val_gender_output_loss: 0.5169 - val_image_quality_output_loss: 0.9607 - val_age_output_loss: 1.4150 - val_weight_output_loss: 1.0104 - val_bag_output_loss: 0.8669 - val_pose_output_loss: 0.6043 - val_footwear_output_loss: 0.8538 - val_emotion_output_loss: 0.9155 - val_gender_output_acc: 0.7746 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.6132 - val_pose_output_acc: 0.7608 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6924\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 26s 858ms/step - loss: 43.8277 - gender_output_loss: 0.5710 - image_quality_output_loss: 0.9202 - age_output_loss: 2.9981 - weight_output_loss: 2.5834 - bag_output_loss: 1.4671 - pose_output_loss: 1.2495 - footwear_output_loss: 1.2122 - emotion_output_loss: 2.3301 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3875 - weight_output_acc: 0.6104 - bag_output_acc: 0.5958 - pose_output_acc: 0.7229 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.7500 - val_loss: 22.4556 - val_gender_output_loss: 0.4786 - val_image_quality_output_loss: 1.0274 - val_age_output_loss: 1.3970 - val_weight_output_loss: 1.0122 - val_bag_output_loss: 0.8619 - val_pose_output_loss: 0.5558 - val_footwear_output_loss: 0.8427 - val_emotion_output_loss: 0.9181 - val_gender_output_acc: 0.7830 - val_image_quality_output_acc: 0.5241 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.5876 - val_bag_output_acc: 0.6181 - val_pose_output_acc: 0.7756 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6949\n",
            "Epoch 15/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 43.0852 - gender_output_loss: 0.5082 - image_quality_output_loss: 0.9288 - age_output_loss: 2.9336 - weight_output_loss: 2.4192 - bag_output_loss: 1.4864 - pose_output_loss: 1.1911 - footwear_output_loss: 1.1809 - emotion_output_loss: 2.4044 - gender_output_acc: 0.7931 - image_quality_output_acc: 0.5603 - age_output_acc: 0.4030 - weight_output_acc: 0.6293 - bag_output_acc: 0.6142 - pose_output_acc: 0.7586 - footwear_output_acc: 0.5862 - emotion_output_acc: 0.7371Epoch 15/50\n",
            "30/30 [==============================] - 26s 865ms/step - loss: 43.0701 - gender_output_loss: 0.5113 - image_quality_output_loss: 0.9231 - age_output_loss: 2.9175 - weight_output_loss: 2.4302 - bag_output_loss: 1.4927 - pose_output_loss: 1.1839 - footwear_output_loss: 1.1999 - emotion_output_loss: 2.4009 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4062 - weight_output_acc: 0.6313 - bag_output_acc: 0.6146 - pose_output_acc: 0.7583 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.7354 - val_loss: 22.5338 - val_gender_output_loss: 0.4873 - val_image_quality_output_loss: 0.9437 - val_age_output_loss: 1.4020 - val_weight_output_loss: 1.0003 - val_bag_output_loss: 0.8645 - val_pose_output_loss: 0.6225 - val_footwear_output_loss: 0.8256 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.7849 - val_image_quality_output_acc: 0.5576 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6078 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7721 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6831\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 26s 858ms/step - loss: 44.3396 - gender_output_loss: 0.4949 - image_quality_output_loss: 0.9226 - age_output_loss: 2.7324 - weight_output_loss: 2.7964 - bag_output_loss: 1.2898 - pose_output_loss: 1.1484 - footwear_output_loss: 1.1426 - emotion_output_loss: 2.8448 - gender_output_acc: 0.8000 - image_quality_output_acc: 0.5521 - age_output_acc: 0.4125 - weight_output_acc: 0.6333 - bag_output_acc: 0.6187 - pose_output_acc: 0.7458 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.6917 - val_loss: 21.8525 - val_gender_output_loss: 0.4472 - val_image_quality_output_loss: 0.9109 - val_age_output_loss: 1.3803 - val_weight_output_loss: 0.9941 - val_bag_output_loss: 0.8497 - val_pose_output_loss: 0.5411 - val_footwear_output_loss: 0.8174 - val_emotion_output_loss: 0.9046 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5664 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6255 - val_pose_output_acc: 0.7835 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6998\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 25s 849ms/step - loss: 42.3839 - gender_output_loss: 0.5611 - image_quality_output_loss: 0.9312 - age_output_loss: 2.6033 - weight_output_loss: 2.6270 - bag_output_loss: 1.4484 - pose_output_loss: 1.1547 - footwear_output_loss: 1.0993 - emotion_output_loss: 2.4726 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4083 - weight_output_acc: 0.6417 - bag_output_acc: 0.5875 - pose_output_acc: 0.7646 - footwear_output_acc: 0.6604 - emotion_output_acc: 0.7146 - val_loss: 22.0077 - val_gender_output_loss: 0.4442 - val_image_quality_output_loss: 0.9081 - val_age_output_loss: 1.3955 - val_weight_output_loss: 0.9993 - val_bag_output_loss: 0.8481 - val_pose_output_loss: 0.5514 - val_footwear_output_loss: 0.8354 - val_emotion_output_loss: 0.9118 - val_gender_output_acc: 0.8012 - val_image_quality_output_acc: 0.5738 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6112 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7844 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.7023\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 44.9735 - gender_output_loss: 0.5431 - image_quality_output_loss: 0.9204 - age_output_loss: 2.8872 - weight_output_loss: 2.7450 - bag_output_loss: 1.6182 - pose_output_loss: 1.0115 - footwear_output_loss: 1.2111 - emotion_output_loss: 2.6863 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3562 - weight_output_acc: 0.6313 - bag_output_acc: 0.6167 - pose_output_acc: 0.7688 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7104 - val_loss: 22.3629 - val_gender_output_loss: 0.4562 - val_image_quality_output_loss: 0.9183 - val_age_output_loss: 1.4060 - val_weight_output_loss: 1.0263 - val_bag_output_loss: 0.8773 - val_pose_output_loss: 0.5802 - val_footwear_output_loss: 0.8383 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5694 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.5778 - val_bag_output_acc: 0.6161 - val_pose_output_acc: 0.7864 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6949\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 25s 830ms/step - loss: 44.7215 - gender_output_loss: 0.5890 - image_quality_output_loss: 0.9373 - age_output_loss: 2.8040 - weight_output_loss: 2.5819 - bag_output_loss: 1.5057 - pose_output_loss: 1.1999 - footwear_output_loss: 1.1555 - emotion_output_loss: 2.7684 - gender_output_acc: 0.7417 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4042 - weight_output_acc: 0.6375 - bag_output_acc: 0.6042 - pose_output_acc: 0.7292 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.6958 - val_loss: 22.3316 - val_gender_output_loss: 0.4620 - val_image_quality_output_loss: 0.9243 - val_age_output_loss: 1.3803 - val_weight_output_loss: 1.0051 - val_bag_output_loss: 0.8517 - val_pose_output_loss: 0.6325 - val_footwear_output_loss: 0.8298 - val_emotion_output_loss: 0.9261 - val_gender_output_acc: 0.7849 - val_image_quality_output_acc: 0.5586 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7544 - val_footwear_output_acc: 0.6501 - val_emotion_output_acc: 0.6865\n",
            "30/30 [==============================] - 25s 844ms/step - loss: 44.9735 - gender_output_loss: 0.5431 - image_quality_output_loss: 0.9204 - age_output_loss: 2.8872 - weight_output_loss: 2.7450 - bag_output_loss: 1.6182 - pose_output_loss: 1.0115 - footwear_output_loss: 1.2111 - emotion_output_loss: 2.6863 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3562 - weight_output_acc: 0.6313 - bag_output_acc: 0.6167 - pose_output_acc: 0.7688 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7104 - val_loss: 22.3629 - val_gender_output_loss: 0.4562 - val_image_quality_output_loss: 0.9183 - val_age_output_loss: 1.4060 - val_weight_output_loss: 1.0263 - val_bag_output_loss: 0.8773 - val_pose_output_loss: 0.5802 - val_footwear_output_loss: 0.8383 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5694 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.5778 - val_bag_output_acc: 0.6161 - val_pose_output_acc: 0.7864 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6949\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 26s 858ms/step - loss: 47.5796 - gender_output_loss: 0.5989 - image_quality_output_loss: 0.9461 - age_output_loss: 2.9574 - weight_output_loss: 2.8382 - bag_output_loss: 1.7615 - pose_output_loss: 1.1686 - footwear_output_loss: 1.2143 - emotion_output_loss: 2.9302 - gender_output_acc: 0.7167 - image_quality_output_acc: 0.5104 - age_output_acc: 0.3979 - weight_output_acc: 0.6125 - bag_output_acc: 0.5938 - pose_output_acc: 0.7521 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.6833 - val_loss: 22.3879 - val_gender_output_loss: 0.5113 - val_image_quality_output_loss: 0.9377 - val_age_output_loss: 1.3871 - val_weight_output_loss: 1.0011 - val_bag_output_loss: 0.8435 - val_pose_output_loss: 0.5727 - val_footwear_output_loss: 0.8391 - val_emotion_output_loss: 0.9514 - val_gender_output_acc: 0.7682 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.5915 - val_bag_output_acc: 0.6348 - val_pose_output_acc: 0.7746 - val_footwear_output_acc: 0.6378 - val_emotion_output_acc: 0.6722\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 26s 858ms/step - loss: 45.1102 - gender_output_loss: 0.5961 - image_quality_output_loss: 0.9386 - age_output_loss: 2.7320 - weight_output_loss: 2.8300 - bag_output_loss: 1.5450 - pose_output_loss: 1.3944 - footwear_output_loss: 1.2054 - emotion_output_loss: 2.5469 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3979 - weight_output_acc: 0.5979 - bag_output_acc: 0.6021 - pose_output_acc: 0.6854 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7208 - val_loss: 22.5064 - val_gender_output_loss: 0.4843 - val_image_quality_output_loss: 1.0035 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9996 - val_bag_output_loss: 0.8809 - val_pose_output_loss: 0.6036 - val_footwear_output_loss: 0.8356 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.7746 - val_image_quality_output_acc: 0.5113 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.5945 - val_bag_output_acc: 0.6043 - val_pose_output_acc: 0.7554 - val_footwear_output_acc: 0.6299 - val_emotion_output_acc: 0.7013\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 26s 855ms/step - loss: 43.5213 - gender_output_loss: 0.5432 - image_quality_output_loss: 0.9430 - age_output_loss: 2.7849 - weight_output_loss: 2.4267 - bag_output_loss: 1.3665 - pose_output_loss: 1.1586 - footwear_output_loss: 1.2474 - emotion_output_loss: 2.7134 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3917 - weight_output_acc: 0.6687 - bag_output_acc: 0.5979 - pose_output_acc: 0.7667 - footwear_output_acc: 0.5958 - emotion_output_acc: 0.6917 - val_loss: 23.6078 - val_gender_output_loss: 0.5728 - val_image_quality_output_loss: 0.9297 - val_age_output_loss: 1.4546 - val_weight_output_loss: 1.0595 - val_bag_output_loss: 0.8733 - val_pose_output_loss: 0.6560 - val_footwear_output_loss: 0.8773 - val_emotion_output_loss: 1.0145 - val_gender_output_acc: 0.7554 - val_image_quality_output_acc: 0.5512 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6093 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7638 - val_footwear_output_acc: 0.6102 - val_emotion_output_acc: 0.6102\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 25s 839ms/step - loss: 44.0343 - gender_output_loss: 0.5274 - image_quality_output_loss: 0.9356 - age_output_loss: 2.6742 - weight_output_loss: 2.4934 - bag_output_loss: 1.2960 - pose_output_loss: 0.9809 - footwear_output_loss: 1.1641 - emotion_output_loss: 3.1420 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3958 - weight_output_acc: 0.6375 - bag_output_acc: 0.6708 - pose_output_acc: 0.7688 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.6729 - val_loss: 22.2623 - val_gender_output_loss: 0.4605 - val_image_quality_output_loss: 0.9627 - val_age_output_loss: 1.3769 - val_weight_output_loss: 1.0063 - val_bag_output_loss: 0.8492 - val_pose_output_loss: 0.5642 - val_footwear_output_loss: 0.8161 - val_emotion_output_loss: 0.9532 - val_gender_output_acc: 0.7923 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.7795 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.6609\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 23s 768ms/step - loss: 40.4579 - gender_output_loss: 0.5567 - image_quality_output_loss: 0.9393 - age_output_loss: 2.6596 - weight_output_loss: 2.5377 - bag_output_loss: 1.3318 - pose_output_loss: 1.1000 - footwear_output_loss: 1.0922 - emotion_output_loss: 2.1327 - gender_output_acc: 0.7750 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3958 - weight_output_acc: 0.6479 - bag_output_acc: 0.6417 - pose_output_acc: 0.7646 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.7479 - val_loss: 22.7395 - val_gender_output_loss: 0.4780 - val_image_quality_output_loss: 0.9711 - val_age_output_loss: 1.4179 - val_weight_output_loss: 1.0230 - val_bag_output_loss: 0.8621 - val_pose_output_loss: 0.5908 - val_footwear_output_loss: 0.8412 - val_emotion_output_loss: 0.9639 - val_gender_output_acc: 0.7854 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6073 - val_bag_output_acc: 0.6265 - val_pose_output_acc: 0.7830 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.6476\n",
            "Epoch 25/50\n",
            "\n",
            "30/30 [==============================] - 28s 928ms/step - loss: 43.6081 - gender_output_loss: 0.5340 - image_quality_output_loss: 0.9364 - age_output_loss: 2.6457 - weight_output_loss: 2.6594 - bag_output_loss: 1.4208 - pose_output_loss: 1.2457 - footwear_output_loss: 1.0895 - emotion_output_loss: 2.6809 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3979 - weight_output_acc: 0.6208 - bag_output_acc: 0.6083 - pose_output_acc: 0.7375 - footwear_output_acc: 0.6146 - emotion_output_acc: 0.7042 - val_loss: 22.3596 - val_gender_output_loss: 0.4617 - val_image_quality_output_loss: 0.9577 - val_age_output_loss: 1.3940 - val_weight_output_loss: 1.0140 - val_bag_output_loss: 0.8548 - val_pose_output_loss: 0.5929 - val_footwear_output_loss: 0.8273 - val_emotion_output_loss: 0.9255 - val_gender_output_acc: 0.7963 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3848 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7589 - val_footwear_output_acc: 0.6309 - val_emotion_output_acc: 0.6801\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 25s 828ms/step - loss: 44.4282 - gender_output_loss: 0.4991 - image_quality_output_loss: 0.9614 - age_output_loss: 2.8820 - weight_output_loss: 2.5571 - bag_output_loss: 1.4222 - pose_output_loss: 1.0100 - footwear_output_loss: 1.1502 - emotion_output_loss: 2.8770 - gender_output_acc: 0.8000 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3917 - weight_output_acc: 0.6250 - bag_output_acc: 0.6417 - pose_output_acc: 0.7896 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.6917 - val_loss: 22.8307 - val_gender_output_loss: 0.4866 - val_image_quality_output_loss: 0.9972 - val_age_output_loss: 1.4170 - val_weight_output_loss: 1.0170 - val_bag_output_loss: 0.8704 - val_pose_output_loss: 0.5721 - val_footwear_output_loss: 0.8337 - val_emotion_output_loss: 0.9867 - val_gender_output_acc: 0.7894 - val_image_quality_output_acc: 0.5261 - val_age_output_acc: 0.3627 - val_weight_output_acc: 0.5955 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7913 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6284\n",
            "Epoch 26/50\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577562708_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577562712_backup\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 32s 1s/step - loss: 46.4128 - gender_output_loss: 0.5475 - image_quality_output_loss: 0.9322 - age_output_loss: 3.0747 - weight_output_loss: 2.5077 - bag_output_loss: 1.4116 - pose_output_loss: 1.2123 - footwear_output_loss: 1.1285 - emotion_output_loss: 3.0729 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4104 - weight_output_acc: 0.6208 - bag_output_acc: 0.6458 - pose_output_acc: 0.7271 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.6812 - val_loss: 21.5672 - val_gender_output_loss: 0.4136 - val_image_quality_output_loss: 0.9528 - val_age_output_loss: 1.3581 - val_weight_output_loss: 0.9919 - val_bag_output_loss: 0.8315 - val_pose_output_loss: 0.5013 - val_footwear_output_loss: 0.8022 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.8159 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.5910 - val_bag_output_acc: 0.6427 - val_pose_output_acc: 0.8150 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 24s 785ms/step - loss: 44.5636 - gender_output_loss: 0.5093 - image_quality_output_loss: 0.9477 - age_output_loss: 3.0081 - weight_output_loss: 2.5090 - bag_output_loss: 1.3686 - pose_output_loss: 1.0634 - footwear_output_loss: 1.2672 - emotion_output_loss: 2.7623 - gender_output_acc: 0.7917 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3604 - weight_output_acc: 0.6458 - bag_output_acc: 0.6500 - pose_output_acc: 0.7792 - footwear_output_acc: 0.5729 - emotion_output_acc: 0.6938 - val_loss: 21.6826 - val_gender_output_loss: 0.4177 - val_image_quality_output_loss: 0.9546 - val_age_output_loss: 1.3586 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.8333 - val_pose_output_loss: 0.5169 - val_footwear_output_loss: 0.8092 - val_emotion_output_loss: 0.9135 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5448 - val_age_output_acc: 0.3903 - val_weight_output_acc: 0.5940 - val_bag_output_acc: 0.6270 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6905\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 25s 825ms/step - loss: 44.3927 - gender_output_loss: 0.6125 - image_quality_output_loss: 0.9488 - age_output_loss: 2.5342 - weight_output_loss: 2.4147 - bag_output_loss: 1.5457 - pose_output_loss: 1.1774 - footwear_output_loss: 1.2743 - emotion_output_loss: 2.9902 - gender_output_acc: 0.7250 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4542 - weight_output_acc: 0.6375 - bag_output_acc: 0.6042 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5500 - emotion_output_acc: 0.6979 - val_loss: 22.3743 - val_gender_output_loss: 0.4957 - val_image_quality_output_loss: 1.0323 - val_age_output_loss: 1.3633 - val_weight_output_loss: 0.9959 - val_bag_output_loss: 0.8721 - val_pose_output_loss: 0.5745 - val_footwear_output_loss: 0.8137 - val_emotion_output_loss: 0.9249 - val_gender_output_acc: 0.7677 - val_image_quality_output_acc: 0.5157 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6019 - val_bag_output_acc: 0.6063 - val_pose_output_acc: 0.7790 - val_footwear_output_acc: 0.6407 - val_emotion_output_acc: 0.6949\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 25s 821ms/step - loss: 43.3288 - gender_output_loss: 0.5889 - image_quality_output_loss: 0.9390 - age_output_loss: 2.6833 - weight_output_loss: 2.6396 - bag_output_loss: 1.6064 - pose_output_loss: 1.3483 - footwear_output_loss: 1.1222 - emotion_output_loss: 2.3254 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4083 - weight_output_acc: 0.6146 - bag_output_acc: 0.6042 - pose_output_acc: 0.7042 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7375 - val_loss: 22.6148 - val_gender_output_loss: 0.4505 - val_image_quality_output_loss: 1.0685 - val_age_output_loss: 1.3887 - val_weight_output_loss: 1.0148 - val_bag_output_loss: 0.9035 - val_pose_output_loss: 0.5597 - val_footwear_output_loss: 0.8731 - val_emotion_output_loss: 0.9077 - val_gender_output_acc: 0.7879 - val_image_quality_output_acc: 0.4828 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5659 - val_bag_output_acc: 0.5910 - val_pose_output_acc: 0.7781 - val_footwear_output_acc: 0.6186 - val_emotion_output_acc: 0.7003\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 26s 854ms/step - loss: 43.7031 - gender_output_loss: 0.5570 - image_quality_output_loss: 0.9541 - age_output_loss: 2.7439 - weight_output_loss: 2.5163 - bag_output_loss: 1.4030 - pose_output_loss: 1.2274 - footwear_output_loss: 1.2393 - emotion_output_loss: 2.6437 - gender_output_acc: 0.7792 - image_quality_output_acc: 0.5271 - age_output_acc: 0.4292 - weight_output_acc: 0.6271 - bag_output_acc: 0.5854 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7229 - val_loss: 22.4256 - val_gender_output_loss: 0.4754 - val_image_quality_output_loss: 0.9897 - val_age_output_loss: 1.4302 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.8766 - val_pose_output_loss: 0.5534 - val_footwear_output_loss: 0.8309 - val_emotion_output_loss: 0.9129 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5089 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6053 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6870\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 25s 838ms/step - loss: 46.4000 - gender_output_loss: 0.6196 - image_quality_output_loss: 0.9529 - age_output_loss: 2.6663 - weight_output_loss: 3.0009 - bag_output_loss: 1.4867 - pose_output_loss: 1.1837 - footwear_output_loss: 1.1946 - emotion_output_loss: 2.9939 - gender_output_acc: 0.7271 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4000 - weight_output_acc: 0.6229 - bag_output_acc: 0.5833 - pose_output_acc: 0.7125 - footwear_output_acc: 0.5729 - emotion_output_acc: 0.6812 - val_loss: 22.0205 - val_gender_output_loss: 0.4651 - val_image_quality_output_loss: 0.9475 - val_age_output_loss: 1.3820 - val_weight_output_loss: 1.0058 - val_bag_output_loss: 0.8568 - val_pose_output_loss: 0.5256 - val_footwear_output_loss: 0.8227 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.7918 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6171 - val_pose_output_acc: 0.8027 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.6929\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 25s 825ms/step - loss: 44.8987 - gender_output_loss: 0.5707 - image_quality_output_loss: 0.9068 - age_output_loss: 2.8302 - weight_output_loss: 2.6721 - bag_output_loss: 1.5628 - pose_output_loss: 1.0945 - footwear_output_loss: 1.0969 - emotion_output_loss: 2.8077 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.6062 - age_output_acc: 0.3583 - weight_output_acc: 0.5958 - bag_output_acc: 0.6104 - pose_output_acc: 0.7667 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.6833 - val_loss: 21.7204 - val_gender_output_loss: 0.4343 - val_image_quality_output_loss: 0.9187 - val_age_output_loss: 1.3662 - val_weight_output_loss: 1.0107 - val_bag_output_loss: 0.8332 - val_pose_output_loss: 0.5306 - val_footwear_output_loss: 0.8101 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.8002 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6029 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.7982 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.7013\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 25s 830ms/step - loss: 43.3217 - gender_output_loss: 0.5519 - image_quality_output_loss: 0.9229 - age_output_loss: 2.8092 - weight_output_loss: 2.7710 - bag_output_loss: 1.4276 - pose_output_loss: 1.0388 - footwear_output_loss: 1.1277 - emotion_output_loss: 2.4896 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4062 - weight_output_acc: 0.6458 - bag_output_acc: 0.5854 - pose_output_acc: 0.7646 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.7188 - val_loss: 22.0177 - val_gender_output_loss: 0.4468 - val_image_quality_output_loss: 0.9337 - val_age_output_loss: 1.3954 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.8458 - val_pose_output_loss: 0.5441 - val_footwear_output_loss: 0.8201 - val_emotion_output_loss: 0.9118 - val_gender_output_acc: 0.7987 - val_image_quality_output_acc: 0.5527 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6289 - val_pose_output_acc: 0.8007 - val_footwear_output_acc: 0.6373 - val_emotion_output_acc: 0.6905\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 25s 818ms/step - loss: 42.8638 - gender_output_loss: 0.5607 - image_quality_output_loss: 0.9289 - age_output_loss: 2.5216 - weight_output_loss: 2.4405 - bag_output_loss: 1.5148 - pose_output_loss: 1.1111 - footwear_output_loss: 1.1533 - emotion_output_loss: 2.7708 - gender_output_acc: 0.7792 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4354 - weight_output_acc: 0.6667 - bag_output_acc: 0.6042 - pose_output_acc: 0.7312 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.7188 - val_loss: 21.8862 - val_gender_output_loss: 0.4698 - val_image_quality_output_loss: 0.9288 - val_age_output_loss: 1.3822 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.8343 - val_pose_output_loss: 0.5265 - val_footwear_output_loss: 0.8152 - val_emotion_output_loss: 0.9130 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6344 - val_pose_output_acc: 0.8086 - val_footwear_output_acc: 0.6417 - val_emotion_output_acc: 0.7028\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 40.9745 - gender_output_loss: 0.5693 - image_quality_output_loss: 0.9319 - age_output_loss: 2.7605 - weight_output_loss: 2.2844 - bag_output_loss: 1.4045 - pose_output_loss: 1.1342 - footwear_output_loss: 1.2041 - emotion_output_loss: 2.2109 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3896 - weight_output_acc: 0.6583 - bag_output_acc: 0.6229 - pose_output_acc: 0.7625 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7583 - val_loss: 22.1388 - val_gender_output_loss: 0.4625 - val_image_quality_output_loss: 0.9300 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9895 - val_bag_output_loss: 0.8458 - val_pose_output_loss: 0.5639 - val_footwear_output_loss: 0.8175 - val_emotion_output_loss: 0.9196 - val_gender_output_acc: 0.7992 - val_image_quality_output_acc: 0.5576 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6299 - val_pose_output_acc: 0.7854 - val_footwear_output_acc: 0.6334 - val_emotion_output_acc: 0.7008\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 24s 817ms/step - loss: 45.5031 - gender_output_loss: 0.5906 - image_quality_output_loss: 0.9493 - age_output_loss: 2.7339 - weight_output_loss: 2.8209 - bag_output_loss: 1.4428 - pose_output_loss: 1.2868 - footwear_output_loss: 1.1702 - emotion_output_loss: 2.8219 - gender_output_acc: 0.7500 - image_quality_output_acc: 0.5417 - age_output_acc: 0.3792 - weight_output_acc: 0.6167 - bag_output_acc: 0.5979 - pose_output_acc: 0.6875 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7021 - val_loss: 22.1553 - val_gender_output_loss: 0.4390 - val_image_quality_output_loss: 0.9252 - val_age_output_loss: 1.3974 - val_weight_output_loss: 1.0179 - val_bag_output_loss: 0.8477 - val_pose_output_loss: 0.5855 - val_footwear_output_loss: 0.8160 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.8046 - val_image_quality_output_acc: 0.5502 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6250 - val_pose_output_acc: 0.7569 - val_footwear_output_acc: 0.6422 - val_emotion_output_acc: 0.6954\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n",
            "Executing augmentation 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_1577560074_1577563000_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 50 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json1577563005_backup\n",
            "Epoch 1/50\n",
            " 2/30 [=>............................] - ETA: 1:52 - loss: 39.8045 - gender_output_loss: 0.5389 - image_quality_output_loss: 0.9091 - age_output_loss: 2.0819 - weight_output_loss: 2.4273 - bag_output_loss: 1.1621 - pose_output_loss: 1.5508 - footwear_output_loss: 0.9768 - emotion_output_loss: 2.4988 - gender_output_acc: 0.6875 - image_quality_output_acc: 0.5938 - age_output_acc: 0.5312 - weight_output_acc: 0.6250 - bag_output_acc: 0.8125 - pose_output_acc: 0.6875 - footwear_output_acc: 0.7500 - emotion_output_acc: 0.6875"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.315896). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 32s 1s/step - loss: 42.5249 - gender_output_loss: 0.5707 - image_quality_output_loss: 0.9611 - age_output_loss: 2.6590 - weight_output_loss: 2.3120 - bag_output_loss: 1.5027 - pose_output_loss: 1.2860 - footwear_output_loss: 1.2183 - emotion_output_loss: 2.4689 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5167 - age_output_acc: 0.4333 - weight_output_acc: 0.6542 - bag_output_acc: 0.6146 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.7271 - val_loss: 21.5265 - val_gender_output_loss: 0.4178 - val_image_quality_output_loss: 0.9393 - val_age_output_loss: 1.3575 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8336 - val_pose_output_loss: 0.5021 - val_footwear_output_loss: 0.8066 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8169 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.8164 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.7018\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 24s 793ms/step - loss: 44.5504 - gender_output_loss: 0.5621 - image_quality_output_loss: 0.9460 - age_output_loss: 2.7056 - weight_output_loss: 2.4747 - bag_output_loss: 1.4851 - pose_output_loss: 1.3372 - footwear_output_loss: 1.1923 - emotion_output_loss: 2.8064 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3771 - weight_output_acc: 0.6438 - bag_output_acc: 0.5917 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.6938 - val_loss: 21.7161 - val_gender_output_loss: 0.4218 - val_image_quality_output_loss: 0.9284 - val_age_output_loss: 1.3523 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8497 - val_pose_output_loss: 0.5650 - val_footwear_output_loss: 0.8050 - val_emotion_output_loss: 0.8977 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.6038 - val_bag_output_acc: 0.6294 - val_pose_output_acc: 0.7835 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.7013\n",
            " - 32s 1s/step - loss: 42.5249 - gender_output_loss: 0.5707 - image_quality_output_loss: 0.9611 - age_output_loss: 2.6590 - weight_output_loss: 2.3120 - bag_output_loss: 1.5027 - pose_output_loss: 1.2860 - footwear_output_loss: 1.2183 - emotion_output_loss: 2.4689 - gender_output_acc: 0.7354 - image_quality_output_acc: 0.5167 - age_output_acc: 0.4333 - weight_output_acc: 0.6542 - bag_output_acc: 0.6146 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.7271 - val_loss: 21.5265 - val_gender_output_loss: 0.4178 - val_image_quality_output_loss: 0.9393 - val_age_output_loss: 1.3575 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8336 - val_pose_output_loss: 0.5021 - val_footwear_output_loss: 0.8066 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8169 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.8164 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.7018\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 25s 826ms/step - loss: 42.7204 - gender_output_loss: 0.5754 - image_quality_output_loss: 0.9649 - age_output_loss: 2.8466 - weight_output_loss: 2.4409 - bag_output_loss: 1.4047 - pose_output_loss: 1.2423 - footwear_output_loss: 1.1293 - emotion_output_loss: 2.3803 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4042 - weight_output_acc: 0.6500 - bag_output_acc: 0.6313 - pose_output_acc: 0.7000 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7271 - val_loss: 21.7810 - val_gender_output_loss: 0.4396 - val_image_quality_output_loss: 0.9128 - val_age_output_loss: 1.3688 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8556 - val_pose_output_loss: 0.5390 - val_footwear_output_loss: 0.8216 - val_emotion_output_loss: 0.9046 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.4090 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6201 - val_pose_output_acc: 0.7830 - val_footwear_output_acc: 0.6368 - val_emotion_output_acc: 0.7052\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 24s 814ms/step - loss: 46.0687 - gender_output_loss: 0.5637 - image_quality_output_loss: 0.9412 - age_output_loss: 2.7397 - weight_output_loss: 2.8233 - bag_output_loss: 1.5767 - pose_output_loss: 1.2220 - footwear_output_loss: 1.1265 - emotion_output_loss: 2.9428 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4062 - weight_output_acc: 0.6271 - bag_output_acc: 0.6167 - pose_output_acc: 0.7333 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.6938 - val_loss: 24.0723 - val_gender_output_loss: 0.4374 - val_image_quality_output_loss: 1.1055 - val_age_output_loss: 1.4465 - val_weight_output_loss: 1.0177 - val_bag_output_loss: 0.8555 - val_pose_output_loss: 1.0247 - val_footwear_output_loss: 0.8419 - val_emotion_output_loss: 0.9033 - val_gender_output_acc: 0.7997 - val_image_quality_output_acc: 0.4877 - val_age_output_acc: 0.3381 - val_weight_output_acc: 0.5792 - val_bag_output_acc: 0.6186 - val_pose_output_acc: 0.6097 - val_footwear_output_acc: 0.6230 - val_emotion_output_acc: 0.7032\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 43.3781 - gender_output_loss: 0.5966 - image_quality_output_loss: 0.9290 - age_output_loss: 2.6797 - weight_output_loss: 2.5541 - bag_output_loss: 1.5865 - pose_output_loss: 1.1357 - footwear_output_loss: 1.2126 - emotion_output_loss: 2.5361 - gender_output_acc: 0.7250 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4313 - weight_output_acc: 0.6479 - bag_output_acc: 0.5792 - pose_output_acc: 0.7417 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.7229 - val_loss: 21.9978 - val_gender_output_loss: 0.4439 - val_image_quality_output_loss: 1.0000 - val_age_output_loss: 1.3705 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8386 - val_pose_output_loss: 0.5621 - val_footwear_output_loss: 0.8453 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.7977 - val_image_quality_output_acc: 0.5138 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6388 - val_pose_output_acc: 0.7677 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.7047\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 6/50\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 45.4090 - gender_output_loss: 0.6342 - image_quality_output_loss: 0.9371 - age_output_loss: 2.9110 - weight_output_loss: 2.6182 - bag_output_loss: 1.5304 - pose_output_loss: 1.1468 - footwear_output_loss: 1.2073 - emotion_output_loss: 2.7782 - gender_output_acc: 0.7229 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3667 - weight_output_acc: 0.6417 - bag_output_acc: 0.6229 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5854 - emotion_output_acc: 0.6938 - val_loss: 22.2295 - val_gender_output_loss: 0.4523 - val_image_quality_output_loss: 0.9332 - val_age_output_loss: 1.3706 - val_weight_output_loss: 1.0030 - val_bag_output_loss: 0.8540 - val_pose_output_loss: 0.6066 - val_footwear_output_loss: 0.8283 - val_emotion_output_loss: 0.9300 - val_gender_output_acc: 0.8022 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.5974 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.7657 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6752\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 42.3479 - gender_output_loss: 0.5464 - image_quality_output_loss: 0.9100 - age_output_loss: 2.6249 - weight_output_loss: 2.3534 - bag_output_loss: 1.3093 - pose_output_loss: 1.1079 - footwear_output_loss: 1.1808 - emotion_output_loss: 2.7636 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5792 - age_output_acc: 0.3896 - weight_output_acc: 0.6354 - bag_output_acc: 0.6229 - pose_output_acc: 0.7417 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7104 - val_loss: 21.7082 - val_gender_output_loss: 0.4286 - val_image_quality_output_loss: 0.9234 - val_age_output_loss: 1.3604 - val_weight_output_loss: 0.9966 - val_bag_output_loss: 0.8352 - val_pose_output_loss: 0.5254 - val_footwear_output_loss: 0.8156 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5571 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.5915 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7938 - val_footwear_output_acc: 0.6373 - val_emotion_output_acc: 0.6983\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 41.7087 - gender_output_loss: 0.5712 - image_quality_output_loss: 0.9468 - age_output_loss: 2.6859 - weight_output_loss: 2.0814 - bag_output_loss: 1.3263 - pose_output_loss: 1.2665 - footwear_output_loss: 1.1709 - emotion_output_loss: 2.5893 - gender_output_acc: 0.7250 - image_quality_output_acc: 0.5375 - age_output_acc: 0.4229 - weight_output_acc: 0.6687 - bag_output_acc: 0.6500 - pose_output_acc: 0.7250 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7104 - val_loss: 21.5845 - val_gender_output_loss: 0.4151 - val_image_quality_output_loss: 0.9182 - val_age_output_loss: 1.3569 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.8267 - val_pose_output_loss: 0.5116 - val_footwear_output_loss: 0.8291 - val_emotion_output_loss: 0.9106 - val_gender_output_acc: 0.8130 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.6358 - val_pose_output_acc: 0.8007 - val_footwear_output_acc: 0.6353 - val_emotion_output_acc: 0.6978\n",
            "Epoch 9/50\n",
            "29/30 [============================>.] - ETA: 0s - loss: 42.9888 - gender_output_loss: 0.5009 - image_quality_output_loss: 0.9298 - age_output_loss: 2.7612 - weight_output_loss: 2.4960 - bag_output_loss: 1.3082 - pose_output_loss: 1.1203 - footwear_output_loss: 1.2445 - emotion_output_loss: 2.6532 - gender_output_acc: 0.7909 - image_quality_output_acc: 0.5819 - age_output_acc: 0.3836 - weight_output_acc: 0.6401 - bag_output_acc: 0.6659 - pose_output_acc: 0.7306 - footwear_output_acc: 0.5862 - emotion_output_acc: 0.7241\n",
            "30/30 [==============================] - 24s 814ms/step - loss: 43.1448 - gender_output_loss: 0.5014 - image_quality_output_loss: 0.9296 - age_output_loss: 2.8061 - weight_output_loss: 2.4985 - bag_output_loss: 1.3211 - pose_output_loss: 1.1187 - footwear_output_loss: 1.2465 - emotion_output_loss: 2.6358 - gender_output_acc: 0.7917 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3833 - weight_output_acc: 0.6354 - bag_output_acc: 0.6646 - pose_output_acc: 0.7312 - footwear_output_acc: 0.5813 - emotion_output_acc: 0.7250 - val_loss: 21.8305 - val_gender_output_loss: 0.4202 - val_image_quality_output_loss: 0.9488 - val_age_output_loss: 1.3596 - val_weight_output_loss: 0.9945 - val_bag_output_loss: 0.8359 - val_pose_output_loss: 0.5279 - val_footwear_output_loss: 0.8303 - val_emotion_output_loss: 0.9279 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5566 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.5901 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.8017 - val_footwear_output_acc: 0.6427 - val_emotion_output_acc: 0.6924\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 44.4357 - gender_output_loss: 0.6045 - image_quality_output_loss: 0.9661 - age_output_loss: 2.7362 - weight_output_loss: 2.8008 - bag_output_loss: 1.3225 - pose_output_loss: 1.2444 - footwear_output_loss: 1.0844 - emotion_output_loss: 2.7179 - gender_output_acc: 0.7271 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3938 - weight_output_acc: 0.6021 - bag_output_acc: 0.6271 - pose_output_acc: 0.7333 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.7104 - val_loss: 21.6692 - val_gender_output_loss: 0.4374 - val_image_quality_output_loss: 0.9298 - val_age_output_loss: 1.3628 - val_weight_output_loss: 0.9950 - val_bag_output_loss: 0.8495 - val_pose_output_loss: 0.5037 - val_footwear_output_loss: 0.8083 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.5832 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.8017 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.7028\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 24s 801ms/step - loss: 45.9202 - gender_output_loss: 0.5675 - image_quality_output_loss: 0.9232 - age_output_loss: 2.8159 - weight_output_loss: 2.6534 - bag_output_loss: 1.5349 - pose_output_loss: 1.1626 - footwear_output_loss: 1.1938 - emotion_output_loss: 3.0073 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4062 - weight_output_acc: 0.6271 - bag_output_acc: 0.5938 - pose_output_acc: 0.7229 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.6938 - val_loss: 22.0127 - val_gender_output_loss: 0.4398 - val_image_quality_output_loss: 0.9561 - val_age_output_loss: 1.4014 - val_weight_output_loss: 0.9992 - val_bag_output_loss: 0.8440 - val_pose_output_loss: 0.5575 - val_footwear_output_loss: 0.8023 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.8007 - val_image_quality_output_acc: 0.5325 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5945 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.7761 - val_footwear_output_acc: 0.6545 - val_emotion_output_acc: 0.7072\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_1577560074.json\n",
            "End of EPOCHS= 50  STEPS_PER_EPOCH= 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKT__bEKI_L",
        "colab_type": "code",
        "outputId": "b22d9f33-2a8f-4eb6-c86e-886c993d2534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "################# Tweaking weights #################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 6, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 5, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 4, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "###################### loss on weight change ################\n",
        "#[-0.0207 -0.0044  0.0069  0.0108 -0.0015 -0.0098 -0.0089 -0.0103]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577567987_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy_1577567748.json\n",
            "Returning new callback array with steps_per_epoch= 30 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100 patience= 25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "30/30 [==============================] - 36s 1s/step - loss: 51.4479 - gender_output_loss: 0.5275 - image_quality_output_loss: 0.9745 - age_output_loss: 2.5789 - weight_output_loss: 2.3144 - bag_output_loss: 1.3235 - pose_output_loss: 1.0347 - footwear_output_loss: 1.1220 - emotion_output_loss: 2.3929 - gender_output_acc: 0.7833 - image_quality_output_acc: 0.5271 - age_output_acc: 0.4313 - weight_output_acc: 0.6708 - bag_output_acc: 0.6375 - pose_output_acc: 0.7792 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.7229 - val_loss: 28.2428 - val_gender_output_loss: 0.4263 - val_image_quality_output_loss: 0.9211 - val_age_output_loss: 1.3731 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.8300 - val_pose_output_loss: 0.5239 - val_footwear_output_loss: 0.8148 - val_emotion_output_loss: 0.9086 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.5994 - val_bag_output_acc: 0.6373 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6496 - val_emotion_output_acc: 0.7077\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 28.24285, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577567987_model.001.h5\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 23s 770ms/step - loss: 54.5760 - gender_output_loss: 0.5129 - image_quality_output_loss: 0.9497 - age_output_loss: 2.8963 - weight_output_loss: 2.3470 - bag_output_loss: 1.5300 - pose_output_loss: 1.0733 - footwear_output_loss: 1.1700 - emotion_output_loss: 2.3495 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5333 - age_output_acc: 0.3917 - weight_output_acc: 0.6583 - bag_output_acc: 0.6313 - pose_output_acc: 0.7542 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7354 - val_loss: 29.4897 - val_gender_output_loss: 0.4464 - val_image_quality_output_loss: 0.9186 - val_age_output_loss: 1.4907 - val_weight_output_loss: 1.0205 - val_bag_output_loss: 0.8491 - val_pose_output_loss: 0.5721 - val_footwear_output_loss: 0.8411 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.7943 - val_image_quality_output_acc: 0.5566 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6358 - val_pose_output_acc: 0.7785 - val_footwear_output_acc: 0.6358 - val_emotion_output_acc: 0.7023\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 28.24285\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 24s 816ms/step - loss: 54.4556 - gender_output_loss: 0.5220 - image_quality_output_loss: 0.9509 - age_output_loss: 2.7236 - weight_output_loss: 2.4443 - bag_output_loss: 1.6265 - pose_output_loss: 0.9057 - footwear_output_loss: 1.0909 - emotion_output_loss: 2.6264 - gender_output_acc: 0.7708 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3833 - weight_output_acc: 0.6438 - bag_output_acc: 0.5896 - pose_output_acc: 0.7833 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7125 - val_loss: 28.9815 - val_gender_output_loss: 0.4478 - val_image_quality_output_loss: 0.9436 - val_age_output_loss: 1.3764 - val_weight_output_loss: 1.0079 - val_bag_output_loss: 0.8450 - val_pose_output_loss: 0.6136 - val_footwear_output_loss: 0.8099 - val_emotion_output_loss: 0.9465 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.5733 - val_bag_output_acc: 0.6299 - val_pose_output_acc: 0.7687 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.6629\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 28.24285\n",
            "Epoch 00003: val_loss did not improve from 28.24285\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 24s 812ms/step - loss: 56.9478 - gender_output_loss: 0.5684 - image_quality_output_loss: 0.9465 - age_output_loss: 2.7737 - weight_output_loss: 2.7032 - bag_output_loss: 1.5289 - pose_output_loss: 1.0872 - footwear_output_loss: 1.1284 - emotion_output_loss: 2.8619 - gender_output_acc: 0.7708 - image_quality_output_acc: 0.5271 - age_output_acc: 0.3750 - weight_output_acc: 0.6271 - bag_output_acc: 0.6250 - pose_output_acc: 0.7396 - footwear_output_acc: 0.6167 - emotion_output_acc: 0.6896 - val_loss: 33.6154 - val_gender_output_loss: 0.7012 - val_image_quality_output_loss: 0.9081 - val_age_output_loss: 1.6289 - val_weight_output_loss: 1.0597 - val_bag_output_loss: 1.1586 - val_pose_output_loss: 0.6480 - val_footwear_output_loss: 1.0153 - val_emotion_output_loss: 0.9462 - val_gender_output_acc: 0.7156 - val_image_quality_output_acc: 0.5571 - val_age_output_acc: 0.2913 - val_weight_output_acc: 0.5591 - val_bag_output_acc: 0.5714 - val_pose_output_acc: 0.7682 - val_footwear_output_acc: 0.5827 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 28.24285\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 25s 817ms/step - loss: 57.4498 - gender_output_loss: 0.5532 - image_quality_output_loss: 0.9281 - age_output_loss: 2.8331 - weight_output_loss: 2.4593 - bag_output_loss: 1.5164 - pose_output_loss: 1.1972 - footwear_output_loss: 1.2236 - emotion_output_loss: 2.9079 - gender_output_acc: 0.7708 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3938 - weight_output_acc: 0.6458 - bag_output_acc: 0.6021 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5750 - emotion_output_acc: 0.6854 - val_loss: 29.0298 - val_gender_output_loss: 0.4348 - val_image_quality_output_loss: 0.9822 - val_age_output_loss: 1.3911 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.8458 - val_pose_output_loss: 0.5282 - val_footwear_output_loss: 0.8781 - val_emotion_output_loss: 0.9487 - val_gender_output_acc: 0.8022 - val_image_quality_output_acc: 0.5290 - val_age_output_acc: 0.3706 - val_weight_output_acc: 0.5886 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7928 - val_footwear_output_acc: 0.6127 - val_emotion_output_acc: 0.6624\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 28.24285\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 24s 805ms/step - loss: 55.8547 - gender_output_loss: 0.5583 - image_quality_output_loss: 0.9592 - age_output_loss: 2.9042 - weight_output_loss: 2.2478 - bag_output_loss: 1.5896 - pose_output_loss: 1.0586 - footwear_output_loss: 1.1220 - emotion_output_loss: 2.6911 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5312 - age_output_acc: 0.4000 - weight_output_acc: 0.6500 - bag_output_acc: 0.6021 - pose_output_acc: 0.7937 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.6854 - val_loss: 30.1359 - val_gender_output_loss: 0.4646 - val_image_quality_output_loss: 1.0111 - val_age_output_loss: 1.4437 - val_weight_output_loss: 1.0162 - val_bag_output_loss: 0.8469 - val_pose_output_loss: 0.7445 - val_footwear_output_loss: 0.8253 - val_emotion_output_loss: 0.9361 - val_gender_output_acc: 0.7864 - val_image_quality_output_acc: 0.5276 - val_age_output_acc: 0.3617 - val_weight_output_acc: 0.5709 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.7151 - val_footwear_output_acc: 0.6309 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 28.24285\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 57.1129 - gender_output_loss: 0.5446 - image_quality_output_loss: 0.9059 - age_output_loss: 2.7887 - weight_output_loss: 2.8933 - bag_output_loss: 1.3521 - pose_output_loss: 1.1083 - footwear_output_loss: 1.0839 - emotion_output_loss: 3.0135 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4062 - weight_output_acc: 0.6354 - bag_output_acc: 0.6333 - pose_output_acc: 0.7729 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.6854 - val_loss: 28.9621 - val_gender_output_loss: 0.4239 - val_image_quality_output_loss: 1.0103 - val_age_output_loss: 1.4120 - val_weight_output_loss: 1.0004 - val_bag_output_loss: 0.8389 - val_pose_output_loss: 0.5023 - val_footwear_output_loss: 0.8339 - val_emotion_output_loss: 0.9663 - val_gender_output_acc: 0.8091 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.3642 - val_weight_output_acc: 0.5881 - val_bag_output_acc: 0.6309 - val_pose_output_acc: 0.8110 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6437\n",
            "30/30 [==============================]\n",
            "Epoch 00007: val_loss did not improve from 28.24285\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 24s 804ms/step - loss: 57.8012 - gender_output_loss: 0.5681 - image_quality_output_loss: 0.9257 - age_output_loss: 2.7754 - weight_output_loss: 2.8801 - bag_output_loss: 1.6128 - pose_output_loss: 1.0831 - footwear_output_loss: 1.1585 - emotion_output_loss: 2.8186 - gender_output_acc: 0.7312 - image_quality_output_acc: 0.5813 - age_output_acc: 0.3896 - weight_output_acc: 0.6187 - bag_output_acc: 0.6062 - pose_output_acc: 0.7688 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7021 - val_loss: 28.2543 - val_gender_output_loss: 0.4519 - val_image_quality_output_loss: 0.9193 - val_age_output_loss: 1.3650 - val_weight_output_loss: 0.9923 - val_bag_output_loss: 0.8406 - val_pose_output_loss: 0.5035 - val_footwear_output_loss: 0.8055 - val_emotion_output_loss: 0.9225 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.5950 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.8159 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.6826\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 28.24285\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 25s 824ms/step - loss: 56.3609 - gender_output_loss: 0.5272 - image_quality_output_loss: 0.9046 - age_output_loss: 2.7231 - weight_output_loss: 2.7199 - bag_output_loss: 1.3876 - pose_output_loss: 1.0028 - footwear_output_loss: 1.2055 - emotion_output_loss: 3.0031 - gender_output_acc: 0.7917 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4000 - weight_output_acc: 0.6146 - bag_output_acc: 0.6333 - pose_output_acc: 0.7792 - footwear_output_acc: 0.5979 - emotion_output_acc: 0.6938 - val_loss: 28.5432 - val_gender_output_loss: 0.4416 - val_image_quality_output_loss: 0.9549 - val_age_output_loss: 1.3903 - val_weight_output_loss: 1.0024 - val_bag_output_loss: 0.8371 - val_pose_output_loss: 0.5191 - val_footwear_output_loss: 0.8020 - val_emotion_output_loss: 0.9291 - val_gender_output_acc: 0.8071 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5891 - val_bag_output_acc: 0.6393 - val_pose_output_acc: 0.8081 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 28.24285\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 24s 810ms/step - loss: 59.7373 - gender_output_loss: 0.5401 - image_quality_output_loss: 0.9484 - age_output_loss: 2.8571 - weight_output_loss: 2.9398 - bag_output_loss: 1.6455 - pose_output_loss: 1.3172 - footwear_output_loss: 1.1137 - emotion_output_loss: 2.9080 - gender_output_acc: 0.7833 - image_quality_output_acc: 0.5417 - age_output_acc: 0.4083 - weight_output_acc: 0.6104 - bag_output_acc: 0.6125 - pose_output_acc: 0.7125 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.7042 - val_loss: 28.6038 - val_gender_output_loss: 0.4436 - val_image_quality_output_loss: 0.9405 - val_age_output_loss: 1.3771 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.8467 - val_pose_output_loss: 0.5662 - val_footwear_output_loss: 0.8165 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.7953 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6309 - val_pose_output_acc: 0.7756 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 28.24285\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 24s 805ms/step - loss: 58.0500 - gender_output_loss: 0.5365 - image_quality_output_loss: 0.8881 - age_output_loss: 2.8172 - weight_output_loss: 2.7994 - bag_output_loss: 1.5580 - pose_output_loss: 1.2143 - footwear_output_loss: 1.1393 - emotion_output_loss: 2.8700 - gender_output_acc: 0.7646 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4292 - weight_output_acc: 0.6500 - bag_output_acc: 0.5833 - pose_output_acc: 0.7417 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.6771 - val_loss: 29.8447 - val_gender_output_loss: 0.4416 - val_image_quality_output_loss: 0.9521 - val_age_output_loss: 1.3688 - val_weight_output_loss: 1.0573 - val_bag_output_loss: 0.8561 - val_pose_output_loss: 0.7842 - val_footwear_output_loss: 0.8427 - val_emotion_output_loss: 0.9174 - val_gender_output_acc: 0.7943 - val_image_quality_output_acc: 0.5507 - val_age_output_acc: 0.3873 - val_weight_output_acc: 0.5689 - val_bag_output_acc: 0.6211 - val_pose_output_acc: 0.6791 - val_footwear_output_acc: 0.6250 - val_emotion_output_acc: 0.6919\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 28.24285\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 24s 803ms/step - loss: 56.6401 - gender_output_loss: 0.5881 - image_quality_output_loss: 0.9166 - age_output_loss: 2.7236 - weight_output_loss: 2.8726 - bag_output_loss: 1.4704 - pose_output_loss: 1.3013 - footwear_output_loss: 1.2676 - emotion_output_loss: 2.4568 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4167 - weight_output_acc: 0.6146 - bag_output_acc: 0.6167 - pose_output_acc: 0.7188 - footwear_output_acc: 0.5604 - emotion_output_acc: 0.7146 - val_loss: 28.5351 - val_gender_output_loss: 0.4659 - val_image_quality_output_loss: 0.9172 - val_age_output_loss: 1.3674 - val_weight_output_loss: 0.9885 - val_bag_output_loss: 0.8654 - val_pose_output_loss: 0.5308 - val_footwear_output_loss: 0.8316 - val_emotion_output_loss: 0.9012 - val_gender_output_acc: 0.7899 - val_image_quality_output_acc: 0.5502 - val_age_output_acc: 0.4119 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.6240 - val_pose_output_acc: 0.7859 - val_footwear_output_acc: 0.6368 - val_emotion_output_acc: 0.7018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 28.24285\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 24s 801ms/step - loss: 55.8235 - gender_output_loss: 0.5612 - image_quality_output_loss: 1.0046 - age_output_loss: 2.5668 - weight_output_loss: 2.5674 - bag_output_loss: 1.5363 - pose_output_loss: 1.3593 - footwear_output_loss: 1.1887 - emotion_output_loss: 2.6242 - gender_output_acc: 0.7542 - image_quality_output_acc: 0.4958 - age_output_acc: 0.4458 - weight_output_acc: 0.6354 - bag_output_acc: 0.5875 - pose_output_acc: 0.7125 - footwear_output_acc: 0.6000 - emotion_output_acc: 0.7104 - val_loss: 28.7367 - val_gender_output_loss: 0.4479 - val_image_quality_output_loss: 0.9156 - val_age_output_loss: 1.3787 - val_weight_output_loss: 0.9880 - val_bag_output_loss: 0.8459 - val_pose_output_loss: 0.5828 - val_footwear_output_loss: 0.8252 - val_emotion_output_loss: 0.9233 - val_gender_output_acc: 0.7987 - val_image_quality_output_acc: 0.5487 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6339 - val_pose_output_acc: 0.7726 - val_footwear_output_acc: 0.6506 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 28.24285\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 24s 807ms/step - loss: 53.2613 - gender_output_loss: 0.5291 - image_quality_output_loss: 0.9285 - age_output_loss: 2.8843 - weight_output_loss: 2.6447 - bag_output_loss: 1.3317 - pose_output_loss: 1.1526 - footwear_output_loss: 1.1436 - emotion_output_loss: 2.0108 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5792 - age_output_acc: 0.3750 - weight_output_acc: 0.6021 - bag_output_acc: 0.6333 - pose_output_acc: 0.7417 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7625 - val_loss: 29.2130 - val_gender_output_loss: 0.4622 - val_image_quality_output_loss: 1.0213 - val_age_output_loss: 1.4036 - val_weight_output_loss: 0.9991 - val_bag_output_loss: 0.8615 - val_pose_output_loss: 0.5492 - val_footwear_output_loss: 0.8322 - val_emotion_output_loss: 0.9435 - val_gender_output_acc: 0.7963 - val_image_quality_output_acc: 0.5143 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.5851 - val_bag_output_acc: 0.6309 - val_pose_output_acc: 0.7849 - val_footwear_output_acc: 0.6491 - val_emotion_output_acc: 0.6634\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 28.24285\n",
            "Epoch 15/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 55.3248 - gender_output_loss: 0.5738 - image_quality_output_loss: 0.9362 - age_output_loss: 2.7227 - weight_output_loss: 2.5666 - bag_output_loss: 1.5482 - pose_output_loss: 1.1655 - footwear_output_loss: 1.1046 - emotion_output_loss: 2.5570 - gender_output_acc: 0.7737 - image_quality_output_acc: 0.5754 - age_output_acc: 0.4009 - weight_output_acc: 0.6272 - bag_output_acc: 0.5991 - pose_output_acc: 0.7629 - footwear_output_acc: 0.6401 - emotion_output_acc: 0.7263\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 24s 809ms/step - loss: 55.0043 - gender_output_loss: 0.5717 - image_quality_output_loss: 0.9342 - age_output_loss: 2.7015 - weight_output_loss: 2.5857 - bag_output_loss: 1.5243 - pose_output_loss: 1.1748 - footwear_output_loss: 1.0976 - emotion_output_loss: 2.5238 - gender_output_acc: 0.7750 - image_quality_output_acc: 0.5771 - age_output_acc: 0.4042 - weight_output_acc: 0.6271 - bag_output_acc: 0.6042 - pose_output_acc: 0.7604 - footwear_output_acc: 0.6458 - emotion_output_acc: 0.7292 - val_loss: 28.3835 - val_gender_output_loss: 0.4576 - val_image_quality_output_loss: 0.9505 - val_age_output_loss: 1.3734 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.8341 - val_pose_output_loss: 0.5047 - val_footwear_output_loss: 0.8203 - val_emotion_output_loss: 0.9240 - val_gender_output_acc: 0.7849 - val_image_quality_output_acc: 0.5512 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6078 - val_bag_output_acc: 0.6388 - val_pose_output_acc: 0.8100 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6831\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 28.24285\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 24s 805ms/step - loss: 55.0323 - gender_output_loss: 0.5109 - image_quality_output_loss: 0.9225 - age_output_loss: 2.8554 - weight_output_loss: 2.4261 - bag_output_loss: 1.4068 - pose_output_loss: 1.1579 - footwear_output_loss: 1.1561 - emotion_output_loss: 2.5614 - gender_output_acc: 0.7875 - image_quality_output_acc: 0.5833 - age_output_acc: 0.3833 - weight_output_acc: 0.6187 - bag_output_acc: 0.5979 - pose_output_acc: 0.7604 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7188 - val_loss: 28.2372 - val_gender_output_loss: 0.4291 - val_image_quality_output_loss: 0.9476 - val_age_output_loss: 1.3571 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8393 - val_pose_output_loss: 0.5031 - val_footwear_output_loss: 0.8123 - val_emotion_output_loss: 0.9263 - val_gender_output_acc: 0.7987 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.8105 - val_footwear_output_acc: 0.6501 - val_emotion_output_acc: 0.6688\n",
            "\n",
            "Epoch 00016: val_loss improved from 28.24285 to 28.23721, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577567987_model.016.h5\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 54.0007 - gender_output_loss: 0.5492 - image_quality_output_loss: 0.9040 - age_output_loss: 2.8035 - weight_output_loss: 2.3742 - bag_output_loss: 1.3595 - pose_output_loss: 1.0246 - footwear_output_loss: 1.1805 - emotion_output_loss: 2.5785 - gender_output_acc: 0.7396 - image_quality_output_acc: 0.5667 - age_output_acc: 0.4042 - weight_output_acc: 0.6375 - bag_output_acc: 0.6271 - pose_output_acc: 0.7521 - footwear_output_acc: 0.6146 - emotion_output_acc: 0.7167 - val_loss: 28.1785 - val_gender_output_loss: 0.4363 - val_image_quality_output_loss: 0.9379 - val_age_output_loss: 1.3633 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8411 - val_pose_output_loss: 0.4987 - val_footwear_output_loss: 0.8114 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.8046 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6422 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00017: val_loss improved from 28.23721 to 28.17847, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577567987_model.017.h5\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 24s 798ms/step - loss: 55.1758 - gender_output_loss: 0.5370 - image_quality_output_loss: 0.9067 - age_output_loss: 2.7536 - weight_output_loss: 2.5126 - bag_output_loss: 1.3731 - pose_output_loss: 1.1008 - footwear_output_loss: 1.2097 - emotion_output_loss: 2.7257 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4104 - weight_output_acc: 0.6271 - bag_output_acc: 0.6083 - pose_output_acc: 0.7396 - footwear_output_acc: 0.5771 - emotion_output_acc: 0.7063 - val_loss: 28.0466 - val_gender_output_loss: 0.4226 - val_image_quality_output_loss: 0.9166 - val_age_output_loss: 1.3494 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.8323 - val_pose_output_loss: 0.5131 - val_footwear_output_loss: 0.8233 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.8027 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.4094 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6412 - val_pose_output_acc: 0.7982 - val_footwear_output_acc: 0.6491 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00018: val_loss improved from 28.17847 to 28.04659, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577567987_model.018.h5\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 55.7980 - gender_output_loss: 0.5315 - image_quality_output_loss: 0.9212 - age_output_loss: 2.7424 - weight_output_loss: 2.7221 - bag_output_loss: 1.5042 - pose_output_loss: 1.2053 - footwear_output_loss: 1.2021 - emotion_output_loss: 2.4757 - gender_output_acc: 0.7792 - image_quality_output_acc: 0.5833 - age_output_acc: 0.3604 - weight_output_acc: 0.6396 - bag_output_acc: 0.6062 - pose_output_acc: 0.7458 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7292 - val_loss: 29.0899 - val_gender_output_loss: 0.5135 - val_image_quality_output_loss: 0.9596 - val_age_output_loss: 1.3887 - val_weight_output_loss: 1.0054 - val_bag_output_loss: 0.8861 - val_pose_output_loss: 0.5398 - val_footwear_output_loss: 0.8173 - val_emotion_output_loss: 0.9295 - val_gender_output_acc: 0.7505 - val_image_quality_output_acc: 0.5394 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5925 - val_bag_output_acc: 0.6009 - val_pose_output_acc: 0.7864 - val_footwear_output_acc: 0.6580 - val_emotion_output_acc: 0.6777\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 28.04659\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 24s 801ms/step - loss: 59.6441 - gender_output_loss: 0.5318 - image_quality_output_loss: 0.9280 - age_output_loss: 2.9201 - weight_output_loss: 2.5608 - bag_output_loss: 1.5682 - pose_output_loss: 1.3066 - footwear_output_loss: 1.1998 - emotion_output_loss: 3.1088 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5667 - age_output_acc: 0.3417 - weight_output_acc: 0.6438 - bag_output_acc: 0.6208 - pose_output_acc: 0.7208 - footwear_output_acc: 0.5938 - emotion_output_acc: 0.6917 - val_loss: 29.5943 - val_gender_output_loss: 0.4718 - val_image_quality_output_loss: 1.0915 - val_age_output_loss: 1.3880 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.8671 - val_pose_output_loss: 0.5692 - val_footwear_output_loss: 0.8334 - val_emotion_output_loss: 1.0001 - val_gender_output_acc: 0.7707 - val_image_quality_output_acc: 0.4498 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.6102 - val_pose_output_acc: 0.7805 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6363\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 28.04659\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 55.8682 - gender_output_loss: 0.6137 - image_quality_output_loss: 0.9576 - age_output_loss: 2.8980 - weight_output_loss: 2.3642 - bag_output_loss: 1.6284 - pose_output_loss: 1.0990 - footwear_output_loss: 1.1232 - emotion_output_loss: 2.4980 - gender_output_acc: 0.7333 - image_quality_output_acc: 0.5312 - age_output_acc: 0.3792 - weight_output_acc: 0.6687 - bag_output_acc: 0.5938 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.7208 - val_loss: 29.5652 - val_gender_output_loss: 0.5544 - val_image_quality_output_loss: 0.9526 - val_age_output_loss: 1.3916 - val_weight_output_loss: 1.0021 - val_bag_output_loss: 0.8717 - val_pose_output_loss: 0.6267 - val_footwear_output_loss: 0.8564 - val_emotion_output_loss: 0.9204 - val_gender_output_acc: 0.7352 - val_image_quality_output_acc: 0.5335 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7436 - val_footwear_output_acc: 0.5915 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 28.04659\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 24s 808ms/step - loss: 52.1540 - gender_output_loss: 0.5548 - image_quality_output_loss: 0.9486 - age_output_loss: 2.7043 - weight_output_loss: 2.4174 - bag_output_loss: 1.2742 - pose_output_loss: 1.1995 - footwear_output_loss: 1.1083 - emotion_output_loss: 2.2111 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3771 - weight_output_acc: 0.6521 - bag_output_acc: 0.6354 - pose_output_acc: 0.7146 - footwear_output_acc: 0.6271 - emotion_output_acc: 0.7458 - val_loss: 28.1868 - val_gender_output_loss: 0.4390 - val_image_quality_output_loss: 0.9238 - val_age_output_loss: 1.3589 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.8377 - val_pose_output_loss: 0.5208 - val_footwear_output_loss: 0.8182 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.7982 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.7968 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 28.04659\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 53.8555 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.9531 - age_output_loss: 2.6635 - weight_output_loss: 2.3094 - bag_output_loss: 1.4798 - pose_output_loss: 1.0896 - footwear_output_loss: 1.2135 - emotion_output_loss: 2.5288 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5292 - age_output_acc: 0.4167 - weight_output_acc: 0.6375 - bag_output_acc: 0.6292 - pose_output_acc: 0.7563 - footwear_output_acc: 0.6021 - emotion_output_acc: 0.7333 - val_loss: 29.4639 - val_gender_output_loss: 0.4687 - val_image_quality_output_loss: 1.0244 - val_age_output_loss: 1.3967 - val_weight_output_loss: 1.0198 - val_bag_output_loss: 0.8678 - val_pose_output_loss: 0.5459 - val_footwear_output_loss: 0.8642 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.7835 - val_image_quality_output_acc: 0.5039 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.5655 - val_bag_output_acc: 0.6299 - val_pose_output_acc: 0.7938 - val_footwear_output_acc: 0.6289 - val_emotion_output_acc: 0.6393\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 28.04659\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 22s 749ms/step - loss: 55.4676 - gender_output_loss: 0.5037 - image_quality_output_loss: 0.8927 - age_output_loss: 2.6787 - weight_output_loss: 2.6273 - bag_output_loss: 1.3489 - pose_output_loss: 1.0157 - footwear_output_loss: 1.1603 - emotion_output_loss: 3.0127 - gender_output_acc: 0.7937 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4417 - weight_output_acc: 0.6229 - bag_output_acc: 0.6604 - pose_output_acc: 0.7896 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.6833 - val_loss: 29.2285 - val_gender_output_loss: 0.4762 - val_image_quality_output_loss: 1.0981 - val_age_output_loss: 1.3915 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.8474 - val_pose_output_loss: 0.5012 - val_footwear_output_loss: 0.8156 - val_emotion_output_loss: 1.0049 - val_gender_output_acc: 0.7751 - val_image_quality_output_acc: 0.4744 - val_age_output_acc: 0.3893 - val_weight_output_acc: 0.5925 - val_bag_output_acc: 0.6275 - val_pose_output_acc: 0.8091 - val_footwear_output_acc: 0.6496 - val_emotion_output_acc: 0.6058\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 28.04659\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 28.04659\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 27s 894ms/step - loss: 54.6776 - gender_output_loss: 0.5623 - image_quality_output_loss: 0.9319 - age_output_loss: 2.8447 - weight_output_loss: 2.4767 - bag_output_loss: 1.4914 - pose_output_loss: 1.0747 - footwear_output_loss: 1.1154 - emotion_output_loss: 2.4379 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3771 - weight_output_acc: 0.6479 - bag_output_acc: 0.6313 - pose_output_acc: 0.7875 - footwear_output_acc: 0.6292 - emotion_output_acc: 0.7271 - val_loss: 28.2667 - val_gender_output_loss: 0.4236 - val_image_quality_output_loss: 0.9353 - val_age_output_loss: 1.3622 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.8591 - val_pose_output_loss: 0.5067 - val_footwear_output_loss: 0.8016 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.8031 - val_image_quality_output_acc: 0.5399 - val_age_output_acc: 0.4021 - val_weight_output_acc: 0.6078 - val_bag_output_acc: 0.6211 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.6841\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 28.04659\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 57.8772 - gender_output_loss: 0.5394 - image_quality_output_loss: 0.9409 - age_output_loss: 2.7799 - weight_output_loss: 3.0838 - bag_output_loss: 1.5701 - pose_output_loss: 1.0134 - footwear_output_loss: 1.1671 - emotion_output_loss: 2.7981 - gender_output_acc: 0.7833 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3875 - weight_output_acc: 0.6167 - bag_output_acc: 0.6146 - pose_output_acc: 0.7750 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.7125 - val_loss: 29.2717 - val_gender_output_loss: 0.4209 - val_image_quality_output_loss: 1.0028 - val_age_output_loss: 1.3837 - val_weight_output_loss: 1.0503 - val_bag_output_loss: 0.8487 - val_pose_output_loss: 0.5400 - val_footwear_output_loss: 0.8124 - val_emotion_output_loss: 1.0243 - val_gender_output_acc: 0.8076 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.5689 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.8007 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.5906\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 28.04659\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 24s 795ms/step - loss: 53.8251 - gender_output_loss: 0.5743 - image_quality_output_loss: 0.9054 - age_output_loss: 2.6317 - weight_output_loss: 2.3724 - bag_output_loss: 1.4452 - pose_output_loss: 1.2529 - footwear_output_loss: 1.1136 - emotion_output_loss: 2.5113 - gender_output_acc: 0.7437 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4146 - weight_output_acc: 0.6542 - bag_output_acc: 0.6250 - pose_output_acc: 0.7375 - footwear_output_acc: 0.6187 - emotion_output_acc: 0.7271 - val_loss: 28.8536 - val_gender_output_loss: 0.4320 - val_image_quality_output_loss: 0.9554 - val_age_output_loss: 1.3877 - val_weight_output_loss: 0.9880 - val_bag_output_loss: 0.8554 - val_pose_output_loss: 0.5684 - val_footwear_output_loss: 0.8335 - val_emotion_output_loss: 0.9205 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7707 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 28.04659\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 56.4997 - gender_output_loss: 0.5550 - image_quality_output_loss: 0.9014 - age_output_loss: 2.7958 - weight_output_loss: 2.8065 - bag_output_loss: 1.4136 - pose_output_loss: 1.2361 - footwear_output_loss: 1.2105 - emotion_output_loss: 2.5787 - gender_output_acc: 0.7979 - image_quality_output_acc: 0.5875 - age_output_acc: 0.4042 - weight_output_acc: 0.6146 - bag_output_acc: 0.6083 - pose_output_acc: 0.7292 - footwear_output_acc: 0.5958 - emotion_output_acc: 0.7125 - val_loss: 28.9915 - val_gender_output_loss: 0.4603 - val_image_quality_output_loss: 0.9542 - val_age_output_loss: 1.3815 - val_weight_output_loss: 1.0184 - val_bag_output_loss: 0.8458 - val_pose_output_loss: 0.5753 - val_footwear_output_loss: 0.8426 - val_emotion_output_loss: 0.9233 - val_gender_output_acc: 0.7795 - val_image_quality_output_acc: 0.5527 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.5782 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.7746 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.6777\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 28.04659\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 24s 802ms/step - loss: 58.1063 - gender_output_loss: 0.5702 - image_quality_output_loss: 0.9688 - age_output_loss: 2.9633 - weight_output_loss: 2.7100 - bag_output_loss: 1.4430 - pose_output_loss: 1.1996 - footwear_output_loss: 1.2307 - emotion_output_loss: 2.7392 - gender_output_acc: 0.7417 - image_quality_output_acc: 0.5375 - age_output_acc: 0.3604 - weight_output_acc: 0.6000 - bag_output_acc: 0.6396 - pose_output_acc: 0.6938 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7063 - val_loss: 28.7852 - val_gender_output_loss: 0.5458 - val_image_quality_output_loss: 0.9143 - val_age_output_loss: 1.3734 - val_weight_output_loss: 1.0011 - val_bag_output_loss: 0.8461 - val_pose_output_loss: 0.5586 - val_footwear_output_loss: 0.8362 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.7530 - val_image_quality_output_acc: 0.5561 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.5920 - val_bag_output_acc: 0.6230 - val_pose_output_acc: 0.7746 - val_footwear_output_acc: 0.6216 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 28.04659\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 24s 801ms/step - loss: 54.7758 - gender_output_loss: 0.5904 - image_quality_output_loss: 0.9598 - age_output_loss: 2.6697 - weight_output_loss: 2.1133 - bag_output_loss: 1.5360 - pose_output_loss: 1.2533 - footwear_output_loss: 1.1934 - emotion_output_loss: 2.6561 - gender_output_acc: 0.7625 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4354 - weight_output_acc: 0.6729 - bag_output_acc: 0.6229 - pose_output_acc: 0.7125 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7083 - val_loss: 28.9797 - val_gender_output_loss: 0.4633 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.3917 - val_weight_output_loss: 0.9929 - val_bag_output_loss: 0.8647 - val_pose_output_loss: 0.5713 - val_footwear_output_loss: 0.8187 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.7781 - val_image_quality_output_acc: 0.5320 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.5999 - val_bag_output_acc: 0.6161 - val_pose_output_acc: 0.7771 - val_footwear_output_acc: 0.6422 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 28.04659\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 53.8868 - gender_output_loss: 0.5094 - image_quality_output_loss: 0.9263 - age_output_loss: 2.4787 - weight_output_loss: 2.7596 - bag_output_loss: 1.5668 - pose_output_loss: 1.1642 - footwear_output_loss: 1.1546 - emotion_output_loss: 2.3823 - gender_output_acc: 0.7979 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4271 - weight_output_acc: 0.6396 - bag_output_acc: 0.6062 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7354 - val_loss: 28.3368 - val_gender_output_loss: 0.4311 - val_image_quality_output_loss: 0.9553 - val_age_output_loss: 1.3685 - val_weight_output_loss: 0.9990 - val_bag_output_loss: 0.8423 - val_pose_output_loss: 0.5061 - val_footwear_output_loss: 0.8140 - val_emotion_output_loss: 0.9095 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5404 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.5920 - val_bag_output_acc: 0.6309 - val_pose_output_acc: 0.8081 - val_footwear_output_acc: 0.6393 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 28.04659\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 24s 795ms/step - loss: 55.5928 - gender_output_loss: 0.4977 - image_quality_output_loss: 0.9145 - age_output_loss: 2.9470 - weight_output_loss: 2.5163 - bag_output_loss: 1.3352 - pose_output_loss: 1.1853 - footwear_output_loss: 1.2272 - emotion_output_loss: 2.4964 - gender_output_acc: 0.8146 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3604 - weight_output_acc: 0.6562 - bag_output_acc: 0.6521 - pose_output_acc: 0.7583 - footwear_output_acc: 0.5792 - emotion_output_acc: 0.7167 - val_loss: 28.2785 - val_gender_output_loss: 0.4560 - val_image_quality_output_loss: 0.9402 - val_age_output_loss: 1.3698 - val_weight_output_loss: 0.9937 - val_bag_output_loss: 0.8413 - val_pose_output_loss: 0.5020 - val_footwear_output_loss: 0.8011 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.7933 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.3789 - val_weight_output_acc: 0.6043 - val_bag_output_acc: 0.6348 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6457 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 28.04659\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 24s 795ms/step - loss: 57.3832 - gender_output_loss: 0.5502 - image_quality_output_loss: 0.9144 - age_output_loss: 2.9578 - weight_output_loss: 2.6897 - bag_output_loss: 1.5098 - pose_output_loss: 1.0186 - footwear_output_loss: 1.0929 - emotion_output_loss: 2.8545 - gender_output_acc: 0.7500 - image_quality_output_acc: 0.5729 - age_output_acc: 0.3667 - weight_output_acc: 0.6313 - bag_output_acc: 0.6146 - pose_output_acc: 0.8021 - footwear_output_acc: 0.6458 - emotion_output_acc: 0.6833 - val_loss: 28.2979 - val_gender_output_loss: 0.4342 - val_image_quality_output_loss: 0.9292 - val_age_output_loss: 1.3788 - val_weight_output_loss: 1.0159 - val_bag_output_loss: 0.8348 - val_pose_output_loss: 0.4986 - val_footwear_output_loss: 0.7936 - val_emotion_output_loss: 0.9207 - val_gender_output_acc: 0.8007 - val_image_quality_output_acc: 0.5571 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.5773 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6535 - val_emotion_output_acc: 0.6698\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 28.04659\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 54.4428 - gender_output_loss: 0.5193 - image_quality_output_loss: 0.9512 - age_output_loss: 2.6555 - weight_output_loss: 2.4960 - bag_output_loss: 1.3848 - pose_output_loss: 1.1496 - footwear_output_loss: 1.1725 - emotion_output_loss: 2.6610 - gender_output_acc: 0.7896 - image_quality_output_acc: 0.5229 - age_output_acc: 0.4042 - weight_output_acc: 0.6708 - bag_output_acc: 0.6583 - pose_output_acc: 0.7771 - footwear_output_acc: 0.6021 - emotion_output_acc: 0.7188 - val_loss: 28.8079 - val_gender_output_loss: 0.4742 - val_image_quality_output_loss: 0.9681 - val_age_output_loss: 1.3869 - val_weight_output_loss: 1.0033 - val_bag_output_loss: 0.8531 - val_pose_output_loss: 0.5140 - val_footwear_output_loss: 0.8121 - val_emotion_output_loss: 0.9494 - val_gender_output_acc: 0.7879 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.8017 - val_footwear_output_acc: 0.6442 - val_emotion_output_acc: 0.6491\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 28.04659\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 24s 794ms/step - loss: 56.1066 - gender_output_loss: 0.5734 - image_quality_output_loss: 0.8976 - age_output_loss: 2.5978 - weight_output_loss: 2.5316 - bag_output_loss: 1.4836 - pose_output_loss: 1.1412 - footwear_output_loss: 1.1907 - emotion_output_loss: 3.0032 - gender_output_acc: 0.7521 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4229 - weight_output_acc: 0.6500 - bag_output_acc: 0.5979 - pose_output_acc: 0.7250 - footwear_output_acc: 0.5833 - emotion_output_acc: 0.6771 - val_loss: 28.5923 - val_gender_output_loss: 0.4402 - val_image_quality_output_loss: 0.9230 - val_age_output_loss: 1.3527 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.8358 - val_pose_output_loss: 0.5617 - val_footwear_output_loss: 0.8502 - val_emotion_output_loss: 0.9412 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5522 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.7761 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.6531\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 28.04659\n",
            "Epoch 36/100\n",
            "29/30 [============================>.] - ETA: 0s - loss: 55.4847 - gender_output_loss: 0.5837 - image_quality_output_loss: 0.9479 - age_output_loss: 2.8586 - weight_output_loss: 2.3767 - bag_output_loss: 1.3198 - pose_output_loss: 1.2111 - footwear_output_loss: 1.1724 - emotion_output_loss: 2.6954 - gender_output_acc: 0.7435 - image_quality_output_acc: 0.5366 - age_output_acc: 0.3966 - weight_output_acc: 0.6422 - bag_output_acc: 0.6185 - pose_output_acc: 0.7220 - footwear_output_acc: 0.5927 - emotion_output_acc: 0.7091\n",
            "Epoch 00035: val_loss did not improve from 28.04659\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 24s 804ms/step - loss: 55.2551 - gender_output_loss: 0.5785 - image_quality_output_loss: 0.9480 - age_output_loss: 2.8702 - weight_output_loss: 2.3629 - bag_output_loss: 1.3108 - pose_output_loss: 1.1931 - footwear_output_loss: 1.1758 - emotion_output_loss: 2.6593 - gender_output_acc: 0.7500 - image_quality_output_acc: 0.5396 - age_output_acc: 0.4000 - weight_output_acc: 0.6458 - bag_output_acc: 0.6208 - pose_output_acc: 0.7229 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7125 - val_loss: 29.1674 - val_gender_output_loss: 0.4779 - val_image_quality_output_loss: 0.9471 - val_age_output_loss: 1.3890 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8692 - val_pose_output_loss: 0.5647 - val_footwear_output_loss: 0.8879 - val_emotion_output_loss: 0.9100 - val_gender_output_acc: 0.7741 - val_image_quality_output_acc: 0.5512 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.6014 - val_bag_output_acc: 0.6137 - val_pose_output_acc: 0.7840 - val_footwear_output_acc: 0.6029 - val_emotion_output_acc: 0.6909\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 28.04659\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 24s 797ms/step - loss: 54.8011 - gender_output_loss: 0.5733 - image_quality_output_loss: 0.8963 - age_output_loss: 2.9447 - weight_output_loss: 2.3687 - bag_output_loss: 1.3799 - pose_output_loss: 1.1298 - footwear_output_loss: 1.1916 - emotion_output_loss: 2.4189 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5875 - age_output_acc: 0.3854 - weight_output_acc: 0.6354 - bag_output_acc: 0.6562 - pose_output_acc: 0.7312 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7104 - val_loss: 29.9505 - val_gender_output_loss: 0.5108 - val_image_quality_output_loss: 0.9169 - val_age_output_loss: 1.4439 - val_weight_output_loss: 0.9976 - val_bag_output_loss: 0.8864 - val_pose_output_loss: 0.5569 - val_footwear_output_loss: 0.9666 - val_emotion_output_loss: 0.9212 - val_gender_output_acc: 0.7697 - val_image_quality_output_acc: 0.5659 - val_age_output_acc: 0.3219 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6166 - val_pose_output_acc: 0.7854 - val_footwear_output_acc: 0.5714 - val_emotion_output_acc: 0.6599\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 28.04659\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 59.2317 - gender_output_loss: 0.5541 - image_quality_output_loss: 0.8861 - age_output_loss: 3.1215 - weight_output_loss: 2.7700 - bag_output_loss: 1.5349 - pose_output_loss: 1.2314 - footwear_output_loss: 1.2303 - emotion_output_loss: 2.6407 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5958 - age_output_acc: 0.3479 - weight_output_acc: 0.6375 - bag_output_acc: 0.5729 - pose_output_acc: 0.7500 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7208 - val_loss: 29.2728 - val_gender_output_loss: 0.4546 - val_image_quality_output_loss: 0.9512 - val_age_output_loss: 1.4214 - val_weight_output_loss: 1.0228 - val_bag_output_loss: 0.8342 - val_pose_output_loss: 0.5901 - val_footwear_output_loss: 0.8646 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5512 - val_age_output_acc: 0.3578 - val_weight_output_acc: 0.5704 - val_bag_output_acc: 0.6358 - val_pose_output_acc: 0.7751 - val_footwear_output_acc: 0.6235 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 28.04659\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 24s 803ms/step - loss: 54.8242 - gender_output_loss: 0.5622 - image_quality_output_loss: 0.9501 - age_output_loss: 2.6846 - weight_output_loss: 2.6374 - bag_output_loss: 1.5452 - pose_output_loss: 1.0513 - footwear_output_loss: 1.1764 - emotion_output_loss: 2.4788 - gender_output_acc: 0.7563 - image_quality_output_acc: 0.5354 - age_output_acc: 0.3812 - weight_output_acc: 0.6417 - bag_output_acc: 0.6250 - pose_output_acc: 0.7688 - footwear_output_acc: 0.6146 - emotion_output_acc: 0.7292 - val_loss: 28.9335 - val_gender_output_loss: 0.4305 - val_image_quality_output_loss: 1.0234 - val_age_output_loss: 1.4072 - val_weight_output_loss: 1.0044 - val_bag_output_loss: 0.8425 - val_pose_output_loss: 0.5341 - val_footwear_output_loss: 0.8361 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.8022 - val_image_quality_output_acc: 0.5325 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.5886 - val_bag_output_acc: 0.6284 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6348 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 28.04659\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 57.4906 - gender_output_loss: 0.5370 - image_quality_output_loss: 0.9335 - age_output_loss: 2.7183 - weight_output_loss: 2.7398 - bag_output_loss: 1.5278 - pose_output_loss: 1.0981 - footwear_output_loss: 1.1675 - emotion_output_loss: 3.0228 - gender_output_acc: 0.7583 - image_quality_output_acc: 0.5479 - age_output_acc: 0.4229 - weight_output_acc: 0.6208 - bag_output_acc: 0.5938 - pose_output_acc: 0.7500 - footwear_output_acc: 0.6104 - emotion_output_acc: 0.6771 - val_loss: 28.6287 - val_gender_output_loss: 0.4269 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.3775 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.8394 - val_pose_output_loss: 0.5316 - val_footwear_output_loss: 0.8080 - val_emotion_output_loss: 0.9458 - val_gender_output_acc: 0.8125 - val_image_quality_output_acc: 0.5453 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5955 - val_bag_output_acc: 0.6324 - val_pose_output_acc: 0.7928 - val_footwear_output_acc: 0.6481 - val_emotion_output_acc: 0.6526\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 28.04659\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 24s 796ms/step - loss: 55.6478 - gender_output_loss: 0.5114 - image_quality_output_loss: 0.9584 - age_output_loss: 2.7475 - weight_output_loss: 2.5409 - bag_output_loss: 1.4428 - pose_output_loss: 1.0083 - footwear_output_loss: 1.1662 - emotion_output_loss: 2.8653 - gender_output_acc: 0.7854 - image_quality_output_acc: 0.5583 - age_output_acc: 0.4521 - weight_output_acc: 0.6271 - bag_output_acc: 0.6271 - pose_output_acc: 0.7896 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.6938 - val_loss: 28.5930 - val_gender_output_loss: 0.4298 - val_image_quality_output_loss: 0.9663 - val_age_output_loss: 1.3822 - val_weight_output_loss: 1.0073 - val_bag_output_loss: 0.8417 - val_pose_output_loss: 0.5181 - val_footwear_output_loss: 0.8051 - val_emotion_output_loss: 0.9393 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3701 - val_weight_output_acc: 0.5832 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.7953 - val_footwear_output_acc: 0.6550 - val_emotion_output_acc: 0.6585\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 28.04659\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 24s 801ms/step - loss: 55.5082 - gender_output_loss: 0.5268 - image_quality_output_loss: 0.9194 - age_output_loss: 2.6353 - weight_output_loss: 2.4818 - bag_output_loss: 1.5791 - pose_output_loss: 1.1226 - footwear_output_loss: 1.1800 - emotion_output_loss: 2.7565 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4021 - weight_output_acc: 0.6458 - bag_output_acc: 0.5854 - pose_output_acc: 0.7521 - footwear_output_acc: 0.6125 - emotion_output_acc: 0.7063 - val_loss: 28.3610 - val_gender_output_loss: 0.4260 - val_image_quality_output_loss: 0.9431 - val_age_output_loss: 1.3638 - val_weight_output_loss: 0.9902 - val_bag_output_loss: 0.8344 - val_pose_output_loss: 0.5226 - val_footwear_output_loss: 0.8160 - val_emotion_output_loss: 0.9291 - val_gender_output_acc: 0.8002 - val_image_quality_output_acc: 0.5522 - val_age_output_acc: 0.3893 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.7982 - val_footwear_output_acc: 0.6324 - val_emotion_output_acc: 0.6604\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 28.04659\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 24s 813ms/step - loss: 56.9494 - gender_output_loss: 0.5162 - image_quality_output_loss: 0.9616 - age_output_loss: 2.7273 - weight_output_loss: 2.8908 - bag_output_loss: 1.4391 - pose_output_loss: 1.0473 - footwear_output_loss: 1.1731 - emotion_output_loss: 2.9134 - gender_output_acc: 0.8042 - image_quality_output_acc: 0.5479 - age_output_acc: 0.3667 - weight_output_acc: 0.6187 - bag_output_acc: 0.6229 - pose_output_acc: 0.7583 - footwear_output_acc: 0.6083 - emotion_output_acc: 0.6938 - val_loss: 28.4447 - val_gender_output_loss: 0.4480 - val_image_quality_output_loss: 0.9332 - val_age_output_loss: 1.3750 - val_weight_output_loss: 1.0009 - val_bag_output_loss: 0.8477 - val_pose_output_loss: 0.5114 - val_footwear_output_loss: 0.8169 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.7972 - val_image_quality_output_acc: 0.5482 - val_age_output_acc: 0.3848 - val_weight_output_acc: 0.5881 - val_bag_output_acc: 0.6314 - val_pose_output_acc: 0.7972 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 28.04659\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy_1577567748.json\n",
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KV66SLXgl7",
        "colab_type": "code",
        "outputId": "b0159c34-d091-4871-8533-4a982b8d4841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=15)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy_1577567748.json\n",
            "Returning new callback array with steps_per_epoch= 721 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100 patience= 15\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy_1577567748.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy_1577567748.json1577569955_backup\n",
            "Epoch 1/100\n",
            "721/721 [==============================] - 245s 340ms/step - loss: 21.4726 - gender_output_loss: 0.4474 - image_quality_output_loss: 0.9209 - age_output_loss: 1.3485 - weight_output_loss: 0.9524 - bag_output_loss: 0.8241 - pose_output_loss: 0.5478 - footwear_output_loss: 0.8502 - emotion_output_loss: 0.8652 - gender_output_acc: 0.7931 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4074 - weight_output_acc: 0.6386 - bag_output_acc: 0.6370 - pose_output_acc: 0.7746 - footwear_output_acc: 0.6071 - emotion_output_acc: 0.7121 - val_loss: 21.8276 - val_gender_output_loss: 0.4262 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.3730 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.8457 - val_pose_output_loss: 0.5080 - val_footwear_output_loss: 0.8245 - val_emotion_output_loss: 0.9142 - val_gender_output_acc: 0.8174 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.5965 - val_bag_output_acc: 0.6403 - val_pose_output_acc: 0.8164 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6998\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 21.82763, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.001.h5\n",
            "Epoch 2/100\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 21.3778 - gender_output_loss: 0.4350 - image_quality_output_loss: 0.9152 - age_output_loss: 1.3462 - weight_output_loss: 0.9498 - bag_output_loss: 0.8252 - pose_output_loss: 0.5415 - footwear_output_loss: 0.8428 - emotion_output_loss: 0.8673 - gender_output_acc: 0.7967 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4102 - weight_output_acc: 0.6382 - bag_output_acc: 0.6407 - pose_output_acc: 0.7782 - footwear_output_acc: 0.6186 - emotion_output_acc: 0.7116 - val_loss: 21.5785 - val_gender_output_loss: 0.4088 - val_image_quality_output_loss: 0.9553 - val_age_output_loss: 1.3690 - val_weight_output_loss: 0.9912 - val_bag_output_loss: 0.8349 - val_pose_output_loss: 0.5011 - val_footwear_output_loss: 0.8111 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.8179 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6029 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.8184 - val_footwear_output_acc: 0.6663 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00002: val_loss improved from 21.82763 to 21.57846, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.002.h5\n",
            "Epoch 3/100\n",
            "721/721 [==============================] - 242s 336ms/step - loss: 21.4074 - gender_output_loss: 0.4402 - image_quality_output_loss: 0.9175 - age_output_loss: 1.3514 - weight_output_loss: 0.9479 - bag_output_loss: 0.8261 - pose_output_loss: 0.5548 - footwear_output_loss: 0.8435 - emotion_output_loss: 0.8645 - gender_output_acc: 0.7920 - image_quality_output_acc: 0.5649 - age_output_acc: 0.4094 - weight_output_acc: 0.6375 - bag_output_acc: 0.6330 - pose_output_acc: 0.7740 - footwear_output_acc: 0.6160 - emotion_output_acc: 0.7125 - val_loss: 21.8947 - val_gender_output_loss: 0.4027 - val_image_quality_output_loss: 1.0365 - val_age_output_loss: 1.3923 - val_weight_output_loss: 1.0048 - val_bag_output_loss: 0.8308 - val_pose_output_loss: 0.4993 - val_footwear_output_loss: 0.8327 - val_emotion_output_loss: 0.9103 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.5034 - val_age_output_acc: 0.3696 - val_weight_output_acc: 0.5901 - val_bag_output_acc: 0.6471 - val_pose_output_acc: 0.8081 - val_footwear_output_acc: 0.6284 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 21.57846\n",
            "Epoch 4/100\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 21.4307 - gender_output_loss: 0.4426 - image_quality_output_loss: 0.9146 - age_output_loss: 1.3538 - weight_output_loss: 0.9531 - bag_output_loss: 0.8337 - pose_output_loss: 0.5538 - footwear_output_loss: 0.8448 - emotion_output_loss: 0.8687 - gender_output_acc: 0.7874 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4037 - weight_output_acc: 0.6375 - bag_output_acc: 0.6298 - pose_output_acc: 0.7733 - footwear_output_acc: 0.6194 - emotion_output_acc: 0.7108 - val_loss: 21.5520 - val_gender_output_loss: 0.4077 - val_image_quality_output_loss: 0.9278 - val_age_output_loss: 1.3732 - val_weight_output_loss: 0.9878 - val_bag_output_loss: 0.8401 - val_pose_output_loss: 0.5138 - val_footwear_output_loss: 0.8401 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5561 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6348 - val_pose_output_acc: 0.8071 - val_footwear_output_acc: 0.6196 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00004: val_loss improved from 21.57846 to 21.55198, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.004.h5\n",
            "Epoch 5/100\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 21.4031 - gender_output_loss: 0.4398 - image_quality_output_loss: 0.9165 - age_output_loss: 1.3563 - weight_output_loss: 0.9540 - bag_output_loss: 0.8278 - pose_output_loss: 0.5616 - footwear_output_loss: 0.8481 - emotion_output_loss: 0.8659 - gender_output_acc: 0.7952 - image_quality_output_acc: 0.5623 - age_output_acc: 0.4107 - weight_output_acc: 0.6381 - bag_output_acc: 0.6316 - pose_output_acc: 0.7718 - footwear_output_acc: 0.6102 - emotion_output_acc: 0.7120 - val_loss: 21.3971 - val_gender_output_loss: 0.3710 - val_image_quality_output_loss: 1.0432 - val_age_output_loss: 1.3450 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.8535 - val_pose_output_loss: 0.4726 - val_footwear_output_loss: 0.7986 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.8337 - val_image_quality_output_acc: 0.5098 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6284 - val_pose_output_acc: 0.8189 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6836\n",
            "\n",
            "Epoch 00005: val_loss improved from 21.55198 to 21.39706, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.005.h5\n",
            "Epoch 6/100\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 21.2051 - gender_output_loss: 0.4227 - image_quality_output_loss: 0.9088 - age_output_loss: 1.3465 - weight_output_loss: 0.9529 - bag_output_loss: 0.8238 - pose_output_loss: 0.5420 - footwear_output_loss: 0.8494 - emotion_output_loss: 0.8651 - gender_output_acc: 0.8096 - image_quality_output_acc: 0.5703 - age_output_acc: 0.4053 - weight_output_acc: 0.6378 - bag_output_acc: 0.6355 - pose_output_acc: 0.7793 - footwear_output_acc: 0.6159 - emotion_output_acc: 0.7114 - val_loss: 21.3675 - val_gender_output_loss: 0.4002 - val_image_quality_output_loss: 0.9929 - val_age_output_loss: 1.3592 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.8358 - val_pose_output_loss: 0.4843 - val_footwear_output_loss: 0.8131 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.8282 - val_image_quality_output_acc: 0.5290 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6437 - val_pose_output_acc: 0.8238 - val_footwear_output_acc: 0.6565 - val_emotion_output_acc: 0.6919\n",
            "\n",
            "Epoch 00006: val_loss improved from 21.39706 to 21.36748, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.006.h5\n",
            "Epoch 7/100\n",
            "685/721 [===========================>..] - ETA: 11s - loss: 20.9591 - gender_output_loss: 0.4016 - image_quality_output_loss: 0.9058 - age_output_loss: 1.3367 - weight_output_loss: 0.9471 - bag_output_loss: 0.8157 - pose_output_loss: 0.5248 - footwear_output_loss: 0.8334 - emotion_output_loss: 0.8631 - gender_output_acc: 0.8162 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4109 - weight_output_acc: 0.6384 - bag_output_acc: 0.6425 - pose_output_acc: 0.7923 - footwear_output_acc: 0.6228 - emotion_output_acc: 0.7119"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNeCQCLveWBk",
        "colab_type": "code",
        "outputId": "e3ea1807-49cc-438d-e3ad-c0a772ce93f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=360 #train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.006.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=15)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 100 patience= 15\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "360/360 [==============================] - 148s 410ms/step - loss: 21.0283 - gender_output_loss: 0.3992 - image_quality_output_loss: 0.9037 - age_output_loss: 1.3409 - weight_output_loss: 0.9509 - bag_output_loss: 0.8176 - pose_output_loss: 0.5180 - footwear_output_loss: 0.8282 - emotion_output_loss: 0.8796 - gender_output_acc: 0.8220 - image_quality_output_acc: 0.5743 - age_output_acc: 0.4113 - weight_output_acc: 0.6354 - bag_output_acc: 0.6403 - pose_output_acc: 0.7943 - footwear_output_acc: 0.6319 - emotion_output_acc: 0.7030 - val_loss: 22.0890 - val_gender_output_loss: 0.3933 - val_image_quality_output_loss: 1.2126 - val_age_output_loss: 1.3536 - val_weight_output_loss: 1.0085 - val_bag_output_loss: 0.8318 - val_pose_output_loss: 0.4864 - val_footwear_output_loss: 0.8209 - val_emotion_output_loss: 0.9547 - val_gender_output_acc: 0.8292 - val_image_quality_output_acc: 0.4395 - val_age_output_acc: 0.3922 - val_weight_output_acc: 0.5896 - val_bag_output_acc: 0.6417 - val_pose_output_acc: 0.8337 - val_footwear_output_acc: 0.6432 - val_emotion_output_acc: 0.6245\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 22.08901, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.001.h5\n",
            "Epoch 2/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.8139 - gender_output_loss: 0.4082 - image_quality_output_loss: 0.9027 - age_output_loss: 1.3362 - weight_output_loss: 0.9370 - bag_output_loss: 0.8158 - pose_output_loss: 0.5176 - footwear_output_loss: 0.8319 - emotion_output_loss: 0.8387 - gender_output_acc: 0.8128 - image_quality_output_acc: 0.5705 - age_output_acc: 0.4113 - weight_output_acc: 0.6438 - bag_output_acc: 0.6422 - pose_output_acc: 0.7972 - footwear_output_acc: 0.6269 - emotion_output_acc: 0.7212 - val_loss: 21.3184 - val_gender_output_loss: 0.3677 - val_image_quality_output_loss: 0.9578 - val_age_output_loss: 1.3441 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.8529 - val_pose_output_loss: 0.4996 - val_footwear_output_loss: 0.8207 - val_emotion_output_loss: 0.8999 - val_gender_output_acc: 0.8351 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6329 - val_pose_output_acc: 0.8076 - val_footwear_output_acc: 0.6462 - val_emotion_output_acc: 0.7003\n",
            "\n",
            "Epoch 00002: val_loss improved from 22.08901 to 21.31840, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.002.h5\n",
            "Epoch 3/100\n",
            "360/360 [==============================] - 130s 361ms/step - loss: 20.9685 - gender_output_loss: 0.4222 - image_quality_output_loss: 0.9195 - age_output_loss: 1.3274 - weight_output_loss: 0.9412 - bag_output_loss: 0.8221 - pose_output_loss: 0.5319 - footwear_output_loss: 0.8296 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8026 - image_quality_output_acc: 0.5618 - age_output_acc: 0.4212 - weight_output_acc: 0.6424 - bag_output_acc: 0.6328 - pose_output_acc: 0.7901 - footwear_output_acc: 0.6293 - emotion_output_acc: 0.7132 - val_loss: 22.5220 - val_gender_output_loss: 0.3938 - val_image_quality_output_loss: 0.9766 - val_age_output_loss: 1.4645 - val_weight_output_loss: 1.0289 - val_bag_output_loss: 0.8468 - val_pose_output_loss: 0.6296 - val_footwear_output_loss: 0.8249 - val_emotion_output_loss: 0.9394 - val_gender_output_acc: 0.8243 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.3479 - val_weight_output_acc: 0.5719 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.7908 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6570\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 21.31840\n",
            "Epoch 4/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 21.2863 - gender_output_loss: 0.4252 - image_quality_output_loss: 0.9077 - age_output_loss: 1.3642 - weight_output_loss: 0.9551 - bag_output_loss: 0.8287 - pose_output_loss: 0.5482 - footwear_output_loss: 0.8407 - emotion_output_loss: 0.8733 - gender_output_acc: 0.8000 - image_quality_output_acc: 0.5738 - age_output_acc: 0.3985 - weight_output_acc: 0.6347 - bag_output_acc: 0.6346 - pose_output_acc: 0.7796 - footwear_output_acc: 0.6168 - emotion_output_acc: 0.7082\n",
            "Epoch 00003: val_loss did not improve from 21.31840\n",
            "Epoch 4/100\n",
            "360/360 [==============================] - 125s 348ms/step - loss: 21.2841 - gender_output_loss: 0.4265 - image_quality_output_loss: 0.9079 - age_output_loss: 1.3641 - weight_output_loss: 0.9545 - bag_output_loss: 0.8280 - pose_output_loss: 0.5482 - footwear_output_loss: 0.8407 - emotion_output_loss: 0.8730 - gender_output_acc: 0.7991 - image_quality_output_acc: 0.5736 - age_output_acc: 0.3986 - weight_output_acc: 0.6351 - bag_output_acc: 0.6349 - pose_output_acc: 0.7795 - footwear_output_acc: 0.6168 - emotion_output_acc: 0.7083 - val_loss: 21.1073 - val_gender_output_loss: 0.3695 - val_image_quality_output_loss: 0.9546 - val_age_output_loss: 1.3493 - val_weight_output_loss: 0.9732 - val_bag_output_loss: 0.8303 - val_pose_output_loss: 0.4852 - val_footwear_output_loss: 0.8032 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.8342 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6368 - val_pose_output_acc: 0.8076 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00004: val_loss improved from 21.31840 to 21.10733, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.004.h5\n",
            "Epoch 5/100\n",
            "360/360 [==============================] - 129s 358ms/step - loss: 21.2380 - gender_output_loss: 0.4349 - image_quality_output_loss: 0.9038 - age_output_loss: 1.3523 - weight_output_loss: 0.9425 - bag_output_loss: 0.8326 - pose_output_loss: 0.5553 - footwear_output_loss: 0.8441 - emotion_output_loss: 0.8731 - gender_output_acc: 0.7936 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4062 - weight_output_acc: 0.6450 - bag_output_acc: 0.6257 - pose_output_acc: 0.7755 - footwear_output_acc: 0.6144 - emotion_output_acc: 0.7069 - val_loss: 21.4546 - val_gender_output_loss: 0.3816 - val_image_quality_output_loss: 0.9923 - val_age_output_loss: 1.3772 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8137 - val_pose_output_loss: 0.5037 - val_footwear_output_loss: 0.8291 - val_emotion_output_loss: 0.9164 - val_gender_output_acc: 0.8297 - val_image_quality_output_acc: 0.5300 - val_age_output_acc: 0.3868 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.8184 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.6737\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 21.10733\n",
            "Epoch 6/100\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 21.0303 - gender_output_loss: 0.4177 - image_quality_output_loss: 0.9172 - age_output_loss: 1.3421 - weight_output_loss: 0.9604 - bag_output_loss: 0.8141 - pose_output_loss: 0.5335 - footwear_output_loss: 0.8332 - emotion_output_loss: 0.8589 - gender_output_acc: 0.8120 - image_quality_output_acc: 0.5707 - age_output_acc: 0.4101 - weight_output_acc: 0.6297 - bag_output_acc: 0.6543 - pose_output_acc: 0.7859 - footwear_output_acc: 0.6148 - emotion_output_acc: 0.7165 - val_loss: 21.3238 - val_gender_output_loss: 0.3781 - val_image_quality_output_loss: 1.0246 - val_age_output_loss: 1.3558 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.8299 - val_pose_output_loss: 0.4867 - val_footwear_output_loss: 0.8246 - val_emotion_output_loss: 0.8985 - val_gender_output_acc: 0.8376 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.4104 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.8155 - val_footwear_output_acc: 0.6452 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 21.10733\n",
            "Epoch 7/100\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 20.8131 - gender_output_loss: 0.3936 - image_quality_output_loss: 0.9056 - age_output_loss: 1.3308 - weight_output_loss: 0.9525 - bag_output_loss: 0.8049 - pose_output_loss: 0.5185 - footwear_output_loss: 0.8173 - emotion_output_loss: 0.8685 - gender_output_acc: 0.8274 - image_quality_output_acc: 0.5712 - age_output_acc: 0.4132 - weight_output_acc: 0.6325 - bag_output_acc: 0.6444 - pose_output_acc: 0.7901 - footwear_output_acc: 0.6314 - emotion_output_acc: 0.7097 - val_loss: 21.3721 - val_gender_output_loss: 0.3743 - val_image_quality_output_loss: 1.0398 - val_age_output_loss: 1.3470 - val_weight_output_loss: 0.9882 - val_bag_output_loss: 0.8239 - val_pose_output_loss: 0.4899 - val_footwear_output_loss: 0.8180 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.8406 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.8253 - val_footwear_output_acc: 0.6535 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 21.10733\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 21.10733\n",
            "Epoch 8/100\n",
            "360/360 [==============================] - 125s 347ms/step - loss: 20.6793 - gender_output_loss: 0.3917 - image_quality_output_loss: 0.9035 - age_output_loss: 1.3307 - weight_output_loss: 0.9341 - bag_output_loss: 0.8088 - pose_output_loss: 0.5094 - footwear_output_loss: 0.8384 - emotion_output_loss: 0.8461 - gender_output_acc: 0.8193 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4160 - weight_output_acc: 0.6417 - bag_output_acc: 0.6535 - pose_output_acc: 0.7969 - footwear_output_acc: 0.6212 - emotion_output_acc: 0.7132 - val_loss: 21.4951 - val_gender_output_loss: 0.3873 - val_image_quality_output_loss: 1.0530 - val_age_output_loss: 1.3430 - val_weight_output_loss: 0.9949 - val_bag_output_loss: 0.8299 - val_pose_output_loss: 0.4765 - val_footwear_output_loss: 0.8547 - val_emotion_output_loss: 0.9206 - val_gender_output_acc: 0.8420 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.4104 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.6452 - val_pose_output_acc: 0.8346 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.6718\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 21.10733\n",
            "Epoch 9/100\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 20.6062 - gender_output_loss: 0.3814 - image_quality_output_loss: 0.8985 - age_output_loss: 1.3264 - weight_output_loss: 0.9487 - bag_output_loss: 0.7947 - pose_output_loss: 0.5000 - footwear_output_loss: 0.8183 - emotion_output_loss: 0.8572 - gender_output_acc: 0.8248 - image_quality_output_acc: 0.5714 - age_output_acc: 0.4094 - weight_output_acc: 0.6399 - bag_output_acc: 0.6498 - pose_output_acc: 0.7976 - footwear_output_acc: 0.6309 - emotion_output_acc: 0.7071 - val_loss: 22.2316 - val_gender_output_loss: 0.4430 - val_image_quality_output_loss: 1.0844 - val_age_output_loss: 1.3709 - val_weight_output_loss: 1.0277 - val_bag_output_loss: 0.8735 - val_pose_output_loss: 0.5173 - val_footwear_output_loss: 0.8722 - val_emotion_output_loss: 0.9376 - val_gender_output_acc: 0.8273 - val_image_quality_output_acc: 0.4961 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.5851 - val_bag_output_acc: 0.6501 - val_pose_output_acc: 0.8248 - val_footwear_output_acc: 0.6358 - val_emotion_output_acc: 0.6658\n",
            "360/360 [==============================]\n",
            "Epoch 00009: val_loss did not improve from 21.10733\n",
            "Epoch 10/100\n",
            "360/360 [==============================] - 126s 349ms/step - loss: 20.7144 - gender_output_loss: 0.3989 - image_quality_output_loss: 0.9104 - age_output_loss: 1.3270 - weight_output_loss: 0.9383 - bag_output_loss: 0.8037 - pose_output_loss: 0.5114 - footwear_output_loss: 0.8311 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8179 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4179 - weight_output_acc: 0.6403 - bag_output_acc: 0.6514 - pose_output_acc: 0.7958 - footwear_output_acc: 0.6278 - emotion_output_acc: 0.7179 - val_loss: 21.1956 - val_gender_output_loss: 0.3929 - val_image_quality_output_loss: 0.9332 - val_age_output_loss: 1.3582 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8271 - val_pose_output_loss: 0.4903 - val_footwear_output_loss: 0.8189 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.8297 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4124 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6467 - val_pose_output_acc: 0.8243 - val_footwear_output_acc: 0.6442 - val_emotion_output_acc: 0.6919\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 21.10733\n",
            "Epoch 11/100\n",
            "360/360 [==============================] - 128s 355ms/step - loss: 20.9481 - gender_output_loss: 0.4025 - image_quality_output_loss: 0.9090 - age_output_loss: 1.3403 - weight_output_loss: 0.9501 - bag_output_loss: 0.8095 - pose_output_loss: 0.5378 - footwear_output_loss: 0.8389 - emotion_output_loss: 0.8658 - gender_output_acc: 0.8148 - image_quality_output_acc: 0.5660 - age_output_acc: 0.4101 - weight_output_acc: 0.6345 - bag_output_acc: 0.6425 - pose_output_acc: 0.7854 - footwear_output_acc: 0.6238 - emotion_output_acc: 0.7122 - val_loss: 21.9289 - val_gender_output_loss: 0.4011 - val_image_quality_output_loss: 1.0548 - val_age_output_loss: 1.3568 - val_weight_output_loss: 1.0046 - val_bag_output_loss: 0.8557 - val_pose_output_loss: 0.5537 - val_footwear_output_loss: 0.8278 - val_emotion_output_loss: 0.9415 - val_gender_output_acc: 0.8406 - val_image_quality_output_acc: 0.4902 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.5989 - val_bag_output_acc: 0.6363 - val_pose_output_acc: 0.7968 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6550\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 21.10733\n",
            "Epoch 12/100\n",
            "360/360 [==============================] - 125s 347ms/step - loss: 21.1837 - gender_output_loss: 0.4390 - image_quality_output_loss: 0.9054 - age_output_loss: 1.3567 - weight_output_loss: 0.9505 - bag_output_loss: 0.8317 - pose_output_loss: 0.5510 - footwear_output_loss: 0.8464 - emotion_output_loss: 0.8634 - gender_output_acc: 0.7915 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4083 - weight_output_acc: 0.6418 - bag_output_acc: 0.6299 - pose_output_acc: 0.7762 - footwear_output_acc: 0.6172 - emotion_output_acc: 0.7118 - val_loss: 22.8535 - val_gender_output_loss: 0.4290 - val_image_quality_output_loss: 1.2009 - val_age_output_loss: 1.3864 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.9120 - val_pose_output_loss: 0.6460 - val_footwear_output_loss: 0.8475 - val_emotion_output_loss: 0.9420 - val_gender_output_acc: 0.8268 - val_image_quality_output_acc: 0.4621 - val_age_output_acc: 0.3676 - val_weight_output_acc: 0.5999 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7781 - val_footwear_output_acc: 0.6344 - val_emotion_output_acc: 0.6924\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 21.10733\n",
            "Epoch 13/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 21.1519 - gender_output_loss: 0.4268 - image_quality_output_loss: 0.9077 - age_output_loss: 1.3500 - weight_output_loss: 0.9611 - bag_output_loss: 0.8221 - pose_output_loss: 0.5568 - footwear_output_loss: 0.8547 - emotion_output_loss: 0.8598 - gender_output_acc: 0.8069 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4014 - weight_output_acc: 0.6339 - bag_output_acc: 0.6396 - pose_output_acc: 0.7714 - footwear_output_acc: 0.6174 - emotion_output_acc: 0.7127 - val_loss: 21.7555 - val_gender_output_loss: 0.4097 - val_image_quality_output_loss: 1.0530 - val_age_output_loss: 1.3969 - val_weight_output_loss: 0.9966 - val_bag_output_loss: 0.8460 - val_pose_output_loss: 0.5007 - val_footwear_output_loss: 0.8137 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.8199 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.3858 - val_weight_output_acc: 0.5974 - val_bag_output_acc: 0.6467 - val_pose_output_acc: 0.8179 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6821\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 21.10733\n",
            "Epoch 14/100\n",
            "360/360 [==============================] - 125s 348ms/step - loss: 20.8589 - gender_output_loss: 0.4058 - image_quality_output_loss: 0.9131 - age_output_loss: 1.3368 - weight_output_loss: 0.9419 - bag_output_loss: 0.8143 - pose_output_loss: 0.5301 - footwear_output_loss: 0.8224 - emotion_output_loss: 0.8664 - gender_output_acc: 0.8092 - image_quality_output_acc: 0.5674 - age_output_acc: 0.4184 - weight_output_acc: 0.6444 - bag_output_acc: 0.6436 - pose_output_acc: 0.7842 - footwear_output_acc: 0.6257 - emotion_output_acc: 0.7101 - val_loss: 21.3691 - val_gender_output_loss: 0.3760 - val_image_quality_output_loss: 1.0371 - val_age_output_loss: 1.3723 - val_weight_output_loss: 1.0157 - val_bag_output_loss: 0.8242 - val_pose_output_loss: 0.4744 - val_footwear_output_loss: 0.7942 - val_emotion_output_loss: 0.9057 - val_gender_output_acc: 0.8337 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.3991 - val_weight_output_acc: 0.5827 - val_bag_output_acc: 0.6535 - val_pose_output_acc: 0.8342 - val_footwear_output_acc: 0.6590 - val_emotion_output_acc: 0.6752\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 21.10733\n",
            "Epoch 15/100\n",
            "360/360 [==============================] - 128s 357ms/step - loss: 20.7161 - gender_output_loss: 0.3933 - image_quality_output_loss: 0.9080 - age_output_loss: 1.3337 - weight_output_loss: 0.9508 - bag_output_loss: 0.8104 - pose_output_loss: 0.5039 - footwear_output_loss: 0.8141 - emotion_output_loss: 0.8649 - gender_output_acc: 0.8227 - image_quality_output_acc: 0.5715 - age_output_acc: 0.4111 - weight_output_acc: 0.6352 - bag_output_acc: 0.6453 - pose_output_acc: 0.7958 - footwear_output_acc: 0.6330 - emotion_output_acc: 0.7069 - val_loss: 21.0386 - val_gender_output_loss: 0.3657 - val_image_quality_output_loss: 0.9968 - val_age_output_loss: 1.3396 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8205 - val_pose_output_loss: 0.4688 - val_footwear_output_loss: 0.8052 - val_emotion_output_loss: 0.9088 - val_gender_output_acc: 0.8440 - val_image_quality_output_acc: 0.5399 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6078 - val_bag_output_acc: 0.6590 - val_pose_output_acc: 0.8361 - val_footwear_output_acc: 0.6516 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00015: val_loss improved from 21.10733 to 21.03856, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.015.h5\n",
            "Epoch 16/100\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 20.4794 - gender_output_loss: 0.3693 - image_quality_output_loss: 0.8949 - age_output_loss: 1.3216 - weight_output_loss: 0.9375 - bag_output_loss: 0.8024 - pose_output_loss: 0.4949 - footwear_output_loss: 0.8297 - emotion_output_loss: 0.8529 - gender_output_acc: 0.8325 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4127 - weight_output_acc: 0.6427 - bag_output_acc: 0.6517 - pose_output_acc: 0.8047 - footwear_output_acc: 0.6255 - emotion_output_acc: 0.7128 - val_loss: 21.3506 - val_gender_output_loss: 0.3564 - val_image_quality_output_loss: 1.0959 - val_age_output_loss: 1.3487 - val_weight_output_loss: 0.9988 - val_bag_output_loss: 0.8254 - val_pose_output_loss: 0.4661 - val_footwear_output_loss: 0.8106 - val_emotion_output_loss: 0.9178 - val_gender_output_acc: 0.8499 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.5989 - val_bag_output_acc: 0.6590 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.6506 - val_emotion_output_acc: 0.6708\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 21.03856\n",
            "Epoch 17/100\n",
            "360/360 [==============================] - 128s 354ms/step - loss: 20.3360 - gender_output_loss: 0.3729 - image_quality_output_loss: 0.8979 - age_output_loss: 1.3120 - weight_output_loss: 0.9416 - bag_output_loss: 0.7846 - pose_output_loss: 0.4884 - footwear_output_loss: 0.8118 - emotion_output_loss: 0.8480 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.5715 - age_output_acc: 0.4292 - weight_output_acc: 0.6441 - bag_output_acc: 0.6613 - pose_output_acc: 0.8061 - footwear_output_acc: 0.6352 - emotion_output_acc: 0.7149 - val_loss: 21.5855 - val_gender_output_loss: 0.3919 - val_image_quality_output_loss: 1.0595 - val_age_output_loss: 1.3440 - val_weight_output_loss: 1.0102 - val_bag_output_loss: 0.8391 - val_pose_output_loss: 0.5013 - val_footwear_output_loss: 0.8313 - val_emotion_output_loss: 0.9269 - val_gender_output_acc: 0.8474 - val_image_quality_output_acc: 0.5202 - val_age_output_acc: 0.4080 - val_weight_output_acc: 0.5861 - val_bag_output_acc: 0.6590 - val_pose_output_acc: 0.8312 - val_footwear_output_acc: 0.6457 - val_emotion_output_acc: 0.6619\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 21.03856\n",
            "Epoch 18/100\n",
            "360/360 [==============================] - 131s 363ms/step - loss: 20.5945 - gender_output_loss: 0.3824 - image_quality_output_loss: 0.8945 - age_output_loss: 1.3329 - weight_output_loss: 0.9348 - bag_output_loss: 0.8062 - pose_output_loss: 0.4967 - footwear_output_loss: 0.8249 - emotion_output_loss: 0.8662 - gender_output_acc: 0.8201 - image_quality_output_acc: 0.5818 - age_output_acc: 0.4108 - weight_output_acc: 0.6384 - bag_output_acc: 0.6462 - pose_output_acc: 0.8061 - footwear_output_acc: 0.6316 - emotion_output_acc: 0.7115 - val_loss: 21.6879 - val_gender_output_loss: 0.4673 - val_image_quality_output_loss: 0.9894 - val_age_output_loss: 1.3671 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8815 - val_pose_output_loss: 0.5328 - val_footwear_output_loss: 0.8447 - val_emotion_output_loss: 0.8925 - val_gender_output_acc: 0.8022 - val_image_quality_output_acc: 0.5236 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6181 - val_pose_output_acc: 0.8086 - val_footwear_output_acc: 0.6270 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 21.03856\n",
            "Epoch 19/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.7989 - gender_output_loss: 0.4009 - image_quality_output_loss: 0.9058 - age_output_loss: 1.3242 - weight_output_loss: 0.9491 - bag_output_loss: 0.8256 - pose_output_loss: 0.5300 - footwear_output_loss: 0.8296 - emotion_output_loss: 0.8603 - gender_output_acc: 0.8130 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4113 - weight_output_acc: 0.6394 - bag_output_acc: 0.6340 - pose_output_acc: 0.7972 - footwear_output_acc: 0.6241 - emotion_output_acc: 0.7099 - val_loss: 21.7049 - val_gender_output_loss: 0.4274 - val_image_quality_output_loss: 1.0631 - val_age_output_loss: 1.3729 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8216 - val_pose_output_loss: 0.5193 - val_footwear_output_loss: 0.8162 - val_emotion_output_loss: 0.9371 - val_gender_output_acc: 0.8297 - val_image_quality_output_acc: 0.5108 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6083 - val_bag_output_acc: 0.6570 - val_pose_output_acc: 0.8155 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 21.03856\n",
            "Epoch 20/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 21.0037 - gender_output_loss: 0.4197 - image_quality_output_loss: 0.9124 - age_output_loss: 1.3515 - weight_output_loss: 0.9490 - bag_output_loss: 0.8066 - pose_output_loss: 0.5371 - footwear_output_loss: 0.8441 - emotion_output_loss: 0.8746 - gender_output_acc: 0.8050 - image_quality_output_acc: 0.5679 - age_output_acc: 0.4090 - weight_output_acc: 0.6373 - bag_output_acc: 0.6406 - pose_output_acc: 0.7767 - footwear_output_acc: 0.6106 - emotion_output_acc: 0.7097 - val_loss: 21.7774 - val_gender_output_loss: 0.5338 - val_image_quality_output_loss: 1.0003 - val_age_output_loss: 1.3663 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.8553 - val_pose_output_loss: 0.5106 - val_footwear_output_loss: 0.8636 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.7785 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6378 - val_pose_output_acc: 0.8027 - val_footwear_output_acc: 0.6073 - val_emotion_output_acc: 0.6988\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 21.03856\n",
            "Epoch 21/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 21.0215 - gender_output_loss: 0.4097 - image_quality_output_loss: 0.9205 - age_output_loss: 1.3504 - weight_output_loss: 0.9366 - bag_output_loss: 0.8255 - pose_output_loss: 0.5584 - footwear_output_loss: 0.8350 - emotion_output_loss: 0.8660 - gender_output_acc: 0.8099 - image_quality_output_acc: 0.5642 - age_output_acc: 0.4064 - weight_output_acc: 0.6438 - bag_output_acc: 0.6462 - pose_output_acc: 0.7672 - footwear_output_acc: 0.6304 - emotion_output_acc: 0.7116 - val_loss: 20.9956 - val_gender_output_loss: 0.3697 - val_image_quality_output_loss: 0.9914 - val_age_output_loss: 1.3429 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.8198 - val_pose_output_loss: 0.4885 - val_footwear_output_loss: 0.7874 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.8351 - val_image_quality_output_acc: 0.5276 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6462 - val_pose_output_acc: 0.8228 - val_footwear_output_acc: 0.6654 - val_emotion_output_acc: 0.6983\n",
            "\n",
            "Epoch 00021: val_loss improved from 21.03856 to 20.99564, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.021.h5\n",
            "Epoch 22/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.8506 - gender_output_loss: 0.4019 - image_quality_output_loss: 0.9007 - age_output_loss: 1.3383 - weight_output_loss: 0.9603 - bag_output_loss: 0.8149 - pose_output_loss: 0.5266 - footwear_output_loss: 0.8363 - emotion_output_loss: 0.8644 - gender_output_acc: 0.8120 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4087 - weight_output_acc: 0.6318 - bag_output_acc: 0.6406 - pose_output_acc: 0.7852 - footwear_output_acc: 0.6236 - emotion_output_acc: 0.7151 - val_loss: 21.4222 - val_gender_output_loss: 0.3755 - val_image_quality_output_loss: 1.0597 - val_age_output_loss: 1.3557 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.8287 - val_pose_output_loss: 0.4715 - val_footwear_output_loss: 0.8801 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.8474 - val_image_quality_output_acc: 0.5138 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6024 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8233 - val_footwear_output_acc: 0.6216 - val_emotion_output_acc: 0.6747\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 20.99564\n",
            "Epoch 23/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.4824 - gender_output_loss: 0.3867 - image_quality_output_loss: 0.9018 - age_output_loss: 1.3284 - weight_output_loss: 0.9295 - bag_output_loss: 0.7964 - pose_output_loss: 0.5076 - footwear_output_loss: 0.8185 - emotion_output_loss: 0.8514 - gender_output_acc: 0.8234 - image_quality_output_acc: 0.5719 - age_output_acc: 0.4139 - weight_output_acc: 0.6514 - bag_output_acc: 0.6595 - pose_output_acc: 0.8033 - footwear_output_acc: 0.6283 - emotion_output_acc: 0.7163 - val_loss: 21.1520 - val_gender_output_loss: 0.3966 - val_image_quality_output_loss: 1.0155 - val_age_output_loss: 1.3391 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8218 - val_pose_output_loss: 0.4804 - val_footwear_output_loss: 0.8232 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.8366 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.4109 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6476 - val_pose_output_acc: 0.8233 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.6762\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 20.99564\n",
            "Epoch 24/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.5949 - gender_output_loss: 0.3727 - image_quality_output_loss: 0.8991 - age_output_loss: 1.3308 - weight_output_loss: 0.9542 - bag_output_loss: 0.7984 - pose_output_loss: 0.4988 - footwear_output_loss: 0.8240 - emotion_output_loss: 0.8707 - gender_output_acc: 0.8344 - image_quality_output_acc: 0.5668 - age_output_acc: 0.4113 - weight_output_acc: 0.6300 - bag_output_acc: 0.6498 - pose_output_acc: 0.8012 - footwear_output_acc: 0.6250 - emotion_output_acc: 0.7054 - val_loss: 20.7048 - val_gender_output_loss: 0.3445 - val_image_quality_output_loss: 0.9918 - val_age_output_loss: 1.3243 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8172 - val_pose_output_loss: 0.4536 - val_footwear_output_loss: 0.7909 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.8489 - val_image_quality_output_acc: 0.5399 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6565 - val_pose_output_acc: 0.8361 - val_footwear_output_acc: 0.6708 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00024: val_loss improved from 20.99564 to 20.70483, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.024.h5\n",
            "Epoch 25/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.2494 - gender_output_loss: 0.3710 - image_quality_output_loss: 0.8921 - age_output_loss: 1.3122 - weight_output_loss: 0.9437 - bag_output_loss: 0.7800 - pose_output_loss: 0.4757 - footwear_output_loss: 0.8109 - emotion_output_loss: 0.8534 - gender_output_acc: 0.8378 - image_quality_output_acc: 0.5764 - age_output_acc: 0.4201 - weight_output_acc: 0.6345 - bag_output_acc: 0.6644 - pose_output_acc: 0.8113 - footwear_output_acc: 0.6363 - emotion_output_acc: 0.7116 - val_loss: 20.8750 - val_gender_output_loss: 0.3479 - val_image_quality_output_loss: 0.9672 - val_age_output_loss: 1.3263 - val_weight_output_loss: 0.9740 - val_bag_output_loss: 0.8251 - val_pose_output_loss: 0.4920 - val_footwear_output_loss: 0.8008 - val_emotion_output_loss: 0.9065 - val_gender_output_acc: 0.8578 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6535 - val_pose_output_acc: 0.8258 - val_footwear_output_acc: 0.6516 - val_emotion_output_acc: 0.6811\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 20.70483\n",
            "Epoch 26/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 20.5473 - gender_output_loss: 0.3756 - image_quality_output_loss: 0.9022 - age_output_loss: 1.3189 - weight_output_loss: 0.9438 - bag_output_loss: 0.8061 - pose_output_loss: 0.5188 - footwear_output_loss: 0.8245 - emotion_output_loss: 0.8562 - gender_output_acc: 0.8330 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4240 - weight_output_acc: 0.6389 - bag_output_acc: 0.6453 - pose_output_acc: 0.7932 - footwear_output_acc: 0.6283 - emotion_output_acc: 0.7116 - val_loss: 21.3653 - val_gender_output_loss: 0.3989 - val_image_quality_output_loss: 1.0515 - val_age_output_loss: 1.3393 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.8221 - val_pose_output_loss: 0.4953 - val_footwear_output_loss: 0.8175 - val_emotion_output_loss: 0.9325 - val_gender_output_acc: 0.8366 - val_image_quality_output_acc: 0.5192 - val_age_output_acc: 0.4055 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6526 - val_pose_output_acc: 0.8228 - val_footwear_output_acc: 0.6393 - val_emotion_output_acc: 0.6496\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 20.70483\n",
            "Epoch 27/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 20.5972 - gender_output_loss: 0.3885 - image_quality_output_loss: 0.9009 - age_output_loss: 1.3280 - weight_output_loss: 0.9467 - bag_output_loss: 0.8222 - pose_output_loss: 0.5111 - footwear_output_loss: 0.8276 - emotion_output_loss: 0.8452 - gender_output_acc: 0.8243 - image_quality_output_acc: 0.5715 - age_output_acc: 0.4043 - weight_output_acc: 0.6384 - bag_output_acc: 0.6361 - pose_output_acc: 0.7965 - footwear_output_acc: 0.6363 - emotion_output_acc: 0.7172 - val_loss: 22.3285 - val_gender_output_loss: 0.4400 - val_image_quality_output_loss: 1.1074 - val_age_output_loss: 1.3549 - val_weight_output_loss: 0.9968 - val_bag_output_loss: 0.9252 - val_pose_output_loss: 0.5825 - val_footwear_output_loss: 0.8129 - val_emotion_output_loss: 0.9616 - val_gender_output_acc: 0.8253 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.5960 - val_bag_output_acc: 0.6127 - val_pose_output_acc: 0.8051 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6344\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 20.70483\n",
            "Epoch 28/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 20.9733 - gender_output_loss: 0.4206 - image_quality_output_loss: 0.9109 - age_output_loss: 1.3500 - weight_output_loss: 0.9451 - bag_output_loss: 0.8022 - pose_output_loss: 0.5496 - footwear_output_loss: 0.8333 - emotion_output_loss: 0.8814 - gender_output_acc: 0.8003 - image_quality_output_acc: 0.5712 - age_output_acc: 0.4097 - weight_output_acc: 0.6413 - bag_output_acc: 0.6554 - pose_output_acc: 0.7762 - footwear_output_acc: 0.6243 - emotion_output_acc: 0.7026 - val_loss: 21.4195 - val_gender_output_loss: 0.4138 - val_image_quality_output_loss: 0.9036 - val_age_output_loss: 1.3465 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8180 - val_pose_output_loss: 0.6185 - val_footwear_output_loss: 0.8324 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.8051 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4124 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6457 - val_pose_output_acc: 0.7761 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 20.70483\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 20.70483\n",
            "Epoch 29/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 20.8680 - gender_output_loss: 0.4142 - image_quality_output_loss: 0.9018 - age_output_loss: 1.3398 - weight_output_loss: 0.9527 - bag_output_loss: 0.8168 - pose_output_loss: 0.5267 - footwear_output_loss: 0.8436 - emotion_output_loss: 0.8689 - gender_output_acc: 0.8082 - image_quality_output_acc: 0.5726 - age_output_acc: 0.4160 - weight_output_acc: 0.6365 - bag_output_acc: 0.6385 - pose_output_acc: 0.7851 - footwear_output_acc: 0.6207 - emotion_output_acc: 0.7109 - val_loss: 21.2750 - val_gender_output_loss: 0.4149 - val_image_quality_output_loss: 1.0344 - val_age_output_loss: 1.3500 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8314 - val_pose_output_loss: 0.4874 - val_footwear_output_loss: 0.8205 - val_emotion_output_loss: 0.9062 - val_gender_output_acc: 0.8194 - val_image_quality_output_acc: 0.5285 - val_age_output_acc: 0.4016 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6506 - val_pose_output_acc: 0.8100 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.6924\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 20.70483\n",
            "Epoch 30/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.7115 - gender_output_loss: 0.4016 - image_quality_output_loss: 0.9092 - age_output_loss: 1.3318 - weight_output_loss: 0.9420 - bag_output_loss: 0.8153 - pose_output_loss: 0.5262 - footwear_output_loss: 0.8303 - emotion_output_loss: 0.8579 - gender_output_acc: 0.8222 - image_quality_output_acc: 0.5677 - age_output_acc: 0.4179 - weight_output_acc: 0.6431 - bag_output_acc: 0.6425 - pose_output_acc: 0.7894 - footwear_output_acc: 0.6231 - emotion_output_acc: 0.7141 - val_loss: 21.0392 - val_gender_output_loss: 0.3535 - val_image_quality_output_loss: 1.0356 - val_age_output_loss: 1.3487 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8148 - val_pose_output_loss: 0.4597 - val_footwear_output_loss: 0.8015 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.8445 - val_image_quality_output_acc: 0.5305 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6560 - val_pose_output_acc: 0.8282 - val_footwear_output_acc: 0.6634 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 20.70483\n",
            "Epoch 31/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.5259 - gender_output_loss: 0.3717 - image_quality_output_loss: 0.8883 - age_output_loss: 1.3180 - weight_output_loss: 0.9338 - bag_output_loss: 0.8090 - pose_output_loss: 0.5224 - footwear_output_loss: 0.8342 - emotion_output_loss: 0.8641 - gender_output_acc: 0.8283 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4149 - weight_output_acc: 0.6403 - bag_output_acc: 0.6438 - pose_output_acc: 0.7917 - footwear_output_acc: 0.6238 - emotion_output_acc: 0.7083 - val_loss: 21.1412 - val_gender_output_loss: 0.3693 - val_image_quality_output_loss: 1.0645 - val_age_output_loss: 1.3282 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8294 - val_pose_output_loss: 0.4536 - val_footwear_output_loss: 0.8101 - val_emotion_output_loss: 0.9307 - val_gender_output_acc: 0.8484 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.4119 - val_weight_output_acc: 0.6004 - val_bag_output_acc: 0.6521 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6639 - val_emotion_output_acc: 0.6531\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 20.70483\n",
            "Epoch 32/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.2499 - gender_output_loss: 0.3589 - image_quality_output_loss: 0.9036 - age_output_loss: 1.3147 - weight_output_loss: 0.9418 - bag_output_loss: 0.7819 - pose_output_loss: 0.4847 - footwear_output_loss: 0.8110 - emotion_output_loss: 0.8524 - gender_output_acc: 0.8467 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4210 - weight_output_acc: 0.6365 - bag_output_acc: 0.6644 - pose_output_acc: 0.8109 - footwear_output_acc: 0.6335 - emotion_output_acc: 0.7135 - val_loss: 20.9766 - val_gender_output_loss: 0.3399 - val_image_quality_output_loss: 1.1286 - val_age_output_loss: 1.3155 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8190 - val_pose_output_loss: 0.4628 - val_footwear_output_loss: 0.7910 - val_emotion_output_loss: 0.8988 - val_gender_output_acc: 0.8548 - val_image_quality_output_acc: 0.4838 - val_age_output_acc: 0.4149 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6531 - val_pose_output_acc: 0.8337 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 20.70483\n",
            "Epoch 33/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 20.0840 - gender_output_loss: 0.3586 - image_quality_output_loss: 0.8830 - age_output_loss: 1.3084 - weight_output_loss: 0.9272 - bag_output_loss: 0.7955 - pose_output_loss: 0.4616 - footwear_output_loss: 0.8013 - emotion_output_loss: 0.8514 - gender_output_acc: 0.8395 - image_quality_output_acc: 0.5876 - age_output_acc: 0.4229 - weight_output_acc: 0.6476 - bag_output_acc: 0.6539 - pose_output_acc: 0.8169 - footwear_output_acc: 0.6388 - emotion_output_acc: 0.7122\n",
            "Epoch 00032: val_loss did not improve from 20.70483\n",
            "Epoch 33/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.0843 - gender_output_loss: 0.3593 - image_quality_output_loss: 0.8828 - age_output_loss: 1.3082 - weight_output_loss: 0.9261 - bag_output_loss: 0.7960 - pose_output_loss: 0.4620 - footwear_output_loss: 0.8015 - emotion_output_loss: 0.8515 - gender_output_acc: 0.8389 - image_quality_output_acc: 0.5877 - age_output_acc: 0.4229 - weight_output_acc: 0.6483 - bag_output_acc: 0.6536 - pose_output_acc: 0.8167 - footwear_output_acc: 0.6387 - emotion_output_acc: 0.7122 - val_loss: 20.8204 - val_gender_output_loss: 0.3476 - val_image_quality_output_loss: 1.0251 - val_age_output_loss: 1.3250 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.8120 - val_pose_output_loss: 0.4507 - val_footwear_output_loss: 0.8029 - val_emotion_output_loss: 0.9095 - val_gender_output_acc: 0.8563 - val_image_quality_output_acc: 0.5271 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8366 - val_footwear_output_acc: 0.6516 - val_emotion_output_acc: 0.6806\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 20.70483\n",
            "Epoch 34/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 20.4367 - gender_output_loss: 0.3654 - image_quality_output_loss: 0.9034 - age_output_loss: 1.3083 - weight_output_loss: 0.9511 - bag_output_loss: 0.7900 - pose_output_loss: 0.5146 - footwear_output_loss: 0.8308 - emotion_output_loss: 0.8589 - gender_output_acc: 0.8393 - image_quality_output_acc: 0.5710 - age_output_acc: 0.4210 - weight_output_acc: 0.6316 - bag_output_acc: 0.6558 - pose_output_acc: 0.7989 - footwear_output_acc: 0.6255 - emotion_output_acc: 0.7084\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.4370 - gender_output_loss: 0.3655 - image_quality_output_loss: 0.9034 - age_output_loss: 1.3085 - weight_output_loss: 0.9510 - bag_output_loss: 0.7906 - pose_output_loss: 0.5140 - footwear_output_loss: 0.8300 - emotion_output_loss: 0.8592 - gender_output_acc: 0.8392 - image_quality_output_acc: 0.5714 - age_output_acc: 0.4210 - weight_output_acc: 0.6318 - bag_output_acc: 0.6556 - pose_output_acc: 0.7991 - footwear_output_acc: 0.6260 - emotion_output_acc: 0.7083 - val_loss: 20.9527 - val_gender_output_loss: 0.3460 - val_image_quality_output_loss: 1.0058 - val_age_output_loss: 1.3370 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.8655 - val_pose_output_loss: 0.4552 - val_footwear_output_loss: 0.8110 - val_emotion_output_loss: 0.8957 - val_gender_output_acc: 0.8543 - val_image_quality_output_acc: 0.5172 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6260 - val_pose_output_acc: 0.8312 - val_footwear_output_acc: 0.6531 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 20.70483\n",
            "Epoch 35/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.4725 - gender_output_loss: 0.3851 - image_quality_output_loss: 0.9056 - age_output_loss: 1.3095 - weight_output_loss: 0.9388 - bag_output_loss: 0.8047 - pose_output_loss: 0.5062 - footwear_output_loss: 0.8173 - emotion_output_loss: 0.8679 - gender_output_acc: 0.8269 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4208 - weight_output_acc: 0.6434 - bag_output_acc: 0.6503 - pose_output_acc: 0.7924 - footwear_output_acc: 0.6389 - emotion_output_acc: 0.7063 - val_loss: 21.1746 - val_gender_output_loss: 0.3731 - val_image_quality_output_loss: 0.9453 - val_age_output_loss: 1.3126 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8796 - val_pose_output_loss: 0.5161 - val_footwear_output_loss: 0.8946 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.8425 - val_image_quality_output_acc: 0.5738 - val_age_output_acc: 0.4218 - val_weight_output_acc: 0.6088 - val_bag_output_acc: 0.6117 - val_pose_output_acc: 0.8056 - val_footwear_output_acc: 0.6181 - val_emotion_output_acc: 0.6929\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 20.70483\n",
            "Epoch 36/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.8804 - gender_output_loss: 0.4081 - image_quality_output_loss: 0.9021 - age_output_loss: 1.3493 - weight_output_loss: 0.9509 - bag_output_loss: 0.8240 - pose_output_loss: 0.5387 - footwear_output_loss: 0.8398 - emotion_output_loss: 0.8613 - gender_output_acc: 0.8118 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4017 - weight_output_acc: 0.6377 - bag_output_acc: 0.6415 - pose_output_acc: 0.7760 - footwear_output_acc: 0.6172 - emotion_output_acc: 0.7165 - val_loss: 21.9509 - val_gender_output_loss: 0.4104 - val_image_quality_output_loss: 1.0868 - val_age_output_loss: 1.3539 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8183 - val_pose_output_loss: 0.5831 - val_footwear_output_loss: 0.8100 - val_emotion_output_loss: 0.9927 - val_gender_output_acc: 0.8302 - val_image_quality_output_acc: 0.5123 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.6038 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.7904 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6294\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 20.70483\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 20.70483\n",
            "Epoch 37/100\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 20.8113 - gender_output_loss: 0.4111 - image_quality_output_loss: 0.9147 - age_output_loss: 1.3267 - weight_output_loss: 0.9581 - bag_output_loss: 0.8258 - pose_output_loss: 0.5324 - footwear_output_loss: 0.8314 - emotion_output_loss: 0.8609 - gender_output_acc: 0.8163 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4137 - weight_output_acc: 0.6295 - bag_output_acc: 0.6316 - pose_output_acc: 0.7814 - footwear_output_acc: 0.6196 - emotion_output_acc: 0.7132 - val_loss: 20.9943 - val_gender_output_loss: 0.3709 - val_image_quality_output_loss: 1.0158 - val_age_output_loss: 1.3580 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 0.8118 - val_pose_output_loss: 0.5026 - val_footwear_output_loss: 0.7891 - val_emotion_output_loss: 0.8891 - val_gender_output_acc: 0.8425 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.4109 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6560 - val_pose_output_acc: 0.8125 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 20.70483\n",
            "Epoch 38/100\n",
            "360/360 [==============================] - 127s 352ms/step - loss: 20.6183 - gender_output_loss: 0.3802 - image_quality_output_loss: 0.9074 - age_output_loss: 1.3347 - weight_output_loss: 0.9428 - bag_output_loss: 0.8010 - pose_output_loss: 0.5155 - footwear_output_loss: 0.8330 - emotion_output_loss: 0.8667 - gender_output_acc: 0.8245 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4205 - weight_output_acc: 0.6453 - bag_output_acc: 0.6469 - pose_output_acc: 0.7906 - footwear_output_acc: 0.6297 - emotion_output_acc: 0.7050 - val_loss: 20.8110 - val_gender_output_loss: 0.3668 - val_image_quality_output_loss: 0.9651 - val_age_output_loss: 1.3373 - val_weight_output_loss: 0.9693 - val_bag_output_loss: 0.8222 - val_pose_output_loss: 0.4600 - val_footwear_output_loss: 0.8529 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.8489 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.4075 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.8302 - val_footwear_output_acc: 0.6339 - val_emotion_output_acc: 0.6988\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 20.70483\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 20.70483\n",
            "Epoch 39/100\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 20.4065 - gender_output_loss: 0.3676 - image_quality_output_loss: 0.8991 - age_output_loss: 1.3144 - weight_output_loss: 0.9298 - bag_output_loss: 0.7990 - pose_output_loss: 0.5019 - footwear_output_loss: 0.8364 - emotion_output_loss: 0.8656 - gender_output_acc: 0.8370 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4208 - weight_output_acc: 0.6451 - bag_output_acc: 0.6533 - pose_output_acc: 0.7977 - footwear_output_acc: 0.6220 - emotion_output_acc: 0.7071 - val_loss: 21.2636 - val_gender_output_loss: 0.3386 - val_image_quality_output_loss: 1.1560 - val_age_output_loss: 1.3282 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.8563 - val_pose_output_loss: 0.4665 - val_footwear_output_loss: 0.8096 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.8666 - val_image_quality_output_acc: 0.4828 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6388 - val_pose_output_acc: 0.8361 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.6634\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 20.70483\n",
            "Saved JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "End of EPOCHS= 100  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYSGZUstwQBm",
        "outputId": "010c8585-45c2-47ad-8b71-404ff977c01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=300\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.024.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=50)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Returning new callback array with steps_per_epoch= 721 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 300 patience= 50\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json1577603315_backup\n",
            "Epoch 1/300\n",
            "721/721 [==============================] - 244s 339ms/step - loss: 20.3378 - gender_output_loss: 0.3662 - image_quality_output_loss: 0.8943 - age_output_loss: 1.3116 - weight_output_loss: 0.9392 - bag_output_loss: 0.7967 - pose_output_loss: 0.4909 - footwear_output_loss: 0.8180 - emotion_output_loss: 0.8536 - gender_output_acc: 0.8370 - image_quality_output_acc: 0.5726 - age_output_acc: 0.4209 - weight_output_acc: 0.6393 - bag_output_acc: 0.6553 - pose_output_acc: 0.8073 - footwear_output_acc: 0.6340 - emotion_output_acc: 0.7120 - val_loss: 21.5579 - val_gender_output_loss: 0.3734 - val_image_quality_output_loss: 1.1021 - val_age_output_loss: 1.3506 - val_weight_output_loss: 1.0047 - val_bag_output_loss: 0.8385 - val_pose_output_loss: 0.4945 - val_footwear_output_loss: 0.8123 - val_emotion_output_loss: 0.9327 - val_gender_output_acc: 0.8455 - val_image_quality_output_acc: 0.5118 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.5974 - val_bag_output_acc: 0.6457 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6614 - val_emotion_output_acc: 0.6580\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 21.55787, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.001.h5\n",
            "Epoch 2/300\n",
            "721/721 [==============================] - 239s 331ms/step - loss: 20.4893 - gender_output_loss: 0.3804 - image_quality_output_loss: 0.8968 - age_output_loss: 1.3213 - weight_output_loss: 0.9412 - bag_output_loss: 0.8017 - pose_output_loss: 0.5054 - footwear_output_loss: 0.8215 - emotion_output_loss: 0.8578 - gender_output_acc: 0.8301 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4152 - weight_output_acc: 0.6410 - bag_output_acc: 0.6494 - pose_output_acc: 0.8016 - footwear_output_acc: 0.6323 - emotion_output_acc: 0.7119 - val_loss: 20.9583 - val_gender_output_loss: 0.3829 - val_image_quality_output_loss: 0.9442 - val_age_output_loss: 1.3314 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8558 - val_pose_output_loss: 0.4707 - val_footwear_output_loss: 0.8115 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.8410 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.4075 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.8292 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00002: val_loss improved from 21.55787 to 20.95826, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.002.h5\n",
            "Epoch 3/300\n",
            "721/721 [==============================] - 239s 331ms/step - loss: 20.6155 - gender_output_loss: 0.3856 - image_quality_output_loss: 0.9047 - age_output_loss: 1.3293 - weight_output_loss: 0.9430 - bag_output_loss: 0.8113 - pose_output_loss: 0.5138 - footwear_output_loss: 0.8257 - emotion_output_loss: 0.8605 - gender_output_acc: 0.8240 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4118 - weight_output_acc: 0.6361 - bag_output_acc: 0.6463 - pose_output_acc: 0.7952 - footwear_output_acc: 0.6286 - emotion_output_acc: 0.7124 - val_loss: 20.8988 - val_gender_output_loss: 0.3599 - val_image_quality_output_loss: 0.9643 - val_age_output_loss: 1.3345 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8074 - val_pose_output_loss: 0.4988 - val_footwear_output_loss: 0.8252 - val_emotion_output_loss: 0.8973 - val_gender_output_acc: 0.8548 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.4090 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6565 - val_pose_output_acc: 0.8046 - val_footwear_output_acc: 0.6516 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00003: val_loss improved from 20.95826 to 20.89876, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.003.h5\n",
            "Epoch 4/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.8671 - gender_output_loss: 0.4068 - image_quality_output_loss: 0.9088 - age_output_loss: 1.3385 - weight_output_loss: 0.9503 - bag_output_loss: 0.8206 - pose_output_loss: 0.5391 - footwear_output_loss: 0.8325 - emotion_output_loss: 0.8682 - gender_output_acc: 0.8149 - image_quality_output_acc: 0.5659 - age_output_acc: 0.4130 - weight_output_acc: 0.6380 - bag_output_acc: 0.6370 - pose_output_acc: 0.7823 - footwear_output_acc: 0.6227 - emotion_output_acc: 0.7114 - val_loss: 21.1898 - val_gender_output_loss: 0.4810 - val_image_quality_output_loss: 0.9511 - val_age_output_loss: 1.3368 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8313 - val_pose_output_loss: 0.5089 - val_footwear_output_loss: 0.8073 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8017 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4011 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6462 - val_pose_output_acc: 0.8130 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.89876\n",
            "Epoch 5/300\n",
            "721/721 [==============================] - 249s 346ms/step - loss: 20.8513 - gender_output_loss: 0.4004 - image_quality_output_loss: 0.9118 - age_output_loss: 1.3398 - weight_output_loss: 0.9512 - bag_output_loss: 0.8196 - pose_output_loss: 0.5395 - footwear_output_loss: 0.8349 - emotion_output_loss: 0.8639 - gender_output_acc: 0.8206 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4103 - weight_output_acc: 0.6390 - bag_output_acc: 0.6408 - pose_output_acc: 0.7782 - footwear_output_acc: 0.6208 - emotion_output_acc: 0.7119 - val_loss: 21.4656 - val_gender_output_loss: 0.3798 - val_image_quality_output_loss: 1.1848 - val_age_output_loss: 1.3321 - val_weight_output_loss: 0.9748 - val_bag_output_loss: 0.8379 - val_pose_output_loss: 0.5080 - val_footwear_output_loss: 0.8333 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.4557 - val_age_output_acc: 0.4045 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6334 - val_pose_output_acc: 0.8012 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.89876\n",
            "Epoch 6/300\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 20.6334 - gender_output_loss: 0.3928 - image_quality_output_loss: 0.9034 - age_output_loss: 1.3288 - weight_output_loss: 0.9432 - bag_output_loss: 0.8120 - pose_output_loss: 0.5195 - footwear_output_loss: 0.8220 - emotion_output_loss: 0.8638 - gender_output_acc: 0.8255 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4216 - weight_output_acc: 0.6394 - bag_output_acc: 0.6403 - pose_output_acc: 0.7911 - footwear_output_acc: 0.6305 - emotion_output_acc: 0.7115 - val_loss: 20.9940 - val_gender_output_loss: 0.3611 - val_image_quality_output_loss: 1.0525 - val_age_output_loss: 1.3373 - val_weight_output_loss: 0.9727 - val_bag_output_loss: 0.8153 - val_pose_output_loss: 0.4658 - val_footwear_output_loss: 0.7976 - val_emotion_output_loss: 0.9161 - val_gender_output_acc: 0.8514 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6486 - val_pose_output_acc: 0.8307 - val_footwear_output_acc: 0.6718 - val_emotion_output_acc: 0.6649\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.89876\n",
            "Epoch 7/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.3580 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.8970 - age_output_loss: 1.3223 - weight_output_loss: 0.9406 - bag_output_loss: 0.7903 - pose_output_loss: 0.4970 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8379 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4188 - weight_output_acc: 0.6401 - bag_output_acc: 0.6601 - pose_output_acc: 0.8032 - footwear_output_acc: 0.6373 - emotion_output_acc: 0.7123 - val_loss: 21.2169 - val_gender_output_loss: 0.3954 - val_image_quality_output_loss: 1.1221 - val_age_output_loss: 1.3389 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.8282 - val_pose_output_loss: 0.4535 - val_footwear_output_loss: 0.8088 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6708\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.89876\n",
            "Epoch 8/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.1027 - gender_output_loss: 0.3561 - image_quality_output_loss: 0.8920 - age_output_loss: 1.3025 - weight_output_loss: 0.9366 - bag_output_loss: 0.7846 - pose_output_loss: 0.4723 - footwear_output_loss: 0.8146 - emotion_output_loss: 0.8512 - gender_output_acc: 0.8427 - image_quality_output_acc: 0.5770 - age_output_acc: 0.4242 - weight_output_acc: 0.6410 - bag_output_acc: 0.6664 - pose_output_acc: 0.8134 - footwear_output_acc: 0.6375 - emotion_output_acc: 0.7126 - val_loss: 20.6763 - val_gender_output_loss: 0.3329 - val_image_quality_output_loss: 1.0473 - val_age_output_loss: 1.3177 - val_weight_output_loss: 0.9757 - val_bag_output_loss: 0.8094 - val_pose_output_loss: 0.4376 - val_footwear_output_loss: 0.8062 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8637 - val_image_quality_output_acc: 0.5354 - val_age_output_acc: 0.4080 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6609 - val_pose_output_acc: 0.8455 - val_footwear_output_acc: 0.6560 - val_emotion_output_acc: 0.6870\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.3580 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.8970 - age_output_loss: 1.3223 - weight_output_loss: 0.9406 - bag_output_loss: 0.7903 - pose_output_loss: 0.4970 - footwear_output_loss: 0.8174 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8379 - image_quality_output_acc: 0.5725 - age_output_acc: 0.4188 - weight_output_acc: 0.6401 - bag_output_acc: 0.6601 - pose_output_acc: 0.8032 - footwear_output_acc: 0.6373 - emotion_output_acc: 0.7123 - val_loss: 21.2169 - val_gender_output_loss: 0.3954 - val_image_quality_output_loss: 1.1221 - val_age_output_loss: 1.3389 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.8282 - val_pose_output_loss: 0.4535 - val_footwear_output_loss: 0.8088 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6708\n",
            "\n",
            "Epoch 00008: val_loss improved from 20.89876 to 20.67635, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.008.h5\n",
            "Epoch 9/300\n",
            "721/721 [==============================] - 245s 340ms/step - loss: 20.0437 - gender_output_loss: 0.3468 - image_quality_output_loss: 0.8906 - age_output_loss: 1.2990 - weight_output_loss: 0.9314 - bag_output_loss: 0.7902 - pose_output_loss: 0.4705 - footwear_output_loss: 0.8073 - emotion_output_loss: 0.8511 - gender_output_acc: 0.8482 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4264 - weight_output_acc: 0.6436 - bag_output_acc: 0.6534 - pose_output_acc: 0.8106 - footwear_output_acc: 0.6410 - emotion_output_acc: 0.7125 - val_loss: 20.5673 - val_gender_output_loss: 0.3716 - val_image_quality_output_loss: 0.9599 - val_age_output_loss: 1.3139 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.8050 - val_pose_output_loss: 0.4514 - val_footwear_output_loss: 0.7967 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.8469 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.4080 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6668 - val_pose_output_acc: 0.8381 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00009: val_loss improved from 20.67635 to 20.56730, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.009.h5\n",
            "Epoch 10/300\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 20.2201 - gender_output_loss: 0.3654 - image_quality_output_loss: 0.8942 - age_output_loss: 1.3155 - weight_output_loss: 0.9371 - bag_output_loss: 0.7912 - pose_output_loss: 0.4824 - footwear_output_loss: 0.8080 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8415 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4172 - weight_output_acc: 0.6403 - bag_output_acc: 0.6622 - pose_output_acc: 0.8089 - footwear_output_acc: 0.6433 - emotion_output_acc: 0.7113 - val_loss: 21.1726 - val_gender_output_loss: 0.3714 - val_image_quality_output_loss: 1.0023 - val_age_output_loss: 1.3685 - val_weight_output_loss: 1.0036 - val_bag_output_loss: 0.8120 - val_pose_output_loss: 0.4844 - val_footwear_output_loss: 0.8149 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.8489 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.4080 - val_weight_output_acc: 0.5763 - val_bag_output_acc: 0.6565 - val_pose_output_acc: 0.8268 - val_footwear_output_acc: 0.6599 - val_emotion_output_acc: 0.6806\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.56730\n",
            "Epoch 11/300\n",
            "721/721 [==============================] - 245s 340ms/step - loss: 20.4863 - gender_output_loss: 0.3848 - image_quality_output_loss: 0.9012 - age_output_loss: 1.3205 - weight_output_loss: 0.9420 - bag_output_loss: 0.8052 - pose_output_loss: 0.5064 - footwear_output_loss: 0.8267 - emotion_output_loss: 0.8634 - gender_output_acc: 0.8281 - image_quality_output_acc: 0.5706 - age_output_acc: 0.4215 - weight_output_acc: 0.6375 - bag_output_acc: 0.6490 - pose_output_acc: 0.7991 - footwear_output_acc: 0.6299 - emotion_output_acc: 0.7102 - val_loss: 20.9467 - val_gender_output_loss: 0.3850 - val_image_quality_output_loss: 0.9256 - val_age_output_loss: 1.3498 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.8195 - val_pose_output_loss: 0.4736 - val_footwear_output_loss: 0.8257 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.8391 - val_image_quality_output_acc: 0.5674 - val_age_output_acc: 0.3937 - val_weight_output_acc: 0.5827 - val_bag_output_acc: 0.6432 - val_pose_output_acc: 0.8312 - val_footwear_output_acc: 0.6481 - val_emotion_output_acc: 0.6762\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 20.56730\n",
            "Epoch 12/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.6958 - gender_output_loss: 0.3980 - image_quality_output_loss: 0.9069 - age_output_loss: 1.3352 - weight_output_loss: 0.9464 - bag_output_loss: 0.8107 - pose_output_loss: 0.5320 - footwear_output_loss: 0.8265 - emotion_output_loss: 0.8644 - gender_output_acc: 0.8194 - image_quality_output_acc: 0.5700 - age_output_acc: 0.4144 - weight_output_acc: 0.6391 - bag_output_acc: 0.6463 - pose_output_acc: 0.7840 - footwear_output_acc: 0.6269 - emotion_output_acc: 0.7120 - val_loss: 21.2787 - val_gender_output_loss: 0.3916 - val_image_quality_output_loss: 0.9524 - val_age_output_loss: 1.3972 - val_weight_output_loss: 0.9847 - val_bag_output_loss: 0.8321 - val_pose_output_loss: 0.5319 - val_footwear_output_loss: 0.7861 - val_emotion_output_loss: 0.9033 - val_gender_output_acc: 0.8219 - val_image_quality_output_acc: 0.5290 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6122 - val_bag_output_acc: 0.6294 - val_pose_output_acc: 0.7982 - val_footwear_output_acc: 0.6688 - val_emotion_output_acc: 0.6875\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 20.56730\n",
            "Epoch 13/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.7181 - gender_output_loss: 0.3976 - image_quality_output_loss: 0.9067 - age_output_loss: 1.3293 - weight_output_loss: 0.9480 - bag_output_loss: 0.8115 - pose_output_loss: 0.5409 - footwear_output_loss: 0.8313 - emotion_output_loss: 0.8642 - gender_output_acc: 0.8180 - image_quality_output_acc: 0.5693 - age_output_acc: 0.4181 - weight_output_acc: 0.6382 - bag_output_acc: 0.6481 - pose_output_acc: 0.7847 - footwear_output_acc: 0.6277 - emotion_output_acc: 0.7119 - val_loss: 21.6243 - val_gender_output_loss: 0.3872 - val_image_quality_output_loss: 1.0356 - val_age_output_loss: 1.3455 - val_weight_output_loss: 0.9670 - val_bag_output_loss: 0.8644 - val_pose_output_loss: 0.5793 - val_footwear_output_loss: 0.8627 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.8406 - val_image_quality_output_acc: 0.5315 - val_age_output_acc: 0.3853 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6403 - val_pose_output_acc: 0.7972 - val_footwear_output_acc: 0.6412 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 20.56730\n",
            "Epoch 14/300\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 20.5054 - gender_output_loss: 0.3787 - image_quality_output_loss: 0.9037 - age_output_loss: 1.3246 - weight_output_loss: 0.9466 - bag_output_loss: 0.8067 - pose_output_loss: 0.5075 - footwear_output_loss: 0.8219 - emotion_output_loss: 0.8618 - gender_output_acc: 0.8278 - image_quality_output_acc: 0.5706 - age_output_acc: 0.4151 - weight_output_acc: 0.6388 - bag_output_acc: 0.6497 - pose_output_acc: 0.7951 - footwear_output_acc: 0.6326 - emotion_output_acc: 0.7113 - val_loss: 20.7578 - val_gender_output_loss: 0.3276 - val_image_quality_output_loss: 1.0125 - val_age_output_loss: 1.3228 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.8080 - val_pose_output_loss: 0.4745 - val_footwear_output_loss: 0.8609 - val_emotion_output_loss: 0.8857 - val_gender_output_acc: 0.8578 - val_image_quality_output_acc: 0.5300 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.6181 - val_bag_output_acc: 0.6580 - val_pose_output_acc: 0.8297 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 20.56730\n",
            "Epoch 15/300\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 20.1777 - gender_output_loss: 0.3565 - image_quality_output_loss: 0.8972 - age_output_loss: 1.3050 - weight_output_loss: 0.9394 - bag_output_loss: 0.7931 - pose_output_loss: 0.4845 - footwear_output_loss: 0.8121 - emotion_output_loss: 0.8537 - gender_output_acc: 0.8438 - image_quality_output_acc: 0.5761 - age_output_acc: 0.4264 - weight_output_acc: 0.6393 - bag_output_acc: 0.6592 - pose_output_acc: 0.8088 - footwear_output_acc: 0.6390 - emotion_output_acc: 0.7128 - val_loss: 20.6914 - val_gender_output_loss: 0.3176 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.3224 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.8094 - val_pose_output_loss: 0.5021 - val_footwear_output_loss: 0.8239 - val_emotion_output_loss: 0.8877 - val_gender_output_acc: 0.8671 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.4213 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6599 - val_pose_output_acc: 0.8194 - val_footwear_output_acc: 0.6427 - val_emotion_output_acc: 0.6959\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 20.56730\n",
            "Epoch 16/300\n",
            "721/721 [==============================] - 238s 331ms/step - loss: 19.9073 - gender_output_loss: 0.3405 - image_quality_output_loss: 0.8885 - age_output_loss: 1.2903 - weight_output_loss: 0.9303 - bag_output_loss: 0.7792 - pose_output_loss: 0.4672 - footwear_output_loss: 0.7984 - emotion_output_loss: 0.8522 - gender_output_acc: 0.8481 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4331 - weight_output_acc: 0.6438 - bag_output_acc: 0.6690 - pose_output_acc: 0.8142 - footwear_output_acc: 0.6438 - emotion_output_acc: 0.7126 - val_loss: 20.5353 - val_gender_output_loss: 0.3178 - val_image_quality_output_loss: 1.0191 - val_age_output_loss: 1.3161 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.8099 - val_pose_output_loss: 0.4324 - val_footwear_output_loss: 0.7932 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.8686 - val_image_quality_output_acc: 0.5374 - val_age_output_acc: 0.4139 - val_weight_output_acc: 0.5989 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8514 - val_footwear_output_acc: 0.6654 - val_emotion_output_acc: 0.6767\n",
            "\n",
            "Epoch 00016: val_loss improved from 20.56730 to 20.53534, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.016.h5\n",
            "Epoch 17/300\n",
            "721/721 [==============================] - 239s 331ms/step - loss: 19.8618 - gender_output_loss: 0.3372 - image_quality_output_loss: 0.8860 - age_output_loss: 1.2912 - weight_output_loss: 0.9281 - bag_output_loss: 0.7790 - pose_output_loss: 0.4563 - footwear_output_loss: 0.8015 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8544 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4298 - weight_output_acc: 0.6419 - bag_output_acc: 0.6672 - pose_output_acc: 0.8234 - footwear_output_acc: 0.6422 - emotion_output_acc: 0.7112 - val_loss: 21.1455 - val_gender_output_loss: 0.3398 - val_image_quality_output_loss: 1.1173 - val_age_output_loss: 1.3279 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8185 - val_pose_output_loss: 0.4746 - val_footwear_output_loss: 0.8530 - val_emotion_output_loss: 0.9153 - val_gender_output_acc: 0.8661 - val_image_quality_output_acc: 0.5148 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.6048 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.8361 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6737\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 20.53534\n",
            "Epoch 18/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.0647 - gender_output_loss: 0.3516 - image_quality_output_loss: 0.8922 - age_output_loss: 1.3033 - weight_output_loss: 0.9343 - bag_output_loss: 0.7917 - pose_output_loss: 0.4741 - footwear_output_loss: 0.8069 - emotion_output_loss: 0.8518 - gender_output_acc: 0.8480 - image_quality_output_acc: 0.5713 - age_output_acc: 0.4275 - weight_output_acc: 0.6422 - bag_output_acc: 0.6579 - pose_output_acc: 0.8135 - footwear_output_acc: 0.6383 - emotion_output_acc: 0.7119 - val_loss: 20.8420 - val_gender_output_loss: 0.3380 - val_image_quality_output_loss: 0.9846 - val_age_output_loss: 1.3381 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.8213 - val_pose_output_loss: 0.4806 - val_footwear_output_loss: 0.7996 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.8538 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.4026 - val_weight_output_acc: 0.5989 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.8322 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 20.53534\n",
            "Epoch 19/300\n",
            "721/721 [==============================] - 239s 331ms/step - loss: 20.3484 - gender_output_loss: 0.3741 - image_quality_output_loss: 0.8966 - age_output_loss: 1.3174 - weight_output_loss: 0.9410 - bag_output_loss: 0.8014 - pose_output_loss: 0.4947 - footwear_output_loss: 0.8207 - emotion_output_loss: 0.8612 - gender_output_acc: 0.8343 - image_quality_output_acc: 0.5736 - age_output_acc: 0.4189 - weight_output_acc: 0.6407 - bag_output_acc: 0.6504 - pose_output_acc: 0.8005 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.7123 - val_loss: 21.0500 - val_gender_output_loss: 0.3615 - val_image_quality_output_loss: 1.0714 - val_age_output_loss: 1.3203 - val_weight_output_loss: 0.9667 - val_bag_output_loss: 0.8142 - val_pose_output_loss: 0.4880 - val_footwear_output_loss: 0.8163 - val_emotion_output_loss: 0.9306 - val_gender_output_acc: 0.8509 - val_image_quality_output_acc: 0.4892 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6009 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8356 - val_footwear_output_acc: 0.6452 - val_emotion_output_acc: 0.6590\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 20.53534\n",
            "Epoch 20/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.6064 - gender_output_loss: 0.3839 - image_quality_output_loss: 0.9071 - age_output_loss: 1.3315 - weight_output_loss: 0.9485 - bag_output_loss: 0.8103 - pose_output_loss: 0.5244 - footwear_output_loss: 0.8239 - emotion_output_loss: 0.8640 - gender_output_acc: 0.8304 - image_quality_output_acc: 0.5684 - age_output_acc: 0.4090 - weight_output_acc: 0.6357 - bag_output_acc: 0.6448 - pose_output_acc: 0.7891 - footwear_output_acc: 0.6266 - emotion_output_acc: 0.7119 - val_loss: 20.7534 - val_gender_output_loss: 0.3678 - val_image_quality_output_loss: 0.9568 - val_age_output_loss: 1.3214 - val_weight_output_loss: 0.9717 - val_bag_output_loss: 0.8439 - val_pose_output_loss: 0.5006 - val_footwear_output_loss: 0.7967 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.8415 - val_image_quality_output_acc: 0.5586 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6304 - val_pose_output_acc: 0.8022 - val_footwear_output_acc: 0.6629 - val_emotion_output_acc: 0.7067\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 20.53534\n",
            "Epoch 21/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.5211 - gender_output_loss: 0.3838 - image_quality_output_loss: 0.9032 - age_output_loss: 1.3265 - weight_output_loss: 0.9429 - bag_output_loss: 0.8074 - pose_output_loss: 0.5114 - footwear_output_loss: 0.8223 - emotion_output_loss: 0.8643 - gender_output_acc: 0.8253 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4162 - weight_output_acc: 0.6415 - bag_output_acc: 0.6526 - pose_output_acc: 0.7968 - footwear_output_acc: 0.6331 - emotion_output_acc: 0.7108 - val_loss: 21.4290 - val_gender_output_loss: 0.3743 - val_image_quality_output_loss: 1.0630 - val_age_output_loss: 1.3519 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8660 - val_pose_output_loss: 0.5207 - val_footwear_output_loss: 0.8276 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.8420 - val_image_quality_output_acc: 0.4946 - val_age_output_acc: 0.3784 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6206 - val_pose_output_acc: 0.7977 - val_footwear_output_acc: 0.6309 - val_emotion_output_acc: 0.6772\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 20.53534\n",
            "Epoch 22/300\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 20.3586 - gender_output_loss: 0.3719 - image_quality_output_loss: 0.8991 - age_output_loss: 1.3144 - weight_output_loss: 0.9413 - bag_output_loss: 0.7957 - pose_output_loss: 0.5071 - footwear_output_loss: 0.8198 - emotion_output_loss: 0.8583 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4181 - weight_output_acc: 0.6399 - bag_output_acc: 0.6569 - pose_output_acc: 0.7979 - footwear_output_acc: 0.6370 - emotion_output_acc: 0.7107 - val_loss: 21.0929 - val_gender_output_loss: 0.3903 - val_image_quality_output_loss: 1.0334 - val_age_output_loss: 1.3644 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.8081 - val_pose_output_loss: 0.4821 - val_footwear_output_loss: 0.8144 - val_emotion_output_loss: 0.9008 - val_gender_output_acc: 0.8376 - val_image_quality_output_acc: 0.5281 - val_age_output_acc: 0.4031 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6580 - val_pose_output_acc: 0.8263 - val_footwear_output_acc: 0.6462 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 20.53534\n",
            "Epoch 23/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 20.0790 - gender_output_loss: 0.3480 - image_quality_output_loss: 0.8930 - age_output_loss: 1.3004 - weight_output_loss: 0.9329 - bag_output_loss: 0.7916 - pose_output_loss: 0.4814 - footwear_output_loss: 0.8070 - emotion_output_loss: 0.8539 - gender_output_acc: 0.8466 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4261 - weight_output_acc: 0.6390 - bag_output_acc: 0.6592 - pose_output_acc: 0.8102 - footwear_output_acc: 0.6390 - emotion_output_acc: 0.7120 - val_loss: 20.5334 - val_gender_output_loss: 0.3302 - val_image_quality_output_loss: 1.0801 - val_age_output_loss: 1.3054 - val_weight_output_loss: 0.9640 - val_bag_output_loss: 0.8035 - val_pose_output_loss: 0.4368 - val_footwear_output_loss: 0.7937 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.8642 - val_image_quality_output_acc: 0.5074 - val_age_output_acc: 0.4237 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6683 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6678 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00023: val_loss improved from 20.53534 to 20.53340, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.023.h5\n",
            "Epoch 24/300\n",
            "721/721 [==============================] - 239s 332ms/step - loss: 19.7515 - gender_output_loss: 0.3259 - image_quality_output_loss: 0.8863 - age_output_loss: 1.2822 - weight_output_loss: 0.9243 - bag_output_loss: 0.7657 - pose_output_loss: 0.4575 - footwear_output_loss: 0.8044 - emotion_output_loss: 0.8515 - gender_output_acc: 0.8607 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4320 - weight_output_acc: 0.6453 - bag_output_acc: 0.6722 - pose_output_acc: 0.8204 - footwear_output_acc: 0.6405 - emotion_output_acc: 0.7119 - val_loss: 20.6650 - val_gender_output_loss: 0.3304 - val_image_quality_output_loss: 1.0087 - val_age_output_loss: 1.3168 - val_weight_output_loss: 0.9659 - val_bag_output_loss: 0.7989 - val_pose_output_loss: 0.4451 - val_footwear_output_loss: 0.8696 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.8696 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.4222 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.6708 - val_pose_output_acc: 0.8514 - val_footwear_output_acc: 0.6393 - val_emotion_output_acc: 0.6821\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 20.53340\n",
            "Epoch 25/300\n",
            "721/721 [==============================] - 240s 332ms/step - loss: 19.6767 - gender_output_loss: 0.3245 - image_quality_output_loss: 0.8836 - age_output_loss: 1.2787 - weight_output_loss: 0.9203 - bag_output_loss: 0.7718 - pose_output_loss: 0.4507 - footwear_output_loss: 0.7946 - emotion_output_loss: 0.8477 - gender_output_acc: 0.8621 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4378 - weight_output_acc: 0.6477 - bag_output_acc: 0.6678 - pose_output_acc: 0.8249 - footwear_output_acc: 0.6420 - emotion_output_acc: 0.7130 - val_loss: 20.7402 - val_gender_output_loss: 0.3273 - val_image_quality_output_loss: 1.1403 - val_age_output_loss: 1.3192 - val_weight_output_loss: 0.9728 - val_bag_output_loss: 0.7993 - val_pose_output_loss: 0.4428 - val_footwear_output_loss: 0.7959 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.8612 - val_image_quality_output_acc: 0.4719 - val_age_output_acc: 0.4085 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6604 - val_pose_output_acc: 0.8386 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 20.53340\n",
            "Epoch 26/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 19.9111 - gender_output_loss: 0.3409 - image_quality_output_loss: 0.8878 - age_output_loss: 1.2879 - weight_output_loss: 0.9297 - bag_output_loss: 0.7811 - pose_output_loss: 0.4710 - footwear_output_loss: 0.8047 - emotion_output_loss: 0.8540 - gender_output_acc: 0.8518 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4288 - weight_output_acc: 0.6407 - bag_output_acc: 0.6644 - pose_output_acc: 0.8136 - footwear_output_acc: 0.6410 - emotion_output_acc: 0.7118 - val_loss: 21.1957 - val_gender_output_loss: 0.3711 - val_image_quality_output_loss: 1.0686 - val_age_output_loss: 1.3495 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.8095 - val_pose_output_loss: 0.4869 - val_footwear_output_loss: 0.8546 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.8465 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.4144 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8268 - val_footwear_output_acc: 0.6378 - val_emotion_output_acc: 0.6772\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 20.53340\n",
            "Epoch 27/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.2225 - gender_output_loss: 0.3642 - image_quality_output_loss: 0.8986 - age_output_loss: 1.3084 - weight_output_loss: 0.9362 - bag_output_loss: 0.7928 - pose_output_loss: 0.4963 - footwear_output_loss: 0.8102 - emotion_output_loss: 0.8589 - gender_output_acc: 0.8367 - image_quality_output_acc: 0.5752 - age_output_acc: 0.4214 - weight_output_acc: 0.6422 - bag_output_acc: 0.6570 - pose_output_acc: 0.8047 - footwear_output_acc: 0.6382 - emotion_output_acc: 0.7113 - val_loss: 20.4870 - val_gender_output_loss: 0.3716 - val_image_quality_output_loss: 0.9099 - val_age_output_loss: 1.3281 - val_weight_output_loss: 0.9705 - val_bag_output_loss: 0.8061 - val_pose_output_loss: 0.4664 - val_footwear_output_loss: 0.8078 - val_emotion_output_loss: 0.8830 - val_gender_output_acc: 0.8455 - val_image_quality_output_acc: 0.5679 - val_age_output_acc: 0.4114 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6678 - val_pose_output_acc: 0.8238 - val_footwear_output_acc: 0.6432 - val_emotion_output_acc: 0.6929\n",
            "\n",
            "Epoch 00027: val_loss improved from 20.53340 to 20.48702, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.027.h5\n",
            "Epoch 28/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.4361 - gender_output_loss: 0.3850 - image_quality_output_loss: 0.9020 - age_output_loss: 1.3237 - weight_output_loss: 0.9445 - bag_output_loss: 0.8034 - pose_output_loss: 0.4979 - footwear_output_loss: 0.8213 - emotion_output_loss: 0.8615 - gender_output_acc: 0.8264 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4224 - weight_output_acc: 0.6393 - bag_output_acc: 0.6468 - pose_output_acc: 0.8018 - footwear_output_acc: 0.6406 - emotion_output_acc: 0.7110 - val_loss: 21.3428 - val_gender_output_loss: 0.3515 - val_image_quality_output_loss: 1.0518 - val_age_output_loss: 1.3416 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.8898 - val_pose_output_loss: 0.4696 - val_footwear_output_loss: 0.8297 - val_emotion_output_loss: 0.9261 - val_gender_output_acc: 0.8583 - val_image_quality_output_acc: 0.5157 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.5901 - val_bag_output_acc: 0.6206 - val_pose_output_acc: 0.8268 - val_footwear_output_acc: 0.6407 - val_emotion_output_acc: 0.6565\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 20.48702\n",
            "Epoch 29/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.4711 - gender_output_loss: 0.3812 - image_quality_output_loss: 0.9039 - age_output_loss: 1.3197 - weight_output_loss: 0.9423 - bag_output_loss: 0.8040 - pose_output_loss: 0.5078 - footwear_output_loss: 0.8273 - emotion_output_loss: 0.8626 - gender_output_acc: 0.8298 - image_quality_output_acc: 0.5683 - age_output_acc: 0.4215 - weight_output_acc: 0.6429 - bag_output_acc: 0.6524 - pose_output_acc: 0.7917 - footwear_output_acc: 0.6285 - emotion_output_acc: 0.7097 - val_loss: 20.8811 - val_gender_output_loss: 0.3248 - val_image_quality_output_loss: 1.0481 - val_age_output_loss: 1.3167 - val_weight_output_loss: 0.9668 - val_bag_output_loss: 0.8045 - val_pose_output_loss: 0.5437 - val_footwear_output_loss: 0.8153 - val_emotion_output_loss: 0.8839 - val_gender_output_acc: 0.8652 - val_image_quality_output_acc: 0.5079 - val_age_output_acc: 0.4109 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6407 - val_pose_output_acc: 0.7864 - val_footwear_output_acc: 0.6388 - val_emotion_output_acc: 0.7018\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 20.48702\n",
            "Epoch 30/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.2652 - gender_output_loss: 0.3579 - image_quality_output_loss: 0.8985 - age_output_loss: 1.3154 - weight_output_loss: 0.9376 - bag_output_loss: 0.7926 - pose_output_loss: 0.4933 - footwear_output_loss: 0.8138 - emotion_output_loss: 0.8591 - gender_output_acc: 0.8432 - image_quality_output_acc: 0.5744 - age_output_acc: 0.4223 - weight_output_acc: 0.6423 - bag_output_acc: 0.6595 - pose_output_acc: 0.8037 - footwear_output_acc: 0.6369 - emotion_output_acc: 0.7106 - val_loss: 21.1339 - val_gender_output_loss: 0.3603 - val_image_quality_output_loss: 1.0040 - val_age_output_loss: 1.3344 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.9025 - val_pose_output_loss: 0.4774 - val_footwear_output_loss: 0.8310 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8578 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.5974 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.8312 - val_footwear_output_acc: 0.6521 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 20.48702\n",
            "Epoch 31/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 19.9744 - gender_output_loss: 0.3431 - image_quality_output_loss: 0.8906 - age_output_loss: 1.2961 - weight_output_loss: 0.9308 - bag_output_loss: 0.7864 - pose_output_loss: 0.4642 - footwear_output_loss: 0.8039 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8547 - image_quality_output_acc: 0.5788 - age_output_acc: 0.4333 - weight_output_acc: 0.6418 - bag_output_acc: 0.6688 - pose_output_acc: 0.8188 - footwear_output_acc: 0.6403 - emotion_output_acc: 0.7117 - val_loss: 21.1604 - val_gender_output_loss: 0.3219 - val_image_quality_output_loss: 1.1089 - val_age_output_loss: 1.3206 - val_weight_output_loss: 1.0323 - val_bag_output_loss: 0.8220 - val_pose_output_loss: 0.4600 - val_footwear_output_loss: 0.8559 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.4104 - val_weight_output_acc: 0.5556 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8371 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6826\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 20.48702\n",
            "Epoch 32/300\n",
            "720/721 [============================>.] - ETA: 0s - loss: 19.6151 - gender_output_loss: 0.3159 - image_quality_output_loss: 0.8842 - age_output_loss: 1.2733 - weight_output_loss: 0.9221 - bag_output_loss: 0.7641 - pose_output_loss: 0.4433 - footwear_output_loss: 0.7980 - emotion_output_loss: 0.8487 - gender_output_acc: 0.8654 - image_quality_output_acc: 0.5785 - age_output_acc: 0.4347 - weight_output_acc: 0.6444 - bag_output_acc: 0.6795 - pose_output_acc: 0.8264 - footwear_output_acc: 0.6471 - emotion_output_acc: 0.7117\n",
            "Epoch 00031: val_loss did not improve from 20.48702\n",
            "Epoch 32/300\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 19.6173 - gender_output_loss: 0.3159 - image_quality_output_loss: 0.8839 - age_output_loss: 1.2734 - weight_output_loss: 0.9219 - bag_output_loss: 0.7641 - pose_output_loss: 0.4436 - footwear_output_loss: 0.7982 - emotion_output_loss: 0.8492 - gender_output_acc: 0.8655 - image_quality_output_acc: 0.5785 - age_output_acc: 0.4347 - weight_output_acc: 0.6445 - bag_output_acc: 0.6795 - pose_output_acc: 0.8263 - footwear_output_acc: 0.6472 - emotion_output_acc: 0.7115 - val_loss: 20.6430 - val_gender_output_loss: 0.3066 - val_image_quality_output_loss: 1.0428 - val_age_output_loss: 1.3090 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.8118 - val_pose_output_loss: 0.4506 - val_footwear_output_loss: 0.8269 - val_emotion_output_loss: 0.8988 - val_gender_output_acc: 0.8848 - val_image_quality_output_acc: 0.5404 - val_age_output_acc: 0.4232 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6786 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 20.48702\n",
            "Epoch 33/300\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 19.5359 - gender_output_loss: 0.3141 - image_quality_output_loss: 0.8841 - age_output_loss: 1.2701 - weight_output_loss: 0.9179 - bag_output_loss: 0.7619 - pose_output_loss: 0.4364 - footwear_output_loss: 0.7915 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8662 - image_quality_output_acc: 0.5785 - age_output_acc: 0.4434 - weight_output_acc: 0.6499 - bag_output_acc: 0.6745 - pose_output_acc: 0.8312 - footwear_output_acc: 0.6502 - emotion_output_acc: 0.7114 - val_loss: 20.9184 - val_gender_output_loss: 0.3211 - val_image_quality_output_loss: 1.0965 - val_age_output_loss: 1.3310 - val_weight_output_loss: 1.0003 - val_bag_output_loss: 0.8096 - val_pose_output_loss: 0.4525 - val_footwear_output_loss: 0.8250 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8720 - val_image_quality_output_acc: 0.5157 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.5763 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 20.48702\n",
            "Epoch 34/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.8153 - gender_output_loss: 0.3346 - image_quality_output_loss: 0.8854 - age_output_loss: 1.2865 - weight_output_loss: 0.9275 - bag_output_loss: 0.7742 - pose_output_loss: 0.4628 - footwear_output_loss: 0.8011 - emotion_output_loss: 0.8499 - gender_output_acc: 0.8538 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4359 - weight_output_acc: 0.6453 - bag_output_acc: 0.6705 - pose_output_acc: 0.8198 - footwear_output_acc: 0.6390 - emotion_output_acc: 0.7123 - val_loss: 20.8084 - val_gender_output_loss: 0.3375 - val_image_quality_output_loss: 1.0149 - val_age_output_loss: 1.3194 - val_weight_output_loss: 0.9678 - val_bag_output_loss: 0.8235 - val_pose_output_loss: 0.4886 - val_footwear_output_loss: 0.7859 - val_emotion_output_loss: 0.9205 - val_gender_output_acc: 0.8652 - val_image_quality_output_acc: 0.5581 - val_age_output_acc: 0.4001 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6570 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6580\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 20.48702\n",
            "Epoch 35/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.1636 - gender_output_loss: 0.3625 - image_quality_output_loss: 0.8976 - age_output_loss: 1.3013 - weight_output_loss: 0.9327 - bag_output_loss: 0.7910 - pose_output_loss: 0.4903 - footwear_output_loss: 0.8119 - emotion_output_loss: 0.8594 - gender_output_acc: 0.8425 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4254 - weight_output_acc: 0.6403 - bag_output_acc: 0.6563 - pose_output_acc: 0.8068 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.7112 - val_loss: 21.1400 - val_gender_output_loss: 0.3233 - val_image_quality_output_loss: 1.0621 - val_age_output_loss: 1.3508 - val_weight_output_loss: 0.9665 - val_bag_output_loss: 0.8174 - val_pose_output_loss: 0.5165 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.9237 - val_gender_output_acc: 0.8652 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6019 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.8287 - val_footwear_output_acc: 0.6506 - val_emotion_output_acc: 0.6604\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 20.48702\n",
            "Epoch 36/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.4081 - gender_output_loss: 0.3765 - image_quality_output_loss: 0.9037 - age_output_loss: 1.3185 - weight_output_loss: 0.9370 - bag_output_loss: 0.8004 - pose_output_loss: 0.5091 - footwear_output_loss: 0.8232 - emotion_output_loss: 0.8602 - gender_output_acc: 0.8327 - image_quality_output_acc: 0.5709 - age_output_acc: 0.4188 - weight_output_acc: 0.6418 - bag_output_acc: 0.6544 - pose_output_acc: 0.7993 - footwear_output_acc: 0.6329 - emotion_output_acc: 0.7104 - val_loss: 20.9139 - val_gender_output_loss: 0.3417 - val_image_quality_output_loss: 0.8873 - val_age_output_loss: 1.3595 - val_weight_output_loss: 0.9711 - val_bag_output_loss: 0.8038 - val_pose_output_loss: 0.4850 - val_footwear_output_loss: 0.8992 - val_emotion_output_loss: 0.9206 - val_gender_output_acc: 0.8588 - val_image_quality_output_acc: 0.5812 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6048 - val_bag_output_acc: 0.6565 - val_pose_output_acc: 0.8322 - val_footwear_output_acc: 0.6225 - val_emotion_output_acc: 0.6713\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 20.48702\n",
            "Epoch 37/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 20.4594 - gender_output_loss: 0.3718 - image_quality_output_loss: 0.9021 - age_output_loss: 1.3183 - weight_output_loss: 0.9444 - bag_output_loss: 0.8010 - pose_output_loss: 0.5151 - footwear_output_loss: 0.8221 - emotion_output_loss: 0.8621 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4192 - weight_output_acc: 0.6401 - bag_output_acc: 0.6548 - pose_output_acc: 0.7935 - footwear_output_acc: 0.6286 - emotion_output_acc: 0.7121 - val_loss: 20.7326 - val_gender_output_loss: 0.3720 - val_image_quality_output_loss: 1.0099 - val_age_output_loss: 1.3088 - val_weight_output_loss: 0.9517 - val_bag_output_loss: 0.8359 - val_pose_output_loss: 0.4638 - val_footwear_output_loss: 0.8235 - val_emotion_output_loss: 0.8912 - val_gender_output_acc: 0.8509 - val_image_quality_output_acc: 0.5133 - val_age_output_acc: 0.4080 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6501 - val_pose_output_acc: 0.8297 - val_footwear_output_acc: 0.6319 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 20.48702\n",
            "Epoch 38/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.1394 - gender_output_loss: 0.3508 - image_quality_output_loss: 0.8914 - age_output_loss: 1.3034 - weight_output_loss: 0.9353 - bag_output_loss: 0.7847 - pose_output_loss: 0.4891 - footwear_output_loss: 0.8095 - emotion_output_loss: 0.8569 - gender_output_acc: 0.8467 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4261 - weight_output_acc: 0.6412 - bag_output_acc: 0.6598 - pose_output_acc: 0.8040 - footwear_output_acc: 0.6387 - emotion_output_acc: 0.7119 - val_loss: 20.6062 - val_gender_output_loss: 0.3741 - val_image_quality_output_loss: 1.0094 - val_age_output_loss: 1.3009 - val_weight_output_loss: 0.9497 - val_bag_output_loss: 0.8406 - val_pose_output_loss: 0.4412 - val_footwear_output_loss: 0.8169 - val_emotion_output_loss: 0.8854 - val_gender_output_acc: 0.8366 - val_image_quality_output_acc: 0.5394 - val_age_output_acc: 0.4247 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6393 - val_pose_output_acc: 0.8420 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6841\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 20.48702\n",
            "Epoch 39/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.8354 - gender_output_loss: 0.3330 - image_quality_output_loss: 0.8882 - age_output_loss: 1.2817 - weight_output_loss: 0.9245 - bag_output_loss: 0.7794 - pose_output_loss: 0.4616 - footwear_output_loss: 0.8034 - emotion_output_loss: 0.8502 - gender_output_acc: 0.8558 - image_quality_output_acc: 0.5802 - age_output_acc: 0.4333 - weight_output_acc: 0.6451 - bag_output_acc: 0.6676 - pose_output_acc: 0.8205 - footwear_output_acc: 0.6435 - emotion_output_acc: 0.7128 - val_loss: 20.5918 - val_gender_output_loss: 0.3267 - val_image_quality_output_loss: 1.1098 - val_age_output_loss: 1.2844 - val_weight_output_loss: 0.9604 - val_bag_output_loss: 0.8073 - val_pose_output_loss: 0.4531 - val_footwear_output_loss: 0.7959 - val_emotion_output_loss: 0.8921 - val_gender_output_acc: 0.8671 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.4247 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.6673 - val_emotion_output_acc: 0.6836\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 20.48702\n",
            "Epoch 40/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.4908 - gender_output_loss: 0.3035 - image_quality_output_loss: 0.8798 - age_output_loss: 1.2639 - weight_output_loss: 0.9152 - bag_output_loss: 0.7627 - pose_output_loss: 0.4402 - footwear_output_loss: 0.7903 - emotion_output_loss: 0.8446 - gender_output_acc: 0.8698 - image_quality_output_acc: 0.5839 - age_output_acc: 0.4406 - weight_output_acc: 0.6447 - bag_output_acc: 0.6826 - pose_output_acc: 0.8295 - footwear_output_acc: 0.6505 - emotion_output_acc: 0.7115 - val_loss: 20.5182 - val_gender_output_loss: 0.3105 - val_image_quality_output_loss: 1.1055 - val_age_output_loss: 1.2887 - val_weight_output_loss: 0.9631 - val_bag_output_loss: 0.7851 - val_pose_output_loss: 0.4450 - val_footwear_output_loss: 0.8001 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.8804 - val_image_quality_output_acc: 0.5059 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6850 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6727 - val_emotion_output_acc: 0.6757\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 20.48702\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 20.48702\n",
            "Epoch 41/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.4485 - gender_output_loss: 0.3054 - image_quality_output_loss: 0.8812 - age_output_loss: 1.2616 - weight_output_loss: 0.9151 - bag_output_loss: 0.7576 - pose_output_loss: 0.4336 - footwear_output_loss: 0.7867 - emotion_output_loss: 0.8462 - gender_output_acc: 0.8696 - image_quality_output_acc: 0.5811 - age_output_acc: 0.4471 - weight_output_acc: 0.6466 - bag_output_acc: 0.6820 - pose_output_acc: 0.8321 - footwear_output_acc: 0.6519 - emotion_output_acc: 0.7132 - val_loss: 20.1527 - val_gender_output_loss: 0.2960 - val_image_quality_output_loss: 1.0200 - val_age_output_loss: 1.2797 - val_weight_output_loss: 0.9586 - val_bag_output_loss: 0.7854 - val_pose_output_loss: 0.4337 - val_footwear_output_loss: 0.7877 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5290 - val_age_output_acc: 0.4336 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6796 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6668 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 20.48702\n",
            "Epoch 00041: val_loss improved from 20.48702 to 20.15274, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.041.h5\n",
            "Epoch 42/300\n",
            "721/721 [==============================] - 240s 333ms/step - loss: 19.7125 - gender_output_loss: 0.3225 - image_quality_output_loss: 0.8882 - age_output_loss: 1.2763 - weight_output_loss: 0.9205 - bag_output_loss: 0.7746 - pose_output_loss: 0.4542 - footwear_output_loss: 0.7973 - emotion_output_loss: 0.8491 - gender_output_acc: 0.8640 - image_quality_output_acc: 0.5784 - age_output_acc: 0.4335 - weight_output_acc: 0.6457 - bag_output_acc: 0.6706 - pose_output_acc: 0.8253 - footwear_output_acc: 0.6483 - emotion_output_acc: 0.7109 - val_loss: 20.6536 - val_gender_output_loss: 0.3499 - val_image_quality_output_loss: 0.9474 - val_age_output_loss: 1.3394 - val_weight_output_loss: 0.9676 - val_bag_output_loss: 0.7995 - val_pose_output_loss: 0.4869 - val_footwear_output_loss: 0.8222 - val_emotion_output_loss: 0.8872 - val_gender_output_acc: 0.8632 - val_image_quality_output_acc: 0.5659 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6088 - val_bag_output_acc: 0.6693 - val_pose_output_acc: 0.8233 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 20.15274\n",
            "Epoch 43/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.0530 - gender_output_loss: 0.3451 - image_quality_output_loss: 0.8905 - age_output_loss: 1.3011 - weight_output_loss: 0.9271 - bag_output_loss: 0.7883 - pose_output_loss: 0.4800 - footwear_output_loss: 0.8092 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8490 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4231 - weight_output_acc: 0.6459 - bag_output_acc: 0.6594 - pose_output_acc: 0.8076 - footwear_output_acc: 0.6380 - emotion_output_acc: 0.7116 - val_loss: 20.8648 - val_gender_output_loss: 0.3568 - val_image_quality_output_loss: 0.9451 - val_age_output_loss: 1.4109 - val_weight_output_loss: 0.9702 - val_bag_output_loss: 0.7958 - val_pose_output_loss: 0.4473 - val_footwear_output_loss: 0.8415 - val_emotion_output_loss: 0.8850 - val_gender_output_acc: 0.8593 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4085 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6506 - val_pose_output_acc: 0.8322 - val_footwear_output_acc: 0.6363 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 20.15274\n",
            "Epoch 44/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.3516 - gender_output_loss: 0.3677 - image_quality_output_loss: 0.9008 - age_output_loss: 1.3126 - weight_output_loss: 0.9385 - bag_output_loss: 0.8018 - pose_output_loss: 0.5039 - footwear_output_loss: 0.8170 - emotion_output_loss: 0.8586 - gender_output_acc: 0.8409 - image_quality_output_acc: 0.5680 - age_output_acc: 0.4219 - weight_output_acc: 0.6428 - bag_output_acc: 0.6524 - pose_output_acc: 0.7962 - footwear_output_acc: 0.6371 - emotion_output_acc: 0.7118 - val_loss: 20.3942 - val_gender_output_loss: 0.3134 - val_image_quality_output_loss: 0.9200 - val_age_output_loss: 1.3284 - val_weight_output_loss: 0.9583 - val_bag_output_loss: 0.7952 - val_pose_output_loss: 0.4935 - val_footwear_output_loss: 0.7872 - val_emotion_output_loss: 0.8811 - val_gender_output_acc: 0.8642 - val_image_quality_output_acc: 0.5492 - val_age_output_acc: 0.4272 - val_weight_output_acc: 0.6142 - val_bag_output_acc: 0.6531 - val_pose_output_acc: 0.8140 - val_footwear_output_acc: 0.6619 - val_emotion_output_acc: 0.6993\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 20.15274\n",
            "Epoch 45/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.3801 - gender_output_loss: 0.3721 - image_quality_output_loss: 0.8981 - age_output_loss: 1.3092 - weight_output_loss: 0.9382 - bag_output_loss: 0.8035 - pose_output_loss: 0.5036 - footwear_output_loss: 0.8150 - emotion_output_loss: 0.8637 - gender_output_acc: 0.8345 - image_quality_output_acc: 0.5699 - age_output_acc: 0.4252 - weight_output_acc: 0.6392 - bag_output_acc: 0.6505 - pose_output_acc: 0.7992 - footwear_output_acc: 0.6357 - emotion_output_acc: 0.7113 - val_loss: 21.0448 - val_gender_output_loss: 0.3316 - val_image_quality_output_loss: 1.0359 - val_age_output_loss: 1.3530 - val_weight_output_loss: 0.9892 - val_bag_output_loss: 0.8095 - val_pose_output_loss: 0.5109 - val_footwear_output_loss: 0.7926 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.8632 - val_image_quality_output_acc: 0.5064 - val_age_output_acc: 0.3898 - val_weight_output_acc: 0.6033 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6678 - val_emotion_output_acc: 0.6806\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 20.15274\n",
            "Epoch 46/300\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 20.0724 - gender_output_loss: 0.3427 - image_quality_output_loss: 0.8937 - age_output_loss: 1.2969 - weight_output_loss: 0.9306 - bag_output_loss: 0.7828 - pose_output_loss: 0.4794 - footwear_output_loss: 0.8069 - emotion_output_loss: 0.8585 - gender_output_acc: 0.8509 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4290 - weight_output_acc: 0.6416 - bag_output_acc: 0.6631 - pose_output_acc: 0.8115 - footwear_output_acc: 0.6435 - emotion_output_acc: 0.7120 - val_loss: 20.8968 - val_gender_output_loss: 0.3140 - val_image_quality_output_loss: 1.0597 - val_age_output_loss: 1.3043 - val_weight_output_loss: 1.0319 - val_bag_output_loss: 0.8034 - val_pose_output_loss: 0.4668 - val_footwear_output_loss: 0.7877 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.8760 - val_image_quality_output_acc: 0.5118 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.5723 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.8425 - val_footwear_output_acc: 0.6688 - val_emotion_output_acc: 0.6634\n",
            "\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 20.15274\n",
            "Epoch 47/300\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 19.8535 - gender_output_loss: 0.3291 - image_quality_output_loss: 0.8842 - age_output_loss: 1.2865 - weight_output_loss: 0.9259 - bag_output_loss: 0.7700 - pose_output_loss: 0.4677 - footwear_output_loss: 0.8027 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8563 - image_quality_output_acc: 0.5807 - age_output_acc: 0.4295 - weight_output_acc: 0.6439 - bag_output_acc: 0.6724 - pose_output_acc: 0.8161 - footwear_output_acc: 0.6395 - emotion_output_acc: 0.7117 - val_loss: 20.8995 - val_gender_output_loss: 0.3213 - val_image_quality_output_loss: 1.1692 - val_age_output_loss: 1.3031 - val_weight_output_loss: 0.9560 - val_bag_output_loss: 0.7968 - val_pose_output_loss: 0.4668 - val_footwear_output_loss: 0.8208 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.4813 - val_age_output_acc: 0.4213 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6762 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.6575\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 20.15274\n",
            "Epoch 48/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.4800 - gender_output_loss: 0.3039 - image_quality_output_loss: 0.8837 - age_output_loss: 1.2599 - weight_output_loss: 0.9126 - bag_output_loss: 0.7623 - pose_output_loss: 0.4358 - footwear_output_loss: 0.7889 - emotion_output_loss: 0.8453 - gender_output_acc: 0.8699 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4424 - weight_output_acc: 0.6463 - bag_output_acc: 0.6780 - pose_output_acc: 0.8309 - footwear_output_acc: 0.6456 - emotion_output_acc: 0.7113 - val_loss: 20.0865 - val_gender_output_loss: 0.2995 - val_image_quality_output_loss: 0.9654 - val_age_output_loss: 1.2816 - val_weight_output_loss: 0.9473 - val_bag_output_loss: 0.7834 - val_pose_output_loss: 0.4332 - val_footwear_output_loss: 0.8220 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.8853 - val_image_quality_output_acc: 0.5576 - val_age_output_acc: 0.4134 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6811 - val_pose_output_acc: 0.8538 - val_footwear_output_acc: 0.6683 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00048: val_loss improved from 20.15274 to 20.08646, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5\n",
            "Epoch 49/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.3341 - gender_output_loss: 0.2952 - image_quality_output_loss: 0.8812 - age_output_loss: 1.2526 - weight_output_loss: 0.9101 - bag_output_loss: 0.7511 - pose_output_loss: 0.4250 - footwear_output_loss: 0.7838 - emotion_output_loss: 0.8438 - gender_output_acc: 0.8748 - image_quality_output_acc: 0.5824 - age_output_acc: 0.4452 - weight_output_acc: 0.6471 - bag_output_acc: 0.6891 - pose_output_acc: 0.8344 - footwear_output_acc: 0.6531 - emotion_output_acc: 0.7099 - val_loss: 20.3795 - val_gender_output_loss: 0.3035 - val_image_quality_output_loss: 1.1288 - val_age_output_loss: 1.2749 - val_weight_output_loss: 0.9439 - val_bag_output_loss: 0.7930 - val_pose_output_loss: 0.4475 - val_footwear_output_loss: 0.7812 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.8780 - val_image_quality_output_acc: 0.4759 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6737 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6772 - val_emotion_output_acc: 0.6831\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 20.08646\n",
            "Epoch 50/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.6797 - gender_output_loss: 0.3179 - image_quality_output_loss: 0.8836 - age_output_loss: 1.2716 - weight_output_loss: 0.9191 - bag_output_loss: 0.7764 - pose_output_loss: 0.4463 - footwear_output_loss: 0.8033 - emotion_output_loss: 0.8483 - gender_output_acc: 0.8622 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4400 - weight_output_acc: 0.6465 - bag_output_acc: 0.6676 - pose_output_acc: 0.8271 - footwear_output_acc: 0.6434 - emotion_output_acc: 0.7119 - val_loss: 21.2201 - val_gender_output_loss: 0.3456 - val_image_quality_output_loss: 1.1447 - val_age_output_loss: 1.3039 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.8013 - val_pose_output_loss: 0.5574 - val_footwear_output_loss: 0.8151 - val_emotion_output_loss: 0.8955 - val_gender_output_acc: 0.8661 - val_image_quality_output_acc: 0.4936 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6654 - val_pose_output_acc: 0.8120 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.6752\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 20.08646\n",
            "Epoch 51/300\n",
            "721/721 [==============================] - 241s 335ms/step - loss: 19.9916 - gender_output_loss: 0.3391 - image_quality_output_loss: 0.8918 - age_output_loss: 1.2913 - weight_output_loss: 0.9281 - bag_output_loss: 0.7871 - pose_output_loss: 0.4719 - footwear_output_loss: 0.8030 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8505 - image_quality_output_acc: 0.5796 - age_output_acc: 0.4282 - weight_output_acc: 0.6428 - bag_output_acc: 0.6674 - pose_output_acc: 0.8156 - footwear_output_acc: 0.6426 - emotion_output_acc: 0.7120 - val_loss: 20.4488 - val_gender_output_loss: 0.3112 - val_image_quality_output_loss: 1.0113 - val_age_output_loss: 1.2900 - val_weight_output_loss: 0.9529 - val_bag_output_loss: 0.7813 - val_pose_output_loss: 0.4780 - val_footwear_output_loss: 0.8076 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.8637 - val_image_quality_output_acc: 0.5153 - val_age_output_acc: 0.4218 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6644 - val_pose_output_acc: 0.8145 - val_footwear_output_acc: 0.6393 - val_emotion_output_acc: 0.6762\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 20.08646\n",
            "Epoch 52/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.2876 - gender_output_loss: 0.3657 - image_quality_output_loss: 0.8965 - age_output_loss: 1.3098 - weight_output_loss: 0.9364 - bag_output_loss: 0.7935 - pose_output_loss: 0.4982 - footwear_output_loss: 0.8153 - emotion_output_loss: 0.8564 - gender_output_acc: 0.8370 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4212 - weight_output_acc: 0.6408 - bag_output_acc: 0.6573 - pose_output_acc: 0.8083 - footwear_output_acc: 0.6345 - emotion_output_acc: 0.7111 - val_loss: 21.7393 - val_gender_output_loss: 0.4320 - val_image_quality_output_loss: 1.1971 - val_age_output_loss: 1.3388 - val_weight_output_loss: 0.9682 - val_bag_output_loss: 0.8515 - val_pose_output_loss: 0.4986 - val_footwear_output_loss: 0.8563 - val_emotion_output_loss: 0.9161 - val_gender_output_acc: 0.8302 - val_image_quality_output_acc: 0.4631 - val_age_output_acc: 0.3809 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6319 - val_pose_output_acc: 0.8105 - val_footwear_output_acc: 0.6176 - val_emotion_output_acc: 0.6624\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 20.08646\n",
            "Epoch 53/300\n",
            "720/721 [============================>.] - ETA: 0s - loss: 20.2839 - gender_output_loss: 0.3647 - image_quality_output_loss: 0.8982 - age_output_loss: 1.3139 - weight_output_loss: 0.9369 - bag_output_loss: 0.7947 - pose_output_loss: 0.4877 - footwear_output_loss: 0.8070 - emotion_output_loss: 0.8567 - gender_output_acc: 0.8380 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4216 - weight_output_acc: 0.6407 - bag_output_acc: 0.6589 - pose_output_acc: 0.8041 - footwear_output_acc: 0.6386 - emotion_output_acc: 0.7109\n",
            "Epoch 00052: val_loss did not improve from 20.08646\n",
            "Epoch 53/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.2840 - gender_output_loss: 0.3648 - image_quality_output_loss: 0.8984 - age_output_loss: 1.3135 - weight_output_loss: 0.9370 - bag_output_loss: 0.7947 - pose_output_loss: 0.4880 - footwear_output_loss: 0.8070 - emotion_output_loss: 0.8567 - gender_output_acc: 0.8380 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4220 - weight_output_acc: 0.6408 - bag_output_acc: 0.6589 - pose_output_acc: 0.8039 - footwear_output_acc: 0.6385 - emotion_output_acc: 0.7109 - val_loss: 21.2357 - val_gender_output_loss: 0.3481 - val_image_quality_output_loss: 1.0991 - val_age_output_loss: 1.3212 - val_weight_output_loss: 0.9651 - val_bag_output_loss: 0.8100 - val_pose_output_loss: 0.5423 - val_footwear_output_loss: 0.8635 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.8607 - val_image_quality_output_acc: 0.5074 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8017 - val_footwear_output_acc: 0.6378 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 20.08646\n",
            "Epoch 54/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.0263 - gender_output_loss: 0.3456 - image_quality_output_loss: 0.8881 - age_output_loss: 1.2924 - weight_output_loss: 0.9302 - bag_output_loss: 0.7838 - pose_output_loss: 0.4722 - footwear_output_loss: 0.8044 - emotion_output_loss: 0.8532 - gender_output_acc: 0.8479 - image_quality_output_acc: 0.5775 - age_output_acc: 0.4281 - weight_output_acc: 0.6423 - bag_output_acc: 0.6653 - pose_output_acc: 0.8163 - footwear_output_acc: 0.6425 - emotion_output_acc: 0.7119 - val_loss: 20.4958 - val_gender_output_loss: 0.3027 - val_image_quality_output_loss: 0.9474 - val_age_output_loss: 1.3201 - val_weight_output_loss: 0.9686 - val_bag_output_loss: 0.8172 - val_pose_output_loss: 0.4722 - val_footwear_output_loss: 0.8082 - val_emotion_output_loss: 0.8792 - val_gender_output_acc: 0.8819 - val_image_quality_output_acc: 0.5669 - val_age_output_acc: 0.4075 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8381 - val_footwear_output_acc: 0.6658 - val_emotion_output_acc: 0.7013\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 20.08646\n",
            "Epoch 55/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.7204 - gender_output_loss: 0.3174 - image_quality_output_loss: 0.8863 - age_output_loss: 1.2745 - weight_output_loss: 0.9179 - bag_output_loss: 0.7662 - pose_output_loss: 0.4559 - footwear_output_loss: 0.7964 - emotion_output_loss: 0.8495 - gender_output_acc: 0.8649 - image_quality_output_acc: 0.5757 - age_output_acc: 0.4359 - weight_output_acc: 0.6461 - bag_output_acc: 0.6779 - pose_output_acc: 0.8240 - footwear_output_acc: 0.6444 - emotion_output_acc: 0.7106 - val_loss: 20.8339 - val_gender_output_loss: 0.3159 - val_image_quality_output_loss: 1.1272 - val_age_output_loss: 1.3141 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.7944 - val_pose_output_loss: 0.4531 - val_footwear_output_loss: 0.8182 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.8696 - val_image_quality_output_acc: 0.4902 - val_age_output_acc: 0.4124 - val_weight_output_acc: 0.5910 - val_bag_output_acc: 0.6781 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6747 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 20.08646\n",
            "Epoch 56/300\n",
            "720/721 [============================>.] - ETA: 0s - loss: 19.4047 - gender_output_loss: 0.3085 - image_quality_output_loss: 0.8784 - age_output_loss: 1.2542 - weight_output_loss: 0.9082 - bag_output_loss: 0.7544 - pose_output_loss: 0.4267 - footwear_output_loss: 0.7863 - emotion_output_loss: 0.8441 - gender_output_acc: 0.8688 - image_quality_output_acc: 0.5852 - age_output_acc: 0.4434 - weight_output_acc: 0.6471 - bag_output_acc: 0.6839 - pose_output_acc: 0.8357 - footwear_output_acc: 0.6542 - emotion_output_acc: 0.7128\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.4052 - gender_output_loss: 0.3082 - image_quality_output_loss: 0.8786 - age_output_loss: 1.2542 - weight_output_loss: 0.9083 - bag_output_loss: 0.7545 - pose_output_loss: 0.4268 - footwear_output_loss: 0.7864 - emotion_output_loss: 0.8440 - gender_output_acc: 0.8689 - image_quality_output_acc: 0.5851 - age_output_acc: 0.4432 - weight_output_acc: 0.6469 - bag_output_acc: 0.6839 - pose_output_acc: 0.8356 - footwear_output_acc: 0.6541 - emotion_output_acc: 0.7130 - val_loss: 20.4841 - val_gender_output_loss: 0.2909 - val_image_quality_output_loss: 1.0859 - val_age_output_loss: 1.2938 - val_weight_output_loss: 0.9557 - val_bag_output_loss: 0.7808 - val_pose_output_loss: 0.4756 - val_footwear_output_loss: 0.7808 - val_emotion_output_loss: 0.8906 - val_gender_output_acc: 0.8824 - val_image_quality_output_acc: 0.5192 - val_age_output_acc: 0.4208 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6845 - val_pose_output_acc: 0.8381 - val_footwear_output_acc: 0.6875 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 20.08646\n",
            "Epoch 57/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.2982 - gender_output_loss: 0.2977 - image_quality_output_loss: 0.8752 - age_output_loss: 1.2481 - weight_output_loss: 0.9047 - bag_output_loss: 0.7542 - pose_output_loss: 0.4228 - footwear_output_loss: 0.7810 - emotion_output_loss: 0.8399 - gender_output_acc: 0.8724 - image_quality_output_acc: 0.5841 - age_output_acc: 0.4470 - weight_output_acc: 0.6496 - bag_output_acc: 0.6846 - pose_output_acc: 0.8366 - footwear_output_acc: 0.6524 - emotion_output_acc: 0.7116 - val_loss: 20.9042 - val_gender_output_loss: 0.3714 - val_image_quality_output_loss: 1.0570 - val_age_output_loss: 1.3077 - val_weight_output_loss: 0.9592 - val_bag_output_loss: 0.8378 - val_pose_output_loss: 0.4554 - val_footwear_output_loss: 0.8461 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.8617 - val_image_quality_output_acc: 0.5285 - val_age_output_acc: 0.4434 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6619 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6909\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 20.08646\n",
            "Epoch 58/300\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.5678 - gender_output_loss: 0.3119 - image_quality_output_loss: 0.8814 - age_output_loss: 1.2624 - weight_output_loss: 0.9137 - bag_output_loss: 0.7624 - pose_output_loss: 0.4499 - footwear_output_loss: 0.7913 - emotion_output_loss: 0.8456 - gender_output_acc: 0.8664 - image_quality_output_acc: 0.5786 - age_output_acc: 0.4399 - weight_output_acc: 0.6447 - bag_output_acc: 0.6823 - pose_output_acc: 0.8238 - footwear_output_acc: 0.6494 - emotion_output_acc: 0.7119 - val_loss: 20.7308 - val_gender_output_loss: 0.3320 - val_image_quality_output_loss: 1.0133 - val_age_output_loss: 1.3208 - val_weight_output_loss: 0.9710 - val_bag_output_loss: 0.7915 - val_pose_output_loss: 0.4781 - val_footwear_output_loss: 0.8177 - val_emotion_output_loss: 0.9030 - val_gender_output_acc: 0.8809 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.4331 - val_weight_output_acc: 0.6284 - val_bag_output_acc: 0.6860 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.6486 - val_emotion_output_acc: 0.6747\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 20.08646\n",
            "Epoch 59/300\n",
            "450/721 [=================>............] - ETA: 1:22 - loss: 19.8037 - gender_output_loss: 0.3303 - image_quality_output_loss: 0.8857 - age_output_loss: 1.2782 - weight_output_loss: 0.9156 - bag_output_loss: 0.7771 - pose_output_loss: 0.4640 - footwear_output_loss: 0.8040 - emotion_output_loss: 0.8478 - gender_output_acc: 0.8546 - image_quality_output_acc: 0.5811 - age_output_acc: 0.4306 - weight_output_acc: 0.6501 - bag_output_acc: 0.6717 - pose_output_acc: 0.8214 - footwear_output_acc: 0.6394 - emotion_output_acc: 0.7150"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-772:\n",
            "Process ForkPoolWorker-769:\n",
            "Process ForkPoolWorker-771:\n",
            "Process ForkPoolWorker-770:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-767:\n",
            "Process ForkPoolWorker-765:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-766:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkPoolWorker-768:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"<ipython-input-6-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-80a5bc4e6929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#class_weight=loss_weights_train,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHxLkazbVFRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "def get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.json\",count=2):\n",
        "    #path = \"/content/gdrive/My Drive/WRN_Extend/\"\n",
        "    file_path = os.path.join(path)+pattern\n",
        "    files = [f for f in glob.iglob(file_path, recursive=False)]\n",
        "    latest_file = sorted(files, key=os.path.getctime,reverse=True)\n",
        "    return latest_file[:count]\n",
        "#files = os.listdir(path)\n",
        "#filenamere.split(r'/',files[len(files)-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGE4A64fVL-l",
        "colab_type": "code",
        "outputId": "be3d5682-85e2-4058-e9f1-b9c6a7613197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw5cAATBUH0e",
        "colab_type": "code",
        "outputId": "b9d1174a-238f-4782-e61f-1be303de8457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=10\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Returning new callback array with steps_per_epoch= 721 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 10 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json1577618037_backup\n",
            "Epoch 1/10\n",
            "721/721 [==============================] - 248s 344ms/step - loss: 19.3692 - gender_output_loss: 0.3011 - image_quality_output_loss: 0.8745 - age_output_loss: 1.2492 - weight_output_loss: 0.9110 - bag_output_loss: 0.7575 - pose_output_loss: 0.4355 - footwear_output_loss: 0.7860 - emotion_output_loss: 0.8419 - gender_output_acc: 0.8701 - image_quality_output_acc: 0.5822 - age_output_acc: 0.4459 - weight_output_acc: 0.6468 - bag_output_acc: 0.6815 - pose_output_acc: 0.8336 - footwear_output_acc: 0.6507 - emotion_output_acc: 0.7135 - val_loss: 21.2337 - val_gender_output_loss: 0.3331 - val_image_quality_output_loss: 1.3140 - val_age_output_loss: 1.2986 - val_weight_output_loss: 0.9675 - val_bag_output_loss: 0.8012 - val_pose_output_loss: 0.4868 - val_footwear_output_loss: 0.8160 - val_emotion_output_loss: 0.8950 - val_gender_output_acc: 0.8750 - val_image_quality_output_acc: 0.4434 - val_age_output_acc: 0.4218 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6732 - val_pose_output_acc: 0.8342 - val_footwear_output_acc: 0.6678 - val_emotion_output_acc: 0.6954\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 21.23374, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.001.h5\n",
            "Epoch 2/10\n",
            "721/721 [==============================] - 239s 331ms/step - loss: 19.6468 - gender_output_loss: 0.3196 - image_quality_output_loss: 0.8858 - age_output_loss: 1.2709 - weight_output_loss: 0.9163 - bag_output_loss: 0.7642 - pose_output_loss: 0.4516 - footwear_output_loss: 0.8010 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8613 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4415 - weight_output_acc: 0.6454 - bag_output_acc: 0.6783 - pose_output_acc: 0.8240 - footwear_output_acc: 0.6481 - emotion_output_acc: 0.7123 - val_loss: 20.6059 - val_gender_output_loss: 0.3064 - val_image_quality_output_loss: 1.1243 - val_age_output_loss: 1.3105 - val_weight_output_loss: 0.9624 - val_bag_output_loss: 0.8070 - val_pose_output_loss: 0.4398 - val_footwear_output_loss: 0.7910 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.8799 - val_image_quality_output_acc: 0.4847 - val_age_output_acc: 0.4060 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.8479 - val_footwear_output_acc: 0.6565 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00002: val_loss improved from 21.23374 to 20.60586, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.002.h5\n",
            "Epoch 3/10\n",
            "721/721 [==============================] - 238s 331ms/step - loss: 20.0167 - gender_output_loss: 0.3461 - image_quality_output_loss: 0.8885 - age_output_loss: 1.2913 - weight_output_loss: 0.9297 - bag_output_loss: 0.7882 - pose_output_loss: 0.4764 - footwear_output_loss: 0.8065 - emotion_output_loss: 0.8548 - gender_output_acc: 0.8480 - image_quality_output_acc: 0.5791 - age_output_acc: 0.4307 - weight_output_acc: 0.6429 - bag_output_acc: 0.6600 - pose_output_acc: 0.8126 - footwear_output_acc: 0.6410 - emotion_output_acc: 0.7129 - val_loss: 20.5662 - val_gender_output_loss: 0.3688 - val_image_quality_output_loss: 1.0364 - val_age_output_loss: 1.3204 - val_weight_output_loss: 0.9482 - val_bag_output_loss: 0.8094 - val_pose_output_loss: 0.4416 - val_footwear_output_loss: 0.7949 - val_emotion_output_loss: 0.8787 - val_gender_output_acc: 0.8489 - val_image_quality_output_acc: 0.5010 - val_age_output_acc: 0.4237 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6457 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6658 - val_emotion_output_acc: 0.7032\n",
            "\n",
            "Epoch 00003: val_loss improved from 20.60586 to 20.56615, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.003.h5\n",
            "Epoch 4/10\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.2851 - gender_output_loss: 0.3674 - image_quality_output_loss: 0.9010 - age_output_loss: 1.3034 - weight_output_loss: 0.9330 - bag_output_loss: 0.7956 - pose_output_loss: 0.4987 - footwear_output_loss: 0.8142 - emotion_output_loss: 0.8605 - gender_output_acc: 0.8356 - image_quality_output_acc: 0.5698 - age_output_acc: 0.4278 - weight_output_acc: 0.6423 - bag_output_acc: 0.6552 - pose_output_acc: 0.7994 - footwear_output_acc: 0.6319 - emotion_output_acc: 0.7134 - val_loss: 20.7558 - val_gender_output_loss: 0.3422 - val_image_quality_output_loss: 0.9871 - val_age_output_loss: 1.3555 - val_weight_output_loss: 0.9568 - val_bag_output_loss: 0.8305 - val_pose_output_loss: 0.4533 - val_footwear_output_loss: 0.7938 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.8661 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3652 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6398 - val_pose_output_acc: 0.8332 - val_footwear_output_acc: 0.6614 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.56615\n",
            "Epoch 5/10\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.3171 - gender_output_loss: 0.3693 - image_quality_output_loss: 0.8982 - age_output_loss: 1.3077 - weight_output_loss: 0.9349 - bag_output_loss: 0.7972 - pose_output_loss: 0.4996 - footwear_output_loss: 0.8110 - emotion_output_loss: 0.8580 - gender_output_acc: 0.8367 - image_quality_output_acc: 0.5716 - age_output_acc: 0.4210 - weight_output_acc: 0.6403 - bag_output_acc: 0.6560 - pose_output_acc: 0.8015 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.7119 - val_loss: 20.8624 - val_gender_output_loss: 0.3249 - val_image_quality_output_loss: 0.9591 - val_age_output_loss: 1.3314 - val_weight_output_loss: 0.9491 - val_bag_output_loss: 0.8516 - val_pose_output_loss: 0.5345 - val_footwear_output_loss: 0.7846 - val_emotion_output_loss: 0.8968 - val_gender_output_acc: 0.8642 - val_image_quality_output_acc: 0.5497 - val_age_output_acc: 0.4075 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6506 - val_pose_output_acc: 0.8036 - val_footwear_output_acc: 0.6619 - val_emotion_output_acc: 0.7028\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.56615\n",
            "Epoch 6/10\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 20.0452 - gender_output_loss: 0.3418 - image_quality_output_loss: 0.8919 - age_output_loss: 1.2957 - weight_output_loss: 0.9259 - bag_output_loss: 0.7820 - pose_output_loss: 0.4774 - footwear_output_loss: 0.8054 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8479 - image_quality_output_acc: 0.5773 - age_output_acc: 0.4282 - weight_output_acc: 0.6447 - bag_output_acc: 0.6652 - pose_output_acc: 0.8159 - footwear_output_acc: 0.6438 - emotion_output_acc: 0.7113 - val_loss: 21.8718 - val_gender_output_loss: 0.4115 - val_image_quality_output_loss: 1.1262 - val_age_output_loss: 1.3685 - val_weight_output_loss: 0.9730 - val_bag_output_loss: 0.8249 - val_pose_output_loss: 0.5488 - val_footwear_output_loss: 0.9195 - val_emotion_output_loss: 0.9092 - val_gender_output_acc: 0.8450 - val_image_quality_output_acc: 0.5074 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8031 - val_footwear_output_acc: 0.6412 - val_emotion_output_acc: 0.6969\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.56615\n",
            "Epoch 7/10\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.7125 - gender_output_loss: 0.3164 - image_quality_output_loss: 0.8835 - age_output_loss: 1.2719 - weight_output_loss: 0.9237 - bag_output_loss: 0.7692 - pose_output_loss: 0.4494 - footwear_output_loss: 0.7956 - emotion_output_loss: 0.8512 - gender_output_acc: 0.8660 - image_quality_output_acc: 0.5805 - age_output_acc: 0.4377 - weight_output_acc: 0.6455 - bag_output_acc: 0.6746 - pose_output_acc: 0.8239 - footwear_output_acc: 0.6460 - emotion_output_acc: 0.7112 - val_loss: 20.9695 - val_gender_output_loss: 0.2905 - val_image_quality_output_loss: 1.1422 - val_age_output_loss: 1.3193 - val_weight_output_loss: 0.9548 - val_bag_output_loss: 0.8359 - val_pose_output_loss: 0.4625 - val_footwear_output_loss: 0.8206 - val_emotion_output_loss: 0.9069 - val_gender_output_acc: 0.8922 - val_image_quality_output_acc: 0.5108 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8499 - val_footwear_output_acc: 0.6624 - val_emotion_output_acc: 0.6663\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.56615\n",
            "Epoch 8/10\n",
            "721/721 [==============================] - 240s 334ms/step - loss: 19.3953 - gender_output_loss: 0.3066 - image_quality_output_loss: 0.8764 - age_output_loss: 1.2483 - weight_output_loss: 0.9080 - bag_output_loss: 0.7577 - pose_output_loss: 0.4355 - footwear_output_loss: 0.7839 - emotion_output_loss: 0.8422 - gender_output_acc: 0.8710 - image_quality_output_acc: 0.5866 - age_output_acc: 0.4450 - weight_output_acc: 0.6500 - bag_output_acc: 0.6798 - pose_output_acc: 0.8330 - footwear_output_acc: 0.6501 - emotion_output_acc: 0.7126 - val_loss: 20.6381 - val_gender_output_loss: 0.2969 - val_image_quality_output_loss: 1.1393 - val_age_output_loss: 1.2947 - val_weight_output_loss: 0.9589 - val_bag_output_loss: 0.7984 - val_pose_output_loss: 0.4388 - val_footwear_output_loss: 0.7975 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.8903 - val_image_quality_output_acc: 0.5094 - val_age_output_acc: 0.4222 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6727 - val_pose_output_acc: 0.8494 - val_footwear_output_acc: 0.6718 - val_emotion_output_acc: 0.6747\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 20.56615\n",
            "Epoch 9/10\n",
            "721/721 [==============================] - 241s 334ms/step - loss: 19.2996 - gender_output_loss: 0.2954 - image_quality_output_loss: 0.8757 - age_output_loss: 1.2475 - weight_output_loss: 0.9064 - bag_output_loss: 0.7489 - pose_output_loss: 0.4277 - footwear_output_loss: 0.7795 - emotion_output_loss: 0.8420 - gender_output_acc: 0.8755 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4515 - weight_output_acc: 0.6494 - bag_output_acc: 0.6841 - pose_output_acc: 0.8370 - footwear_output_acc: 0.6520 - emotion_output_acc: 0.7129 - val_loss: 20.1438 - val_gender_output_loss: 0.2848 - val_image_quality_output_loss: 1.0424 - val_age_output_loss: 1.2766 - val_weight_output_loss: 0.9400 - val_bag_output_loss: 0.7891 - val_pose_output_loss: 0.4356 - val_footwear_output_loss: 0.7856 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.8848 - val_image_quality_output_acc: 0.5281 - val_age_output_acc: 0.4400 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6737 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6713 - val_emotion_output_acc: 0.6860\n",
            "\n",
            "Epoch 00009: val_loss improved from 20.56615 to 20.14383, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.009.h5\n",
            "Epoch 10/10\n",
            "721/721 [==============================] - 242s 335ms/step - loss: 19.5851 - gender_output_loss: 0.3165 - image_quality_output_loss: 0.8860 - age_output_loss: 1.2648 - weight_output_loss: 0.9121 - bag_output_loss: 0.7637 - pose_output_loss: 0.4458 - footwear_output_loss: 0.7890 - emotion_output_loss: 0.8479 - gender_output_acc: 0.8647 - image_quality_output_acc: 0.5805 - age_output_acc: 0.4348 - weight_output_acc: 0.6469 - bag_output_acc: 0.6791 - pose_output_acc: 0.8287 - footwear_output_acc: 0.6525 - emotion_output_acc: 0.7127 - val_loss: 20.3988 - val_gender_output_loss: 0.3032 - val_image_quality_output_loss: 1.0289 - val_age_output_loss: 1.2878 - val_weight_output_loss: 0.9525 - val_bag_output_loss: 0.7898 - val_pose_output_loss: 0.4724 - val_footwear_output_loss: 0.7951 - val_emotion_output_loss: 0.8907 - val_gender_output_acc: 0.8755 - val_image_quality_output_acc: 0.5266 - val_age_output_acc: 0.4321 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6796 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.14383\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json1577620465_backup\n",
            "End of EPOCHS= 10  STEPS_PER_EPOCH= 721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBlGV4gKikGh",
        "colab_type": "code",
        "outputId": "fe3e9810-c47b-4037-98ed-f4214446d48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        }
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=10\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file[0])\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.009.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 10 patience= 10\n",
            "Epoch 1/10\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 19.3736 - gender_output_loss: 0.3038 - image_quality_output_loss: 0.8809 - age_output_loss: 1.2445 - weight_output_loss: 0.9062 - bag_output_loss: 0.7561 - pose_output_loss: 0.4332 - footwear_output_loss: 0.7853 - emotion_output_loss: 0.8453 - gender_output_acc: 0.8703 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4550 - weight_output_acc: 0.6483 - bag_output_acc: 0.6844 - pose_output_acc: 0.8274 - footwear_output_acc: 0.6460 - emotion_output_acc: 0.7123 - val_loss: 20.5468 - val_gender_output_loss: 0.3107 - val_image_quality_output_loss: 1.0824 - val_age_output_loss: 1.3138 - val_weight_output_loss: 0.9511 - val_bag_output_loss: 0.7943 - val_pose_output_loss: 0.4386 - val_footwear_output_loss: 0.8197 - val_emotion_output_loss: 0.8818 - val_gender_output_acc: 0.8834 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.4232 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6757 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6993\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 20.54684, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.001.h5\n",
            "Epoch 2/10\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 19.5620 - gender_output_loss: 0.3183 - image_quality_output_loss: 0.8809 - age_output_loss: 1.2739 - weight_output_loss: 0.9200 - bag_output_loss: 0.7617 - pose_output_loss: 0.4416 - footwear_output_loss: 0.7738 - emotion_output_loss: 0.8412 - gender_output_acc: 0.8599 - image_quality_output_acc: 0.5859 - age_output_acc: 0.4347 - weight_output_acc: 0.6488 - bag_output_acc: 0.6750 - pose_output_acc: 0.8267 - footwear_output_acc: 0.6571 - emotion_output_acc: 0.7128 - val_loss: 21.4302 - val_gender_output_loss: 0.3305 - val_image_quality_output_loss: 1.0286 - val_age_output_loss: 1.3311 - val_weight_output_loss: 0.9626 - val_bag_output_loss: 0.9000 - val_pose_output_loss: 0.5441 - val_footwear_output_loss: 0.8563 - val_emotion_output_loss: 0.9174 - val_gender_output_acc: 0.8730 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.4267 - val_weight_output_acc: 0.6294 - val_bag_output_acc: 0.6378 - val_pose_output_acc: 0.8204 - val_footwear_output_acc: 0.6491 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 20.54684\n",
            "Epoch 3/10\n",
            "360/360 [==============================] - 131s 363ms/step - loss: 19.9350 - gender_output_loss: 0.3428 - image_quality_output_loss: 0.8914 - age_output_loss: 1.2834 - weight_output_loss: 0.9297 - bag_output_loss: 0.7694 - pose_output_loss: 0.4635 - footwear_output_loss: 0.7942 - emotion_output_loss: 0.8673 - gender_output_acc: 0.8523 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4387 - weight_output_acc: 0.6399 - bag_output_acc: 0.6687 - pose_output_acc: 0.8161 - footwear_output_acc: 0.6488 - emotion_output_acc: 0.7089 - val_loss: 20.5731 - val_gender_output_loss: 0.3430 - val_image_quality_output_loss: 1.0073 - val_age_output_loss: 1.3181 - val_weight_output_loss: 0.9672 - val_bag_output_loss: 0.7910 - val_pose_output_loss: 0.4677 - val_footwear_output_loss: 0.8058 - val_emotion_output_loss: 0.8799 - val_gender_output_acc: 0.8494 - val_image_quality_output_acc: 0.5128 - val_age_output_acc: 0.4296 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6683 - val_pose_output_acc: 0.8184 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.7057\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.54684\n",
            "Epoch 4/10\n",
            "360/360 [==============================] - 126s 349ms/step - loss: 20.3156 - gender_output_loss: 0.3577 - image_quality_output_loss: 0.9069 - age_output_loss: 1.3096 - weight_output_loss: 0.9283 - bag_output_loss: 0.8020 - pose_output_loss: 0.5050 - footwear_output_loss: 0.8283 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8418 - image_quality_output_acc: 0.5658 - age_output_acc: 0.4170 - weight_output_acc: 0.6431 - bag_output_acc: 0.6498 - pose_output_acc: 0.7995 - footwear_output_acc: 0.6283 - emotion_output_acc: 0.7132 - val_loss: 20.7002 - val_gender_output_loss: 0.3604 - val_image_quality_output_loss: 0.9192 - val_age_output_loss: 1.3375 - val_weight_output_loss: 0.9641 - val_bag_output_loss: 0.8244 - val_pose_output_loss: 0.4797 - val_footwear_output_loss: 0.8155 - val_emotion_output_loss: 0.8876 - val_gender_output_acc: 0.8499 - val_image_quality_output_acc: 0.5689 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.8169 - val_footwear_output_acc: 0.6476 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.54684\n",
            "Epoch 5/10\n",
            "359/360 [============================>.] - ETA: 0s - loss: 20.3513 - gender_output_loss: 0.3651 - image_quality_output_loss: 0.8967 - age_output_loss: 1.3120 - weight_output_loss: 0.9420 - bag_output_loss: 0.7877 - pose_output_loss: 0.4983 - footwear_output_loss: 0.8213 - emotion_output_loss: 0.8605 - gender_output_acc: 0.8360 - image_quality_output_acc: 0.5796 - age_output_acc: 0.4269 - weight_output_acc: 0.6361 - bag_output_acc: 0.6569 - pose_output_acc: 0.8034 - footwear_output_acc: 0.6334 - emotion_output_acc: 0.7077\n",
            "Epoch 00004: val_loss did not improve from 20.54684\n",
            "Epoch 5/10\n",
            "360/360 [==============================] - 129s 360ms/step - loss: 20.3491 - gender_output_loss: 0.3650 - image_quality_output_loss: 0.8968 - age_output_loss: 1.3127 - weight_output_loss: 0.9414 - bag_output_loss: 0.7875 - pose_output_loss: 0.4997 - footwear_output_loss: 0.8216 - emotion_output_loss: 0.8588 - gender_output_acc: 0.8359 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4266 - weight_output_acc: 0.6366 - bag_output_acc: 0.6573 - pose_output_acc: 0.8033 - footwear_output_acc: 0.6328 - emotion_output_acc: 0.7085 - val_loss: 21.0810 - val_gender_output_loss: 0.3505 - val_image_quality_output_loss: 0.9683 - val_age_output_loss: 1.3595 - val_weight_output_loss: 0.9979 - val_bag_output_loss: 0.7911 - val_pose_output_loss: 0.5231 - val_footwear_output_loss: 0.8043 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.8588 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.3927 - val_weight_output_acc: 0.5999 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8194 - val_footwear_output_acc: 0.6476 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.54684\n",
            "Epoch 6/10\n",
            "360/360 [==============================] - 126s 349ms/step - loss: 19.9680 - gender_output_loss: 0.3387 - image_quality_output_loss: 0.8952 - age_output_loss: 1.2856 - weight_output_loss: 0.9235 - bag_output_loss: 0.7922 - pose_output_loss: 0.4675 - footwear_output_loss: 0.7945 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8495 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4271 - weight_output_acc: 0.6486 - bag_output_acc: 0.6528 - pose_output_acc: 0.8205 - footwear_output_acc: 0.6465 - emotion_output_acc: 0.7146 - val_loss: 20.7585 - val_gender_output_loss: 0.3237 - val_image_quality_output_loss: 0.9927 - val_age_output_loss: 1.3077 - val_weight_output_loss: 0.9672 - val_bag_output_loss: 0.8277 - val_pose_output_loss: 0.4578 - val_footwear_output_loss: 0.8154 - val_emotion_output_loss: 0.9229 - val_gender_output_acc: 0.8716 - val_image_quality_output_acc: 0.5374 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.5965 - val_bag_output_acc: 0.6511 - val_pose_output_acc: 0.8391 - val_footwear_output_acc: 0.6698 - val_emotion_output_acc: 0.6604\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.54684\n",
            "Epoch 7/10\n",
            "360/360 [==============================] - 131s 364ms/step - loss: 19.5897 - gender_output_loss: 0.3128 - image_quality_output_loss: 0.8873 - age_output_loss: 1.2608 - weight_output_loss: 0.9009 - bag_output_loss: 0.7737 - pose_output_loss: 0.4552 - footwear_output_loss: 0.7827 - emotion_output_loss: 0.8447 - gender_output_acc: 0.8655 - image_quality_output_acc: 0.5832 - age_output_acc: 0.4476 - weight_output_acc: 0.6519 - bag_output_acc: 0.6740 - pose_output_acc: 0.8200 - footwear_output_acc: 0.6557 - emotion_output_acc: 0.7109 - val_loss: 20.7758 - val_gender_output_loss: 0.3114 - val_image_quality_output_loss: 1.1364 - val_age_output_loss: 1.2962 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.7956 - val_pose_output_loss: 0.4558 - val_footwear_output_loss: 0.7810 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.8814 - val_image_quality_output_acc: 0.4985 - val_age_output_acc: 0.4139 - val_weight_output_acc: 0.6088 - val_bag_output_acc: 0.6644 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6732 - val_emotion_output_acc: 0.6609\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.54684\n",
            "Epoch 8/10\n",
            "360/360 [==============================] - 126s 349ms/step - loss: 19.5597 - gender_output_loss: 0.3138 - image_quality_output_loss: 0.8766 - age_output_loss: 1.2619 - weight_output_loss: 0.9269 - bag_output_loss: 0.7644 - pose_output_loss: 0.4324 - footwear_output_loss: 0.7894 - emotion_output_loss: 0.8431 - gender_output_acc: 0.8672 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4318 - weight_output_acc: 0.6413 - bag_output_acc: 0.6736 - pose_output_acc: 0.8271 - footwear_output_acc: 0.6438 - emotion_output_acc: 0.7125 - val_loss: 20.1998 - val_gender_output_loss: 0.2899 - val_image_quality_output_loss: 1.0443 - val_age_output_loss: 1.2799 - val_weight_output_loss: 0.9454 - val_bag_output_loss: 0.7756 - val_pose_output_loss: 0.4451 - val_footwear_output_loss: 0.7733 - val_emotion_output_loss: 0.8897 - val_gender_output_acc: 0.8789 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.4306 - val_weight_output_acc: 0.6299 - val_bag_output_acc: 0.6663 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6722 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00008: val_loss improved from 20.54684 to 20.19984, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.008.h5\n",
            "Epoch 9/10\n",
            "360/360 [==============================] - 130s 362ms/step - loss: 19.2805 - gender_output_loss: 0.2866 - image_quality_output_loss: 0.8714 - age_output_loss: 1.2383 - weight_output_loss: 0.9133 - bag_output_loss: 0.7473 - pose_output_loss: 0.4246 - footwear_output_loss: 0.7879 - emotion_output_loss: 0.8431 - gender_output_acc: 0.8793 - image_quality_output_acc: 0.5859 - age_output_acc: 0.4488 - weight_output_acc: 0.6411 - bag_output_acc: 0.6915 - pose_output_acc: 0.8313 - footwear_output_acc: 0.6422 - emotion_output_acc: 0.7134 - val_loss: 21.1225 - val_gender_output_loss: 0.3120 - val_image_quality_output_loss: 1.1129 - val_age_output_loss: 1.3139 - val_weight_output_loss: 0.9678 - val_bag_output_loss: 0.8986 - val_pose_output_loss: 0.4561 - val_footwear_output_loss: 0.8139 - val_emotion_output_loss: 0.9041 - val_gender_output_acc: 0.8893 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.8479 - val_footwear_output_acc: 0.6599 - val_emotion_output_acc: 0.6777\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.19984\n",
            "Epoch 10/10\n",
            "360/360 [==============================] - 127s 353ms/step - loss: 19.6242 - gender_output_loss: 0.3154 - image_quality_output_loss: 0.8895 - age_output_loss: 1.2689 - weight_output_loss: 0.9194 - bag_output_loss: 0.7638 - pose_output_loss: 0.4410 - footwear_output_loss: 0.7881 - emotion_output_loss: 0.8465 - gender_output_acc: 0.8649 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4420 - weight_output_acc: 0.6450 - bag_output_acc: 0.6748 - pose_output_acc: 0.8304 - footwear_output_acc: 0.6559 - emotion_output_acc: 0.7099 - val_loss: 21.0760 - val_gender_output_loss: 0.3758 - val_image_quality_output_loss: 0.9908 - val_age_output_loss: 1.3740 - val_weight_output_loss: 0.9599 - val_bag_output_loss: 0.8248 - val_pose_output_loss: 0.4869 - val_footwear_output_loss: 0.8140 - val_emotion_output_loss: 0.9002 - val_gender_output_acc: 0.8597 - val_image_quality_output_acc: 0.5477 - val_age_output_acc: 0.3912 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6521 - val_pose_output_acc: 0.8287 - val_footwear_output_acc: 0.6619 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.19984\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json1577622782_backup\n",
            "End of EPOCHS= 10  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGswX2HOosur",
        "colab_type": "code",
        "outputId": "df3605bb-e80d-4fdf-ba00-3772a89f785b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file[0])\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.008.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 20 patience= 10\n",
            "Epoch 1/20\n",
            "  2/360 [..............................] - ETA: 22:27 - loss: 20.1980 - gender_output_loss: 0.3671 - image_quality_output_loss: 0.9147 - age_output_loss: 1.2631 - weight_output_loss: 1.0153 - bag_output_loss: 0.7794 - pose_output_loss: 0.4663 - footwear_output_loss: 0.7984 - emotion_output_loss: 0.8485 - gender_output_acc: 0.8125 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3125 - weight_output_acc: 0.6250 - bag_output_acc: 0.5625 - pose_output_acc: 0.7812 - footwear_output_acc: 0.6250 - emotion_output_acc: 0.7188"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.269534). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 133s 369ms/step - loss: 19.3729 - gender_output_loss: 0.2924 - image_quality_output_loss: 0.8749 - age_output_loss: 1.2509 - weight_output_loss: 0.9045 - bag_output_loss: 0.7522 - pose_output_loss: 0.4265 - footwear_output_loss: 0.7897 - emotion_output_loss: 0.8497 - gender_output_acc: 0.8766 - image_quality_output_acc: 0.5875 - age_output_acc: 0.4585 - weight_output_acc: 0.6524 - bag_output_acc: 0.6811 - pose_output_acc: 0.8330 - footwear_output_acc: 0.6491 - emotion_output_acc: 0.7109 - val_loss: 20.8405 - val_gender_output_loss: 0.3326 - val_image_quality_output_loss: 1.1180 - val_age_output_loss: 1.3155 - val_weight_output_loss: 0.9654 - val_bag_output_loss: 0.8002 - val_pose_output_loss: 0.4429 - val_footwear_output_loss: 0.7951 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.8814 - val_image_quality_output_acc: 0.5157 - val_age_output_acc: 0.4247 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6742 - val_pose_output_acc: 0.8509 - val_footwear_output_acc: 0.6663 - val_emotion_output_acc: 0.6639\n",
            "Epoch 1/20\n",
            "Epoch 00001: val_loss improved from inf to 20.84050, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 125s 347ms/step - loss: 19.5728 - gender_output_loss: 0.3152 - image_quality_output_loss: 0.8861 - age_output_loss: 1.2621 - weight_output_loss: 0.9148 - bag_output_loss: 0.7658 - pose_output_loss: 0.4429 - footwear_output_loss: 0.7865 - emotion_output_loss: 0.8435 - gender_output_acc: 0.8639 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4347 - weight_output_acc: 0.6453 - bag_output_acc: 0.6741 - pose_output_acc: 0.8283 - footwear_output_acc: 0.6517 - emotion_output_acc: 0.7113 - val_loss: 20.5909 - val_gender_output_loss: 0.3125 - val_image_quality_output_loss: 0.9612 - val_age_output_loss: 1.2953 - val_weight_output_loss: 0.9456 - val_bag_output_loss: 0.7928 - val_pose_output_loss: 0.5128 - val_footwear_output_loss: 0.8478 - val_emotion_output_loss: 0.9024 - val_gender_output_acc: 0.8745 - val_image_quality_output_acc: 0.5443 - val_age_output_acc: 0.4286 - val_weight_output_acc: 0.6314 - val_bag_output_acc: 0.6629 - val_pose_output_acc: 0.8155 - val_footwear_output_acc: 0.6501 - val_emotion_output_acc: 0.6978\n",
            "\n",
            "Epoch 00002: val_loss improved from 20.84050 to 20.59087, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.002.h5\n",
            "Epoch 3/20\n",
            "  1/360 [..............................] - ETA: 2:03 - loss: 16.5118 - gender_output_loss: 0.2364 - image_quality_output_loss: 1.0874 - age_output_loss: 1.1897 - weight_output_loss: 0.8411 - bag_output_loss: 0.8014 - pose_output_loss: 0.3526 - footwear_output_loss: 0.6014 - emotion_output_loss: 0.2786 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.3750 - age_output_acc: 0.5000 - weight_output_acc: 0.7500 - bag_output_acc: 0.6250 - pose_output_acc: 0.8750 - footwear_output_acc: 0.7500 - emotion_output_acc: 0.9375"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.352554). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "360/360 [==============================] - 129s 360ms/step - loss: 19.7858 - gender_output_loss: 0.3266 - image_quality_output_loss: 0.8890 - age_output_loss: 1.2817 - weight_output_loss: 0.9055 - bag_output_loss: 0.7727 - pose_output_loss: 0.4682 - footwear_output_loss: 0.8014 - emotion_output_loss: 0.8452 - gender_output_acc: 0.8540 - image_quality_output_acc: 0.5701 - age_output_acc: 0.4380 - weight_output_acc: 0.6472 - bag_output_acc: 0.6687 - pose_output_acc: 0.8102 - footwear_output_acc: 0.6394 - emotion_output_acc: 0.7144 - val_loss: 20.7908 - val_gender_output_loss: 0.3669 - val_image_quality_output_loss: 1.1200 - val_age_output_loss: 1.3117 - val_weight_output_loss: 0.9617 - val_bag_output_loss: 0.8059 - val_pose_output_loss: 0.4565 - val_footwear_output_loss: 0.7887 - val_emotion_output_loss: 0.8782 - val_gender_output_acc: 0.8440 - val_image_quality_output_acc: 0.4744 - val_age_output_acc: 0.4242 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6609 - val_pose_output_acc: 0.8273 - val_footwear_output_acc: 0.6614 - val_emotion_output_acc: 0.7018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.59087\n",
            "Epoch 4/20\n",
            "\n",
            "360/360 [==============================] - 126s 350ms/step - loss: 20.3437 - gender_output_loss: 0.3641 - image_quality_output_loss: 0.8939 - age_output_loss: 1.3040 - weight_output_loss: 0.9440 - bag_output_loss: 0.7990 - pose_output_loss: 0.4948 - footwear_output_loss: 0.8143 - emotion_output_loss: 0.8642 - gender_output_acc: 0.8431 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4269 - weight_output_acc: 0.6339 - bag_output_acc: 0.6556 - pose_output_acc: 0.8031 - footwear_output_acc: 0.6363 - emotion_output_acc: 0.7092 - val_loss: 22.8904 - val_gender_output_loss: 0.3804 - val_image_quality_output_loss: 1.3849 - val_age_output_loss: 1.3991 - val_weight_output_loss: 1.0500 - val_bag_output_loss: 0.8336 - val_pose_output_loss: 0.6176 - val_footwear_output_loss: 0.8113 - val_emotion_output_loss: 0.9545 - val_gender_output_acc: 0.8686 - val_image_quality_output_acc: 0.4281 - val_age_output_acc: 0.4104 - val_weight_output_acc: 0.5674 - val_bag_output_acc: 0.6590 - val_pose_output_acc: 0.7987 - val_footwear_output_acc: 0.6531 - val_emotion_output_acc: 0.6973\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.59087\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 129s 359ms/step - loss: 20.2026 - gender_output_loss: 0.3687 - image_quality_output_loss: 0.9058 - age_output_loss: 1.2974 - weight_output_loss: 0.9292 - bag_output_loss: 0.7848 - pose_output_loss: 0.4926 - footwear_output_loss: 0.8047 - emotion_output_loss: 0.8522 - gender_output_acc: 0.8373 - image_quality_output_acc: 0.5677 - age_output_acc: 0.4231 - weight_output_acc: 0.6448 - bag_output_acc: 0.6689 - pose_output_acc: 0.8019 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.7167 - val_loss: 20.6387 - val_gender_output_loss: 0.3544 - val_image_quality_output_loss: 0.9427 - val_age_output_loss: 1.3426 - val_weight_output_loss: 0.9595 - val_bag_output_loss: 0.7999 - val_pose_output_loss: 0.4574 - val_footwear_output_loss: 0.8273 - val_emotion_output_loss: 0.8848 - val_gender_output_acc: 0.8538 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4237 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6550 - val_pose_output_acc: 0.8332 - val_footwear_output_acc: 0.6467 - val_emotion_output_acc: 0.6998\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.59087\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 125s 348ms/step - loss: 20.1588 - gender_output_loss: 0.3460 - image_quality_output_loss: 0.8877 - age_output_loss: 1.2920 - weight_output_loss: 0.9361 - bag_output_loss: 0.7916 - pose_output_loss: 0.4764 - footwear_output_loss: 0.8197 - emotion_output_loss: 0.8604 - gender_output_acc: 0.8472 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4366 - weight_output_acc: 0.6410 - bag_output_acc: 0.6568 - pose_output_acc: 0.8161 - footwear_output_acc: 0.6368 - emotion_output_acc: 0.7078 - val_loss: 20.8136 - val_gender_output_loss: 0.3166 - val_image_quality_output_loss: 1.0488 - val_age_output_loss: 1.3105 - val_weight_output_loss: 0.9546 - val_bag_output_loss: 0.8109 - val_pose_output_loss: 0.4779 - val_footwear_output_loss: 0.7861 - val_emotion_output_loss: 0.9272 - val_gender_output_acc: 0.8706 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.4247 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8391 - val_footwear_output_acc: 0.6698 - val_emotion_output_acc: 0.6575\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.59087\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 128s 354ms/step - loss: 19.7399 - gender_output_loss: 0.3085 - image_quality_output_loss: 0.8847 - age_output_loss: 1.2705 - weight_output_loss: 0.9107 - bag_output_loss: 0.7814 - pose_output_loss: 0.4557 - footwear_output_loss: 0.7994 - emotion_output_loss: 0.8506 - gender_output_acc: 0.8686 - image_quality_output_acc: 0.5818 - age_output_acc: 0.4398 - weight_output_acc: 0.6488 - bag_output_acc: 0.6667 - pose_output_acc: 0.8208 - footwear_output_acc: 0.6411 - emotion_output_acc: 0.7134 - val_loss: 20.4825 - val_gender_output_loss: 0.3083 - val_image_quality_output_loss: 1.0325 - val_age_output_loss: 1.2944 - val_weight_output_loss: 0.9489 - val_bag_output_loss: 0.8001 - val_pose_output_loss: 0.4535 - val_footwear_output_loss: 0.7991 - val_emotion_output_loss: 0.8981 - val_gender_output_acc: 0.8839 - val_image_quality_output_acc: 0.5369 - val_age_output_acc: 0.4218 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6727 - val_pose_output_acc: 0.8494 - val_footwear_output_acc: 0.6713 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00007: val_loss improved from 20.59087 to 20.48249, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.007.h5\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 125s 348ms/step - loss: 19.4804 - gender_output_loss: 0.3056 - image_quality_output_loss: 0.8789 - age_output_loss: 1.2537 - weight_output_loss: 0.9168 - bag_output_loss: 0.7495 - pose_output_loss: 0.4368 - footwear_output_loss: 0.7806 - emotion_output_loss: 0.8507 - gender_output_acc: 0.8708 - image_quality_output_acc: 0.5821 - age_output_acc: 0.4415 - weight_output_acc: 0.6457 - bag_output_acc: 0.6837 - pose_output_acc: 0.8273 - footwear_output_acc: 0.6564 - emotion_output_acc: 0.7083 - val_loss: 20.3996 - val_gender_output_loss: 0.2991 - val_image_quality_output_loss: 1.0321 - val_age_output_loss: 1.2906 - val_weight_output_loss: 0.9435 - val_bag_output_loss: 0.7932 - val_pose_output_loss: 0.4304 - val_footwear_output_loss: 0.8146 - val_emotion_output_loss: 0.9055 - val_gender_output_acc: 0.8839 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6678 - val_pose_output_acc: 0.8529 - val_footwear_output_acc: 0.6639 - val_emotion_output_acc: 0.6683\n",
            "\n",
            "Epoch 00008: val_loss improved from 20.48249 to 20.39960, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.008.h5\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 128s 356ms/step - loss: 19.2623 - gender_output_loss: 0.2898 - image_quality_output_loss: 0.8666 - age_output_loss: 1.2421 - weight_output_loss: 0.9003 - bag_output_loss: 0.7558 - pose_output_loss: 0.4154 - footwear_output_loss: 0.7867 - emotion_output_loss: 0.8430 - gender_output_acc: 0.8766 - image_quality_output_acc: 0.5851 - age_output_acc: 0.4434 - weight_output_acc: 0.6474 - bag_output_acc: 0.6847 - pose_output_acc: 0.8432 - footwear_output_acc: 0.6495 - emotion_output_acc: 0.7127 - val_loss: 20.8706 - val_gender_output_loss: 0.3202 - val_image_quality_output_loss: 1.0747 - val_age_output_loss: 1.3215 - val_weight_output_loss: 0.9463 - val_bag_output_loss: 0.8098 - val_pose_output_loss: 0.4539 - val_footwear_output_loss: 0.8381 - val_emotion_output_loss: 0.9173 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5241 - val_age_output_acc: 0.4090 - val_weight_output_acc: 0.6284 - val_bag_output_acc: 0.6644 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6708\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.39960\n",
            "Epoch 10/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 19.5770 - gender_output_loss: 0.3087 - image_quality_output_loss: 0.8928 - age_output_loss: 1.2626 - weight_output_loss: 0.8999 - bag_output_loss: 0.7682 - pose_output_loss: 0.4519 - footwear_output_loss: 0.7844 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8724 - image_quality_output_acc: 0.5752 - age_output_acc: 0.4424 - weight_output_acc: 0.6475 - bag_output_acc: 0.6722 - pose_output_acc: 0.8275 - footwear_output_acc: 0.6563 - emotion_output_acc: 0.7120\n",
            "360/360 [==============================] - 127s 354ms/step - loss: 19.5775 - gender_output_loss: 0.3086 - image_quality_output_loss: 0.8919 - age_output_loss: 1.2626 - weight_output_loss: 0.8997 - bag_output_loss: 0.7681 - pose_output_loss: 0.4523 - footwear_output_loss: 0.7846 - emotion_output_loss: 0.8446 - gender_output_acc: 0.8726 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4424 - weight_output_acc: 0.6476 - bag_output_acc: 0.6724 - pose_output_acc: 0.8273 - footwear_output_acc: 0.6562 - emotion_output_acc: 0.7118 - val_loss: 21.6949 - val_gender_output_loss: 0.3832 - val_image_quality_output_loss: 1.0587 - val_age_output_loss: 1.3336 - val_weight_output_loss: 0.9575 - val_bag_output_loss: 0.8507 - val_pose_output_loss: 0.6608 - val_footwear_output_loss: 0.7977 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.8469 - val_image_quality_output_acc: 0.5344 - val_age_output_acc: 0.4050 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6437 - val_pose_output_acc: 0.7677 - val_footwear_output_acc: 0.6683 - val_emotion_output_acc: 0.7008\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.39960\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 129s 359ms/step - loss: 19.8019 - gender_output_loss: 0.3292 - image_quality_output_loss: 0.8895 - age_output_loss: 1.2741 - weight_output_loss: 0.9101 - bag_output_loss: 0.7675 - pose_output_loss: 0.4688 - footwear_output_loss: 0.7985 - emotion_output_loss: 0.8532 - gender_output_acc: 0.8521 - image_quality_output_acc: 0.5809 - age_output_acc: 0.4467 - weight_output_acc: 0.6488 - bag_output_acc: 0.6776 - pose_output_acc: 0.8196 - footwear_output_acc: 0.6427 - emotion_output_acc: 0.7085 - val_loss: 22.6547 - val_gender_output_loss: 0.3641 - val_image_quality_output_loss: 1.4580 - val_age_output_loss: 1.3274 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.9760 - val_pose_output_loss: 0.4757 - val_footwear_output_loss: 0.8918 - val_emotion_output_loss: 0.9465 - val_gender_output_acc: 0.8514 - val_image_quality_output_acc: 0.3676 - val_age_output_acc: 0.3971 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6196 - val_pose_output_acc: 0.8273 - val_footwear_output_acc: 0.5871 - val_emotion_output_acc: 0.6334\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 20.39960\n",
            "Epoch 12/20\n",
            "223/360 [=================>............] - ETA: 42s - loss: 20.3778 - gender_output_loss: 0.3640 - image_quality_output_loss: 0.9066 - age_output_loss: 1.3104 - weight_output_loss: 0.9499 - bag_output_loss: 0.7951 - pose_output_loss: 0.4895 - footwear_output_loss: 0.8069 - emotion_output_loss: 0.8631 - gender_output_acc: 0.8383 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4182 - weight_output_acc: 0.6342 - bag_output_acc: 0.6547 - pose_output_acc: 0.8100 - footwear_output_acc: 0.6449 - emotion_output_acc: 0.7108"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t887bmlTyBSw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "4e736307-5e32-48c1-8ebe-fe7c0598c1a3"
      },
      "source": [
        "get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=20)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.008.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.007.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.002.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577622866_model.001.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.008.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577621486_model.001.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.009.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.003.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.002.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577618033_model.001.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.041.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.027.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.023.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.016.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.009.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.008.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.003.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.002.h5',\n",
              " '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.001.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pXD3J4ex-z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "295b2b90-e333-411d-c072-a0b331b2e463"
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 20 patience= 10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/20\n",
            "360/360 [==============================] - 153s 425ms/step - loss: 19.2634 - gender_output_loss: 0.2992 - image_quality_output_loss: 0.8741 - age_output_loss: 1.2485 - weight_output_loss: 0.9054 - bag_output_loss: 0.7574 - pose_output_loss: 0.4104 - footwear_output_loss: 0.7845 - emotion_output_loss: 0.8409 - gender_output_acc: 0.8733 - image_quality_output_acc: 0.5845 - age_output_acc: 0.4483 - weight_output_acc: 0.6481 - bag_output_acc: 0.6814 - pose_output_acc: 0.8351 - footwear_output_acc: 0.6505 - emotion_output_acc: 0.7109 - val_loss: 20.9435 - val_gender_output_loss: 0.3385 - val_image_quality_output_loss: 1.0870 - val_age_output_loss: 1.3133 - val_weight_output_loss: 1.0086 - val_bag_output_loss: 0.8154 - val_pose_output_loss: 0.4707 - val_footwear_output_loss: 0.8124 - val_emotion_output_loss: 0.8902 - val_gender_output_acc: 0.8711 - val_image_quality_output_acc: 0.5246 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.5748 - val_bag_output_acc: 0.6585 - val_pose_output_acc: 0.8312 - val_footwear_output_acc: 0.6688 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 20.94352, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 144s 400ms/step - loss: 19.6526 - gender_output_loss: 0.3187 - image_quality_output_loss: 0.8907 - age_output_loss: 1.2816 - weight_output_loss: 0.9146 - bag_output_loss: 0.7577 - pose_output_loss: 0.4565 - footwear_output_loss: 0.7875 - emotion_output_loss: 0.8444 - gender_output_acc: 0.8649 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4375 - weight_output_acc: 0.6476 - bag_output_acc: 0.6780 - pose_output_acc: 0.8234 - footwear_output_acc: 0.6512 - emotion_output_acc: 0.7123 - val_loss: 20.7421 - val_gender_output_loss: 0.3642 - val_image_quality_output_loss: 1.0315 - val_age_output_loss: 1.2827 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 0.8986 - val_pose_output_loss: 0.4506 - val_footwear_output_loss: 0.7685 - val_emotion_output_loss: 0.8882 - val_gender_output_acc: 0.8524 - val_image_quality_output_acc: 0.5261 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.5950 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.8474 - val_footwear_output_acc: 0.6747 - val_emotion_output_acc: 0.6777\n",
            "\n",
            "Epoch 00002: val_loss improved from 20.94352 to 20.74210, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.002.h5\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 138s 383ms/step - loss: 19.9279 - gender_output_loss: 0.3357 - image_quality_output_loss: 0.8811 - age_output_loss: 1.2854 - weight_output_loss: 0.9291 - bag_output_loss: 0.7810 - pose_output_loss: 0.4638 - footwear_output_loss: 0.8060 - emotion_output_loss: 0.8623 - gender_output_acc: 0.8533 - image_quality_output_acc: 0.5903 - age_output_acc: 0.4349 - weight_output_acc: 0.6439 - bag_output_acc: 0.6705 - pose_output_acc: 0.8167 - footwear_output_acc: 0.6420 - emotion_output_acc: 0.7101 - val_loss: 21.4105 - val_gender_output_loss: 0.3807 - val_image_quality_output_loss: 1.1339 - val_age_output_loss: 1.3149 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8315 - val_pose_output_loss: 0.5458 - val_footwear_output_loss: 0.8316 - val_emotion_output_loss: 0.9013 - val_gender_output_acc: 0.8533 - val_image_quality_output_acc: 0.4985 - val_age_output_acc: 0.4291 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6580 - val_pose_output_acc: 0.7963 - val_footwear_output_acc: 0.6462 - val_emotion_output_acc: 0.6993\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.74210\n",
            "Epoch 4/20\n",
            "360/360 [==============================] - 133s 368ms/step - loss: 20.3143 - gender_output_loss: 0.3679 - image_quality_output_loss: 0.9081 - age_output_loss: 1.3064 - weight_output_loss: 0.9409 - bag_output_loss: 0.7960 - pose_output_loss: 0.5028 - footwear_output_loss: 0.8140 - emotion_output_loss: 0.8531 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.5623 - age_output_acc: 0.4247 - weight_output_acc: 0.6392 - bag_output_acc: 0.6582 - pose_output_acc: 0.8040 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.7128 - val_loss: 21.6009 - val_gender_output_loss: 0.3649 - val_image_quality_output_loss: 1.0742 - val_age_output_loss: 1.4291 - val_weight_output_loss: 0.9978 - val_bag_output_loss: 0.8158 - val_pose_output_loss: 0.4961 - val_footwear_output_loss: 0.7886 - val_emotion_output_loss: 0.9290 - val_gender_output_acc: 0.8474 - val_image_quality_output_acc: 0.4729 - val_age_output_acc: 0.3824 - val_weight_output_acc: 0.5950 - val_bag_output_acc: 0.6476 - val_pose_output_acc: 0.8214 - val_footwear_output_acc: 0.6471 - val_emotion_output_acc: 0.6545\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.74210\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.74210\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 20.2913 - gender_output_loss: 0.3655 - image_quality_output_loss: 0.9047 - age_output_loss: 1.3064 - weight_output_loss: 0.9306 - bag_output_loss: 0.7950 - pose_output_loss: 0.4930 - footwear_output_loss: 0.8195 - emotion_output_loss: 0.8602 - gender_output_acc: 0.8372 - image_quality_output_acc: 0.5682 - age_output_acc: 0.4200 - weight_output_acc: 0.6469 - bag_output_acc: 0.6568 - pose_output_acc: 0.8062 - footwear_output_acc: 0.6351 - emotion_output_acc: 0.7108 - val_loss: 22.4823 - val_gender_output_loss: 0.3619 - val_image_quality_output_loss: 1.2460 - val_age_output_loss: 1.3383 - val_weight_output_loss: 1.0166 - val_bag_output_loss: 0.8022 - val_pose_output_loss: 0.7515 - val_footwear_output_loss: 0.8125 - val_emotion_output_loss: 0.9463 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.4434 - val_age_output_acc: 0.4085 - val_weight_output_acc: 0.5925 - val_bag_output_acc: 0.6609 - val_pose_output_acc: 0.7347 - val_footwear_output_acc: 0.6383 - val_emotion_output_acc: 0.6393\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.74210\n",
            "Epoch 6/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 20.0221 - gender_output_loss: 0.3517 - image_quality_output_loss: 0.8929 - age_output_loss: 1.2884 - weight_output_loss: 0.9265 - bag_output_loss: 0.7857 - pose_output_loss: 0.4776 - footwear_output_loss: 0.8028 - emotion_output_loss: 0.8529 - gender_output_acc: 0.8484 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4290 - weight_output_acc: 0.6433 - bag_output_acc: 0.6610 - pose_output_acc: 0.8075 - footwear_output_acc: 0.6447 - emotion_output_acc: 0.7127\n",
            "Epoch 00005: val_loss did not improve from 20.74210\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 132s 368ms/step - loss: 20.0223 - gender_output_loss: 0.3516 - image_quality_output_loss: 0.8932 - age_output_loss: 1.2887 - weight_output_loss: 0.9264 - bag_output_loss: 0.7862 - pose_output_loss: 0.4776 - footwear_output_loss: 0.8030 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8486 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4286 - weight_output_acc: 0.6431 - bag_output_acc: 0.6608 - pose_output_acc: 0.8076 - footwear_output_acc: 0.6446 - emotion_output_acc: 0.7130 - val_loss: 20.9425 - val_gender_output_loss: 0.3377 - val_image_quality_output_loss: 1.1045 - val_age_output_loss: 1.3060 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8148 - val_pose_output_loss: 0.4816 - val_footwear_output_loss: 0.7950 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.8637 - val_image_quality_output_acc: 0.4975 - val_age_output_acc: 0.4208 - val_weight_output_acc: 0.5979 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6821\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.74210\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 19.6947 - gender_output_loss: 0.3238 - image_quality_output_loss: 0.8794 - age_output_loss: 1.2740 - weight_output_loss: 0.9150 - bag_output_loss: 0.7704 - pose_output_loss: 0.4458 - footwear_output_loss: 0.8023 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8573 - image_quality_output_acc: 0.5809 - age_output_acc: 0.4365 - weight_output_acc: 0.6470 - bag_output_acc: 0.6691 - pose_output_acc: 0.8253 - footwear_output_acc: 0.6448 - emotion_output_acc: 0.7122 - val_loss: 20.7767 - val_gender_output_loss: 0.3337 - val_image_quality_output_loss: 1.0841 - val_age_output_loss: 1.2995 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.8013 - val_pose_output_loss: 0.4628 - val_footwear_output_loss: 0.8046 - val_emotion_output_loss: 0.9056 - val_gender_output_acc: 0.8656 - val_image_quality_output_acc: 0.5074 - val_age_output_acc: 0.4237 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6688 - val_pose_output_acc: 0.8499 - val_footwear_output_acc: 0.6624 - val_emotion_output_acc: 0.6737\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.74210\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 133s 368ms/step - loss: 19.5791 - gender_output_loss: 0.3104 - image_quality_output_loss: 0.8895 - age_output_loss: 1.2629 - weight_output_loss: 0.9170 - bag_output_loss: 0.7616 - pose_output_loss: 0.4447 - footwear_output_loss: 0.7834 - emotion_output_loss: 0.8509 - gender_output_acc: 0.8670 - image_quality_output_acc: 0.5800 - age_output_acc: 0.4474 - weight_output_acc: 0.6432 - bag_output_acc: 0.6816 - pose_output_acc: 0.8297 - footwear_output_acc: 0.6521 - emotion_output_acc: 0.7106 - val_loss: 19.9243 - val_gender_output_loss: 0.2943 - val_image_quality_output_loss: 0.9389 - val_age_output_loss: 1.2690 - val_weight_output_loss: 0.9497 - val_bag_output_loss: 0.7801 - val_pose_output_loss: 0.4231 - val_footwear_output_loss: 0.7797 - val_emotion_output_loss: 0.8946 - val_gender_output_acc: 0.8863 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.4350 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6703 - val_pose_output_acc: 0.8553 - val_footwear_output_acc: 0.6708 - val_emotion_output_acc: 0.6791\n",
            "360/360 [==============================]\n",
            "Epoch 00008: val_loss improved from 20.74210 to 19.92431, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 19.4102 - gender_output_loss: 0.3010 - image_quality_output_loss: 0.8815 - age_output_loss: 1.2447 - weight_output_loss: 0.9217 - bag_output_loss: 0.7519 - pose_output_loss: 0.4308 - footwear_output_loss: 0.7898 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8755 - image_quality_output_acc: 0.5845 - age_output_acc: 0.4599 - weight_output_acc: 0.6443 - bag_output_acc: 0.6826 - pose_output_acc: 0.8349 - footwear_output_acc: 0.6479 - emotion_output_acc: 0.7087 - val_loss: 20.0367 - val_gender_output_loss: 0.2994 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.2687 - val_weight_output_loss: 0.9445 - val_bag_output_loss: 0.7769 - val_pose_output_loss: 0.4406 - val_footwear_output_loss: 0.7880 - val_emotion_output_loss: 0.8949 - val_gender_output_acc: 0.8853 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.4350 - val_weight_output_acc: 0.6284 - val_bag_output_acc: 0.6841 - val_pose_output_acc: 0.8474 - val_footwear_output_acc: 0.6673 - val_emotion_output_acc: 0.6934\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 19.92431\n",
            "Epoch 10/20\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 19.5818 - gender_output_loss: 0.3087 - image_quality_output_loss: 0.8856 - age_output_loss: 1.2766 - weight_output_loss: 0.9068 - bag_output_loss: 0.7624 - pose_output_loss: 0.4481 - footwear_output_loss: 0.7911 - emotion_output_loss: 0.8426 - gender_output_acc: 0.8696 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4292 - weight_output_acc: 0.6469 - bag_output_acc: 0.6809 - pose_output_acc: 0.8259 - footwear_output_acc: 0.6484 - emotion_output_acc: 0.7158 - val_loss: 20.7214 - val_gender_output_loss: 0.3333 - val_image_quality_output_loss: 1.0579 - val_age_output_loss: 1.3167 - val_weight_output_loss: 0.9695 - val_bag_output_loss: 0.7906 - val_pose_output_loss: 0.4793 - val_footwear_output_loss: 0.8003 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.8755 - val_image_quality_output_acc: 0.5246 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6678 - val_pose_output_acc: 0.8307 - val_footwear_output_acc: 0.6624 - val_emotion_output_acc: 0.6988\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 19.92431\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 138s 384ms/step - loss: 19.8470 - gender_output_loss: 0.3429 - image_quality_output_loss: 0.8834 - age_output_loss: 1.2815 - weight_output_loss: 0.9226 - bag_output_loss: 0.7693 - pose_output_loss: 0.4621 - footwear_output_loss: 0.8077 - emotion_output_loss: 0.8519 - gender_output_acc: 0.8495 - image_quality_output_acc: 0.5816 - age_output_acc: 0.4410 - weight_output_acc: 0.6491 - bag_output_acc: 0.6712 - pose_output_acc: 0.8238 - footwear_output_acc: 0.6436 - emotion_output_acc: 0.7120 - val_loss: 21.3537 - val_gender_output_loss: 0.3656 - val_image_quality_output_loss: 1.1749 - val_age_output_loss: 1.3345 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8041 - val_pose_output_loss: 0.5224 - val_footwear_output_loss: 0.7998 - val_emotion_output_loss: 0.9056 - val_gender_output_acc: 0.8632 - val_image_quality_output_acc: 0.4759 - val_age_output_acc: 0.3907 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6718 - val_pose_output_acc: 0.8228 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 19.92431\n",
            "Epoch 12/20\n",
            "360/360 [==============================] - 140s 390ms/step - loss: 20.3383 - gender_output_loss: 0.3540 - image_quality_output_loss: 0.9040 - age_output_loss: 1.3136 - weight_output_loss: 0.9373 - bag_output_loss: 0.8046 - pose_output_loss: 0.5014 - footwear_output_loss: 0.8135 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8443 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4219 - weight_output_acc: 0.6356 - bag_output_acc: 0.6490 - pose_output_acc: 0.8078 - footwear_output_acc: 0.6342 - emotion_output_acc: 0.7120 - val_loss: 20.7169 - val_gender_output_loss: 0.3268 - val_image_quality_output_loss: 0.9542 - val_age_output_loss: 1.3287 - val_weight_output_loss: 0.9664 - val_bag_output_loss: 0.8044 - val_pose_output_loss: 0.5024 - val_footwear_output_loss: 0.8369 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.8656 - val_image_quality_output_acc: 0.5566 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6649 - val_pose_output_acc: 0.8159 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.6924\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 19.92431\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 135s 376ms/step - loss: 20.3254 - gender_output_loss: 0.3632 - image_quality_output_loss: 0.9070 - age_output_loss: 1.3124 - weight_output_loss: 0.9324 - bag_output_loss: 0.7929 - pose_output_loss: 0.4915 - footwear_output_loss: 0.8220 - emotion_output_loss: 0.8596 - gender_output_acc: 0.8342 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4118 - weight_output_acc: 0.6387 - bag_output_acc: 0.6582 - pose_output_acc: 0.8003 - footwear_output_acc: 0.6326 - emotion_output_acc: 0.7122 - val_loss: 21.7071 - val_gender_output_loss: 0.3346 - val_image_quality_output_loss: 1.4120 - val_age_output_loss: 1.3058 - val_weight_output_loss: 1.0227 - val_bag_output_loss: 0.7910 - val_pose_output_loss: 0.4601 - val_footwear_output_loss: 0.8457 - val_emotion_output_loss: 0.9181 - val_gender_output_acc: 0.8652 - val_image_quality_output_acc: 0.3976 - val_age_output_acc: 0.4247 - val_weight_output_acc: 0.5846 - val_bag_output_acc: 0.6727 - val_pose_output_acc: 0.8391 - val_footwear_output_acc: 0.6275 - val_emotion_output_acc: 0.6599\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 19.92431\n",
            "Epoch 14/20\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 19.8767 - gender_output_loss: 0.3358 - image_quality_output_loss: 0.8819 - age_output_loss: 1.2836 - weight_output_loss: 0.9202 - bag_output_loss: 0.7878 - pose_output_loss: 0.4625 - footwear_output_loss: 0.7942 - emotion_output_loss: 0.8503 - gender_output_acc: 0.8524 - image_quality_output_acc: 0.5818 - age_output_acc: 0.4297 - weight_output_acc: 0.6427 - bag_output_acc: 0.6658 - pose_output_acc: 0.8179 - footwear_output_acc: 0.6450 - emotion_output_acc: 0.7118 - val_loss: 21.2733 - val_gender_output_loss: 0.3481 - val_image_quality_output_loss: 1.1019 - val_age_output_loss: 1.3331 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8375 - val_pose_output_loss: 0.4909 - val_footwear_output_loss: 0.8046 - val_emotion_output_loss: 0.9248 - val_gender_output_acc: 0.8666 - val_image_quality_output_acc: 0.4931 - val_age_output_acc: 0.4119 - val_weight_output_acc: 0.6122 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.6491 - val_emotion_output_acc: 0.6649\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 19.92431\n",
            "Epoch 15/20\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 19.7830 - gender_output_loss: 0.3234 - image_quality_output_loss: 0.8908 - age_output_loss: 1.2702 - weight_output_loss: 0.9157 - bag_output_loss: 0.7830 - pose_output_loss: 0.4622 - footwear_output_loss: 0.8056 - emotion_output_loss: 0.8438 - gender_output_acc: 0.8628 - image_quality_output_acc: 0.5783 - age_output_acc: 0.4391 - weight_output_acc: 0.6450 - bag_output_acc: 0.6651 - pose_output_acc: 0.8168 - footwear_output_acc: 0.6359 - emotion_output_acc: 0.7156 - val_loss: 21.0095 - val_gender_output_loss: 0.3115 - val_image_quality_output_loss: 1.1870 - val_age_output_loss: 1.3004 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.7989 - val_pose_output_loss: 0.4949 - val_footwear_output_loss: 0.7720 - val_emotion_output_loss: 0.9134 - val_gender_output_acc: 0.8720 - val_image_quality_output_acc: 0.4660 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6821 - val_pose_output_acc: 0.8307 - val_footwear_output_acc: 0.6772 - val_emotion_output_acc: 0.6983\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00015: val_loss did not improve from 19.92431\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 134s 373ms/step - loss: 19.4171 - gender_output_loss: 0.3036 - image_quality_output_loss: 0.8722 - age_output_loss: 1.2640 - weight_output_loss: 0.9100 - bag_output_loss: 0.7423 - pose_output_loss: 0.4273 - footwear_output_loss: 0.7902 - emotion_output_loss: 0.8474 - gender_output_acc: 0.8677 - image_quality_output_acc: 0.5859 - age_output_acc: 0.4453 - weight_output_acc: 0.6488 - bag_output_acc: 0.6880 - pose_output_acc: 0.8363 - footwear_output_acc: 0.6446 - emotion_output_acc: 0.7101 - val_loss: 20.1678 - val_gender_output_loss: 0.3062 - val_image_quality_output_loss: 0.9942 - val_age_output_loss: 1.2766 - val_weight_output_loss: 0.9426 - val_bag_output_loss: 0.7858 - val_pose_output_loss: 0.4295 - val_footwear_output_loss: 0.7757 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.8799 - val_image_quality_output_acc: 0.5472 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6806 - val_pose_output_acc: 0.8519 - val_footwear_output_acc: 0.6772 - val_emotion_output_acc: 0.6762\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 19.92431\n",
            "Epoch 17/20\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 19.1488 - gender_output_loss: 0.2911 - image_quality_output_loss: 0.8871 - age_output_loss: 1.2358 - weight_output_loss: 0.9011 - bag_output_loss: 0.7417 - pose_output_loss: 0.4171 - footwear_output_loss: 0.7776 - emotion_output_loss: 0.8288 - gender_output_acc: 0.8809 - image_quality_output_acc: 0.5821 - age_output_acc: 0.4554 - weight_output_acc: 0.6533 - bag_output_acc: 0.6882 - pose_output_acc: 0.8436 - footwear_output_acc: 0.6575 - emotion_output_acc: 0.7167 - val_loss: 20.7410 - val_gender_output_loss: 0.3907 - val_image_quality_output_loss: 1.0011 - val_age_output_loss: 1.2867 - val_weight_output_loss: 0.9606 - val_bag_output_loss: 0.8026 - val_pose_output_loss: 0.4644 - val_footwear_output_loss: 0.8230 - val_emotion_output_loss: 0.9212 - val_gender_output_acc: 0.8622 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4213 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6855 - val_pose_output_acc: 0.8494 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6747\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 19.92431\n",
            "Epoch 18/20\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 19.7261 - gender_output_loss: 0.3147 - image_quality_output_loss: 0.8744 - age_output_loss: 1.2827 - weight_output_loss: 0.9205 - bag_output_loss: 0.7731 - pose_output_loss: 0.4413 - footwear_output_loss: 0.7925 - emotion_output_loss: 0.8579 - gender_output_acc: 0.8616 - image_quality_output_acc: 0.5922 - age_output_acc: 0.4349 - weight_output_acc: 0.6403 - bag_output_acc: 0.6736 - pose_output_acc: 0.8319 - footwear_output_acc: 0.6481 - emotion_output_acc: 0.7082 - val_loss: 20.4611 - val_gender_output_loss: 0.3061 - val_image_quality_output_loss: 1.0458 - val_age_output_loss: 1.3007 - val_weight_output_loss: 0.9716 - val_bag_output_loss: 0.8034 - val_pose_output_loss: 0.4430 - val_footwear_output_loss: 0.7787 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.8794 - val_image_quality_output_acc: 0.5118 - val_age_output_acc: 0.4296 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6732 - val_pose_output_acc: 0.8410 - val_footwear_output_acc: 0.6742 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 19.92431\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577628048_backup\n",
            "End of EPOCHS= 20  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcVeHFyq-HcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save('/content/gdrive/My Drive/WRN_Extend/Latest_val_loss_19924306.h5py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMOGPX7I9l1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e2dbc29-1bcc-4202-c250-b9227cfe5f66"
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "\n",
        "\n",
        "################################# Below are multiple Trial and Errors ######################\n",
        "################################# No big difference observed in the results ###############################\n",
        "######### Trying with clr param changes clr_mode='triangular2', clr_multiplier=8\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   clr_mode='triangular2',\n",
        "                                   clr_multiplier=8)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577629721_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 20 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577629725_backup\n",
            "Epoch 1/20\n",
            "360/360 [==============================] - 140s 389ms/step - loss: 19.2879 - gender_output_loss: 0.2955 - image_quality_output_loss: 0.8713 - age_output_loss: 1.2463 - weight_output_loss: 0.9057 - bag_output_loss: 0.7574 - pose_output_loss: 0.4257 - footwear_output_loss: 0.7872 - emotion_output_loss: 0.8357 - gender_output_acc: 0.8733 - image_quality_output_acc: 0.5811 - age_output_acc: 0.4408 - weight_output_acc: 0.6491 - bag_output_acc: 0.6835 - pose_output_acc: 0.8349 - footwear_output_acc: 0.6562 - emotion_output_acc: 0.7127 - val_loss: 20.7866 - val_gender_output_loss: 0.3228 - val_image_quality_output_loss: 1.0899 - val_age_output_loss: 1.3002 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.7944 - val_pose_output_loss: 0.4693 - val_footwear_output_loss: 0.8209 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.8794 - val_image_quality_output_acc: 0.5207 - val_age_output_acc: 0.4301 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6757 - val_pose_output_acc: 0.8415 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 20.78656, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577629721_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 131s 365ms/step - loss: 19.3805 - gender_output_loss: 0.2922 - image_quality_output_loss: 0.8846 - age_output_loss: 1.2513 - weight_output_loss: 0.9145 - bag_output_loss: 0.7558 - pose_output_loss: 0.4233 - footwear_output_loss: 0.7815 - emotion_output_loss: 0.8488 - gender_output_acc: 0.8790 - image_quality_output_acc: 0.5823 - age_output_acc: 0.4535 - weight_output_acc: 0.6491 - bag_output_acc: 0.6807 - pose_output_acc: 0.8344 - footwear_output_acc: 0.6493 - emotion_output_acc: 0.7108 - val_loss: 20.7353 - val_gender_output_loss: 0.3471 - val_image_quality_output_loss: 0.9483 - val_age_output_loss: 1.3133 - val_weight_output_loss: 0.9531 - val_bag_output_loss: 0.8182 - val_pose_output_loss: 0.5185 - val_footwear_output_loss: 0.8264 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.8656 - val_image_quality_output_acc: 0.5586 - val_age_output_acc: 0.4193 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6629 - val_pose_output_acc: 0.8258 - val_footwear_output_acc: 0.6614 - val_emotion_output_acc: 0.6929\n",
            "\n",
            "Epoch 00002: val_loss improved from 20.78656 to 20.73529, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577629721_model.002.h5\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 137s 382ms/step - loss: 19.7462 - gender_output_loss: 0.3008 - image_quality_output_loss: 0.8866 - age_output_loss: 1.2735 - weight_output_loss: 0.9261 - bag_output_loss: 0.7744 - pose_output_loss: 0.4594 - footwear_output_loss: 0.7921 - emotion_output_loss: 0.8585 - gender_output_acc: 0.8717 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4377 - weight_output_acc: 0.6408 - bag_output_acc: 0.6783 - pose_output_acc: 0.8210 - footwear_output_acc: 0.6509 - emotion_output_acc: 0.7075 - val_loss: 20.8386 - val_gender_output_loss: 0.3066 - val_image_quality_output_loss: 1.0522 - val_age_output_loss: 1.3422 - val_weight_output_loss: 0.9723 - val_bag_output_loss: 0.8228 - val_pose_output_loss: 0.4540 - val_footwear_output_loss: 0.7947 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.8819 - val_image_quality_output_acc: 0.5089 - val_age_output_acc: 0.4154 - val_weight_output_acc: 0.6097 - val_bag_output_acc: 0.6462 - val_pose_output_acc: 0.8342 - val_footwear_output_acc: 0.6693 - val_emotion_output_acc: 0.6663\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.73529\n",
            "Epoch 4/20\n",
            "360/360 [==============================] - 133s 371ms/step - loss: 19.6066 - gender_output_loss: 0.3311 - image_quality_output_loss: 0.8795 - age_output_loss: 1.2729 - weight_output_loss: 0.9119 - bag_output_loss: 0.7626 - pose_output_loss: 0.4541 - footwear_output_loss: 0.7986 - emotion_output_loss: 0.8332 - gender_output_acc: 0.8613 - image_quality_output_acc: 0.5826 - age_output_acc: 0.4398 - weight_output_acc: 0.6498 - bag_output_acc: 0.6759 - pose_output_acc: 0.8227 - footwear_output_acc: 0.6411 - emotion_output_acc: 0.7177 - val_loss: 21.2300 - val_gender_output_loss: 0.3356 - val_image_quality_output_loss: 1.1392 - val_age_output_loss: 1.3435 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.8032 - val_pose_output_loss: 0.5431 - val_footwear_output_loss: 0.8060 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.8661 - val_image_quality_output_acc: 0.4828 - val_age_output_acc: 0.4311 - val_weight_output_acc: 0.6206 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8061 - val_footwear_output_acc: 0.6639 - val_emotion_output_acc: 0.6993\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.73529\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 136s 379ms/step - loss: 19.7194 - gender_output_loss: 0.3301 - image_quality_output_loss: 0.8911 - age_output_loss: 1.2769 - weight_output_loss: 0.9217 - bag_output_loss: 0.7679 - pose_output_loss: 0.4627 - footwear_output_loss: 0.7838 - emotion_output_loss: 0.8416 - gender_output_acc: 0.8571 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4332 - weight_output_acc: 0.6396 - bag_output_acc: 0.6759 - pose_output_acc: 0.8203 - footwear_output_acc: 0.6547 - emotion_output_acc: 0.7191 - val_loss: 20.8281 - val_gender_output_loss: 0.3659 - val_image_quality_output_loss: 1.0729 - val_age_output_loss: 1.3062 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.8083 - val_pose_output_loss: 0.4511 - val_footwear_output_loss: 0.8265 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.8538 - val_image_quality_output_acc: 0.5059 - val_age_output_acc: 0.4109 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6752 - val_pose_output_acc: 0.8386 - val_footwear_output_acc: 0.6663 - val_emotion_output_acc: 0.6909\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.73529\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 133s 368ms/step - loss: 20.1662 - gender_output_loss: 0.3497 - image_quality_output_loss: 0.8866 - age_output_loss: 1.2929 - weight_output_loss: 0.9345 - bag_output_loss: 0.7918 - pose_output_loss: 0.4900 - footwear_output_loss: 0.8189 - emotion_output_loss: 0.8632 - gender_output_acc: 0.8477 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4323 - weight_output_acc: 0.6443 - bag_output_acc: 0.6634 - pose_output_acc: 0.8030 - footwear_output_acc: 0.6323 - emotion_output_acc: 0.7052 - val_loss: 20.8940 - val_gender_output_loss: 0.3015 - val_image_quality_output_loss: 1.1568 - val_age_output_loss: 1.3056 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.7945 - val_pose_output_loss: 0.4878 - val_footwear_output_loss: 0.7967 - val_emotion_output_loss: 0.8990 - val_gender_output_acc: 0.8755 - val_image_quality_output_acc: 0.4724 - val_age_output_acc: 0.4139 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8179 - val_footwear_output_acc: 0.6476 - val_emotion_output_acc: 0.6781\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.73529\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 136s 379ms/step - loss: 20.1036 - gender_output_loss: 0.3507 - image_quality_output_loss: 0.8982 - age_output_loss: 1.3053 - weight_output_loss: 0.9298 - bag_output_loss: 0.7853 - pose_output_loss: 0.4878 - footwear_output_loss: 0.8043 - emotion_output_loss: 0.8447 - gender_output_acc: 0.8479 - image_quality_output_acc: 0.5785 - age_output_acc: 0.4243 - weight_output_acc: 0.6377 - bag_output_acc: 0.6648 - pose_output_acc: 0.8050 - footwear_output_acc: 0.6415 - emotion_output_acc: 0.7186 - val_loss: 20.5859 - val_gender_output_loss: 0.3323 - val_image_quality_output_loss: 0.9353 - val_age_output_loss: 1.3267 - val_weight_output_loss: 0.9615 - val_bag_output_loss: 0.8130 - val_pose_output_loss: 0.4679 - val_footwear_output_loss: 0.7876 - val_emotion_output_loss: 0.9121 - val_gender_output_acc: 0.8627 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4134 - val_weight_output_acc: 0.6132 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.8337 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6658\n",
            "\n",
            "Epoch 00007: val_loss improved from 20.73529 to 20.58588, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577629721_model.007.h5\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 132s 366ms/step - loss: 20.3071 - gender_output_loss: 0.3630 - image_quality_output_loss: 0.8951 - age_output_loss: 1.3072 - weight_output_loss: 0.9324 - bag_output_loss: 0.7998 - pose_output_loss: 0.4864 - footwear_output_loss: 0.8188 - emotion_output_loss: 0.8675 - gender_output_acc: 0.8363 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4262 - weight_output_acc: 0.6432 - bag_output_acc: 0.6536 - pose_output_acc: 0.8059 - footwear_output_acc: 0.6366 - emotion_output_acc: 0.7054 - val_loss: 22.1917 - val_gender_output_loss: 0.3556 - val_image_quality_output_loss: 1.4282 - val_age_output_loss: 1.3311 - val_weight_output_loss: 1.0085 - val_bag_output_loss: 0.8138 - val_pose_output_loss: 0.4861 - val_footwear_output_loss: 0.8808 - val_emotion_output_loss: 0.9518 - val_gender_output_acc: 0.8583 - val_image_quality_output_acc: 0.3971 - val_age_output_acc: 0.4326 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6516 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6019 - val_emotion_output_acc: 0.6417\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 20.58588\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 20.3503 - gender_output_loss: 0.3767 - image_quality_output_loss: 0.8844 - age_output_loss: 1.3096 - weight_output_loss: 0.9331 - bag_output_loss: 0.7976 - pose_output_loss: 0.5001 - footwear_output_loss: 0.8148 - emotion_output_loss: 0.8642 - gender_output_acc: 0.8302 - image_quality_output_acc: 0.5806 - age_output_acc: 0.4181 - weight_output_acc: 0.6411 - bag_output_acc: 0.6592 - pose_output_acc: 0.8007 - footwear_output_acc: 0.6401 - emotion_output_acc: 0.7092 - val_loss: 21.3558 - val_gender_output_loss: 0.4052 - val_image_quality_output_loss: 1.1485 - val_age_output_loss: 1.3118 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.8128 - val_pose_output_loss: 0.4730 - val_footwear_output_loss: 0.8381 - val_emotion_output_loss: 0.9269 - val_gender_output_acc: 0.8312 - val_image_quality_output_acc: 0.4636 - val_age_output_acc: 0.4090 - val_weight_output_acc: 0.6117 - val_bag_output_acc: 0.6457 - val_pose_output_acc: 0.8248 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6531\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.58588\n",
            "Epoch 10/20\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 20.2206 - gender_output_loss: 0.3398 - image_quality_output_loss: 0.9110 - age_output_loss: 1.3006 - weight_output_loss: 0.9332 - bag_output_loss: 0.7983 - pose_output_loss: 0.4856 - footwear_output_loss: 0.8103 - emotion_output_loss: 0.8569 - gender_output_acc: 0.8547 - image_quality_output_acc: 0.5646 - age_output_acc: 0.4368 - weight_output_acc: 0.6370 - bag_output_acc: 0.6528 - pose_output_acc: 0.8040 - footwear_output_acc: 0.6328 - emotion_output_acc: 0.7122 - val_loss: 21.2861 - val_gender_output_loss: 0.3831 - val_image_quality_output_loss: 0.9932 - val_age_output_loss: 1.3375 - val_weight_output_loss: 0.9988 - val_bag_output_loss: 0.8218 - val_pose_output_loss: 0.5086 - val_footwear_output_loss: 0.8338 - val_emotion_output_loss: 0.9273 - val_gender_output_acc: 0.8519 - val_image_quality_output_acc: 0.5497 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.5920 - val_bag_output_acc: 0.6594 - val_pose_output_acc: 0.8278 - val_footwear_output_acc: 0.6437 - val_emotion_output_acc: 0.6550\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.58588\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 138s 384ms/step - loss: 20.1142 - gender_output_loss: 0.3492 - image_quality_output_loss: 0.8983 - age_output_loss: 1.2933 - weight_output_loss: 0.9403 - bag_output_loss: 0.7915 - pose_output_loss: 0.4567 - footwear_output_loss: 0.8041 - emotion_output_loss: 0.8633 - gender_output_acc: 0.8448 - image_quality_output_acc: 0.5734 - age_output_acc: 0.4306 - weight_output_acc: 0.6392 - bag_output_acc: 0.6571 - pose_output_acc: 0.8207 - footwear_output_acc: 0.6394 - emotion_output_acc: 0.7080 - val_loss: 23.0623 - val_gender_output_loss: 0.4987 - val_image_quality_output_loss: 1.2748 - val_age_output_loss: 1.4281 - val_weight_output_loss: 1.0193 - val_bag_output_loss: 0.8636 - val_pose_output_loss: 0.5429 - val_footwear_output_loss: 0.8255 - val_emotion_output_loss: 1.0138 - val_gender_output_acc: 0.8199 - val_image_quality_output_acc: 0.4478 - val_age_output_acc: 0.3548 - val_weight_output_acc: 0.5753 - val_bag_output_acc: 0.6496 - val_pose_output_acc: 0.8273 - val_footwear_output_acc: 0.6560 - val_emotion_output_acc: 0.6029\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.58588\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 20.58588\n",
            "Epoch 12/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 19.9096 - gender_output_loss: 0.3444 - image_quality_output_loss: 0.8816 - age_output_loss: 1.2872 - weight_output_loss: 0.9196 - bag_output_loss: 0.7759 - pose_output_loss: 0.4756 - footwear_output_loss: 0.7966 - emotion_output_loss: 0.8458 - gender_output_acc: 0.8507 - image_quality_output_acc: 0.5828 - age_output_acc: 0.4370 - weight_output_acc: 0.6490 - bag_output_acc: 0.6665 - pose_output_acc: 0.8181 - footwear_output_acc: 0.6399 - emotion_output_acc: 0.7141 - val_loss: 20.8710 - val_gender_output_loss: 0.3141 - val_image_quality_output_loss: 1.2115 - val_age_output_loss: 1.3011 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.7911 - val_pose_output_loss: 0.4262 - val_footwear_output_loss: 0.8167 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.8755 - val_image_quality_output_acc: 0.4503 - val_age_output_acc: 0.4296 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6727 - val_pose_output_acc: 0.8578 - val_footwear_output_acc: 0.6560 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 20.58588\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 19.7389 - gender_output_loss: 0.3144 - image_quality_output_loss: 0.8969 - age_output_loss: 1.2754 - weight_output_loss: 0.9306 - bag_output_loss: 0.7592 - pose_output_loss: 0.4458 - footwear_output_loss: 0.8024 - emotion_output_loss: 0.8468 - gender_output_acc: 0.8656 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4340 - weight_output_acc: 0.6432 - bag_output_acc: 0.6708 - pose_output_acc: 0.8215 - footwear_output_acc: 0.6446 - emotion_output_acc: 0.7118 - val_loss: 20.1509 - val_gender_output_loss: 0.3131 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.2901 - val_weight_output_loss: 0.9485 - val_bag_output_loss: 0.7894 - val_pose_output_loss: 0.4343 - val_footwear_output_loss: 0.7851 - val_emotion_output_loss: 0.8774 - val_gender_output_acc: 0.8775 - val_image_quality_output_acc: 0.5458 - val_age_output_acc: 0.4321 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6565 - val_pose_output_acc: 0.8474 - val_footwear_output_acc: 0.6629 - val_emotion_output_acc: 0.6988\n",
            "\n",
            "Epoch 00013: val_loss improved from 20.58588 to 20.15086, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577629721_model.013.h5\n",
            "Epoch 14/20\n",
            "360/360 [==============================] - 135s 376ms/step - loss: 19.6412 - gender_output_loss: 0.3180 - image_quality_output_loss: 0.8779 - age_output_loss: 1.2641 - weight_output_loss: 0.9084 - bag_output_loss: 0.7813 - pose_output_loss: 0.4445 - footwear_output_loss: 0.7884 - emotion_output_loss: 0.8502 - gender_output_acc: 0.8597 - image_quality_output_acc: 0.5908 - age_output_acc: 0.4490 - weight_output_acc: 0.6460 - bag_output_acc: 0.6679 - pose_output_acc: 0.8262 - footwear_output_acc: 0.6542 - emotion_output_acc: 0.7134 - val_loss: 20.7126 - val_gender_output_loss: 0.3238 - val_image_quality_output_loss: 1.0433 - val_age_output_loss: 1.2936 - val_weight_output_loss: 0.9659 - val_bag_output_loss: 0.8072 - val_pose_output_loss: 0.4975 - val_footwear_output_loss: 0.7924 - val_emotion_output_loss: 0.8993 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5389 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.5930 - val_bag_output_acc: 0.6693 - val_pose_output_acc: 0.8248 - val_footwear_output_acc: 0.6722 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 20.15086\n",
            "Epoch 15/20\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 19.4496 - gender_output_loss: 0.3091 - image_quality_output_loss: 0.8728 - age_output_loss: 1.2455 - weight_output_loss: 0.9124 - bag_output_loss: 0.7571 - pose_output_loss: 0.4281 - footwear_output_loss: 0.7895 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8672 - image_quality_output_acc: 0.5847 - age_output_acc: 0.4578 - weight_output_acc: 0.6448 - bag_output_acc: 0.6804 - pose_output_acc: 0.8340 - footwear_output_acc: 0.6438 - emotion_output_acc: 0.7090 - val_loss: 20.6798 - val_gender_output_loss: 0.3055 - val_image_quality_output_loss: 1.0839 - val_age_output_loss: 1.2929 - val_weight_output_loss: 0.9571 - val_bag_output_loss: 0.7984 - val_pose_output_loss: 0.4474 - val_footwear_output_loss: 0.8596 - val_emotion_output_loss: 0.8986 - val_gender_output_acc: 0.8868 - val_image_quality_output_acc: 0.5256 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6732 - val_pose_output_acc: 0.8494 - val_footwear_output_acc: 0.6412 - val_emotion_output_acc: 0.6777\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 20.15086\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 19.2430 - gender_output_loss: 0.2906 - image_quality_output_loss: 0.8821 - age_output_loss: 1.2459 - weight_output_loss: 0.9023 - bag_output_loss: 0.7547 - pose_output_loss: 0.4277 - footwear_output_loss: 0.7725 - emotion_output_loss: 0.8272 - gender_output_acc: 0.8743 - image_quality_output_acc: 0.5842 - age_output_acc: 0.4439 - weight_output_acc: 0.6521 - bag_output_acc: 0.6845 - pose_output_acc: 0.8351 - footwear_output_acc: 0.6569 - emotion_output_acc: 0.7151 - val_loss: 20.5412 - val_gender_output_loss: 0.3074 - val_image_quality_output_loss: 1.0919 - val_age_output_loss: 1.2788 - val_weight_output_loss: 0.9520 - val_bag_output_loss: 0.8021 - val_pose_output_loss: 0.4408 - val_footwear_output_loss: 0.8074 - val_emotion_output_loss: 0.9057 - val_gender_output_acc: 0.8863 - val_image_quality_output_acc: 0.5167 - val_age_output_acc: 0.4281 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6777 - val_pose_output_acc: 0.8533 - val_footwear_output_acc: 0.6654 - val_emotion_output_acc: 0.6703\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 20.15086\n",
            "Epoch 17/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 19.2055 - gender_output_loss: 0.2922 - image_quality_output_loss: 0.8750 - age_output_loss: 1.2429 - weight_output_loss: 0.9119 - bag_output_loss: 0.7489 - pose_output_loss: 0.4179 - footwear_output_loss: 0.7786 - emotion_output_loss: 0.8252 - gender_output_acc: 0.8783 - image_quality_output_acc: 0.5840 - age_output_acc: 0.4474 - weight_output_acc: 0.6531 - bag_output_acc: 0.6793 - pose_output_acc: 0.8425 - footwear_output_acc: 0.6609 - emotion_output_acc: 0.7184 - val_loss: 20.4346 - val_gender_output_loss: 0.2951 - val_image_quality_output_loss: 1.1013 - val_age_output_loss: 1.2715 - val_weight_output_loss: 0.9630 - val_bag_output_loss: 0.7944 - val_pose_output_loss: 0.4320 - val_footwear_output_loss: 0.7923 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.8873 - val_image_quality_output_acc: 0.5123 - val_age_output_acc: 0.4311 - val_weight_output_acc: 0.6048 - val_bag_output_acc: 0.6786 - val_pose_output_acc: 0.8538 - val_footwear_output_acc: 0.6727 - val_emotion_output_acc: 0.6718\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 20.15086\n",
            "Epoch 18/20\n",
            "360/360 [==============================] - 136s 377ms/step - loss: 19.2396 - gender_output_loss: 0.2950 - image_quality_output_loss: 0.8812 - age_output_loss: 1.2410 - weight_output_loss: 0.8940 - bag_output_loss: 0.7469 - pose_output_loss: 0.4178 - footwear_output_loss: 0.7750 - emotion_output_loss: 0.8484 - gender_output_acc: 0.8745 - image_quality_output_acc: 0.5858 - age_output_acc: 0.4589 - weight_output_acc: 0.6540 - bag_output_acc: 0.6799 - pose_output_acc: 0.8370 - footwear_output_acc: 0.6564 - emotion_output_acc: 0.7057 - val_loss: 20.4661 - val_gender_output_loss: 0.3085 - val_image_quality_output_loss: 1.0408 - val_age_output_loss: 1.2888 - val_weight_output_loss: 0.9601 - val_bag_output_loss: 0.7926 - val_pose_output_loss: 0.4383 - val_footwear_output_loss: 0.8082 - val_emotion_output_loss: 0.9052 - val_gender_output_acc: 0.8834 - val_image_quality_output_acc: 0.5251 - val_age_output_acc: 0.4301 - val_weight_output_acc: 0.6196 - val_bag_output_acc: 0.6693 - val_pose_output_acc: 0.8489 - val_footwear_output_acc: 0.6718 - val_emotion_output_acc: 0.6786\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 20.15086\n",
            "Epoch 19/20\n",
            "360/360 [==============================] - 136s 377ms/step - loss: 19.2409 - gender_output_loss: 0.2844 - image_quality_output_loss: 0.8699 - age_output_loss: 1.2377 - weight_output_loss: 0.9051 - bag_output_loss: 0.7601 - pose_output_loss: 0.4130 - footwear_output_loss: 0.7712 - emotion_output_loss: 0.8509 - gender_output_acc: 0.8859 - image_quality_output_acc: 0.5941 - age_output_acc: 0.4531 - weight_output_acc: 0.6519 - bag_output_acc: 0.6823 - pose_output_acc: 0.8415 - footwear_output_acc: 0.6642 - emotion_output_acc: 0.7083 - val_loss: 20.5989 - val_gender_output_loss: 0.3028 - val_image_quality_output_loss: 1.0100 - val_age_output_loss: 1.2899 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.7921 - val_pose_output_loss: 0.5231 - val_footwear_output_loss: 0.7904 - val_emotion_output_loss: 0.8902 - val_gender_output_acc: 0.8844 - val_image_quality_output_acc: 0.5349 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8155 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 20.15086\n",
            "Epoch 20/20\n",
            "360/360 [==============================] - 136s 377ms/step - loss: 19.2882 - gender_output_loss: 0.2977 - image_quality_output_loss: 0.8828 - age_output_loss: 1.2467 - weight_output_loss: 0.9027 - bag_output_loss: 0.7455 - pose_output_loss: 0.4252 - footwear_output_loss: 0.7825 - emotion_output_loss: 0.8395 - gender_output_acc: 0.8778 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4429 - weight_output_acc: 0.6455 - bag_output_acc: 0.6835 - pose_output_acc: 0.8380 - footwear_output_acc: 0.6576 - emotion_output_acc: 0.7144 - val_loss: 20.3478 - val_gender_output_loss: 0.2948 - val_image_quality_output_loss: 1.0546 - val_age_output_loss: 1.2767 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.7756 - val_pose_output_loss: 0.4380 - val_footwear_output_loss: 0.7913 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.8868 - val_image_quality_output_acc: 0.5128 - val_age_output_acc: 0.4257 - val_weight_output_acc: 0.5969 - val_bag_output_acc: 0.6713 - val_pose_output_acc: 0.8509 - val_footwear_output_acc: 0.6722 - val_emotion_output_acc: 0.6757\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 20.15086\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577632444_backup\n",
            "End of EPOCHS= 20  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBMGW_uANYbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b8fe9e8-0ace-4e93-8367-414945ef0129"
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=30\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 30 patience= 10\n",
            "Epoch 1/30\n",
            "360/360 [==============================] - 142s 394ms/step - loss: 19.3110 - gender_output_loss: 0.2961 - image_quality_output_loss: 0.8892 - age_output_loss: 1.2530 - weight_output_loss: 0.9108 - bag_output_loss: 0.7493 - pose_output_loss: 0.4230 - footwear_output_loss: 0.7922 - emotion_output_loss: 0.8274 - gender_output_acc: 0.8740 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4441 - weight_output_acc: 0.6483 - bag_output_acc: 0.6950 - pose_output_acc: 0.8349 - footwear_output_acc: 0.6524 - emotion_output_acc: 0.7158 - val_loss: 20.6898 - val_gender_output_loss: 0.3147 - val_image_quality_output_loss: 1.0766 - val_age_output_loss: 1.2993 - val_weight_output_loss: 0.9640 - val_bag_output_loss: 0.8038 - val_pose_output_loss: 0.4488 - val_footwear_output_loss: 0.8494 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8814 - val_image_quality_output_acc: 0.5064 - val_age_output_acc: 0.4129 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6688 - val_pose_output_acc: 0.8460 - val_footwear_output_acc: 0.6555 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 20.68976, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.001.h5\n",
            "Epoch 2/30\n",
            "360/360 [==============================] - 133s 370ms/step - loss: 19.6489 - gender_output_loss: 0.3189 - image_quality_output_loss: 0.8724 - age_output_loss: 1.2646 - weight_output_loss: 0.9187 - bag_output_loss: 0.7711 - pose_output_loss: 0.4569 - footwear_output_loss: 0.7825 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8611 - image_quality_output_acc: 0.5837 - age_output_acc: 0.4405 - weight_output_acc: 0.6450 - bag_output_acc: 0.6741 - pose_output_acc: 0.8210 - footwear_output_acc: 0.6554 - emotion_output_acc: 0.7082 - val_loss: 20.3971 - val_gender_output_loss: 0.3141 - val_image_quality_output_loss: 0.8967 - val_age_output_loss: 1.3163 - val_weight_output_loss: 0.9502 - val_bag_output_loss: 0.7789 - val_pose_output_loss: 0.5107 - val_footwear_output_loss: 0.8307 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.8770 - val_image_quality_output_acc: 0.5906 - val_age_output_acc: 0.4321 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6752 - val_pose_output_acc: 0.8169 - val_footwear_output_acc: 0.6516 - val_emotion_output_acc: 0.6983\n",
            "\n",
            "Epoch 00002: val_loss improved from 20.68976 to 20.39708, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.002.h5\n",
            "Epoch 3/30\n",
            "360/360 [==============================] - 138s 383ms/step - loss: 19.8486 - gender_output_loss: 0.3355 - image_quality_output_loss: 0.8945 - age_output_loss: 1.2767 - weight_output_loss: 0.9243 - bag_output_loss: 0.7730 - pose_output_loss: 0.4775 - footwear_output_loss: 0.8024 - emotion_output_loss: 0.8425 - gender_output_acc: 0.8587 - image_quality_output_acc: 0.5677 - age_output_acc: 0.4382 - weight_output_acc: 0.6405 - bag_output_acc: 0.6698 - pose_output_acc: 0.8054 - footwear_output_acc: 0.6396 - emotion_output_acc: 0.7161 - val_loss: 20.9274 - val_gender_output_loss: 0.3217 - val_image_quality_output_loss: 0.9639 - val_age_output_loss: 1.3527 - val_weight_output_loss: 0.9710 - val_bag_output_loss: 0.8104 - val_pose_output_loss: 0.5374 - val_footwear_output_loss: 0.7976 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.8656 - val_image_quality_output_acc: 0.5497 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6127 - val_bag_output_acc: 0.6555 - val_pose_output_acc: 0.7869 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.7072\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.39708\n",
            "Epoch 4/30\n",
            "360/360 [==============================] - 134s 373ms/step - loss: 20.3259 - gender_output_loss: 0.3658 - image_quality_output_loss: 0.8999 - age_output_loss: 1.3099 - weight_output_loss: 0.9330 - bag_output_loss: 0.8031 - pose_output_loss: 0.4923 - footwear_output_loss: 0.8068 - emotion_output_loss: 0.8664 - gender_output_acc: 0.8365 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4196 - weight_output_acc: 0.6438 - bag_output_acc: 0.6526 - pose_output_acc: 0.8031 - footwear_output_acc: 0.6415 - emotion_output_acc: 0.7054 - val_loss: 20.5280 - val_gender_output_loss: 0.3505 - val_image_quality_output_loss: 0.9409 - val_age_output_loss: 1.2993 - val_weight_output_loss: 0.9683 - val_bag_output_loss: 0.8103 - val_pose_output_loss: 0.4837 - val_footwear_output_loss: 0.7949 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.8474 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.4336 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.8189 - val_footwear_output_acc: 0.6560 - val_emotion_output_acc: 0.6831\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.39708\n",
            "Epoch 5/30\n",
            "360/360 [==============================] - 138s 382ms/step - loss: 20.4345 - gender_output_loss: 0.3609 - image_quality_output_loss: 0.8969 - age_output_loss: 1.3081 - weight_output_loss: 0.9474 - bag_output_loss: 0.8024 - pose_output_loss: 0.5148 - footwear_output_loss: 0.8172 - emotion_output_loss: 0.8642 - gender_output_acc: 0.8425 - image_quality_output_acc: 0.5776 - age_output_acc: 0.4243 - weight_output_acc: 0.6312 - bag_output_acc: 0.6512 - pose_output_acc: 0.7955 - footwear_output_acc: 0.6382 - emotion_output_acc: 0.7087 - val_loss: 20.8075 - val_gender_output_loss: 0.3428 - val_image_quality_output_loss: 0.9910 - val_age_output_loss: 1.2974 - val_weight_output_loss: 0.9560 - val_bag_output_loss: 0.8528 - val_pose_output_loss: 0.4555 - val_footwear_output_loss: 0.8087 - val_emotion_output_loss: 0.9339 - val_gender_output_acc: 0.8519 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.4331 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6097 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6531\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.39708\n",
            "Epoch 6/30\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 20.0538 - gender_output_loss: 0.3478 - image_quality_output_loss: 0.8968 - age_output_loss: 1.2958 - weight_output_loss: 0.9193 - bag_output_loss: 0.7884 - pose_output_loss: 0.4692 - footwear_output_loss: 0.8137 - emotion_output_loss: 0.8548 - gender_output_acc: 0.8491 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4293 - weight_output_acc: 0.6472 - bag_output_acc: 0.6620 - pose_output_acc: 0.8167 - footwear_output_acc: 0.6339 - emotion_output_acc: 0.7132 - val_loss: 21.5701 - val_gender_output_loss: 0.3913 - val_image_quality_output_loss: 1.2832 - val_age_output_loss: 1.3142 - val_weight_output_loss: 0.9573 - val_bag_output_loss: 0.8249 - val_pose_output_loss: 0.4884 - val_footwear_output_loss: 0.8038 - val_emotion_output_loss: 0.9355 - val_gender_output_acc: 0.8533 - val_image_quality_output_acc: 0.4311 - val_age_output_acc: 0.4203 - val_weight_output_acc: 0.6107 - val_bag_output_acc: 0.6560 - val_pose_output_acc: 0.8381 - val_footwear_output_acc: 0.6658 - val_emotion_output_acc: 0.6570\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.39708\n",
            "Epoch 7/30\n",
            "359/360 [============================>.] - ETA: 0s - loss: 19.6573 - gender_output_loss: 0.3217 - image_quality_output_loss: 0.8828 - age_output_loss: 1.2626 - weight_output_loss: 0.9211 - bag_output_loss: 0.7746 - pose_output_loss: 0.4554 - footwear_output_loss: 0.7874 - emotion_output_loss: 0.8422 - gender_output_acc: 0.8663 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4415 - weight_output_acc: 0.6429 - bag_output_acc: 0.6744 - pose_output_acc: 0.8219 - footwear_output_acc: 0.6459 - emotion_output_acc: 0.7176\n",
            "Epoch 7/30\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 19.6548 - gender_output_loss: 0.3216 - image_quality_output_loss: 0.8826 - age_output_loss: 1.2629 - weight_output_loss: 0.9201 - bag_output_loss: 0.7744 - pose_output_loss: 0.4547 - footwear_output_loss: 0.7876 - emotion_output_loss: 0.8427 - gender_output_acc: 0.8660 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4413 - weight_output_acc: 0.6438 - bag_output_acc: 0.6741 - pose_output_acc: 0.8224 - footwear_output_acc: 0.6460 - emotion_output_acc: 0.7172 - val_loss: 21.0048 - val_gender_output_loss: 0.3190 - val_image_quality_output_loss: 1.2249 - val_age_output_loss: 1.3237 - val_weight_output_loss: 0.9650 - val_bag_output_loss: 0.7918 - val_pose_output_loss: 0.4409 - val_footwear_output_loss: 0.7921 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.8711 - val_image_quality_output_acc: 0.4537 - val_age_output_acc: 0.4149 - val_weight_output_acc: 0.6137 - val_bag_output_acc: 0.6767 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6688 - val_emotion_output_acc: 0.6658\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.39708\n",
            "Epoch 8/30\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 19.4507 - gender_output_loss: 0.2988 - image_quality_output_loss: 0.8778 - age_output_loss: 1.2596 - weight_output_loss: 0.9078 - bag_output_loss: 0.7467 - pose_output_loss: 0.4421 - footwear_output_loss: 0.7873 - emotion_output_loss: 0.8492 - gender_output_acc: 0.8738 - image_quality_output_acc: 0.5804 - age_output_acc: 0.4392 - weight_output_acc: 0.6457 - bag_output_acc: 0.6856 - pose_output_acc: 0.8264 - footwear_output_acc: 0.6530 - emotion_output_acc: 0.7089 - val_loss: 20.0387 - val_gender_output_loss: 0.2914 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.2733 - val_weight_output_loss: 0.9411 - val_bag_output_loss: 0.7864 - val_pose_output_loss: 0.4443 - val_footwear_output_loss: 0.7769 - val_emotion_output_loss: 0.8880 - val_gender_output_acc: 0.8814 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.4321 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6890 - val_pose_output_acc: 0.8509 - val_footwear_output_acc: 0.6777 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00008: val_loss improved from 20.39708 to 20.03874, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.008.h5\n",
            "Epoch 9/30\n",
            "360/360 [==============================] - 136s 379ms/step - loss: 19.1860 - gender_output_loss: 0.2877 - image_quality_output_loss: 0.8724 - age_output_loss: 1.2422 - weight_output_loss: 0.8900 - bag_output_loss: 0.7513 - pose_output_loss: 0.4299 - footwear_output_loss: 0.7724 - emotion_output_loss: 0.8357 - gender_output_acc: 0.8800 - image_quality_output_acc: 0.5925 - age_output_acc: 0.4505 - weight_output_acc: 0.6547 - bag_output_acc: 0.6828 - pose_output_acc: 0.8351 - footwear_output_acc: 0.6594 - emotion_output_acc: 0.7155 - val_loss: 20.4718 - val_gender_output_loss: 0.3169 - val_image_quality_output_loss: 0.9652 - val_age_output_loss: 1.3016 - val_weight_output_loss: 0.9581 - val_bag_output_loss: 0.7993 - val_pose_output_loss: 0.4906 - val_footwear_output_loss: 0.8144 - val_emotion_output_loss: 0.8836 - val_gender_output_acc: 0.8878 - val_image_quality_output_acc: 0.5463 - val_age_output_acc: 0.4193 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6644 - val_pose_output_acc: 0.8282 - val_footwear_output_acc: 0.6747 - val_emotion_output_acc: 0.6978\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.03874\n",
            "Epoch 10/30\n",
            "360/360 [==============================] - 132s 367ms/step - loss: 19.7068 - gender_output_loss: 0.3222 - image_quality_output_loss: 0.8867 - age_output_loss: 1.2701 - weight_output_loss: 0.9352 - bag_output_loss: 0.7612 - pose_output_loss: 0.4402 - footwear_output_loss: 0.7912 - emotion_output_loss: 0.8560 - gender_output_acc: 0.8627 - image_quality_output_acc: 0.5786 - age_output_acc: 0.4335 - weight_output_acc: 0.6394 - bag_output_acc: 0.6774 - pose_output_acc: 0.8299 - footwear_output_acc: 0.6573 - emotion_output_acc: 0.7083 - val_loss: 21.2621 - val_gender_output_loss: 0.3357 - val_image_quality_output_loss: 1.1930 - val_age_output_loss: 1.3088 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.7866 - val_pose_output_loss: 0.4878 - val_footwear_output_loss: 0.8120 - val_emotion_output_loss: 0.9468 - val_gender_output_acc: 0.8563 - val_image_quality_output_acc: 0.4813 - val_age_output_acc: 0.4311 - val_weight_output_acc: 0.6033 - val_bag_output_acc: 0.6855 - val_pose_output_acc: 0.8342 - val_footwear_output_acc: 0.6634 - val_emotion_output_acc: 0.6398\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.03874\n",
            "Epoch 11/30\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 20.0160 - gender_output_loss: 0.3422 - image_quality_output_loss: 0.8895 - age_output_loss: 1.2834 - weight_output_loss: 0.9307 - bag_output_loss: 0.7841 - pose_output_loss: 0.4786 - footwear_output_loss: 0.8127 - emotion_output_loss: 0.8550 - gender_output_acc: 0.8530 - image_quality_output_acc: 0.5809 - age_output_acc: 0.4347 - weight_output_acc: 0.6432 - bag_output_acc: 0.6611 - pose_output_acc: 0.8097 - footwear_output_acc: 0.6340 - emotion_output_acc: 0.7137 - val_loss: 21.3737 - val_gender_output_loss: 0.4022 - val_image_quality_output_loss: 1.0613 - val_age_output_loss: 1.3921 - val_weight_output_loss: 0.9705 - val_bag_output_loss: 0.8115 - val_pose_output_loss: 0.5179 - val_footwear_output_loss: 0.8092 - val_emotion_output_loss: 0.8911 - val_gender_output_acc: 0.8376 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.4085 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6486 - val_pose_output_acc: 0.8071 - val_footwear_output_acc: 0.6329 - val_emotion_output_acc: 0.7028\n",
            "360/360 [==============================]\n",
            "Epoch 00011: val_loss did not improve from 20.03874\n",
            "Epoch 12/30\n",
            "360/360 [==============================] - 132s 366ms/step - loss: 20.1897 - gender_output_loss: 0.3596 - image_quality_output_loss: 0.8982 - age_output_loss: 1.3035 - weight_output_loss: 0.9232 - bag_output_loss: 0.7833 - pose_output_loss: 0.4933 - footwear_output_loss: 0.8023 - emotion_output_loss: 0.8637 - gender_output_acc: 0.8391 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4210 - weight_output_acc: 0.6469 - bag_output_acc: 0.6611 - pose_output_acc: 0.8061 - footwear_output_acc: 0.6495 - emotion_output_acc: 0.7104 - val_loss: 20.6479 - val_gender_output_loss: 0.3670 - val_image_quality_output_loss: 0.9079 - val_age_output_loss: 1.2996 - val_weight_output_loss: 0.9552 - val_bag_output_loss: 0.8132 - val_pose_output_loss: 0.5159 - val_footwear_output_loss: 0.8253 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.8474 - val_image_quality_output_acc: 0.5694 - val_age_output_acc: 0.4040 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6344 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6491 - val_emotion_output_acc: 0.6831\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 20.03874\n",
            "Epoch 13/30\n",
            "360/360 [==============================] - 135s 376ms/step - loss: 20.3256 - gender_output_loss: 0.3656 - image_quality_output_loss: 0.8967 - age_output_loss: 1.3043 - weight_output_loss: 0.9573 - bag_output_loss: 0.7943 - pose_output_loss: 0.4695 - footwear_output_loss: 0.8037 - emotion_output_loss: 0.8749 - gender_output_acc: 0.8394 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4212 - weight_output_acc: 0.6306 - bag_output_acc: 0.6545 - pose_output_acc: 0.8082 - footwear_output_acc: 0.6439 - emotion_output_acc: 0.7064 - val_loss: 20.4795 - val_gender_output_loss: 0.3160 - val_image_quality_output_loss: 0.9056 - val_age_output_loss: 1.3431 - val_weight_output_loss: 0.9527 - val_bag_output_loss: 0.8262 - val_pose_output_loss: 0.4704 - val_footwear_output_loss: 0.7910 - val_emotion_output_loss: 0.8794 - val_gender_output_acc: 0.8706 - val_image_quality_output_acc: 0.5699 - val_age_output_acc: 0.4065 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6467 - val_pose_output_acc: 0.8258 - val_footwear_output_acc: 0.6590 - val_emotion_output_acc: 0.7037\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 20.03874\n",
            "Epoch 14/30\n",
            "360/360 [==============================] - 132s 366ms/step - loss: 19.9220 - gender_output_loss: 0.3407 - image_quality_output_loss: 0.8934 - age_output_loss: 1.2803 - weight_output_loss: 0.9075 - bag_output_loss: 0.7784 - pose_output_loss: 0.4894 - footwear_output_loss: 0.8079 - emotion_output_loss: 0.8436 - gender_output_acc: 0.8500 - image_quality_output_acc: 0.5710 - age_output_acc: 0.4332 - weight_output_acc: 0.6535 - bag_output_acc: 0.6651 - pose_output_acc: 0.8068 - footwear_output_acc: 0.6384 - emotion_output_acc: 0.7141 - val_loss: 20.7371 - val_gender_output_loss: 0.3191 - val_image_quality_output_loss: 1.0294 - val_age_output_loss: 1.3217 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.7930 - val_pose_output_loss: 0.4570 - val_footwear_output_loss: 0.8025 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.8696 - val_image_quality_output_acc: 0.5281 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.6063 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.8406 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6806\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 20.03874\n",
            "Epoch 15/30\n",
            "360/360 [==============================] - 136s 376ms/step - loss: 19.5388 - gender_output_loss: 0.3144 - image_quality_output_loss: 0.8883 - age_output_loss: 1.2578 - weight_output_loss: 0.8916 - bag_output_loss: 0.7688 - pose_output_loss: 0.4390 - footwear_output_loss: 0.7871 - emotion_output_loss: 0.8539 - gender_output_acc: 0.8663 - image_quality_output_acc: 0.5698 - age_output_acc: 0.4450 - weight_output_acc: 0.6589 - bag_output_acc: 0.6783 - pose_output_acc: 0.8326 - footwear_output_acc: 0.6517 - emotion_output_acc: 0.7083 - val_loss: 20.5552 - val_gender_output_loss: 0.3225 - val_image_quality_output_loss: 1.0529 - val_age_output_loss: 1.2856 - val_weight_output_loss: 0.9477 - val_bag_output_loss: 0.7956 - val_pose_output_loss: 0.4429 - val_footwear_output_loss: 0.8134 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.8780 - val_image_quality_output_acc: 0.5310 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6152 - val_bag_output_acc: 0.6624 - val_pose_output_acc: 0.8543 - val_footwear_output_acc: 0.6654 - val_emotion_output_acc: 0.6668\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 20.03874\n",
            "Epoch 16/30\n",
            "360/360 [==============================] - 132s 367ms/step - loss: 19.5510 - gender_output_loss: 0.3018 - image_quality_output_loss: 0.8726 - age_output_loss: 1.2596 - weight_output_loss: 0.9341 - bag_output_loss: 0.7611 - pose_output_loss: 0.4426 - footwear_output_loss: 0.7919 - emotion_output_loss: 0.8390 - gender_output_acc: 0.8670 - image_quality_output_acc: 0.5894 - age_output_acc: 0.4358 - weight_output_acc: 0.6354 - bag_output_acc: 0.6748 - pose_output_acc: 0.8269 - footwear_output_acc: 0.6507 - emotion_output_acc: 0.7134 - val_loss: 20.3654 - val_gender_output_loss: 0.3044 - val_image_quality_output_loss: 1.0280 - val_age_output_loss: 1.2846 - val_weight_output_loss: 0.9593 - val_bag_output_loss: 0.7855 - val_pose_output_loss: 0.4369 - val_footwear_output_loss: 0.8106 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8922 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.4272 - val_weight_output_acc: 0.6058 - val_bag_output_acc: 0.6786 - val_pose_output_acc: 0.8504 - val_footwear_output_acc: 0.6698 - val_emotion_output_acc: 0.6742\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00016: val_loss did not improve from 20.03874\n",
            "Epoch 17/30\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 19.3253 - gender_output_loss: 0.3024 - image_quality_output_loss: 0.8832 - age_output_loss: 1.2375 - weight_output_loss: 0.8993 - bag_output_loss: 0.7585 - pose_output_loss: 0.4113 - footwear_output_loss: 0.8004 - emotion_output_loss: 0.8468 - gender_output_acc: 0.8712 - image_quality_output_acc: 0.5849 - age_output_acc: 0.4503 - weight_output_acc: 0.6512 - bag_output_acc: 0.6837 - pose_output_acc: 0.8384 - footwear_output_acc: 0.6441 - emotion_output_acc: 0.7097 - val_loss: 20.6352 - val_gender_output_loss: 0.3057 - val_image_quality_output_loss: 1.1202 - val_age_output_loss: 1.2926 - val_weight_output_loss: 0.9707 - val_bag_output_loss: 0.7844 - val_pose_output_loss: 0.4472 - val_footwear_output_loss: 0.7839 - val_emotion_output_loss: 0.9078 - val_gender_output_acc: 0.8824 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.4119 - val_weight_output_acc: 0.5955 - val_bag_output_acc: 0.6781 - val_pose_output_acc: 0.8553 - val_footwear_output_acc: 0.6767 - val_emotion_output_acc: 0.6649\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 20.03874\n",
            "Epoch 18/30\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 19.3253 - gender_output_loss: 0.3024 - image_quality_output_loss: 0.8832 - age_output_loss: 1.2375 - weight_output_loss: 0.8993 - bag_output_loss: 0.7585 - pose_output_loss: 0.4113 - footwear_output_loss: 0.8004 - emotion_output_loss: 0.8468 - gender_output_acc: 0.8712 - image_quality_output_acc: 0.5849 - age_output_acc: 0.4503 - weight_output_acc: 0.6512 - bag_output_acc: 0.6837 - pose_output_acc: 0.8384 - footwear_output_acc: 0.6441 - emotion_output_acc: 0.7097 - val_loss: 20.6352 - val_gender_output_loss: 0.3057 - val_image_quality_output_loss: 1.1202 - val_age_output_loss: 1.2926 - val_weight_output_loss: 0.9707 - val_bag_output_loss: 0.7844 - val_pose_output_loss: 0.4472 - val_footwear_output_loss: 0.7839 - val_emotion_output_loss: 0.9078 - val_gender_output_acc: 0.8824 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.4119 - val_weight_output_acc: 0.5955 - val_bag_output_acc: 0.6781 - val_pose_output_acc: 0.8553 - val_footwear_output_acc: 0.6767 - val_emotion_output_acc: 0.6649\n",
            " - 133s 370ms/step - loss: 19.6591 - gender_output_loss: 0.3106 - image_quality_output_loss: 0.8797 - age_output_loss: 1.2865 - weight_output_loss: 0.9195 - bag_output_loss: 0.7630 - pose_output_loss: 0.4509 - footwear_output_loss: 0.7888 - emotion_output_loss: 0.8373 - gender_output_acc: 0.8677 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4356 - weight_output_acc: 0.6460 - bag_output_acc: 0.6783 - pose_output_acc: 0.8226 - footwear_output_acc: 0.6576 - emotion_output_acc: 0.7125 - val_loss: 21.7915 - val_gender_output_loss: 0.4482 - val_image_quality_output_loss: 1.1670 - val_age_output_loss: 1.3740 - val_weight_output_loss: 0.9618 - val_bag_output_loss: 0.8916 - val_pose_output_loss: 0.4694 - val_footwear_output_loss: 0.8263 - val_emotion_output_loss: 0.9099 - val_gender_output_acc: 0.8455 - val_image_quality_output_acc: 0.4921 - val_age_output_acc: 0.4109 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6511 - val_emotion_output_acc: 0.6649\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 20.03874\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577635049_backup\n",
            "End of EPOCHS= 30  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZJ7uq_nZ5Ub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5d20aec-5aa2-477d-a23a-3f8850b345b1"
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#last_saved_file ='/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 1, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 20 patience= 10\n",
            "Backing up history file: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json  to: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577637130_backup\n",
            "Epoch 1/20\n",
            "360/360 [==============================] - 143s 396ms/step - loss: 13.0692 - gender_output_loss: 0.2912 - image_quality_output_loss: 0.8684 - age_output_loss: 1.2607 - weight_output_loss: 0.9078 - bag_output_loss: 0.7454 - pose_output_loss: 0.4361 - footwear_output_loss: 0.7615 - emotion_output_loss: 0.8443 - gender_output_acc: 0.8748 - image_quality_output_acc: 0.5938 - age_output_acc: 0.4443 - weight_output_acc: 0.6443 - bag_output_acc: 0.6854 - pose_output_acc: 0.8313 - footwear_output_acc: 0.6653 - emotion_output_acc: 0.7141 - val_loss: 14.0112 - val_gender_output_loss: 0.3085 - val_image_quality_output_loss: 1.0650 - val_age_output_loss: 1.2894 - val_weight_output_loss: 0.9571 - val_bag_output_loss: 0.7978 - val_pose_output_loss: 0.4235 - val_footwear_output_loss: 0.8132 - val_emotion_output_loss: 0.9022 - val_gender_output_acc: 0.8804 - val_image_quality_output_acc: 0.5167 - val_age_output_acc: 0.4286 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6796 - val_pose_output_acc: 0.8533 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6772\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 14.01122, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 143s 396ms/step - loss: 13.0692 - gender_output_loss: 0.2912 - image_quality_output_loss: 0.8684 - age_output_loss: 1.2607 - weight_output_loss: 0.9078 - bag_output_loss: 0.7454 - pose_output_loss: 0.4361 - footwear_output_loss: 0.7615 - emotion_output_loss: 0.8443 - gender_output_acc: 0.8748 - image_quality_output_acc: 0.5938 - age_output_acc: 0.4443 - weight_output_acc: 0.6443 - bag_output_acc: 0.6854 - pose_output_acc: 0.8313 - footwear_output_acc: 0.6653 - emotion_output_acc: 0.7141 - val_loss: 14.0112 - val_gender_output_loss: 0.3085 - val_image_quality_output_loss: 1.0650 - val_age_output_loss: 1.2894 - val_weight_output_loss: 0.9571 - val_bag_output_loss: 0.7978 - val_pose_output_loss: 0.4235 - val_footwear_output_loss: 0.8132 - val_emotion_output_loss: 0.9022 - val_gender_output_acc: 0.8804 - val_image_quality_output_acc: 0.5167 - val_age_output_acc: 0.4286 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6796 - val_pose_output_acc: 0.8533 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6772\n",
            "360/360 [==============================] - 131s 365ms/step - loss: 13.1955 - gender_output_loss: 0.3105 - image_quality_output_loss: 0.8853 - age_output_loss: 1.2492 - weight_output_loss: 0.9148 - bag_output_loss: 0.7587 - pose_output_loss: 0.4368 - footwear_output_loss: 0.7914 - emotion_output_loss: 0.8463 - gender_output_acc: 0.8694 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4549 - weight_output_acc: 0.6510 - bag_output_acc: 0.6821 - pose_output_acc: 0.8311 - footwear_output_acc: 0.6540 - emotion_output_acc: 0.7125 - val_loss: 14.0129 - val_gender_output_loss: 0.3072 - val_image_quality_output_loss: 1.0749 - val_age_output_loss: 1.2779 - val_weight_output_loss: 0.9594 - val_bag_output_loss: 0.8045 - val_pose_output_loss: 0.4375 - val_footwear_output_loss: 0.7934 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.8829 - val_image_quality_output_acc: 0.5049 - val_age_output_acc: 0.4316 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6693 - val_pose_output_acc: 0.8553 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6673\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 14.01122\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 138s 384ms/step - loss: 13.3210 - gender_output_loss: 0.3263 - image_quality_output_loss: 0.8841 - age_output_loss: 1.2640 - weight_output_loss: 0.9174 - bag_output_loss: 0.7737 - pose_output_loss: 0.4423 - footwear_output_loss: 0.8051 - emotion_output_loss: 0.8527 - gender_output_acc: 0.8594 - image_quality_output_acc: 0.5825 - age_output_acc: 0.4436 - weight_output_acc: 0.6477 - bag_output_acc: 0.6747 - pose_output_acc: 0.8257 - footwear_output_acc: 0.6401 - emotion_output_acc: 0.7087 - val_loss: 13.7732 - val_gender_output_loss: 0.3154 - val_image_quality_output_loss: 0.9368 - val_age_output_loss: 1.2961 - val_weight_output_loss: 0.9592 - val_bag_output_loss: 0.8215 - val_pose_output_loss: 0.4505 - val_footwear_output_loss: 0.8033 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6422 - val_pose_output_acc: 0.8317 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6796\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00003: val_loss improved from 14.01122 to 13.77325, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.003.h5\n",
            "Epoch 4/20\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 13.3964 - gender_output_loss: 0.3309 - image_quality_output_loss: 0.8959 - age_output_loss: 1.2816 - weight_output_loss: 0.9197 - bag_output_loss: 0.7751 - pose_output_loss: 0.4751 - footwear_output_loss: 0.7942 - emotion_output_loss: 0.8541 - gender_output_acc: 0.8542 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4321 - weight_output_acc: 0.6457 - bag_output_acc: 0.6651 - pose_output_acc: 0.8144 - footwear_output_acc: 0.6477 - emotion_output_acc: 0.7135 - val_loss: 14.6497 - val_gender_output_loss: 0.3382 - val_image_quality_output_loss: 1.2629 - val_age_output_loss: 1.3179 - val_weight_output_loss: 0.9603 - val_bag_output_loss: 0.8019 - val_pose_output_loss: 0.4937 - val_footwear_output_loss: 0.8447 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.8666 - val_image_quality_output_acc: 0.4547 - val_age_output_acc: 0.4198 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6663 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6708\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 13.77325\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 13.3964 - gender_output_loss: 0.3309 - image_quality_output_loss: 0.8959 - age_output_loss: 1.2816 - weight_output_loss: 0.9197 - bag_output_loss: 0.7751 - pose_output_loss: 0.4751 - footwear_output_loss: 0.7942 - emotion_output_loss: 0.8541 - gender_output_acc: 0.8542 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4321 - weight_output_acc: 0.6457 - bag_output_acc: 0.6651 - pose_output_acc: 0.8144 - footwear_output_acc: 0.6477 - emotion_output_acc: 0.7135 - val_loss: 14.6497 - val_gender_output_loss: 0.3382 - val_image_quality_output_loss: 1.2629 - val_age_output_loss: 1.3179 - val_weight_output_loss: 0.9603 - val_bag_output_loss: 0.8019 - val_pose_output_loss: 0.4937 - val_footwear_output_loss: 0.8447 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.8666 - val_image_quality_output_acc: 0.4547 - val_age_output_acc: 0.4198 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6663 - val_pose_output_acc: 0.8209 - val_footwear_output_acc: 0.6403 - val_emotion_output_acc: 0.6708\n",
            "360/360 [==============================] - 138s 382ms/step - loss: 13.3898 - gender_output_loss: 0.3353 - image_quality_output_loss: 0.8899 - age_output_loss: 1.2714 - weight_output_loss: 0.9300 - bag_output_loss: 0.7722 - pose_output_loss: 0.4547 - footwear_output_loss: 0.8039 - emotion_output_loss: 0.8609 - gender_output_acc: 0.8575 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4398 - weight_output_acc: 0.6411 - bag_output_acc: 0.6719 - pose_output_acc: 0.8229 - footwear_output_acc: 0.6365 - emotion_output_acc: 0.7090 - val_loss: 13.7862 - val_gender_output_loss: 0.2947 - val_image_quality_output_loss: 0.9409 - val_age_output_loss: 1.3239 - val_weight_output_loss: 0.9884 - val_bag_output_loss: 0.7978 - val_pose_output_loss: 0.4521 - val_footwear_output_loss: 0.7967 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.8809 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4070 - val_weight_output_acc: 0.5910 - val_bag_output_acc: 0.6654 - val_pose_output_acc: 0.8415 - val_footwear_output_acc: 0.6526 - val_emotion_output_acc: 0.6900\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 13.77325\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 13.2953 - gender_output_loss: 0.3192 - image_quality_output_loss: 0.8949 - age_output_loss: 1.2673 - weight_output_loss: 0.9106 - bag_output_loss: 0.7729 - pose_output_loss: 0.4578 - footwear_output_loss: 0.7933 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8686 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4351 - weight_output_acc: 0.6486 - bag_output_acc: 0.6780 - pose_output_acc: 0.8262 - footwear_output_acc: 0.6422 - emotion_output_acc: 0.7139 - val_loss: 13.8249 - val_gender_output_loss: 0.3104 - val_image_quality_output_loss: 1.0213 - val_age_output_loss: 1.3071 - val_weight_output_loss: 0.9566 - val_bag_output_loss: 0.7778 - val_pose_output_loss: 0.4473 - val_footwear_output_loss: 0.7908 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.8765 - val_image_quality_output_acc: 0.5320 - val_age_output_acc: 0.4149 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6841 - val_pose_output_acc: 0.8410 - val_footwear_output_acc: 0.6634 - val_emotion_output_acc: 0.6998\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 13.77325\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 13.2953 - gender_output_loss: 0.3192 - image_quality_output_loss: 0.8949 - age_output_loss: 1.2673 - weight_output_loss: 0.9106 - bag_output_loss: 0.7729 - pose_output_loss: 0.4578 - footwear_output_loss: 0.7933 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8686 - image_quality_output_acc: 0.5778 - age_output_acc: 0.4351 - weight_output_acc: 0.6486 - bag_output_acc: 0.6780 - pose_output_acc: 0.8262 - footwear_output_acc: 0.6422 - emotion_output_acc: 0.7139 - val_loss: 13.8249 - val_gender_output_loss: 0.3104 - val_image_quality_output_loss: 1.0213 - val_age_output_loss: 1.3071 - val_weight_output_loss: 0.9566 - val_bag_output_loss: 0.7778 - val_pose_output_loss: 0.4473 - val_footwear_output_loss: 0.7908 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.8765 - val_image_quality_output_acc: 0.5320 - val_age_output_acc: 0.4149 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6841 - val_pose_output_acc: 0.8410 - val_footwear_output_acc: 0.6634 - val_emotion_output_acc: 0.6998\n",
            "360/360 [==============================] - 138s 382ms/step - loss: 13.1516 - gender_output_loss: 0.3140 - image_quality_output_loss: 0.8848 - age_output_loss: 1.2618 - weight_output_loss: 0.9132 - bag_output_loss: 0.7541 - pose_output_loss: 0.4450 - footwear_output_loss: 0.7837 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8627 - image_quality_output_acc: 0.5790 - age_output_acc: 0.4431 - weight_output_acc: 0.6484 - bag_output_acc: 0.6858 - pose_output_acc: 0.8283 - footwear_output_acc: 0.6592 - emotion_output_acc: 0.7123 - val_loss: 13.8921 - val_gender_output_loss: 0.3016 - val_image_quality_output_loss: 1.0370 - val_age_output_loss: 1.2894 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.7858 - val_pose_output_loss: 0.4371 - val_footwear_output_loss: 0.8084 - val_emotion_output_loss: 0.9017 - val_gender_output_acc: 0.8775 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.4380 - val_weight_output_acc: 0.6063 - val_bag_output_acc: 0.6777 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 13.77325\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 138s 382ms/step - loss: 13.1516 - gender_output_loss: 0.3140 - image_quality_output_loss: 0.8848 - age_output_loss: 1.2618 - weight_output_loss: 0.9132 - bag_output_loss: 0.7541 - pose_output_loss: 0.4450 - footwear_output_loss: 0.7837 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8627 - image_quality_output_acc: 0.5790 - age_output_acc: 0.4431 - weight_output_acc: 0.6484 - bag_output_acc: 0.6858 - pose_output_acc: 0.8283 - footwear_output_acc: 0.6592 - emotion_output_acc: 0.7123 - val_loss: 13.8921 - val_gender_output_loss: 0.3016 - val_image_quality_output_loss: 1.0370 - val_age_output_loss: 1.2894 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.7858 - val_pose_output_loss: 0.4371 - val_footwear_output_loss: 0.8084 - val_emotion_output_loss: 0.9017 - val_gender_output_acc: 0.8775 - val_image_quality_output_acc: 0.5162 - val_age_output_acc: 0.4380 - val_weight_output_acc: 0.6063 - val_bag_output_acc: 0.6777 - val_pose_output_acc: 0.8465 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6796\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 13.0416 - gender_output_loss: 0.2940 - image_quality_output_loss: 0.8742 - age_output_loss: 1.2498 - weight_output_loss: 0.9171 - bag_output_loss: 0.7584 - pose_output_loss: 0.4234 - footwear_output_loss: 0.7775 - emotion_output_loss: 0.8396 - gender_output_acc: 0.8753 - image_quality_output_acc: 0.5936 - age_output_acc: 0.4503 - weight_output_acc: 0.6408 - bag_output_acc: 0.6830 - pose_output_acc: 0.8405 - footwear_output_acc: 0.6604 - emotion_output_acc: 0.7118 - val_loss: 14.0107 - val_gender_output_loss: 0.2917 - val_image_quality_output_loss: 1.1387 - val_age_output_loss: 1.2817 - val_weight_output_loss: 0.9525 - val_bag_output_loss: 0.7861 - val_pose_output_loss: 0.4682 - val_footwear_output_loss: 0.7884 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.8863 - val_image_quality_output_acc: 0.4995 - val_age_output_acc: 0.4242 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6836 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6718 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 13.77325\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 13.0416 - gender_output_loss: 0.2940 - image_quality_output_loss: 0.8742 - age_output_loss: 1.2498 - weight_output_loss: 0.9171 - bag_output_loss: 0.7584 - pose_output_loss: 0.4234 - footwear_output_loss: 0.7775 - emotion_output_loss: 0.8396 - gender_output_acc: 0.8753 - image_quality_output_acc: 0.5936 - age_output_acc: 0.4503 - weight_output_acc: 0.6408 - bag_output_acc: 0.6830 - pose_output_acc: 0.8405 - footwear_output_acc: 0.6604 - emotion_output_acc: 0.7118 - val_loss: 14.0107 - val_gender_output_loss: 0.2917 - val_image_quality_output_loss: 1.1387 - val_age_output_loss: 1.2817 - val_weight_output_loss: 0.9525 - val_bag_output_loss: 0.7861 - val_pose_output_loss: 0.4682 - val_footwear_output_loss: 0.7884 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.8863 - val_image_quality_output_acc: 0.4995 - val_age_output_acc: 0.4242 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6836 - val_pose_output_acc: 0.8445 - val_footwear_output_acc: 0.6718 - val_emotion_output_acc: 0.6801\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 12.9003 - gender_output_loss: 0.2936 - image_quality_output_loss: 0.8681 - age_output_loss: 1.2431 - weight_output_loss: 0.9086 - bag_output_loss: 0.7461 - pose_output_loss: 0.4219 - footwear_output_loss: 0.7678 - emotion_output_loss: 0.8249 - gender_output_acc: 0.8792 - image_quality_output_acc: 0.5854 - age_output_acc: 0.4538 - weight_output_acc: 0.6441 - bag_output_acc: 0.6896 - pose_output_acc: 0.8330 - footwear_output_acc: 0.6635 - emotion_output_acc: 0.7160 - val_loss: 14.4898 - val_gender_output_loss: 0.3204 - val_image_quality_output_loss: 1.2306 - val_age_output_loss: 1.3000 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.7970 - val_pose_output_loss: 0.4686 - val_footwear_output_loss: 0.8030 - val_emotion_output_loss: 0.9348 - val_gender_output_acc: 0.8824 - val_image_quality_output_acc: 0.4813 - val_age_output_acc: 0.4257 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6836 - val_pose_output_acc: 0.8430 - val_footwear_output_acc: 0.6599 - val_emotion_output_acc: 0.6550\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 13.77325\n",
            "Epoch 10/20\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 13.1467 - gender_output_loss: 0.3038 - image_quality_output_loss: 0.8855 - age_output_loss: 1.2510 - weight_output_loss: 0.9010 - bag_output_loss: 0.7587 - pose_output_loss: 0.4325 - footwear_output_loss: 0.7859 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8750 - image_quality_output_acc: 0.5745 - age_output_acc: 0.4410 - weight_output_acc: 0.6533 - bag_output_acc: 0.6830 - pose_output_acc: 0.8352 - footwear_output_acc: 0.6517 - emotion_output_acc: 0.7073 - val_loss: 14.0068 - val_gender_output_loss: 0.3145 - val_image_quality_output_loss: 1.0981 - val_age_output_loss: 1.3374 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.7868 - val_pose_output_loss: 0.4599 - val_footwear_output_loss: 0.7848 - val_emotion_output_loss: 0.8783 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5094 - val_age_output_acc: 0.4006 - val_weight_output_acc: 0.5945 - val_bag_output_acc: 0.6722 - val_pose_output_acc: 0.8327 - val_footwear_output_acc: 0.6747 - val_emotion_output_acc: 0.6929\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 13.77325\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 138s 382ms/step - loss: 13.2267 - gender_output_loss: 0.3197 - image_quality_output_loss: 0.8817 - age_output_loss: 1.2606 - weight_output_loss: 0.9175 - bag_output_loss: 0.7743 - pose_output_loss: 0.4553 - footwear_output_loss: 0.7872 - emotion_output_loss: 0.8528 - gender_output_acc: 0.8606 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4441 - weight_output_acc: 0.6408 - bag_output_acc: 0.6717 - pose_output_acc: 0.8236 - footwear_output_acc: 0.6497 - emotion_output_acc: 0.7083 - val_loss: 13.7399 - val_gender_output_loss: 0.3418 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.2967 - val_weight_output_loss: 0.9547 - val_bag_output_loss: 0.7954 - val_pose_output_loss: 0.4418 - val_footwear_output_loss: 0.7757 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.8676 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.4267 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8450 - val_footwear_output_acc: 0.6767 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00011: val_loss improved from 13.77325 to 13.73993, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.011.h5\n",
            "Epoch 12/20\n",
            "360/360 [==============================] - 148s 410ms/step - loss: 13.3870 - gender_output_loss: 0.3401 - image_quality_output_loss: 0.8928 - age_output_loss: 1.2825 - weight_output_loss: 0.9190 - bag_output_loss: 0.7773 - pose_output_loss: 0.4806 - footwear_output_loss: 0.8107 - emotion_output_loss: 0.8527 - gender_output_acc: 0.8519 - image_quality_output_acc: 0.5710 - age_output_acc: 0.4342 - weight_output_acc: 0.6517 - bag_output_acc: 0.6667 - pose_output_acc: 0.8111 - footwear_output_acc: 0.6439 - emotion_output_acc: 0.7139 - val_loss: 13.6431 - val_gender_output_loss: 0.3077 - val_image_quality_output_loss: 0.9198 - val_age_output_loss: 1.2935 - val_weight_output_loss: 0.9503 - val_bag_output_loss: 0.8043 - val_pose_output_loss: 0.5061 - val_footwear_output_loss: 0.7848 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.8701 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.4311 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8125 - val_footwear_output_acc: 0.6639 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00012: val_loss improved from 13.73993 to 13.64306, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.012.h5\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 13.3265 - gender_output_loss: 0.3199 - image_quality_output_loss: 0.9006 - age_output_loss: 1.2841 - weight_output_loss: 0.9193 - bag_output_loss: 0.7874 - pose_output_loss: 0.4671 - footwear_output_loss: 0.7992 - emotion_output_loss: 0.8433 - gender_output_acc: 0.8585 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4323 - weight_output_acc: 0.6398 - bag_output_acc: 0.6597 - pose_output_acc: 0.8177 - footwear_output_acc: 0.6431 - emotion_output_acc: 0.7130 - val_loss: 13.6319 - val_gender_output_loss: 0.3216 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.2951 - val_weight_output_loss: 0.9525 - val_bag_output_loss: 0.7867 - val_pose_output_loss: 0.4487 - val_footwear_output_loss: 0.7777 - val_emotion_output_loss: 0.8914 - val_gender_output_acc: 0.8652 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6147 - val_bag_output_acc: 0.6732 - val_pose_output_acc: 0.8420 - val_footwear_output_acc: 0.6594 - val_emotion_output_acc: 0.6909\n",
            "360/360 [==============================] - 148s 410ms/step - loss: 13.3870 - gender_output_loss: 0.3401 - image_quality_output_loss: 0.8928 - age_output_loss: 1.2825 - weight_output_loss: 0.9190 - bag_output_loss: 0.7773 - pose_output_loss: 0.4806 - footwear_output_loss: 0.8107 - emotion_output_loss: 0.8527 - gender_output_acc: 0.8519 - image_quality_output_acc: 0.5710 - age_output_acc: 0.4342 - weight_output_acc: 0.6517 - bag_output_acc: 0.6667 - pose_output_acc: 0.8111 - footwear_output_acc: 0.6439 - emotion_output_acc: 0.7139 - val_loss: 13.6431 - val_gender_output_loss: 0.3077 - val_image_quality_output_loss: 0.9198 - val_age_output_loss: 1.2935 - val_weight_output_loss: 0.9503 - val_bag_output_loss: 0.8043 - val_pose_output_loss: 0.5061 - val_footwear_output_loss: 0.7848 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.8701 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.4311 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8125 - val_footwear_output_acc: 0.6639 - val_emotion_output_acc: 0.7052\n",
            "\n",
            "Epoch 00013: val_loss improved from 13.64306 to 13.63192, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.013.h5\n",
            "Epoch 14/20\n",
            "360/360 [==============================] - 135s 374ms/step - loss: 13.2505 - gender_output_loss: 0.3267 - image_quality_output_loss: 0.8799 - age_output_loss: 1.2750 - weight_output_loss: 0.9295 - bag_output_loss: 0.7584 - pose_output_loss: 0.4538 - footwear_output_loss: 0.7910 - emotion_output_loss: 0.8587 - gender_output_acc: 0.8597 - image_quality_output_acc: 0.5861 - age_output_acc: 0.4326 - weight_output_acc: 0.6398 - bag_output_acc: 0.6891 - pose_output_acc: 0.8240 - footwear_output_acc: 0.6488 - emotion_output_acc: 0.7087 - val_loss: 13.7752 - val_gender_output_loss: 0.3114 - val_image_quality_output_loss: 1.0137 - val_age_output_loss: 1.2945 - val_weight_output_loss: 0.9505 - val_bag_output_loss: 0.7989 - val_pose_output_loss: 0.4500 - val_footwear_output_loss: 0.7890 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.8740 - val_image_quality_output_acc: 0.5438 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6191 - val_bag_output_acc: 0.6831 - val_pose_output_acc: 0.8327 - val_footwear_output_acc: 0.6575 - val_emotion_output_acc: 0.6742\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 13.63192\n",
            "Epoch 15/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 13.0112 - gender_output_loss: 0.3029 - image_quality_output_loss: 0.8751 - age_output_loss: 1.2469 - weight_output_loss: 0.9196 - bag_output_loss: 0.7702 - pose_output_loss: 0.4369 - footwear_output_loss: 0.7794 - emotion_output_loss: 0.8295 - gender_output_acc: 0.8748 - image_quality_output_acc: 0.5943 - age_output_acc: 0.4378 - weight_output_acc: 0.6441 - bag_output_acc: 0.6745 - pose_output_acc: 0.8330 - footwear_output_acc: 0.6604 - emotion_output_acc: 0.7194 - val_loss: 13.7311 - val_gender_output_loss: 0.2926 - val_image_quality_output_loss: 1.0145 - val_age_output_loss: 1.2805 - val_weight_output_loss: 0.9488 - val_bag_output_loss: 0.7988 - val_pose_output_loss: 0.4441 - val_footwear_output_loss: 0.8031 - val_emotion_output_loss: 0.8911 - val_gender_output_acc: 0.8873 - val_image_quality_output_acc: 0.5384 - val_age_output_acc: 0.4252 - val_weight_output_acc: 0.6225 - val_bag_output_acc: 0.6826 - val_pose_output_acc: 0.8435 - val_footwear_output_acc: 0.6663 - val_emotion_output_acc: 0.6801\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 13.63192\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 136s 378ms/step - loss: 12.9893 - gender_output_loss: 0.3036 - image_quality_output_loss: 0.8721 - age_output_loss: 1.2383 - weight_output_loss: 0.9020 - bag_output_loss: 0.7444 - pose_output_loss: 0.4379 - footwear_output_loss: 0.7755 - emotion_output_loss: 0.8499 - gender_output_acc: 0.8733 - image_quality_output_acc: 0.5816 - age_output_acc: 0.4543 - weight_output_acc: 0.6514 - bag_output_acc: 0.6828 - pose_output_acc: 0.8335 - footwear_output_acc: 0.6547 - emotion_output_acc: 0.7052 - val_loss: 13.5052 - val_gender_output_loss: 0.2845 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.2728 - val_weight_output_loss: 0.9516 - val_bag_output_loss: 0.7762 - val_pose_output_loss: 0.4290 - val_footwear_output_loss: 0.7855 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.8858 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.4350 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6855 - val_pose_output_acc: 0.8524 - val_footwear_output_acc: 0.6713 - val_emotion_output_acc: 0.6821\n",
            "\n",
            "Epoch 00016: val_loss improved from 13.63192 to 13.50516, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.016.h5\n",
            "Epoch 17/20\n",
            "360/360 [==============================] - 136s 377ms/step - loss: 12.9680 - gender_output_loss: 0.3013 - image_quality_output_loss: 0.8779 - age_output_loss: 1.2408 - weight_output_loss: 0.9041 - bag_output_loss: 0.7564 - pose_output_loss: 0.4278 - footwear_output_loss: 0.7732 - emotion_output_loss: 0.8386 - gender_output_acc: 0.8712 - image_quality_output_acc: 0.5845 - age_output_acc: 0.4380 - weight_output_acc: 0.6472 - bag_output_acc: 0.6814 - pose_output_acc: 0.8354 - footwear_output_acc: 0.6491 - emotion_output_acc: 0.7094 - val_loss: 13.6758 - val_gender_output_loss: 0.2937 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.2893 - val_weight_output_loss: 0.9466 - val_bag_output_loss: 0.7797 - val_pose_output_loss: 0.4447 - val_footwear_output_loss: 0.8041 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.8844 - val_image_quality_output_acc: 0.5468 - val_age_output_acc: 0.4218 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6841 - val_pose_output_acc: 0.8524 - val_footwear_output_acc: 0.6624 - val_emotion_output_acc: 0.6772\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 13.50516\n",
            "Epoch 18/20\n",
            "360/360 [==============================] - 135s 375ms/step - loss: 12.9995 - gender_output_loss: 0.2998 - image_quality_output_loss: 0.8690 - age_output_loss: 1.2473 - weight_output_loss: 0.9151 - bag_output_loss: 0.7487 - pose_output_loss: 0.4405 - footwear_output_loss: 0.7859 - emotion_output_loss: 0.8407 - gender_output_acc: 0.8708 - image_quality_output_acc: 0.5858 - age_output_acc: 0.4547 - weight_output_acc: 0.6467 - bag_output_acc: 0.6891 - pose_output_acc: 0.8318 - footwear_output_acc: 0.6484 - emotion_output_acc: 0.7120 - val_loss: 13.7282 - val_gender_output_loss: 0.3159 - val_image_quality_output_loss: 1.0018 - val_age_output_loss: 1.2902 - val_weight_output_loss: 0.9520 - val_bag_output_loss: 0.7890 - val_pose_output_loss: 0.4708 - val_footwear_output_loss: 0.7977 - val_emotion_output_loss: 0.8889 - val_gender_output_acc: 0.8799 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.4419 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6757 - val_pose_output_acc: 0.8297 - val_footwear_output_acc: 0.6683 - val_emotion_output_acc: 0.6944\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 13.50516\n",
            "Epoch 19/20\n",
            "360/360 [==============================] - 135s 376ms/step - loss: 13.2317 - gender_output_loss: 0.3313 - image_quality_output_loss: 0.8769 - age_output_loss: 1.2703 - weight_output_loss: 0.9324 - bag_output_loss: 0.7790 - pose_output_loss: 0.4464 - footwear_output_loss: 0.7898 - emotion_output_loss: 0.8538 - gender_output_acc: 0.8580 - image_quality_output_acc: 0.5880 - age_output_acc: 0.4391 - weight_output_acc: 0.6328 - bag_output_acc: 0.6677 - pose_output_acc: 0.8304 - footwear_output_acc: 0.6427 - emotion_output_acc: 0.7069 - val_loss: 14.1044 - val_gender_output_loss: 0.3189 - val_image_quality_output_loss: 1.1260 - val_age_output_loss: 1.3063 - val_weight_output_loss: 0.9520 - val_bag_output_loss: 0.8139 - val_pose_output_loss: 0.4732 - val_footwear_output_loss: 0.8115 - val_emotion_output_loss: 0.8931 - val_gender_output_acc: 0.8666 - val_image_quality_output_acc: 0.4808 - val_age_output_acc: 0.4306 - val_weight_output_acc: 0.6284 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8204 - val_footwear_output_acc: 0.6457 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 13.50516\n",
            "Epoch 20/20\n",
            "360/360 [==============================] - 134s 373ms/step - loss: 13.3101 - gender_output_loss: 0.3427 - image_quality_output_loss: 0.8964 - age_output_loss: 1.2836 - weight_output_loss: 0.9137 - bag_output_loss: 0.7746 - pose_output_loss: 0.4858 - footwear_output_loss: 0.8084 - emotion_output_loss: 0.8426 - gender_output_acc: 0.8542 - image_quality_output_acc: 0.5637 - age_output_acc: 0.4300 - weight_output_acc: 0.6493 - bag_output_acc: 0.6712 - pose_output_acc: 0.8148 - footwear_output_acc: 0.6351 - emotion_output_acc: 0.7161 - val_loss: 13.9086 - val_gender_output_loss: 0.3075 - val_image_quality_output_loss: 1.0589 - val_age_output_loss: 1.2821 - val_weight_output_loss: 0.9643 - val_bag_output_loss: 0.8001 - val_pose_output_loss: 0.4887 - val_footwear_output_loss: 0.7735 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.8720 - val_image_quality_output_acc: 0.4833 - val_age_output_acc: 0.4400 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.6762 - val_pose_output_acc: 0.8253 - val_footwear_output_acc: 0.6649 - val_emotion_output_acc: 0.7042\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 13.50516\n",
            "Current JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Final JSON PATH: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json1577639870_backup\n",
            "End of EPOCHS= 20  STEPS_PER_EPOCH= 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALFXF7ZPpwkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cdf10d2-aa70-430c-f8de-38773f22cc1b"
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.016.h5'\n",
        "#last_saved_file ='/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 1, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.016.h5\n",
            "assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577639975_model.{epoch:03d}.h5\n",
            "JSON path: /content/gdrive/My Drive/WRN_Extend/json_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977.json\n",
            "Returning new callback array with steps_per_epoch= 360 min_lr= 0.0002511886 max_lr= 0.02511886 epoch_count= 20 patience= 10\n",
            "Epoch 1/20\n",
            "360/360 [==============================] - 141s 392ms/step - loss: 12.8544 - gender_output_loss: 0.3092 - image_quality_output_loss: 0.8716 - age_output_loss: 1.2309 - weight_output_loss: 0.9032 - bag_output_loss: 0.7534 - pose_output_loss: 0.4351 - footwear_output_loss: 0.7751 - emotion_output_loss: 0.8153 - gender_output_acc: 0.8648 - image_quality_output_acc: 0.5839 - age_output_acc: 0.4587 - weight_output_acc: 0.6524 - bag_output_acc: 0.6823 - pose_output_acc: 0.8321 - footwear_output_acc: 0.6582 - emotion_output_acc: 0.7203 - val_loss: 14.2956 - val_gender_output_loss: 0.3010 - val_image_quality_output_loss: 1.2481 - val_age_output_loss: 1.3114 - val_weight_output_loss: 0.9569 - val_bag_output_loss: 0.7857 - val_pose_output_loss: 0.4857 - val_footwear_output_loss: 0.8065 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.8937 - val_image_quality_output_acc: 0.4749 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6826 - val_pose_output_acc: 0.8292 - val_footwear_output_acc: 0.6585 - val_emotion_output_acc: 0.6939\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 14.29563, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577639975_model.001.h5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 132s 366ms/step - loss: 13.0654 - gender_output_loss: 0.2947 - image_quality_output_loss: 0.8740 - age_output_loss: 1.2565 - weight_output_loss: 0.9099 - bag_output_loss: 0.7475 - pose_output_loss: 0.4379 - footwear_output_loss: 0.7816 - emotion_output_loss: 0.8561 - gender_output_acc: 0.8757 - image_quality_output_acc: 0.5859 - age_output_acc: 0.4425 - weight_output_acc: 0.6491 - bag_output_acc: 0.6880 - pose_output_acc: 0.8285 - footwear_output_acc: 0.6510 - emotion_output_acc: 0.7056 - val_loss: 14.1091 - val_gender_output_loss: 0.2983 - val_image_quality_output_loss: 1.0545 - val_age_output_loss: 1.2958 - val_weight_output_loss: 0.9654 - val_bag_output_loss: 0.8015 - val_pose_output_loss: 0.4673 - val_footwear_output_loss: 0.7938 - val_emotion_output_loss: 0.9525 - val_gender_output_acc: 0.8853 - val_image_quality_output_acc: 0.5108 - val_age_output_acc: 0.4365 - val_weight_output_acc: 0.6068 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8391 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6319\n",
            "\n",
            "Epoch 00002: val_loss improved from 14.29563 to 14.10907, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577639975_model.002.h5\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 137s 380ms/step - loss: 13.1024 - gender_output_loss: 0.3206 - image_quality_output_loss: 0.8766 - age_output_loss: 1.2562 - weight_output_loss: 0.8981 - bag_output_loss: 0.7574 - pose_output_loss: 0.4437 - footwear_output_loss: 0.7921 - emotion_output_loss: 0.8502 - gender_output_acc: 0.8668 - image_quality_output_acc: 0.5885 - age_output_acc: 0.4417 - weight_output_acc: 0.6488 - bag_output_acc: 0.6785 - pose_output_acc: 0.8247 - footwear_output_acc: 0.6517 - emotion_output_acc: 0.7127 - val_loss: 15.2118 - val_gender_output_loss: 0.3298 - val_image_quality_output_loss: 1.1663 - val_age_output_loss: 1.3330 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 1.2457 - val_pose_output_loss: 0.4727 - val_footwear_output_loss: 0.8039 - val_emotion_output_loss: 0.9161 - val_gender_output_acc: 0.8765 - val_image_quality_output_acc: 0.4675 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6093 - val_bag_output_acc: 0.5551 - val_pose_output_acc: 0.8514 - val_footwear_output_acc: 0.6629 - val_emotion_output_acc: 0.6639\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 14.10907\n",
            "Epoch 4/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 13.2951 - gender_output_loss: 0.3279 - image_quality_output_loss: 0.8897 - age_output_loss: 1.2787 - weight_output_loss: 0.9376 - bag_output_loss: 0.7884 - pose_output_loss: 0.4623 - footwear_output_loss: 0.8011 - emotion_output_loss: 0.8452 - gender_output_acc: 0.8593 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4479 - weight_output_acc: 0.6384 - bag_output_acc: 0.6657 - pose_output_acc: 0.8165 - footwear_output_acc: 0.6391 - emotion_output_acc: 0.7138\n",
            "Epoch 4/20\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 13.2957 - gender_output_loss: 0.3282 - image_quality_output_loss: 0.8902 - age_output_loss: 1.2786 - weight_output_loss: 0.9374 - bag_output_loss: 0.7886 - pose_output_loss: 0.4622 - footwear_output_loss: 0.8012 - emotion_output_loss: 0.8449 - gender_output_acc: 0.8590 - image_quality_output_acc: 0.5729 - age_output_acc: 0.4484 - weight_output_acc: 0.6385 - bag_output_acc: 0.6660 - pose_output_acc: 0.8165 - footwear_output_acc: 0.6389 - emotion_output_acc: 0.7139 - val_loss: 15.2742 - val_gender_output_loss: 0.3848 - val_image_quality_output_loss: 1.3946 - val_age_output_loss: 1.3616 - val_weight_output_loss: 1.0475 - val_bag_output_loss: 0.8277 - val_pose_output_loss: 0.5560 - val_footwear_output_loss: 0.8697 - val_emotion_output_loss: 0.9272 - val_gender_output_acc: 0.8573 - val_image_quality_output_acc: 0.4286 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.5571 - val_bag_output_acc: 0.6634 - val_pose_output_acc: 0.8095 - val_footwear_output_acc: 0.6260 - val_emotion_output_acc: 0.6531\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 14.10907\n",
            "Epoch 5/20\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 13.2789 - gender_output_loss: 0.3465 - image_quality_output_loss: 0.8864 - age_output_loss: 1.2760 - weight_output_loss: 0.9235 - bag_output_loss: 0.7817 - pose_output_loss: 0.4663 - footwear_output_loss: 0.8045 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8443 - image_quality_output_acc: 0.5769 - age_output_acc: 0.4441 - weight_output_acc: 0.6446 - bag_output_acc: 0.6578 - pose_output_acc: 0.8191 - footwear_output_acc: 0.6405 - emotion_output_acc: 0.7155 - val_loss: 14.2716 - val_gender_output_loss: 0.3134 - val_image_quality_output_loss: 1.1255 - val_age_output_loss: 1.3225 - val_weight_output_loss: 1.0022 - val_bag_output_loss: 0.8166 - val_pose_output_loss: 0.4641 - val_footwear_output_loss: 0.8075 - val_emotion_output_loss: 0.9204 - val_gender_output_acc: 0.8696 - val_image_quality_output_acc: 0.5153 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6019 - val_bag_output_acc: 0.6575 - val_pose_output_acc: 0.8371 - val_footwear_output_acc: 0.6629 - val_emotion_output_acc: 0.6535\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 14.10907\n",
            "Epoch 6/20\n",
            "360/360 [==============================] - 133s 369ms/step - loss: 13.3049 - gender_output_loss: 0.3326 - image_quality_output_loss: 0.8879 - age_output_loss: 1.2902 - weight_output_loss: 0.9250 - bag_output_loss: 0.7694 - pose_output_loss: 0.4716 - footwear_output_loss: 0.7955 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8557 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4252 - weight_output_acc: 0.6391 - bag_output_acc: 0.6750 - pose_output_acc: 0.8155 - footwear_output_acc: 0.6458 - emotion_output_acc: 0.7073 - val_loss: 13.7549 - val_gender_output_loss: 0.3311 - val_image_quality_output_loss: 1.0010 - val_age_output_loss: 1.2872 - val_weight_output_loss: 0.9463 - val_bag_output_loss: 0.8001 - val_pose_output_loss: 0.4443 - val_footwear_output_loss: 0.7840 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.8622 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.4242 - val_weight_output_acc: 0.6211 - val_bag_output_acc: 0.6663 - val_pose_output_acc: 0.8401 - val_footwear_output_acc: 0.6708 - val_emotion_output_acc: 0.6796\n",
            "\n",
            "Epoch 00006: val_loss improved from 14.10907 to 13.75486, saving model to /content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577639975_model.006.h5\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 138s 383ms/step - loss: 13.0368 - gender_output_loss: 0.3063 - image_quality_output_loss: 0.8838 - age_output_loss: 1.2501 - weight_output_loss: 0.9053 - bag_output_loss: 0.7643 - pose_output_loss: 0.4345 - footwear_output_loss: 0.7772 - emotion_output_loss: 0.8456 - gender_output_acc: 0.8698 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4500 - weight_output_acc: 0.6514 - bag_output_acc: 0.6769 - pose_output_acc: 0.8326 - footwear_output_acc: 0.6575 - emotion_output_acc: 0.7085 - val_loss: 13.7902 - val_gender_output_loss: 0.2922 - val_image_quality_output_loss: 1.0592 - val_age_output_loss: 1.3011 - val_weight_output_loss: 0.9608 - val_bag_output_loss: 0.7811 - val_pose_output_loss: 0.4356 - val_footwear_output_loss: 0.7831 - val_emotion_output_loss: 0.8995 - val_gender_output_acc: 0.8863 - val_image_quality_output_acc: 0.5148 - val_age_output_acc: 0.4188 - val_weight_output_acc: 0.6201 - val_bag_output_acc: 0.6767 - val_pose_output_acc: 0.8474 - val_footwear_output_acc: 0.6678 - val_emotion_output_acc: 0.6767\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 13.75486\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 133s 370ms/step - loss: 12.8750 - gender_output_loss: 0.3008 - image_quality_output_loss: 0.8651 - age_output_loss: 1.2473 - weight_output_loss: 0.9266 - bag_output_loss: 0.7337 - pose_output_loss: 0.4298 - footwear_output_loss: 0.7742 - emotion_output_loss: 0.8310 - gender_output_acc: 0.8741 - image_quality_output_acc: 0.5943 - age_output_acc: 0.4505 - weight_output_acc: 0.6354 - bag_output_acc: 0.6953 - pose_output_acc: 0.8300 - footwear_output_acc: 0.6549 - emotion_output_acc: 0.7182 - val_loss: 13.8977 - val_gender_output_loss: 0.3046 - val_image_quality_output_loss: 1.0929 - val_age_output_loss: 1.2811 - val_weight_output_loss: 0.9721 - val_bag_output_loss: 0.8022 - val_pose_output_loss: 0.4348 - val_footwear_output_loss: 0.7955 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.8844 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.4405 - val_weight_output_acc: 0.6102 - val_bag_output_acc: 0.6703 - val_pose_output_acc: 0.8563 - val_footwear_output_acc: 0.6644 - val_emotion_output_acc: 0.6722\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 13.75486\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 12.7877 - gender_output_loss: 0.2882 - image_quality_output_loss: 0.8649 - age_output_loss: 1.2351 - weight_output_loss: 0.8978 - bag_output_loss: 0.7368 - pose_output_loss: 0.4217 - footwear_output_loss: 0.7706 - emotion_output_loss: 0.8285 - gender_output_acc: 0.8750 - image_quality_output_acc: 0.5925 - age_output_acc: 0.4517 - weight_output_acc: 0.6550 - bag_output_acc: 0.6918 - pose_output_acc: 0.8373 - footwear_output_acc: 0.6592 - emotion_output_acc: 0.7139 - val_loss: 13.7555 - val_gender_output_loss: 0.3082 - val_image_quality_output_loss: 1.0285 - val_age_output_loss: 1.2782 - val_weight_output_loss: 0.9483 - val_bag_output_loss: 0.7902 - val_pose_output_loss: 0.4384 - val_footwear_output_loss: 0.7991 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.8834 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.4360 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6786 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6752\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 13.75486\n",
            "Epoch 10/20\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 12.7877 - gender_output_loss: 0.2882 - image_quality_output_loss: 0.8649 - age_output_loss: 1.2351 - weight_output_loss: 0.8978 - bag_output_loss: 0.7368 - pose_output_loss: 0.4217 - footwear_output_loss: 0.7706 - emotion_output_loss: 0.8285 - gender_output_acc: 0.8750 - image_quality_output_acc: 0.5925 - age_output_acc: 0.4517 - weight_output_acc: 0.6550 - bag_output_acc: 0.6918 - pose_output_acc: 0.8373 - footwear_output_acc: 0.6592 - emotion_output_acc: 0.7139 - val_loss: 13.7555 - val_gender_output_loss: 0.3082 - val_image_quality_output_loss: 1.0285 - val_age_output_loss: 1.2782 - val_weight_output_loss: 0.9483 - val_bag_output_loss: 0.7902 - val_pose_output_loss: 0.4384 - val_footwear_output_loss: 0.7991 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.8834 - val_image_quality_output_acc: 0.5359 - val_age_output_acc: 0.4360 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6786 - val_pose_output_acc: 0.8469 - val_footwear_output_acc: 0.6609 - val_emotion_output_acc: 0.6752\n",
            "360/360 [==============================] - 134s 372ms/step - loss: 12.9852 - gender_output_loss: 0.3055 - image_quality_output_loss: 0.8863 - age_output_loss: 1.2568 - weight_output_loss: 0.9230 - bag_output_loss: 0.7527 - pose_output_loss: 0.4344 - footwear_output_loss: 0.7687 - emotion_output_loss: 0.8365 - gender_output_acc: 0.8681 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4474 - weight_output_acc: 0.6356 - bag_output_acc: 0.6854 - pose_output_acc: 0.8304 - footwear_output_acc: 0.6608 - emotion_output_acc: 0.7116 - val_loss: 14.4008 - val_gender_output_loss: 0.3518 - val_image_quality_output_loss: 1.1627 - val_age_output_loss: 1.3136 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.8429 - val_pose_output_loss: 0.4669 - val_footwear_output_loss: 0.8086 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.8671 - val_image_quality_output_acc: 0.4911 - val_age_output_acc: 0.4090 - val_weight_output_acc: 0.6009 - val_bag_output_acc: 0.6535 - val_pose_output_acc: 0.8381 - val_footwear_output_acc: 0.6658 - val_emotion_output_acc: 0.6467\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 13.75486\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 137s 381ms/step - loss: 13.1873 - gender_output_loss: 0.3182 - image_quality_output_loss: 0.8854 - age_output_loss: 1.2700 - weight_output_loss: 0.9334 - bag_output_loss: 0.7554 - pose_output_loss: 0.4459 - footwear_output_loss: 0.7991 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8720 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4339 - weight_output_acc: 0.6385 - bag_output_acc: 0.6873 - pose_output_acc: 0.8276 - footwear_output_acc: 0.6458 - emotion_output_acc: 0.7068 - val_loss: 14.3231 - val_gender_output_loss: 0.3538 - val_image_quality_output_loss: 1.1567 - val_age_output_loss: 1.3019 - val_weight_output_loss: 0.9535 - val_bag_output_loss: 0.8525 - val_pose_output_loss: 0.5423 - val_footwear_output_loss: 0.8038 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.8558 - val_image_quality_output_acc: 0.4744 - val_age_output_acc: 0.4272 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6353 - val_pose_output_acc: 0.7987 - val_footwear_output_acc: 0.6565 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 13.75486\n",
            "Epoch 12/20\n",
            "360/360 [==============================] - 134s 371ms/step - loss: 13.2300 - gender_output_loss: 0.3431 - image_quality_output_loss: 0.8937 - age_output_loss: 1.2791 - weight_output_loss: 0.9176 - bag_output_loss: 0.7811 - pose_output_loss: 0.4868 - footwear_output_loss: 0.7949 - emotion_output_loss: 0.8356 - gender_output_acc: 0.8542 - image_quality_output_acc: 0.5786 - age_output_acc: 0.4366 - weight_output_acc: 0.6483 - bag_output_acc: 0.6660 - pose_output_acc: 0.8068 - footwear_output_acc: 0.6495 - emotion_output_acc: 0.7174 - val_loss: 14.3707 - val_gender_output_loss: 0.3472 - val_image_quality_output_loss: 1.1182 - val_age_output_loss: 1.3056 - val_weight_output_loss: 0.9615 - val_bag_output_loss: 0.8101 - val_pose_output_loss: 0.4585 - val_footwear_output_loss: 0.9310 - val_emotion_output_loss: 0.9077 - val_gender_output_acc: 0.8647 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.4099 - val_weight_output_acc: 0.6216 - val_bag_output_acc: 0.6639 - val_pose_output_acc: 0.8351 - val_footwear_output_acc: 0.5591 - val_emotion_output_acc: 0.6718\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 13.75486\n",
            "Epoch 13/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 13.2577 - gender_output_loss: 0.3354 - image_quality_output_loss: 0.8829 - age_output_loss: 1.2777 - weight_output_loss: 0.9183 - bag_output_loss: 0.7767 - pose_output_loss: 0.4779 - footwear_output_loss: 0.7928 - emotion_output_loss: 0.8559 - gender_output_acc: 0.8520 - image_quality_output_acc: 0.5743 - age_output_acc: 0.4394 - weight_output_acc: 0.6492 - bag_output_acc: 0.6697 - pose_output_acc: 0.8134 - footwear_output_acc: 0.6502 - emotion_output_acc: 0.7114\n",
            "Epoch 00012: val_loss did not improve from 13.75486\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 138s 384ms/step - loss: 13.2534 - gender_output_loss: 0.3354 - image_quality_output_loss: 0.8829 - age_output_loss: 1.2776 - weight_output_loss: 0.9189 - bag_output_loss: 0.7766 - pose_output_loss: 0.4775 - footwear_output_loss: 0.7924 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8519 - image_quality_output_acc: 0.5743 - age_output_acc: 0.4392 - weight_output_acc: 0.6491 - bag_output_acc: 0.6698 - pose_output_acc: 0.8135 - footwear_output_acc: 0.6507 - emotion_output_acc: 0.7118 - val_loss: 13.9186 - val_gender_output_loss: 0.3844 - val_image_quality_output_loss: 1.0453 - val_age_output_loss: 1.3047 - val_weight_output_loss: 0.9562 - val_bag_output_loss: 0.8228 - val_pose_output_loss: 0.4600 - val_footwear_output_loss: 0.7854 - val_emotion_output_loss: 0.8902 - val_gender_output_acc: 0.8524 - val_image_quality_output_acc: 0.5177 - val_age_output_acc: 0.4306 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6649 - val_pose_output_acc: 0.8332 - val_footwear_output_acc: 0.6604 - val_emotion_output_acc: 0.6914\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 13.75486\n",
            "Epoch 14/20\n",
            "227/360 [=================>............] - ETA: 43s - loss: 13.2323 - gender_output_loss: 0.3276 - image_quality_output_loss: 0.8898 - age_output_loss: 1.2812 - weight_output_loss: 0.9316 - bag_output_loss: 0.7878 - pose_output_loss: 0.4531 - footwear_output_loss: 0.7769 - emotion_output_loss: 0.8522 - gender_output_acc: 0.8634 - image_quality_output_acc: 0.5743 - age_output_acc: 0.4364 - weight_output_acc: 0.6360 - bag_output_acc: 0.6644 - pose_output_acc: 0.8219 - footwear_output_acc: 0.6630 - emotion_output_acc: 0.7104"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-656:\n",
            "Process ForkPoolWorker-657:\n",
            "Process ForkPoolWorker-658:\n",
            "Process ForkPoolWorker-659:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-653:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-652:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-654:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "Process ForkPoolWorker-655:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"<ipython-input-7-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-7-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"<ipython-input-7-bb393f4f08f0>\", line 52, in __getitem__\n",
            "    image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-7-bb393f4f08f0>\", line 55, in __getitem__\n",
            "    train_std = np.std(image, axis=(0,1,2))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 884, in apply_transform\n",
            "    x = apply_brightness_shift(x, transform_parameters['brightness'])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 215, in apply_brightness_shift\n",
            "    x = array_to_img(x)\n",
            "  File \"<__array_function__ internals>\", line 6, in std\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 271, in array_to_img\n",
            "    return pil_image.fromarray(x.astype('uint8'), 'RGB')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 3381, in std\n",
            "    **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2434, in fromarray\n",
            "    obj = obj.tobytes()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 217, in _std\n",
            "    keepdims=keepdims)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 183, in _var\n",
            "    arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8a84e7316d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#class_weight=loss_weights_train,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lsZxVEwBTWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722.json\" \"/content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722_round2.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx79InfALyOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(weights_file+\"py\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwUEJewRFxtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BespGPlFHKHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict['val_age_output_acc'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-39qKvhHFHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict_new=get_indexed_results(y_pred_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2VU5EiI1dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def get_indexed_results(y_pred_results):\n",
        "    class_output_arr = ['gender',\n",
        "                        'imagequality',\n",
        "                        'age',\n",
        "                        'weight',\n",
        "                        'carryingbag',\n",
        "                        'bodypose',    \n",
        "                        'footwear',   \n",
        "                        'emotion']\n",
        "    y_pred_dict = {}\n",
        "    for len_val in range(len(y_pred_results)):\n",
        "        y_pred_local = y_pred_results[len_val][:-12]\n",
        "        y_pred_final = np.argmax(y_pred_local, axis=1)\n",
        "        y_pred_dict[class_output_arr[len_val]]=y_pred_final\n",
        "    return y_pred_dict\n",
        "def print_confusion_matrix(y_pred_dict, val_df_to_use, attribute_to_select):\n",
        "    cols_to_select=[col for col in one_hot_df.columns if col.startswith(attribute_to_select)]\n",
        "    y_true = val_df_to_use[cols_to_select].values\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    y_pred=y_pred_dict[attribute_to_select]\n",
        "    #print(y_pred_results[].shape, y_true.shape)\n",
        "    #print(y_true_classes)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    print(\"Confusion Matrix for\", attribute_to_select)\n",
        "    matrix = confusion_matrix(y_true_classes, y_pred)\n",
        "    print(matrix)\n",
        "    true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "    print(\"True Class Dist\",true_class_dist)\n",
        "    pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "    print(\"Predicted class dist\",pred_class_dist)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)\n",
        "\n",
        "y_pred_dict_new=get_indexed_results(y_pred_results)\n",
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvBCk7s2M62K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNRMbh8Jy4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=y_pred_dict['val_emotion_output_acc']\n",
        "true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "print(\"True Class Dist\",true_class_dist)\n",
        "pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "print(\"Predicted class dist\",pred_class_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkFGgymGq47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_gender_output_acc   \n",
        "val_image_quality_output_acc    \n",
        "val_age_output_acc    \n",
        "val_weight_output_acc    \n",
        "val_bag_output_acc   \n",
        "val_pose_output_acc    \n",
        "val_footwear_output_acc   \n",
        "val_emotion_output_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LRFinder_new:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            print(\"\")\n",
        "            print(\"Training stopped due to high loss\",loss)\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        # If x_train contains data for multiple inputs, use length of the first input.\n",
        "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
        "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
        "\n",
        "        # Compute number of batches and LR multiplier\n",
        "        num_batches = epochs * N / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=batch_size, epochs=epochs,\n",
        "                       callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "        if steps_per_epoch is None:\n",
        "            try:\n",
        "                steps_per_epoch = len(generator)\n",
        "            except (ValueError, NotImplementedError) as e:\n",
        "                raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                        ' generator based on the '\n",
        "                        '`keras.utils.Sequence`'\n",
        "                        ' class. Please specify `steps_per_epoch` '\n",
        "                        'or use the `keras.utils.Sequence` class.')\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit_generator(generator=generator,\n",
        "                                 epochs=epochs,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 callbacks=[callback],\n",
        "                                 **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(x_scale)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIcGpeISVW0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}